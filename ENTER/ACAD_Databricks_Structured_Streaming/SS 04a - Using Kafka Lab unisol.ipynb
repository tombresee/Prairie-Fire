{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n<img src=\"https://files.training.databricks.com/images/Apache-Spark-Logo_TM_200px.png\" style=\"float: left: margin: 20px\"/>\n\n# Structured Streaming with Kafka Lab\n\n## Instructions\n* Insert solutions wherever it says `FILL_IN`\n* Feel free to copy/paste code from the previous notebook, where applicable\n* Run test cells to verify that your solution is correct\n\n## Prerequisites\n* Web browser: **Chrome**\n* Familiarity with Kafka\n* A cluster configured with **8 cores** and **DBR 6.3**\n* External Services\n  - Familiarity with Kafka is helpful, but not required\n* Suggested Courses from <a href=\"https://academy.databricks.com/\" target=\"_blank\">Databricks Academy</a>:\n  - ETL Part 1\n  - Spark-SQL"],"metadata":{}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Setup & Classroom-Cleanup<br>\n\nFor each lesson to execute correctly, please make sure to run the **`Classroom-Setup`** cell at the start of each lesson (see the next cell) and the **`Classroom-Cleanup`** cell at the end of each lesson."],"metadata":{}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["Define the name of the stream we are to use later in this lesson:"],"metadata":{}},{"cell_type":"code","source":["myStreamName = \"lab04a_ps\""],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Exercise 1: Use Kafka to read a stream</h2>\n\nIn this example, we are looking at a series of `ERROR`, `WARNING` and `INFO` log messages that are coming in via our Kafka server.\n\nWe want to analyze how many log messages are coming from each IP address?\n\nCreate `initialDF` with the following Kafka parameters:\n\n0. `format` is `kafka`\n0. `kafka.bootstrap.server` (pick the server closest to you)\n  * is `server1.databricks.training:9092` US (Oregon)\n  * or `server2.databricks.training:9092` Singapore\n0. `subscribe` is `logdata`\n\nWhen you are done, run the TEST cell that follows to verify your results."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nspark.conf.set(\"spark.sql.shuffle.partitions\", sc.defaultParallelism)\n\nkafkaServer = \"server1.databricks.training:9092\"   # US (Oregon)\n# kafkaServer = \"server2.databricks.training:9092\" # Singapore\n\n# Create our initial DataFrame\ninitialDF = (spark.readStream\n  .format(\"kafka\")                                                             # Specify \"kafka\" as the type of the stream\n  .option(\"kafka.bootstrap.servers\", kafkaServer)                              # Set the location of the kafka serve\n  .option(\"subscribe\", \"logdata\")                                              # Indicate which topics to listen to\n  .option(\"startingOffsets\", \"earliest\")                                       # Rewind stream to beginning when we restart notebook\n  .option(\"maxOffsetsPerTrigger\", 1000)                                        # Throttle Kafka's processing of the streams\n  .load()                                                                      # Load the input data stream in as a DataFrame\n)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\ninitSchemaStr = str(initialDF.schema)\n\ndbTest(\"SS-04-key\",       True, \"(key,BinaryType,true)\" in initSchemaStr)\ndbTest(\"SS-04-value\",     True, \"(value,BinaryType,true)\" in initSchemaStr)\ndbTest(\"SS-04-topic\",     True, \"(topic,StringType,true)\" in initSchemaStr)\ndbTest(\"SS-04-partition\", True, \"(partition,IntegerType,true)\" in initSchemaStr)\ndbTest(\"SS-04-offset\",    True, \"(offset,LongType,true)\" in initSchemaStr)\ndbTest(\"SS-04-timestamp\", True, \"(timestamp,TimestampType,true)\" in initSchemaStr)\ndbTest(\"SS-04-timestampType\", True, \"(timestampType,IntegerType,true)\" in initSchemaStr)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Exercise 2: Do Some ETL Processing</h2>\n\nPerform the following ETL steps:\n\n0. Cast `value` column to STRING\n0. `ts_string` is derived from `value` at positions 14 to 24,\n0. `epoc` is derived from `unix_timestamp` of `ts_string` using format \"yyyy/MM/dd HH:mm:ss.SSS\"\n0. `capturedAt` is derived from casting `epoc` to `timestamp` format\n0. `logData` is created by applying `regexp_extract` on `value`.. use this string `\"\"\"^.*\\]\\s+(.*)$\"\"\"`\n\nWhen you are done, run the TEST cell that follows to verify your results."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import col, unix_timestamp, regexp_extract\n\ncleanDF = (initialDF\n  .withColumn(\"value\", col(\"value\").cast(\"STRING\"))                            # cast \"value\" column to STRING\n  .withColumn(\"ts_string\", col(\"value\").substr(14, 24))                        # Select the \"value\" column, pull substring(2,23) from it and rename to \"ts_string\"\n  .withColumn(\"epoc\", unix_timestamp(\"ts_string\", \"yyyy/MM/dd HH:mm:ss.SSS\"))  # Select the \"ts_string\" column, apply unix_timestamp to it and rename to \"epoc\"\n  .withColumn(\"capturedAt\", col(\"epoc\").cast(\"timestamp\"))                     # Select the \"epoc\" column and cast to a timestamp and rename it to \"capturedAt\"               \n  .withColumn(\"logData\", regexp_extract(\"value\", \"\"\"^.*\\]\\s+(.*)$\"\"\", 1))      # Select the \"logData\" column and apply the regexp `\"\"\"^.*\\]\\s+(.*)$\"\"\"`\n)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\nschemaStr = str(cleanDF.schema)\n\ndbTest(\"SS-04-schema-value\",     True, \"(value,StringType,true)\" in schemaStr)\ndbTest(\"SS-04-schema-ts_string\",  True, \"(ts_string,StringType,true)\" in schemaStr)\ndbTest(\"SS-04-schema-epoc\",   True, \"(epoc,LongType,true)\" in schemaStr)\ndbTest(\"SS-04-schema-capturedAt\", True, \"(capturedAt,TimestampType,true)\" in schemaStr)\ndbTest(\"SS-04-schema-logData\",  True, \"(logData,StringType,true)\" in schemaStr)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Exercise 3: Classify and Count IP Addresses Over a 10s Window</h2>\n\nTo solve this problem, you need to:\n\n0. Parse the first part of an IP address from the column `logData` with the `regexp_extract()` function\n  * You will need a regular expression which we have already provided below as `IP_REG_EX`\n  * Hint: take the 1st matching value\n0. Filter out the records that don't contain IP addresses\n0. Form another column called `ipClass` that classifies IP addresses based on the first part of an IP address \n  * 1 to 126: \"Class A\"\n  * 127: \"Loopback\"\n  * 128 to 191: \"Class B\"\n  * 192 to 223: \"Class C\"\n  * 224 to 239: \"Class D\"\n  * 240 to 256: \"Class E\"\n  * anything else is invalid\n0. Perform an aggregation over a window of time, grouping by the `capturedAt` window and `ipClass`\n  * For this lab, use a 10-second window\n0. Count the number of IP values that belong to a specific `ipClass`\n0. Sort by `ipClass`"],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import col,length, window, when\n\n# This is the regular expression pattern that we will use \nIP_REG_EX = \"\"\"^.*\\s+(\\d{1,3})\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}.*$\"\"\"\n\nipDF = (cleanDF\n  .withColumn(\"ip\", regexp_extract(col(\"logData\"), IP_REG_EX, 1))  # apply regexp_extract on IP_REG_EX with value of 1 to \"logData\" and rename it \"ip\"\n  .filter(length(col(\"ip\"))> 0)                                    # keep only \"ip\" that have non-zero length\n  .withColumn(\"ipClass\"                                            # figure out class of IP address based on first two octets\n     ,when(col(\"ip\") < 127, \"Class A\")\n     .when(col(\"ip\") == 127, \"Loopback\")\n     .when((col(\"ip\") > 127) & (col(\"ip\") < 192), \"Class B\")\n     .when((col(\"ip\") >= 192) & (col(\"ip\") < 224), \"Class C\")\n     .when((col(\"ip\") >= 224) & (col(\"ip\") < 240), \"Class D\")\n     .when((col(\"ip\") >= 240) & (col(\"ip\") < 256), \"Class C\")\n     .otherwise(\"Invalid\"))\n  .groupBy( window(col(\"capturedAt\"), \"10 seconds\").alias(\"time\"), col(\"ipClass\"))    # form 10 second windows of \"capturedAt\", call them \"time\" \n  .count()                                                          # add up total\n  .orderBy(\"ipClass\")                                               # sort by IP class\n)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\nschemaStr = str(ipDF.schema)\n\ndbTest(\"SS-04-schema-ipClass\", True, \"(ipClass,StringType,false)\" in schemaStr)\ndbTest(\"SS-04-schema-count\",   True, \"(count,LongType,false)\" in schemaStr)\ndbTest(\"SS-04-schema-start\",   True, \"(start,TimestampType,true)\" in schemaStr)\ndbTest(\"SS-04-schema-end\",     True, \"(end,TimestampType,true)\" in schemaStr)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Exercise 4: Display a LIVE Plot</h2>\n\nThe `DataFrame` that you pass to `display()` should have three columns:\n\n* `time`: The time window structure\n* `ipClass`: The class the first part of the IP address belongs to\n* `count`: The number of times that said class of IP address appeared in the window\n\nUnder <b>Plot Options</b>, use the following:\n* <b>Keys:</b> `ipClass`\n* <b>Values:</b> `count`\n\n<b>Display type:</b> is \n* <b>Pie Chart</b>\n\n<img src=\"https://files.training.databricks.com/images/eLearning/Structured-Streaming/plot-options-pie.png\"/>"],"metadata":{}},{"cell_type":"code","source":["# ANSWER\ndisplay(ipDF, streamName = myStreamName)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\ndbTest(\"SS-04-numActiveStreams\", True, len(spark.streams.active) > 0)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["Wait until stream is done initializing..."],"metadata":{}},{"cell_type":"code","source":["untilStreamIsReady(myStreamName)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Exercise 5: Stop streaming jobs</h2>\n\nBefore we can conclude, we need to shut down all active streams."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\n\nfor s in spark.streams.active:   # Iterate over all the active streams\n  try:\n    s.stop()                     # Stop the stream\n    s.awaitTermination()         # Wait for it to stop\n    \n  except Exception as e:\n    # In extream cases, this funtion may throw an ignorable error.\n    print(\"An [ignorable] error has occured while stoping the stream.\\n\".str(e))    "],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\ndbTest(\"SS-04-numActiveStreams\", 0, len(spark.streams.active))\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Cleanup<br>\n\nRun the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Cleanup\""],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> All done!</h2>\n\nThank you for your participation!"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"SS 04a - Using Kafka Lab","notebookId":4416930934983066},"nbformat":4,"nbformat_minor":0}
