{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n<img src=\"https://files.training.databricks.com/images/Apache-Spark-Logo_TM_200px.png\" style=\"float: left: margin: 20px\"/>\n\n# Structured Streaming Concepts Lab\n\n## Instructions\n* Insert solutions wherever it says `FILL_IN`\n* Feel free to copy/paste code from the previous notebook, where applicable\n* Run test cells to verify that your solution is correct\n\n## Prerequisites\n* Web browser: **Chrome**\n* A cluster configured with **8 cores** and **DBR 6.3**\n* Suggested Courses from <a href=\"https://academy.databricks.com/\" target=\"_blank\">Databricks Academy</a>:\n  - ETL Part 1\n  - Spark-SQL"],"metadata":{}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Setup & Classroom-Cleanup<br>\n\nFor each lesson to execute correctly, please make sure to run the **`Classroom-Setup`** cell at the start of each lesson (see the next cell) and the **`Classroom-Cleanup`** cell at the end of each lesson."],"metadata":{}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["Define the name of the stream we are to use later in this lesson:"],"metadata":{}},{"cell_type":"code","source":["myStreamName = \"lab02_ps\""],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Exercise 1: Read Stream</h2>\n\nThe dataset used in this exercise consists of flight information about flights from/to various airports in 2007.\n\nRun the following cell to see what the streaming data will look like."],"metadata":{}},{"cell_type":"code","source":["display(\n  spark.read.parquet(\"dbfs:/mnt/training/asa/flights/2007-01-stream.parquet/part-00000-tid-9167815511861375854-22d81a30-d5b4-43d0-9216-0c20d14c3f54-178-c000.snappy.parquet\")\n)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Start by reading a stream. \n\nFor this step you will need to:\n0. Starting with `spark`, an instance of `SparkSession`, and get the `DataStreamReader`\n0. Make sure to only consume only 1 file per trigger\n0. Specify the stream's schema using the instance `dataSchema` (already provided for you)\n0. Use `dsr.parquet()` to specify the stream's file type and source directory, `dataPath` \n\nWhen you are done, run the TEST cell that follows to verify your results."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\ndataSchema = \"DepartureAt timestamp, FlightDate string, DepTime string, CRSDepTime string, ArrTime string, CRSArrTime string, UniqueCarrier string, FlightNum integer, TailNum string, ActualElapsedTime string, CRSElapsedTime string, AirTime string, ArrDelay string, DepDelay string, Origin string, Dest string, Distance string, TaxiIn string, TaxiOut string, Cancelled integer, CancellationCode string, Diverted integer, CarrierDelay string, WeatherDelay string, NASDelay string, SecurityDelay string, LateAircraftDelay string\"\n\ndataPath = \"dbfs:/mnt/training/asa/flights/2007-01-stream.parquet\"\n\ninitialDF = (spark\n  .readStream                       # Get a DataStreamReader\n  .option(\"maxFilesPerTrigger\", 1)  # Force processing of only 1 file per trigger \n  .schema(dataSchema)               # Use the schema \"dataSchema\"\n  .parquet(dataPath)                # Read in stream's file type and source directory\n)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\nschemaStr = str(initialDF.schema)\n\ndbTest(\"SS-02-schema-01\", True, \"(DepartureAt,TimestampType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-02\", True, \"(FlightDate,StringType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-03\", True, \"(DepTime,StringType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-04\", True, \"(CRSDepTime,StringType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-05\", True, \"(ArrTime,StringType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-06\", True, \"(CRSArrTime,StringType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-07\", True, \"(UniqueCarrier,StringType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-08\", True, \"(FlightNum,IntegerType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-09\", True, \"(TailNum,StringType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-10\", True, \"(ActualElapsedTime,StringType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-11\", True, \"(CRSElapsedTime,StringType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-12\", True, \"(AirTime,StringType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-13\", True, \"(ArrDelay,StringType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-14\", True, \"(DepDelay,StringType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-15\", True, \"(Origin,StringType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-16\", True, \"(Dest,StringType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-17\", True, \"(Distance,StringType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-18\", True, \"(TaxiIn,StringType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-19\", True, \"(TaxiOut,StringType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-20\", True, \"(Cancelled,IntegerType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-21\", True, \"(CancellationCode,StringType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-22\", True, \"(Diverted,IntegerType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-23\", True, \"(CarrierDelay,StringType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-24\", True, \"(WeatherDelay,StringType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-25\", True, \"(NASDelay,StringType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-26\", True, \"(SecurityDelay,StringType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-27\", True, \"(LateAircraftDelay,StringType,true)\" in schemaStr) \n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["-sandbox\n\n<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Exercise 2: Calculate the total of all delays</h2>\n\nWe want to calculate (and later graph) the total delay of each flight\n0. Start with `initialDF` from the previous exercise \n0. Convert the following columns from `String` to `Integer`: `CarrierDelay`, `WeatherDelay`, `NASDelay`, `SecurityDelay` and `LateAircraftDelay`\n0. Add the column `TotalDelay` which is the sum of the other 5 delays\n0. Filter the flights by `UniqueCarrier` down to the carriers **AS**, **AQ**, **HA** and **F9**\n0. Filter the results to non-zero delay's (`TotalDelay` > 0)\n0. Assign the final DataFrame to `delaysDF`\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> The `display()` function will only plot the first 1000 records. By limiting ourselves to four carriers and non-zero delays, we can help to ensure that we get a reasonable demonstration of a live plot."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import col\n\ndelaysDF = (initialDF\n  .withColumn(\"CarrierDelay\", col(\"CarrierDelay\").cast(\"integer\"))\n  .withColumn(\"WeatherDelay\", col(\"WeatherDelay\").cast(\"integer\"))\n  .withColumn(\"NASDelay\", col(\"NASDelay\").cast(\"integer\"))\n  .withColumn(\"SecurityDelay\", col(\"SecurityDelay\").cast(\"integer\"))\n  .withColumn(\"LateAircraftDelay\", col(\"LateAircraftDelay\").cast(\"integer\"))\n  .withColumn(\"TotalDelay\", col(\"CarrierDelay\") + col(\"WeatherDelay\") + col(\"NASDelay\") + col(\"SecurityDelay\") + col(\"LateAircraftDelay\"))\n  .filter(col(\"UniqueCarrier\").isin(\"AS\", \"AQ\", \"HA\", \"F9\"))\n  .filter(col(\"TotalDelay\") > 0)\n)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\nschemaStr = str(delaysDF.schema)\n\ndbTest(\"SS-02-schema-01\", True, \"(UniqueCarrier,StringType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-02\", True, \"(TotalDelay,IntegerType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-03\", True, \"(CarrierDelay,IntegerType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-04\", True, \"(WeatherDelay,IntegerType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-05\", True, \"(NASDelay,IntegerType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-06\", True, \"(SecurityDelay,IntegerType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-07\", True, \"(LateAircraftDelay,IntegerType,true)\" in schemaStr) \ndbTest(\"SS-02-schema-08\", True, \"(DepartureAt,TimestampType,true)\" in schemaStr) \n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["-sandbox\n<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Exercise 3: Plot a LIVE graph</h2>\n\nPlot `delaysDF` and give the stream the name \"delays_python\"\n\nOnce the data is loaded, render a line graph with \n* **Keys** is set to `DepartureAt`\n* **Series groupings** is set to `UniqueCarrier`\n* **Values** is set to `TotalDelay`\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Because of the `display()` function's 1000 record limit, the stream will appear to stop shortly after January 5th."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\ndisplay(delaysDF, streamName = myStreamName)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\ncount = 0\nfor s in spark.streams.active:\n  if (s.name == myStreamName):\n    count = count + 1\n\ndbTest(\"SS-02-runningCount\", 1, count)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["When you are done, stop the stream:"],"metadata":{}},{"cell_type":"code","source":["stopAllStreams()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Exercise 4: Write Stream</h2>\n\nWrite the stream to an in-memory table\n0. Use appropriate `format`\n0. For this exercise, we want to append new records to the results table\n0. Configure a 15 second trigger\n0. Name the query \"delays_python\"\n0. Start the query\n0. Assign the query to `delayQuery`"],"metadata":{}},{"cell_type":"code","source":["# ANSWER\ndelayQuery = (delaysDF \n .writeStream                           # From the DataFrame get the DataStreamWriter\n .format(\"memory\")                      # Specify the sink format as \"memory\"\n .outputMode(\"append\")                  # Configure the output mode as \"append\"\n .queryName(myStreamName)               # Name the query with myStreamName\n .trigger(processingTime=\"15 seconds\")  # Use a 15 second trigger\n .start()                               # Start the query\n)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\ndbTest(\"SS-02-isActive\", True, delayQuery.isActive)\ndbTest(\"SS-02-name\", myStreamName, delayQuery.name)\n# The query's trigger is not available via the Python API\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["Wait until stream is done initializing..."],"metadata":{}},{"cell_type":"code","source":["untilStreamIsReady(myStreamName)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Exercise 5: Stop streaming jobs</h2>\n\nBefore we can conclude, we need to shut down all active streams."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nfor s in spark.streams.active: # Iterate over all active streams\n  try:\n    print(\"stopping \" + s.name)  # A little console output\n    s.stop()                     # Stop the stream\n\n  except Exception as e:\n    # In extream cases, this funtion may throw an ignorable error.\n    print(\"An [ignorable] error has occured while stoping the stream.\\n\".str(e))\n    "],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\ndbTest(\"SS-02-numActiveStreams\", 0, len(spark.streams.active))\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Cleanup<br>\n\nRun the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Cleanup\""],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Next Steps</h2>\n\nStart the next lesson, [Time Windows]($../SS 03 - Time Windows)."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"SS 02 - Streaming Concepts Lab","notebookId":4416930934983118},"nbformat":4,"nbformat_minor":0}
