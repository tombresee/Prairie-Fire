{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n<img src=\"https://files.training.databricks.com/images/Apache-Spark-Logo_TM_200px.png\" style=\"float: left: margin: 20px\"/>\n\n# Working with Time Windows Lab\n\n## Instructions\n* Insert solutions wherever it says `FILL_IN`\n* Feel free to copy/paste code from the previous notebook, where applicable\n* Run test cells to verify that your solution is correct\n\n## Prerequisites\n* Web browser: **Chrome**\n* A cluster configured with **8 cores** and **DBR 6.3**\n* Suggested Courses from <a href=\"https://academy.databricks.com/\" target=\"_blank\">Databricks Academy</a>:\n  - ETL Part 1\n  - Spark-SQL"],"metadata":{}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Setup & Classroom-Cleanup<br>\n\nFor each lesson to execute correctly, please make sure to run the **`Classroom-Setup`** cell at the start of each lesson (see the next cell) and the **`Classroom-Cleanup`** cell at the end of each lesson."],"metadata":{}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["Define the name of the stream we are to use later in this lesson:"],"metadata":{}},{"cell_type":"code","source":["myStreamName = \"lab03_ps\""],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Exercise 1: Read data into a stream</h2>\n\nThe dataset used in this exercise consists of flight information about flights from/to various airports in 2007.\n\nYou have already seen this dataset in Exercise 1 of Notebook 02.\n\nTo refresh your memory, take a look at the first few lines of the dataset."],"metadata":{}},{"cell_type":"code","source":["path = \"dbfs:/mnt/training/asa/flights/2007-01-stream.parquet/part-00000-tid-9167815511861375854-22d81a30-d5b4-43d0-9216-0c20d14c3f54-178-c000.snappy.parquet\"\ndf = spark.read.parquet(path)\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["For this exercise you will need to complete the following tasks:\n0. Start a stream that reads parquet files dumped to the directory `dataPath`\n0. Control the size of each partition by forcing Spark to processes only 1 file per trigger.\n\nOther notes:\n0. The source data has already been defined as `dataPath`\n0. The schema has already be defined as `parquetSchema`"],"metadata":{}},{"cell_type":"code","source":["# ANSWER\ndataPath = \"/mnt/training/asa/flights/2007-01-stream.parquet/\"\n\nparquetSchema = \"DepartureAt timestamp, FlightDate string, DepTime string, CRSDepTime string, ArrTime string, CRSArrTime string, UniqueCarrier string, FlightNum integer, TailNum string, ActualElapsedTime string, CRSElapsedTime string, AirTime string, ArrDelay string, DepDelay string, Origin string, Dest string, Distance string, TaxiIn string, TaxiOut string, Cancelled integer, CancellationCode string, Diverted integer, CarrierDelay string, WeatherDelay string, NASDelay string, SecurityDelay string, LateAircraftDelay string\"\n  \n# Configure the shuffle partitions to match the number of cores  \nspark.conf.set(\"spark.sql.shuffle.partitions\", sc.defaultParallelism)\n\nstreamDF = (spark                   # Start with the SparkSesion\n  .readStream                       # Get the DataStreamReader\n  .format(\"parquet\")                # Configure the stream's source for the appropriate file type\n  .schema(parquetSchema)            # Specify the parquet files' schema\n  .option(\"maxFilesPerTrigger\", 1)  # Restrict Spark to processing only 1 file per trigger\n  .load(dataPath)                   # Load the DataFrame specifying its location as dataPath\n)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\nschemaStr = str(streamDF.schema)\n\ndbTest(\"SS-03-shuffles\",  sc.defaultParallelism, spark.conf.get(\"spark.sql.shuffle.partitions\"))\n\ndbTest(\"SS-03-schema-1\",  True, \"(DepartureAt,TimestampType,true)\" in schemaStr)\ndbTest(\"SS-03-schema-2\",  True, \"(FlightDate,StringType,true)\" in schemaStr)\ndbTest(\"SS-03-schema-3\",  True, \"(DepTime,StringType,true)\" in schemaStr)\ndbTest(\"SS-03-schema-4\",  True, \"(CRSDepTime,StringType,true)\" in schemaStr)\ndbTest(\"SS-03-schema-5\",  True, \"(ArrTime,StringType,true)\" in schemaStr)\ndbTest(\"SS-03-schema-6\",  True, \"(CRSArrTime,StringType,true)\" in schemaStr)\ndbTest(\"SS-03-schema-7\",  True, \"(UniqueCarrier,StringType,true)\" in schemaStr)\ndbTest(\"SS-03-schema-8\",  True, \"(FlightNum,IntegerType,true)\" in schemaStr)\ndbTest(\"SS-03-schema-9\",  True, \"(TailNum,StringType,true)\" in schemaStr)\ndbTest(\"SS-03-schema-10\",  True, \"(ActualElapsedTime,StringType,true)\" in schemaStr)\ndbTest(\"SS-03-schema-11\",  True, \"(CRSElapsedTime,StringType,true)\" in schemaStr)\ndbTest(\"SS-03-schema-12\",  True, \"(AirTime,StringType,true)\" in schemaStr)\ndbTest(\"SS-03-schema-13\",  True, \"(ArrDelay,StringType,true)\" in schemaStr)\ndbTest(\"SS-03-schema-14\",  True, \"(DepDelay,StringType,true)\" in schemaStr)\ndbTest(\"SS-03-schema-15\",  True, \"(Origin,StringType,true)\" in schemaStr)\ndbTest(\"SS-03-schema-16\",  True, \"(Dest,StringType,true)\" in schemaStr)\ndbTest(\"SS-03-schema-17\",  True, \"(Distance,StringType,true)\" in schemaStr)\ndbTest(\"SS-03-schema-18\",  True, \"(TaxiIn,StringType,true)\" in schemaStr)\ndbTest(\"SS-03-schema-19\",  True, \"(TaxiOut,StringType,true)\" in schemaStr)\ndbTest(\"SS-03-schema-20\",  True, \"(Cancelled,IntegerType,true)\" in schemaStr)\ndbTest(\"SS-03-schema-21\",  True, \"(CancellationCode,StringType,true)\" in schemaStr)\ndbTest(\"SS-03-schema-22\",  True, \"(Diverted,IntegerType,true)\" in schemaStr)\ndbTest(\"SS-03-schema-23\",  True, \"(CarrierDelay,StringType,true)\" in schemaStr)\ndbTest(\"SS-03-schema-24\",  True, \"(WeatherDelay,StringType,true)\" in schemaStr)\ndbTest(\"SS-03-schema-25\",  True, \"(NASDelay,StringType,true)\" in schemaStr)\ndbTest(\"SS-03-schema-26\",  True, \"(SecurityDelay,StringType,true)\" in schemaStr)\ndbTest(\"SS-03-schema-27\",  True, \"(LateAircraftDelay,StringType,true)\" in schemaStr)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Exercise 2: Plot grouped events</h2>\n\nPlot the count of all flights aggregated by a 30 minute window and `UniqueCarrier`. \n\nIgnore any events delayed by 300 minutes or more.\n\nYou will need to:\n0. Use a watermark to discard events not received within 300 minutes\n0. Configure the stream for a 30 minute sliding window\n0. Aggregate by the 30 minute window and the column `UniqueCarrier`\n0. Add the column `start` by extracting it from `window.start`\n0. Sort the stream by `start`\n\nIn order to create a LIVE bar chart of the data, you'll need to specify the following <b>Plot Options</b>:\n* **Keys** is set to `start`\n* **Series groupings** is set to `UniqueCarrier`\n* **Values** is set to `count`"],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import hour, window, col\n\ncountsDF = (streamDF                                             # Start with the DataFrame\n  .withWatermark(\"DepartureAt\", \"300 minutes\")                   # Specify the watermark\n  .groupBy(window(\"DepartureAt\", \"30 minute\"), \"UniqueCarrier\" ) # Aggregate the data\n  .count()                                                       # Produce a count for each aggreate\n  .withColumn(\"start\", col(\"window.start\"))                      # Add the column \"start\", extracting it from \"window.start\"\n  .orderBy(\"start\")                                              # Sort the stream by \"start\" \n)\ndisplay(countsDF,  streamName = myStreamName)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Wait until stream is done initializing..."],"metadata":{}},{"cell_type":"code","source":["untilStreamIsReady(myStreamName)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\nschemaStr = str(countsDF.schema)\n\ndbTest(\"SS-03-schema-1\",  True, \"(UniqueCarrier,StringType,true)\" in schemaStr)\ndbTest(\"SS-03-schema-2\",  True, \"(count,LongType,false)\" in schemaStr)\ndbTest(\"SS-03-schema-5\",  True, \"(start,TimestampType,true)\" in schemaStr)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Exercise 3: Stop streaming jobs</h2>\n\nBefore we can conclude, we need to shut down all active streams."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\n\nfor s in spark.streams.active:   # Iterate over all the active streams\n  try:\n    s.stop()                     # Stop the stream\n    s.awaitTermination()         # Wait for it to stop\n    \n  except Exception as e:\n    # In extream cases, this funtion may throw an ignorable error.\n    print(\"An [ignorable] error has occured while stoping the stream.\\n\".str(e))    "],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\ndbTest(\"SS-03-numActiveStreams\", 0, len(spark.streams.active))\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Cleanup<br>\n\nRun the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Cleanup\""],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Next Steps</h2>\n\nStart the next lesson, [Using Kafka]($../SS 04a - Using Kafka)."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"SS 03 - Time Windows Lab","notebookId":4416930934983094},"nbformat":4,"nbformat_minor":0}
