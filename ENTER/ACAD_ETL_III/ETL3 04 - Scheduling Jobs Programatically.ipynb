{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n<img src=\"https://files.training.databricks.com/images/Apache-Spark-Logo_TM_200px.png\" style=\"float: left: margin: 20px\"/>\n# Scheduling Jobs Programatically\n\nApache Spark&trade; and Databricks&reg; can be automated through the Jobs UI, REST API, or the command line\n\n## In this lesson you:\n* Submit jobs using the Jobs UI and REST API\n* Monitor jobs using the REST API\n\n## Audience\n* Primary Audience: Data Engineers\n* Additional Audiences: Data Scientists and Data Pipeline Engineers\n\n## Prerequisites\n* Web browser: Chrome\n* A cluster configured with **8 cores** and **DBR 6.2**\n* Course: ETL Part 1 from <a href=\"https://academy.databricks.com/\" target=\"_blank\">Databricks Academy</a>\n* Course: ETL Part 2 from <a href=\"https://academy.databricks.com/\" target=\"_blank\">Databricks Academy</a>"],"metadata":{}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Setup\n\nFor each lesson to execute correctly, please make sure to run the **`Classroom-Setup`** cell at the<br/>\nstart of each lesson (see the next cell) and the **`Classroom-Cleanup`** cell at the end of each lesson."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["<iframe  \nsrc=\"//fast.wistia.net/embed/iframe/1ie2iv3vou?videoFoam=true\"\nstyle=\"border:1px solid #1cb1c2;\"\nallowtransparency=\"true\" scrolling=\"no\" class=\"wistia_embed\"\nname=\"wistia_embed\" allowfullscreen mozallowfullscreen webkitallowfullscreen\noallowfullscreen msallowfullscreen width=\"640\" height=\"360\" ></iframe>\n<div>\n<a target=\"_blank\" href=\"https://fast.wistia.net/embed/iframe/1ie2iv3vou?seo=false\">\n  <img alt=\"Opens in new tab\" src=\"https://files.training.databricks.com/static/images/external-link-icon-16x16.png\"/>&nbsp;Watch full-screen.</a>\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Automating ETL Workloads\n\nSince recurring production jobs are the goal of ETL workloads, Spark needs a way to integrate with other automation and scheduling tools.  We also need to be able to run Python files and Scala/Java jars.\n\nRecall from <a href=\"https://academy.databricks.com/collections/frontpage/products/etl-part-1-data-extraction\" target=\"_blank\">ETL Part 1 course from Databricks Academy</a> how we can schedule jobs using the Databricks user interface.  In this lesson, we'll explore more robust solutions to schedule jobs.\n\n<div><img src=\"https://files.training.databricks.com/images/eLearning/ETL-Part-3/jobs.png\" style=\"height: 400px; margin: 20px\"/></div>\n\nThere are a number of different automation and scheduling tools including the following:<br><br>\n\n* Command line tools integrated with the UNIX scheduler Cron\n* The workflow scheduler Apache Airflow\n* Microsoft's Scheduler or Data Factory\n\nThe gateway into job scheduling is programmatic access to Databricks, which can be achieved either through the REST API or the Databricks Command Line Interface (CLI)."],"metadata":{}},{"cell_type":"markdown","source":["### Access Tokens\n\nAccess tokens provide programmatic access to the Databricks CLI and REST API.  This lesson uses the REST API but could also be completed <a href=\"https://docs.azuredatabricks.net/user-guide/dev-tools/databricks-cli.html\" target=\"_blank\">using the command line alternative.</a>\n\nTo get started, first generate an access token."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\nIn order to generate a token:<br><br>\n\n1. Click on the person icon in the upper-right corner of the screen.\n2. Click **User Settings**\n<div><img src=\"https://files.training.databricks.com/images/eLearning/ETL-Part-3/token-1.png\" style=\"height: 400px; margin: 20px\"/></div>\n3. Click on **Access Tokens**\n4. Click on **Generate New Token**\n<div><img src=\"https://files.training.databricks.com/images/eLearning/ETL-Part-3/token-2-azure.png\" style=\"height: 400px; margin: 20px\"/></div>\n\n5. Name your token\n6. Designate a lifespan (a shorter lifespan is generally better to minimize risk exposure)\n7. Click **Generate**\n<div><img src=\"https://files.training.databricks.com/images/eLearning/ETL-Part-3/token-3.png\" style=\"height: 400px; margin: 20px\"/></div>\n8. Copy your token.  You'll only be able to see it once.\n<div><img src=\"https://files.training.databricks.com/images/eLearning/ETL-Part-3/token-4.png\" style=\"height: 400px; margin: 20px\"/></div>\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Be sure to keep this key secure.  This grants the holder full programmatic access to Databricks, including both resources and data that's available to your Databricks environment."],"metadata":{}},{"cell_type":"markdown","source":["Paste your token into the following cell along with the domain of your Databricks deployment (you can see this in the notebook's URL).  The deployment should look something like `https://westus2.azuredatabricks.net`"],"metadata":{}},{"cell_type":"code","source":["# ANSWER\n\ntoken = \"no\"\n\n\ndomain = \"https://westus2.azuredatabricks.net\"\n\n#domain = \"https://example.cloud.databricks.com/api/2.0/\"\n\nheader = {'Authorization': \"Bearer \"+ token}\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["Test that the connection works by listing all files in the root directory of DBFS."],"metadata":{}},{"cell_type":"code","source":["try:\n  import json\n  import requests\n\n  endPoint = domain+\"dbfs/list?path=/\"\n  r = requests.get(endPoint, headers=header)\n\n  [i.get(\"path\") for i in json.loads(r.text).get(\"files\")]  \n\nexcept Exception as e:\n  print(e)\n  print(\"\\n** Double check your previous settings **\\n\")\n  \n  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">HTTPSConnectionPool(host=&#39;westus2.azuredatabricks.netdbfs&#39;, port=443): Max retries exceeded with url: /list?path=/ (Caused by NewConnectionError(&#39;&lt;urllib3.connection.VerifiedHTTPSConnection object at 0x7fbf8a132b38&gt;: Failed to establish a new connection: [Errno -2] Name or service not known&#39;))\n\n** Double check your previous settings **\n\n</div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["-sandbox\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> The REST API can be used at the command line using a command like `curl -s -H \"Authorization: Bearer token\" https://domain.cloud.databricks.com/api/2.0/dbfs/list\\?path\\=/`\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> The CLI can be used with the command `databricks fs ls dbfs:/` once it has been installed and configured with your access token"],"metadata":{}},{"cell_type":"code","source":["# ! curl -s -H \"Authorization: Bearer token\" https://domain.cloud.databricks.com/api/2.0/dbfs/list\\?path\\=/"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["### Scheduling with the REST API and CLI\n\nJobs can either be scheduled for running on a consistent basis or they can be run every time the API call is made.  Since there are many parameters in scheduling jobs, it's often best to schedule a job through the user interface, parse the configuration settings, and then run later jobs using the API.\n\nRun the following cell to get the sense of what a basic job accomplishes."],"metadata":{}},{"cell_type":"code","source":["path = dbutils.notebook.run(\"./Runnable/Runnable-4\", 120, {\"username\": getUsername(), \"ranBy\": \"NOTEBOOK\"})\ndisplay(spark.read.parquet(path))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o2240._run.\n: java.lang.IllegalArgumentException: requirement failed: To enable notebook workflows, please upgrade your Databricks subscription.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat com.databricks.dbutils_v1.impl.NotebookUtilsImpl.checkEnabled(NotebookUtilsImpl.scala:53)\n\tat com.databricks.dbutils_v1.impl.NotebookUtilsImpl.setContext(NotebookUtilsImpl.scala:165)\n\tat com.databricks.dbutils_v1.impl.NotebookUtilsImpl._run(NotebookUtilsImpl.scala:85)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">IllegalArgumentException</span>                  Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1055298992984389&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>path <span class=\"ansi-blue-fg\">=</span> dbutils<span class=\"ansi-blue-fg\">.</span>notebook<span class=\"ansi-blue-fg\">.</span>run<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;./Runnable/Runnable-4&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">120</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">{</span><span class=\"ansi-blue-fg\">&#34;username&#34;</span><span class=\"ansi-blue-fg\">:</span> getUsername<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;ranBy&#34;</span><span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">&#34;NOTEBOOK&#34;</span><span class=\"ansi-blue-fg\">}</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> display<span class=\"ansi-blue-fg\">(</span>spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>parquet<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/local_disk0/tmp/1587764623688-0/dbutils.py</span> in <span class=\"ansi-cyan-fg\">run</span><span class=\"ansi-blue-fg\">(self, path, timeout_seconds, arguments, _NotebookHandler__databricks_internal_cluster_spec)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    134</span>                 arguments<span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    135</span>                 __databricks_internal_cluster_spec<span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-fg\">--&gt; 136</span><span class=\"ansi-red-fg\">                 self.shell.currentJobGroup)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    137</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    138</span>         <span class=\"ansi-green-fg\">def</span> __repr__<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     77</span>                 <span class=\"ansi-green-fg\">raise</span> QueryExecutionException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     78</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;java.lang.IllegalArgumentException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 79</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> IllegalArgumentException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     80</span>             <span class=\"ansi-green-fg\">raise</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     81</span>     <span class=\"ansi-green-fg\">return</span> deco\n\n<span class=\"ansi-red-fg\">IllegalArgumentException</span>: &#39;requirement failed: To enable notebook workflows, please upgrade your Databricks subscription.&#39;</div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["-sandbox\nThe notebook `Runnable-4` logs a timestamp and how the notebook is run.  This will log our jobs.\n\nSchedule this job notebook as a job using parameters by first navigating to the jobs panel on the left-hand side of the screen and creating a new job.  Customize the job as follows:<br><br>\n\n1. Give the job a name\n2. Choose the notebook `Runnable-4` in the `Runnable` directory of this course\n3. Add parameters for `username`, which is your Databricks login email (this gives you a unique path to save your data), and set `ranBy` as `JOB`\n4. Choose a cluster of 2 workers and 1 driver (the default is too large for our needs).  **You can also choose to run a job against an already active cluster, reducing the time to spin up new resources.**\n5. Click **Run now** to execute the job.\n\n<div><img src=\"https://files.training.databricks.com/images/eLearning/ETL-Part-3/runnable-4-execution.png\" style=\"height: 400px; margin: 20px\"/></div>\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Set recurring jobs in the same way by adding a schedule\n<img alt=\"Best Practice\" title=\"Best Practice\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-blue-ribbon.svg\"/> Set email alerts in case of job failure"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\nWhen the job completes, paste the `Run ID` that appears under completed runs below.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> On the jobs page, you can access the logs to determine the cause of any failures."],"metadata":{}},{"cell_type":"code","source":["try:\n  runId = \"FILL_IN\"\n  endPoint = domain + \"jobs/runs/get?run_id={}\".format(runId)\n\n  json.loads(requests.get(endPoint, headers=header).text)\n  \nexcept Exception as e:\n  print(e)\n  print(\"\\n** Double check your runId and domain **\\n\")"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["Now take a look at the table to see the update"],"metadata":{}},{"cell_type":"code","source":["display(spark.read.parquet(path))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["-sandbox\nWith this design pattern, you can have full, programmatic access to Databricks.  <a href=\"https://docs.databricks.com/api/latest/examples.html#jobs-api-examples\" target=\"_blank\">See the documentation</a> for examples on submitting jobs from Python files and JARs and other API examples.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Always run production jobs on a new cluster to minimize the chance of unexpected behavior.  Autoscaling clusters allows for elastically allocating more resources to a job as needed"],"metadata":{}},{"cell_type":"markdown","source":["## Exercise 1: Create and Submit a Job using the REST API\n\nNow that a job has been submitted through the UI, we can easily capture and re-run that job.  Re-run the job using the REST API and different parameters."],"metadata":{}},{"cell_type":"markdown","source":["### Step 1: Create the `POST` Request Payload\n\nTo create a new job, communicate the specifications about the job using a `POST` request.  First, define the following variables:<br><br>\n\n* `name`: The name of your job\n* `notebook_path`: The path to the notebook `Runnable-4`.  This will be the `noteboook_path` variable listed in the API call above."],"metadata":{}},{"cell_type":"code","source":["\n\nimport json\n\nname = \"Lesson-04-Lab\"\n\nnotebook_path = \"/Shared/ETL-Part-3/Python/Runnable/Runnable-4\"\n\ndata = {\n  \"name\": name,\n  \"new_cluster\": {\n    \"spark_version\": \"4.2.x-scala2.11\",\n    \"node_type_id\": \"Standard_DS3_v2\",\n    \"num_workers\": 2,\n    \"spark_conf\": {\"spark.databricks.delta.preview.enabled\": \"true\"}\n  },\n  \"notebook_task\": {\n    \"notebook_path\": notebook_path,\n    \"base_parameters\": {\n      \"username\": username, \"ranBy\": \"REST-API\"\n    }\n  }\n}\n\ndata_str = json.dumps(data)\nprint(data_str)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">{&#34;name&#34;: &#34;Lesson-04-Lab&#34;, &#34;new_cluster&#34;: {&#34;spark_version&#34;: &#34;4.2.x-scala2.11&#34;, &#34;node_type_id&#34;: &#34;Standard_DS3_v2&#34;, &#34;num_workers&#34;: 2, &#34;spark_conf&#34;: {&#34;spark.databricks.delta.preview.enabled&#34;: &#34;true&#34;}}, &#34;notebook_task&#34;: {&#34;notebook_path&#34;: &#34;/Shared/ETL-Part-3/Python/Runnable/Runnable-4&#34;, &#34;base_parameters&#34;: {&#34;username&#34;: &#34;tbresee@umich.edu&#34;, &#34;ranBy&#34;: &#34;REST-API&#34;}}}\n</div>"]}}],"execution_count":25},{"cell_type":"markdown","source":["### Step 2: Create the Job\n\nUse the base `domain` defined above to create a URL for the REST endpoint `jobs/create`.  Then, submit a `POST` request using `data_str` as the payload."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\n# createEndPoint = domain + \"jobs/create\"\n# r = requests.post(createEndPoint, headers=header, data=data_str)\n\n# job_id = json.loads(r.text).get(\"job_id\")\n# print(job_id)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["### Step 3: Run the Job\n\nRun the job using the `job_id` from above.  You'll need to submit the post request to the `RunEndPoint` URL of `jobs/run-now`"],"metadata":{}},{"cell_type":"code","source":["# ANSWER\n# RunEndPoint = domain + \"jobs/run-now\"\n\n# data2 = {\"job_id\": job_id}\n# data2_str = json.dumps(data2)\n\n# r = requests.post(RunEndPoint, headers=header, data=data2_str)\n\n# r.text"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["### Step 4: Confirm that the Job Ran\n\nConfirm that the job ran by checking the parquet file.  It can take a few minutes for the job to run and update this file."],"metadata":{}},{"cell_type":"code","source":["display(spark.read.parquet(path))"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\nfrom pyspark.sql.functions import col\n\nAPICounts = (spark.read.parquet(path)\n  .filter(col(\"ranBy\") == \"REST-API\")\n  .count()\n)\n\nif APICounts > 0:\n  print(\"Tests passed!\")\nelse:\n  print(\"Test failed, no records found\")"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["## Review\n**Question:** What ways can you schedule jobs on Databricks?  \n**Answer:** Jobs can be scheduled using the UI, REST API, or Databricks CLI.\n\n**Question:** How can you gain programmatic access to Databricks?  \n**Answer:** Generating a token will give programmatic access to most Databricks services."],"metadata":{}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Cleanup<br>\n\nRun the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Cleanup\""],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["## Next Steps\n\nStart the next lesson, [Job Failure]($./ETL3 05 - Job Failure )."],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n**Q:** Where can I get more information on the REST API?  \n**A:** Check out the <a href=\"https://docs.azuredatabricks.net/api/index.html\" target=\"_blank\">Databricks documentation.</a>\n\n**Q:** How can I set up the Databricks CLI?  \n**A:** Check out the <a href=\"https://docs.azuredatabricks.net/user-guide/dev-tools/databricks-cli.html#set-up-the-cli\" target=\"_blank\">Databricks documentation for step-by-step instructions.</a>\n\n**Q:** How can I do a `spark-submit` job using the API?  \n**A:** Check out the <a href=\"https://docs.azuredatabricks.net/api/latest/examples.html#spark-submit-api-example\" target=\"_blank\">Databricks documentation for API examples.</a>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"ETL3 04 - Scheduling Jobs Programatically","notebookId":1055298992984374},"nbformat":4,"nbformat_minor":0}
