{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n<img src=\"https://files.training.databricks.com/images/Apache-Spark-Logo_TM_200px.png\" style=\"float: left: margin: 20px\"/>\n\n# Streaming ETL\n\nApache Spark&trade; and Databricks&reg; makes it easy to build scalable and fault-tolerant streaming ETL applications.\n\n## In this lesson you:\n* Define logic to read from a stream of data\n* Perform a basic ETL job on a streaming data source\n* Join a stream to historical data\n* Write to an always up-to-date Databricks Delta table\n\n## Audience\n* Primary Audience: Data Engineers\n* Additional Audiences: Data Scientists and Data Pipeline Engineers\n\n## Prerequisites\n* Web browser: Chrome\n* A cluster configured with **8 cores** and **DBR 6.2**\n* Course: ETL Part 1 from <a href=\"https://academy.databricks.com/\" target=\"_blank\">Databricks Academy</a>\n* Course: ETL Part 2 from <a href=\"https://academy.databricks.com/\" target=\"_blank\">Databricks Academy</a>"],"metadata":{}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Deprecation Warning\n\nThis lesson as been replaced by the Structured Streaming course.\n\nIf you have not completed an Instructor-Led or Self-Paced course on Structured Stream, we encourage you to do so now at <a href=\"https://academy.databricks.com/\" target=\"_blank\">Databricks Academy</a>.\n\nIn addition to the Structured Streaming API and concepts, our prerequisites course also introduce a number of utility methods for controlling streams which are employed in this lesson:\n* **`untilStreamIsReady(name)`**\n* **`stopAllStreams()`**\n\nBoth of these methods are defined above in the call to **`Classroom-Setup`** and can be found in the notebook **`./Includes/Common-Notebooks/Utility-Methods`**."],"metadata":{}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Setup\n\nFor each lesson to execute correctly, please make sure to run the **`Classroom-Setup`** cell at the<br/>\nstart of each lesson (see the next cell) and the **`Classroom-Cleanup`** cell at the end of each lesson."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["<iframe  \nsrc=\"//fast.wistia.net/embed/iframe/y36vjvhx5e?videoFoam=true\"\nstyle=\"border:1px solid #1cb1c2;\"\nallowtransparency=\"true\" scrolling=\"no\" class=\"wistia_embed\"\nname=\"wistia_embed\" allowfullscreen mozallowfullscreen webkitallowfullscreen\noallowfullscreen msallowfullscreen width=\"640\" height=\"360\" ></iframe>\n<div>\n<a target=\"_blank\" href=\"https://fast.wistia.net/embed/iframe/y36vjvhx5e?seo=false\">\n  <img alt=\"Opens in new tab\" src=\"https://files.training.databricks.com/static/images/external-link-icon-16x16.png\"/>&nbsp;Watch full-screen.</a>\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### ETL on Streaming Data\n\nSpark Streaming enables scalable and fault-tolerant ETL operations that continuously clean and aggregate data before pushing it to data stores.  Streaming applications can also incorporate machine learning and other Spark features to trigger actions in real time, such as flagging potentially fraudulent user activity.  This lesson is meant as an introduction to streaming applications as they pertain to production ETL jobs.  \n\nStreaming poses a number of specific obstacles. These obstacles include:<br><br>\n\n* *End-to-end reliability and correctness:* Applications must be resilient to failures of any element of the pipeline caused by network issues, traffic spikes, and/or hardware malfunctions\n* *Handle complex transformations:* applications receive many data formats that often involve complex business logic\n* *Late and out-of-order data:* network issues can result in data that arrives late and out of its intended order\n* *Integrate with other systems:* Applications must integrate with the rest of a data infrastructure\n\nStreaming data sources in Spark offer the same DataFrames API for interacting with your data.  The crucial difference is that in structured streaming, the DataFrame is unbounded.  In other words, data arrives in an input stream and new records are appended to the input DataFrame.\n\n<div><img src=\"https://files.training.databricks.com/images/eLearning/ETL-Part-3/structured-streamining-model.png\" style=\"height: 400px; margin: 20px\"/></div>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Connecting to the Stream\n\nAs data technology matures, the industry has been converging on a set of technologies.  Apache Kafka and the Azure managed alternative Event Hubs has become the ingestion engine at the heart of many pipelines.  \n\nThis technology brokers messages between producers, such as an IoT device writing data, and consumers, such as a Spark cluster reading data to perform real time analytics. There can be a many-to-many relationship between producers and consumers and the broker itself is scalable and fault tolerant.\n\nConnect to a Kafka topic that is streaming Wikipedia event data.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/>  There are a number of ways to stream data.  One other common design pattern is to stream from an Azure Blob Container where any new files that appear will be read by the stream.  In this example, we'll be streaming directly from Kafka."],"metadata":{}},{"cell_type":"markdown","source":["Start by defining the schema of the data in the stream."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import BooleanType, IntegerType, StructType, StringType, StructField, TimestampType\n\nschema = (StructType()\n  .add(\"timestamp\", TimestampType())\n  .add(\"url\", StringType())\n  .add(\"userURL\", StringType())\n  .add(\"pageURL\", StringType())\n  .add(\"isNewPage\", BooleanType())\n  .add(\"geocoding\", StructType()\n    .add(\"countryCode2\", StringType())\n    .add(\"city\", StringType())\n    .add(\"latitude\", StringType())\n    .add(\"country\", StringType())\n    .add(\"longitude\", StringType())\n    .add(\"stateProvince\", StringType())\n    .add(\"countryCode3\", StringType())\n    .add(\"user\", StringType())\n    .add(\"namespace\", StringType()))\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["To read from a stream, use `spark.readStream()`, which returns a `DataStreamReader` class.  Then, configure the stream by adding the following options:<br><br>\n\n* The server endpoint: `server1.databricks.training:9092`\n* The topic to subscribe to: `en`\n* A location to log checkpoint metadata (more on this below)\n* The format: `kafka` \n\nFinally, use the load method, which loads the data stream from the Kafka source and returns it as an unbounded `DataFrame`."],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", sc.defaultParallelism)\n\nkafkaDF = (spark\n  .readStream\n  .option(\"kafka.bootstrap.servers\", \"server1.databricks.training:9092\")\n  .option(\"subscribe\", \"en\")\n  .format(\"kafka\")\n  .load()\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["-sandbox\nKafka transmits information using a key, value, and metadata such as topic and partition.  The information we're interested in is the `value` column.  Since this is a binary value, we must first cast it to a `StringType`.  We must also provide it with a schema.  Finally, we can expand the full structure of the JSON.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Wait until the stream finishes initializing.  \n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Often streams are started with the `.start()` method.  In this example, `display()`, provided in Databricks environments, is running that command for you."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col, from_json\nfrom pyspark.sql.types import StringType\n\nkafkaCleanDF = (kafkaDF\n  .select(from_json(col(\"value\").cast(StringType()), schema).alias(\"message\"))\n  .select(\"message.*\")\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"code","source":["myStreamName = \"lesson02_ps\"\ndisplay(kafkaCleanDF, streamName = myStreamName)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>timestamp</th><th>url</th><th>userURL</th><th>pageURL</th><th>isNewPage</th><th>geocoding</th></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":15},{"cell_type":"code","source":["# Wait until the stream is ready...\nuntilStreamIsReady(myStreamName)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The stream lesson02_ps is active and ready.\n</div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["Now we have a live stream of Wikipedia data.  Now stop the stream (and any other active streams) to reduce resource consumption."],"metadata":{}},{"cell_type":"code","source":["for s in spark.streams.active:\n  s.stop()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["### Transform the Stream\n\nWe can now start to apply transformation logic to our data in real time.  Parse out only the non-null country data."],"metadata":{}},{"cell_type":"code","source":["goeocodingDF = (kafkaCleanDF\n  .filter(col(\"geocoding.country\").isNotNull())\n  .select(\"timestamp\", \"pageURL\", \"geocoding.countryCode2\", \"geocoding.city\")\n)\n\ndisplay(goeocodingDF, streamName = myStreamName)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>timestamp</th><th>pageURL</th><th>countryCode2</th><th>city</th></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":20},{"cell_type":"code","source":["# Wait until the stream is ready...\nuntilStreamIsReady(myStreamName)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The stream lesson02_ps is active and ready.\n</div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["Stop the stream"],"metadata":{}},{"cell_type":"code","source":["for s in spark.streams.active:\n  s.stop()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"markdown","source":["### Combine with Historical Data\n\nJoins between historical data and streams operate in much the same way that other joins work.  Join the stream of Wikipedia data on country metadata"],"metadata":{}},{"cell_type":"markdown","source":["Import a lookup table of country data."],"metadata":{}},{"cell_type":"code","source":["countries = spark.read.parquet(\"/mnt/training/countries/ISOCountryCodes/ISOCountryLookup.parquet\")\n\ndisplay(countries)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>EnglishShortName</th><th>alpha2Code</th><th>alpha3Code</th><th>numericCode</th><th>ISO31662SubdivisionCode</th><th>independentTerritory</th></tr></thead><tbody><tr><td>Afghanistan</td><td>AF</td><td>AFG</td><td>004</td><td>ISO 3166-2:AF</td><td>Yes</td></tr><tr><td>Åland Islands</td><td>AX</td><td>ALA</td><td>248</td><td>ISO 3166-2:AX</td><td>No</td></tr><tr><td>Albania</td><td>AL</td><td>ALB</td><td>008</td><td>ISO 3166-2:AL</td><td>Yes</td></tr><tr><td>Algeria</td><td>DZ</td><td>DZA</td><td>012</td><td>ISO 3166-2:DZ</td><td>Yes</td></tr><tr><td>American Samoa</td><td>AS</td><td>ASM</td><td>016</td><td>ISO 3166-2:AS</td><td>No</td></tr><tr><td>Andorra</td><td>AD</td><td>AND</td><td>020</td><td>ISO 3166-2:AD</td><td>Yes</td></tr><tr><td>Angola</td><td>AO</td><td>AGO</td><td>024</td><td>ISO 3166-2:AO</td><td>Yes</td></tr><tr><td>Anguilla</td><td>AI</td><td>AIA</td><td>660</td><td>ISO 3166-2:AI</td><td>No</td></tr><tr><td>Antarctica</td><td>AQ</td><td>ATA</td><td>010</td><td>ISO 3166-2:AQ</td><td>No</td></tr><tr><td>Antigua and Barbuda</td><td>AG</td><td>ATG</td><td>028</td><td>ISO 3166-2:AG</td><td>Yes</td></tr><tr><td>Argentina</td><td>AR</td><td>ARG</td><td>032</td><td>ISO 3166-2:AR</td><td>Yes</td></tr><tr><td>Armenia</td><td>AM</td><td>ARM</td><td>051</td><td>ISO 3166-2:AM</td><td>Yes</td></tr><tr><td>Aruba</td><td>AW</td><td>ABW</td><td>533</td><td>ISO 3166-2:AW</td><td>No</td></tr><tr><td>Australia</td><td>AU</td><td>AUS</td><td>036</td><td>ISO 3166-2:AU</td><td>Yes</td></tr><tr><td>Austria</td><td>AT</td><td>AUT</td><td>040</td><td>ISO 3166-2:AT</td><td>Yes</td></tr><tr><td>Azerbaijan</td><td>AZ</td><td>AZE</td><td>031</td><td>ISO 3166-2:AZ</td><td>Yes</td></tr><tr><td>Bahamas</td><td>BS</td><td>BHS</td><td>044</td><td>ISO 3166-2:BS</td><td>Yes</td></tr><tr><td>Bahrain</td><td>BH</td><td>BHR</td><td>048</td><td>ISO 3166-2:BH</td><td>Yes</td></tr><tr><td>Bangladesh</td><td>BD</td><td>BGD</td><td>050</td><td>ISO 3166-2:BD</td><td>Yes</td></tr><tr><td>Barbados</td><td>BB</td><td>BRB</td><td>052</td><td>ISO 3166-2:BB</td><td>Yes</td></tr><tr><td>Belarus</td><td>BY</td><td>BLR</td><td>112</td><td>ISO 3166-2:BY</td><td>Yes</td></tr><tr><td>Belgium</td><td>BE</td><td>BEL</td><td>056</td><td>ISO 3166-2:BE</td><td>Yes</td></tr><tr><td>Belize</td><td>BZ</td><td>BLZ</td><td>084</td><td>ISO 3166-2:BZ</td><td>Yes</td></tr><tr><td>Benin</td><td>BJ</td><td>BEN</td><td>204</td><td>ISO 3166-2:BJ</td><td>Yes</td></tr><tr><td>Bermuda</td><td>BM</td><td>BMU</td><td>060</td><td>ISO 3166-2:BM</td><td>No</td></tr><tr><td>Bhutan</td><td>BT</td><td>BTN</td><td>064</td><td>ISO 3166-2:BT</td><td>Yes</td></tr><tr><td>Bolivia (Plurinational State of)</td><td>BO</td><td>BOL</td><td>068</td><td>ISO 3166-2:BO</td><td>Yes</td></tr><tr><td>Netherlands Bonaire Sint Eustatius and Saba</td><td>BQ</td><td>BES</td><td>535</td><td>ISO 3166-2:BQ</td><td>No</td></tr><tr><td>Bosnia and Herzegovina</td><td>BA</td><td>BIH</td><td>070</td><td>ISO 3166-2:BA</td><td>Yes</td></tr><tr><td>Botswana</td><td>BW</td><td>BWA</td><td>072</td><td>ISO 3166-2:BW</td><td>Yes</td></tr><tr><td>Bouvet Island</td><td>BV</td><td>BVT</td><td>074</td><td>ISO 3166-2:BV</td><td>No</td></tr><tr><td>Brazil</td><td>BR</td><td>BRA</td><td>076</td><td>ISO 3166-2:BR</td><td>Yes</td></tr><tr><td>British Indian Ocean Territory</td><td>IO</td><td>IOT</td><td>086</td><td>ISO 3166-2:IO</td><td>No</td></tr><tr><td>Brunei Darussalam</td><td>BN</td><td>BRN</td><td>096</td><td>ISO 3166-2:BN</td><td>Yes</td></tr><tr><td>Bulgaria</td><td>BG</td><td>BGR</td><td>100</td><td>ISO 3166-2:BG</td><td>Yes</td></tr><tr><td>Burkina Faso</td><td>BF</td><td>BFA</td><td>854</td><td>ISO 3166-2:BF</td><td>Yes</td></tr><tr><td>Burundi</td><td>BI</td><td>BDI</td><td>108</td><td>ISO 3166-2:BI</td><td>Yes</td></tr><tr><td>Cabo Verde</td><td>CV</td><td>CPV</td><td>132</td><td>ISO 3166-2:CV</td><td>Yes</td></tr><tr><td>Cambodia</td><td>KH</td><td>KHM</td><td>116</td><td>ISO 3166-2:KH</td><td>Yes</td></tr><tr><td>Cameroon</td><td>CM</td><td>CMR</td><td>120</td><td>ISO 3166-2:CM</td><td>Yes</td></tr><tr><td>Canada</td><td>CA</td><td>CAN</td><td>124</td><td>ISO 3166-2:CA</td><td>Yes</td></tr><tr><td>Cayman Islands</td><td>KY</td><td>CYM</td><td>136</td><td>ISO 3166-2:KY</td><td>No</td></tr><tr><td>Central African Republic</td><td>CF</td><td>CAF</td><td>140</td><td>ISO 3166-2:CF</td><td>Yes</td></tr><tr><td>Chad</td><td>TD</td><td>TCD</td><td>148</td><td>ISO 3166-2:TD</td><td>Yes</td></tr><tr><td>Chile</td><td>CL</td><td>CHL</td><td>152</td><td>ISO 3166-2:CL</td><td>Yes</td></tr><tr><td>China</td><td>CN</td><td>CHN</td><td>156</td><td>ISO 3166-2:CN</td><td>Yes</td></tr><tr><td>Christmas Island</td><td>CX</td><td>CXR</td><td>162</td><td>ISO 3166-2:CX</td><td>No</td></tr><tr><td>Cocos (Keeling) Islands</td><td>CC</td><td>CCK</td><td>166</td><td>ISO 3166-2:CC</td><td>No</td></tr><tr><td>Colombia</td><td>CO</td><td>COL</td><td>170</td><td>ISO 3166-2:CO</td><td>Yes</td></tr><tr><td>Comoros</td><td>KM</td><td>COM</td><td>174</td><td>ISO 3166-2:KM</td><td>Yes</td></tr><tr><td>Congo</td><td>CG</td><td>COG</td><td>178</td><td>ISO 3166-2:CG</td><td>Yes</td></tr><tr><td>Congo (Democratic Republic of the)</td><td>CD</td><td>COD</td><td>180</td><td>ISO 3166-2:CD</td><td>Yes</td></tr><tr><td>Cook Islands</td><td>CK</td><td>COK</td><td>184</td><td>ISO 3166-2:CK</td><td>No</td></tr><tr><td>Costa Rica</td><td>CR</td><td>CRI</td><td>188</td><td>ISO 3166-2:CR</td><td>Yes</td></tr><tr><td>Côte d'Ivoire</td><td>CI</td><td>CIV</td><td>384</td><td>ISO 3166-2:CI</td><td>Yes</td></tr><tr><td>Croatia</td><td>HR</td><td>HRV</td><td>191</td><td>ISO 3166-2:HR</td><td>Yes</td></tr><tr><td>Cuba</td><td>CU</td><td>CUB</td><td>192</td><td>ISO 3166-2:CU</td><td>Yes</td></tr><tr><td>Curaçao</td><td>CW</td><td>CUW</td><td>531</td><td>ISO 3166-2:CW</td><td>No</td></tr><tr><td>Cyprus</td><td>CY</td><td>CYP</td><td>196</td><td>ISO 3166-2:CY</td><td>Yes</td></tr><tr><td>Republic Czechia</td><td>CZ</td><td>CZE</td><td>203</td><td>ISO 3166-2:CZ</td><td>Yes</td></tr><tr><td>Denmark</td><td>DK</td><td>DNK</td><td>208</td><td>ISO 3166-2:DK</td><td>Yes</td></tr><tr><td>Djibouti</td><td>DJ</td><td>DJI</td><td>262</td><td>ISO 3166-2:DJ</td><td>Yes</td></tr><tr><td>Dominica</td><td>DM</td><td>DMA</td><td>212</td><td>ISO 3166-2:DM</td><td>Yes</td></tr><tr><td>Dominican Republic</td><td>DO</td><td>DOM</td><td>214</td><td>ISO 3166-2:DO</td><td>Yes</td></tr><tr><td>Ecuador</td><td>EC</td><td>ECU</td><td>218</td><td>ISO 3166-2:EC</td><td>Yes</td></tr><tr><td>Egypt</td><td>EG</td><td>EGY</td><td>818</td><td>ISO 3166-2:EG</td><td>Yes</td></tr><tr><td>El Salvador</td><td>SV</td><td>SLV</td><td>222</td><td>ISO 3166-2:SV</td><td>Yes</td></tr><tr><td>Equatorial Guinea</td><td>GQ</td><td>GNQ</td><td>226</td><td>ISO 3166-2:GQ</td><td>Yes</td></tr><tr><td>Eritrea</td><td>ER</td><td>ERI</td><td>232</td><td>ISO 3166-2:ER</td><td>Yes</td></tr><tr><td>Estonia</td><td>EE</td><td>EST</td><td>233</td><td>ISO 3166-2:EE</td><td>Yes</td></tr><tr><td>Ethiopia</td><td>ET</td><td>ETH</td><td>231</td><td>ISO 3166-2:ET</td><td>Yes</td></tr><tr><td>Falkland Islands (Malvinas)</td><td>FK</td><td>FLK</td><td>238</td><td>ISO 3166-2:FK</td><td>No</td></tr><tr><td>Faroe Islands</td><td>FO</td><td>FRO</td><td>234</td><td>ISO 3166-2:FO</td><td>No</td></tr><tr><td>Fiji</td><td>FJ</td><td>FJI</td><td>242</td><td>ISO 3166-2:FJ</td><td>Yes</td></tr><tr><td>Finland</td><td>FI</td><td>FIN</td><td>246</td><td>ISO 3166-2:FI</td><td>Yes</td></tr><tr><td>France</td><td>FR</td><td>FRA</td><td>250</td><td>ISO 3166-2:FR</td><td>Yes</td></tr><tr><td>French Guiana</td><td>GF</td><td>GUF</td><td>254</td><td>ISO 3166-2:GF</td><td>No</td></tr><tr><td>French Polynesia</td><td>PF</td><td>PYF</td><td>258</td><td>ISO 3166-2:PF</td><td>No</td></tr><tr><td>French Southern Territories</td><td>TF</td><td>ATF</td><td>260</td><td>ISO 3166-2:TF</td><td>No</td></tr><tr><td>Gabon</td><td>GA</td><td>GAB</td><td>266</td><td>ISO 3166-2:GA</td><td>Yes</td></tr><tr><td>Gambia</td><td>GM</td><td>GMB</td><td>270</td><td>ISO 3166-2:GM</td><td>Yes</td></tr><tr><td>Georgia</td><td>GE</td><td>GEO</td><td>268</td><td>ISO 3166-2:GE</td><td>Yes</td></tr><tr><td>Germany</td><td>DE</td><td>DEU</td><td>276</td><td>ISO 3166-2:DE</td><td>Yes</td></tr><tr><td>Ghana</td><td>GH</td><td>GHA</td><td>288</td><td>ISO 3166-2:GH</td><td>Yes</td></tr><tr><td>Gibraltar</td><td>GI</td><td>GIB</td><td>292</td><td>ISO 3166-2:GI</td><td>No</td></tr><tr><td>Greece</td><td>GR</td><td>GRC</td><td>300</td><td>ISO 3166-2:GR</td><td>Yes</td></tr><tr><td>Greenland</td><td>GL</td><td>GRL</td><td>304</td><td>ISO 3166-2:GL</td><td>No</td></tr><tr><td>Grenada</td><td>GD</td><td>GRD</td><td>308</td><td>ISO 3166-2:GD</td><td>Yes</td></tr><tr><td>Guadeloupe</td><td>GP</td><td>GLP</td><td>312</td><td>ISO 3166-2:GP</td><td>No</td></tr><tr><td>Guam</td><td>GU</td><td>GUM</td><td>316</td><td>ISO 3166-2:GU</td><td>No</td></tr><tr><td>Guatemala</td><td>GT</td><td>GTM</td><td>320</td><td>ISO 3166-2:GT</td><td>Yes</td></tr><tr><td>Guernsey</td><td>GG</td><td>GGY</td><td>831</td><td>ISO 3166-2:GG</td><td>No</td></tr><tr><td>Guinea</td><td>GN</td><td>GIN</td><td>324</td><td>ISO 3166-2:GN</td><td>Yes</td></tr><tr><td>Guinea-Bissau</td><td>GW</td><td>GNB</td><td>624</td><td>ISO 3166-2:GW</td><td>Yes</td></tr><tr><td>Guyana</td><td>GY</td><td>GUY</td><td>328</td><td>ISO 3166-2:GY</td><td>Yes</td></tr><tr><td>Haiti</td><td>HT</td><td>HTI</td><td>332</td><td>ISO 3166-2:HT</td><td>Yes</td></tr><tr><td>Heard Island and McDonald Islands</td><td>HM</td><td>HMD</td><td>334</td><td>ISO 3166-2:HM</td><td>No</td></tr><tr><td>Holy See</td><td>VA</td><td>VAT</td><td>336</td><td>ISO 3166-2:VA</td><td>Yes</td></tr><tr><td>Honduras</td><td>HN</td><td>HND</td><td>340</td><td>ISO 3166-2:HN</td><td>Yes</td></tr><tr><td>Hong Kong</td><td>HK</td><td>HKG</td><td>344</td><td>ISO 3166-2:HK</td><td>No</td></tr><tr><td>Hungary</td><td>HU</td><td>HUN</td><td>348</td><td>ISO 3166-2:HU</td><td>Yes</td></tr><tr><td>Iceland</td><td>IS</td><td>ISL</td><td>352</td><td>ISO 3166-2:IS</td><td>Yes</td></tr><tr><td>India</td><td>IN</td><td>IND</td><td>356</td><td>ISO 3166-2:IN</td><td>Yes</td></tr><tr><td>Indonesia</td><td>ID</td><td>IDN</td><td>360</td><td>ISO 3166-2:ID</td><td>Yes</td></tr><tr><td>Iran (Islamic Republic of)</td><td>IR</td><td>IRN</td><td>364</td><td>ISO 3166-2:IR</td><td>Yes</td></tr><tr><td>Iraq</td><td>IQ</td><td>IRQ</td><td>368</td><td>ISO 3166-2:IQ</td><td>Yes</td></tr><tr><td>Ireland</td><td>IE</td><td>IRL</td><td>372</td><td>ISO 3166-2:IE</td><td>Yes</td></tr><tr><td>Isle of Man</td><td>IM</td><td>IMN</td><td>833</td><td>ISO 3166-2:IM</td><td>No</td></tr><tr><td>Israel</td><td>IL</td><td>ISR</td><td>376</td><td>ISO 3166-2:IL</td><td>Yes</td></tr><tr><td>Italy</td><td>IT</td><td>ITA</td><td>380</td><td>ISO 3166-2:IT</td><td>Yes</td></tr><tr><td>Jamaica</td><td>JM</td><td>JAM</td><td>388</td><td>ISO 3166-2:JM</td><td>Yes</td></tr><tr><td>Japan</td><td>JP</td><td>JPN</td><td>392</td><td>ISO 3166-2:JP</td><td>Yes</td></tr><tr><td>Jersey</td><td>JE</td><td>JEY</td><td>832</td><td>ISO 3166-2:JE</td><td>No</td></tr><tr><td>Jordan</td><td>JO</td><td>JOR</td><td>400</td><td>ISO 3166-2:JO</td><td>Yes</td></tr><tr><td>Kazakhstan</td><td>KZ</td><td>KAZ</td><td>398</td><td>ISO 3166-2:KZ</td><td>Yes</td></tr><tr><td>Kenya</td><td>KE</td><td>KEN</td><td>404</td><td>ISO 3166-2:KE</td><td>Yes</td></tr><tr><td>Kiribati</td><td>KI</td><td>KIR</td><td>296</td><td>ISO 3166-2:KI</td><td>Yes</td></tr><tr><td>Korea (Democratic People's Republic of)</td><td>KP</td><td>PRK</td><td>408</td><td>ISO 3166-2:KP</td><td>Yes</td></tr><tr><td>Korea (Republic of)</td><td>KR</td><td>KOR</td><td>410</td><td>ISO 3166-2:KR</td><td>Yes</td></tr><tr><td>Kuwait</td><td>KW</td><td>KWT</td><td>414</td><td>ISO 3166-2:KW</td><td>Yes</td></tr><tr><td>Kyrgyzstan</td><td>KG</td><td>KGZ</td><td>417</td><td>ISO 3166-2:KG</td><td>Yes</td></tr><tr><td>People's Democratic Republic</td><td>LA</td><td>LAO</td><td>418</td><td>ISO 3166-2:LA</td><td>Yes</td></tr><tr><td>Latvia</td><td>LV</td><td>LVA</td><td>428</td><td>ISO 3166-2:LV</td><td>Yes</td></tr><tr><td>Lebanon</td><td>LB</td><td>LBN</td><td>422</td><td>ISO 3166-2:LB</td><td>Yes</td></tr><tr><td>Lesotho</td><td>LS</td><td>LSO</td><td>426</td><td>ISO 3166-2:LS</td><td>Yes</td></tr><tr><td>Liberia</td><td>LR</td><td>LBR</td><td>430</td><td>ISO 3166-2:LR</td><td>Yes</td></tr><tr><td>Libya</td><td>LY</td><td>LBY</td><td>434</td><td>ISO 3166-2:LY</td><td>Yes</td></tr><tr><td>Liechtenstein</td><td>LI</td><td>LIE</td><td>438</td><td>ISO 3166-2:LI</td><td>Yes</td></tr><tr><td>Lithuania</td><td>LT</td><td>LTU</td><td>440</td><td>ISO 3166-2:LT</td><td>Yes</td></tr><tr><td>Luxembourg</td><td>LU</td><td>LUX</td><td>442</td><td>ISO 3166-2:LU</td><td>Yes</td></tr><tr><td>Macao</td><td>MO</td><td>MAC</td><td>446</td><td>ISO 3166-2:MO</td><td>No</td></tr><tr><td>Macedonia (the former Yugoslav Republic of)</td><td>MK</td><td>MKD</td><td>807</td><td>ISO 3166-2:MK</td><td>Yes</td></tr><tr><td>Madagascar</td><td>MG</td><td>MDG</td><td>450</td><td>ISO 3166-2:MG</td><td>Yes</td></tr><tr><td>Malawi</td><td>MW</td><td>MWI</td><td>454</td><td>ISO 3166-2:MW</td><td>Yes</td></tr><tr><td>Malaysia</td><td>MY</td><td>MYS</td><td>458</td><td>ISO 3166-2:MY</td><td>Yes</td></tr><tr><td>Maldives</td><td>MV</td><td>MDV</td><td>462</td><td>ISO 3166-2:MV</td><td>Yes</td></tr><tr><td>Mali</td><td>ML</td><td>MLI</td><td>466</td><td>ISO 3166-2:ML</td><td>Yes</td></tr><tr><td>Malta</td><td>MT</td><td>MLT</td><td>470</td><td>ISO 3166-2:MT</td><td>Yes</td></tr><tr><td>Marshall Islands</td><td>MH</td><td>MHL</td><td>584</td><td>ISO 3166-2:MH</td><td>Yes</td></tr><tr><td>Martinique</td><td>MQ</td><td>MTQ</td><td>474</td><td>ISO 3166-2:MQ</td><td>No</td></tr><tr><td>Mauritania</td><td>MR</td><td>MRT</td><td>478</td><td>ISO 3166-2:MR</td><td>Yes</td></tr><tr><td>Mauritius</td><td>MU</td><td>MUS</td><td>480</td><td>ISO 3166-2:MU</td><td>Yes</td></tr><tr><td>Mayotte</td><td>YT</td><td>MYT</td><td>175</td><td>ISO 3166-2:YT</td><td>No</td></tr><tr><td>Mexico</td><td>MX</td><td>MEX</td><td>484</td><td>ISO 3166-2:MX</td><td>Yes</td></tr><tr><td>Micronesia (Federated States of)</td><td>FM</td><td>FSM</td><td>583</td><td>ISO 3166-2:FM</td><td>Yes</td></tr><tr><td>Moldova (Republic of)</td><td>MD</td><td>MDA</td><td>498</td><td>ISO 3166-2:MD</td><td>Yes</td></tr><tr><td>Monaco</td><td>MC</td><td>MCO</td><td>492</td><td>ISO 3166-2:MC</td><td>Yes</td></tr><tr><td>Mongolia</td><td>MN</td><td>MNG</td><td>496</td><td>ISO 3166-2:MN</td><td>Yes</td></tr><tr><td>Montenegro</td><td>ME</td><td>MNE</td><td>499</td><td>ISO 3166-2:ME</td><td>Yes</td></tr><tr><td>Montserrat</td><td>MS</td><td>MSR</td><td>500</td><td>ISO 3166-2:MS</td><td>No</td></tr><tr><td>Morocco</td><td>MA</td><td>MAR</td><td>504</td><td>ISO 3166-2:MA</td><td>Yes</td></tr><tr><td>Mozambique</td><td>MZ</td><td>MOZ</td><td>508</td><td>ISO 3166-2:MZ</td><td>Yes</td></tr><tr><td>Myanmar</td><td>MM</td><td>MMR</td><td>104</td><td>ISO 3166-2:MM</td><td>Yes</td></tr><tr><td>Namibia</td><td>NA</td><td>NAM</td><td>516</td><td>ISO 3166-2:NA</td><td>Yes</td></tr><tr><td>Nauru</td><td>NR</td><td>NRU</td><td>520</td><td>ISO 3166-2:NR</td><td>Yes</td></tr><tr><td>Nepal</td><td>NP</td><td>NPL</td><td>524</td><td>ISO 3166-2:NP</td><td>Yes</td></tr><tr><td>Netherlands</td><td>NL</td><td>NLD</td><td>528</td><td>ISO 3166-2:NL</td><td>Yes</td></tr><tr><td>New Caledonia</td><td>NC</td><td>NCL</td><td>540</td><td>ISO 3166-2:NC</td><td>No</td></tr><tr><td>New Zealand</td><td>NZ</td><td>NZL</td><td>554</td><td>ISO 3166-2:NZ</td><td>Yes</td></tr><tr><td>Nicaragua</td><td>NI</td><td>NIC</td><td>558</td><td>ISO 3166-2:NI</td><td>Yes</td></tr><tr><td>Niger</td><td>NE</td><td>NER</td><td>562</td><td>ISO 3166-2:NE</td><td>Yes</td></tr><tr><td>Nigeria</td><td>NG</td><td>NGA</td><td>566</td><td>ISO 3166-2:NG</td><td>Yes</td></tr><tr><td>Niue</td><td>NU</td><td>NIU</td><td>570</td><td>ISO 3166-2:NU</td><td>No</td></tr><tr><td>Norfolk Island</td><td>NF</td><td>NFK</td><td>574</td><td>ISO 3166-2:NF</td><td>No</td></tr><tr><td>Northern Mariana Islands</td><td>MP</td><td>MNP</td><td>580</td><td>ISO 3166-2:MP</td><td>No</td></tr><tr><td>Norway</td><td>NO</td><td>NOR</td><td>578</td><td>ISO 3166-2:NO</td><td>Yes</td></tr><tr><td>Oman</td><td>OM</td><td>OMN</td><td>512</td><td>ISO 3166-2:OM</td><td>Yes</td></tr><tr><td>Pakistan</td><td>PK</td><td>PAK</td><td>586</td><td>ISO 3166-2:PK</td><td>Yes</td></tr><tr><td>Palau</td><td>PW</td><td>PLW</td><td>585</td><td>ISO 3166-2:PW</td><td>Yes</td></tr><tr><td>State of Palestine</td><td>PS</td><td>PSE</td><td>275</td><td>ISO 3166-2:PS</td><td>No</td></tr><tr><td>Panama</td><td>PA</td><td>PAN</td><td>591</td><td>ISO 3166-2:PA</td><td>Yes</td></tr><tr><td>Papua New Guinea</td><td>PG</td><td>PNG</td><td>598</td><td>ISO 3166-2:PG</td><td>Yes</td></tr><tr><td>Paraguay</td><td>PY</td><td>PRY</td><td>600</td><td>ISO 3166-2:PY</td><td>Yes</td></tr><tr><td>Peru</td><td>PE</td><td>PER</td><td>604</td><td>ISO 3166-2:PE</td><td>Yes</td></tr><tr><td>Philippines</td><td>PH</td><td>PHL</td><td>608</td><td>ISO 3166-2:PH</td><td>Yes</td></tr><tr><td>Pitcairn</td><td>PN</td><td>PCN</td><td>612</td><td>ISO 3166-2:PN</td><td>No</td></tr><tr><td>Poland</td><td>PL</td><td>POL</td><td>616</td><td>ISO 3166-2:PL</td><td>Yes</td></tr><tr><td>Portugal</td><td>PT</td><td>PRT</td><td>620</td><td>ISO 3166-2:PT</td><td>Yes</td></tr><tr><td>Puerto Rico</td><td>PR</td><td>PRI</td><td>630</td><td>ISO 3166-2:PR</td><td>No</td></tr><tr><td>Qatar</td><td>QA</td><td>QAT</td><td>634</td><td>ISO 3166-2:QA</td><td>Yes</td></tr><tr><td>Réunion</td><td>RE</td><td>REU</td><td>638</td><td>ISO 3166-2:RE</td><td>No</td></tr><tr><td>Romania</td><td>RO</td><td>ROU</td><td>642</td><td>ISO 3166-2:RO</td><td>Yes</td></tr><tr><td>Russian Federation</td><td>RU</td><td>RUS</td><td>643</td><td>ISO 3166-2:RU</td><td>Yes</td></tr><tr><td>Rwanda</td><td>RW</td><td>RWA</td><td>646</td><td>ISO 3166-2:RW</td><td>Yes</td></tr><tr><td>Saint Barthélemy</td><td>BL</td><td>BLM</td><td>652</td><td>ISO 3166-2:BL</td><td>No</td></tr><tr><td>Saint Helena Ascension and Tristan da Cunha</td><td>SH</td><td>SHN</td><td>654</td><td>ISO 3166-2:SH</td><td>No</td></tr><tr><td>Saint Kitts and Nevis</td><td>KN</td><td>KNA</td><td>659</td><td>ISO 3166-2:KN</td><td>Yes</td></tr><tr><td>Saint Lucia</td><td>LC</td><td>LCA</td><td>662</td><td>ISO 3166-2:LC</td><td>Yes</td></tr><tr><td>Saint Martin (French part)</td><td>MF</td><td>MAF</td><td>663</td><td>ISO 3166-2:MF</td><td>No</td></tr><tr><td>Saint Pierre and Miquelon</td><td>PM</td><td>SPM</td><td>666</td><td>ISO 3166-2:PM</td><td>No</td></tr><tr><td>Saint Vincent and the Grenadines</td><td>VC</td><td>VCT</td><td>670</td><td>ISO 3166-2:VC</td><td>Yes</td></tr><tr><td>Samoa</td><td>WS</td><td>WSM</td><td>882</td><td>ISO 3166-2:WS</td><td>Yes</td></tr><tr><td>San Marino</td><td>SM</td><td>SMR</td><td>674</td><td>ISO 3166-2:SM</td><td>Yes</td></tr><tr><td>Sao Tome and Principe</td><td>ST</td><td>STP</td><td>678</td><td>ISO 3166-2:ST</td><td>Yes</td></tr><tr><td>Saudi Arabia</td><td>SA</td><td>SAU</td><td>682</td><td>ISO 3166-2:SA</td><td>Yes</td></tr><tr><td>Senegal</td><td>SN</td><td>SEN</td><td>686</td><td>ISO 3166-2:SN</td><td>Yes</td></tr><tr><td>Serbia</td><td>RS</td><td>SRB</td><td>688</td><td>ISO 3166-2:RS</td><td>Yes</td></tr><tr><td>Seychelles</td><td>SC</td><td>SYC</td><td>690</td><td>ISO 3166-2:SC</td><td>Yes</td></tr><tr><td>Sierra Leone</td><td>SL</td><td>SLE</td><td>694</td><td>ISO 3166-2:SL</td><td>Yes</td></tr><tr><td>Singapore</td><td>SG</td><td>SGP</td><td>702</td><td>ISO 3166-2:SG</td><td>Yes</td></tr><tr><td>Sint Maarten (Dutch part)</td><td>SX</td><td>SXM</td><td>534</td><td>ISO 3166-2:SX</td><td>No</td></tr><tr><td>Slovakia</td><td>SK</td><td>SVK</td><td>703</td><td>ISO 3166-2:SK</td><td>Yes</td></tr><tr><td>Slovenia</td><td>SI</td><td>SVN</td><td>705</td><td>ISO 3166-2:SI</td><td>Yes</td></tr><tr><td>Solomon Islands</td><td>SB</td><td>SLB</td><td>090</td><td>ISO 3166-2:SB</td><td>Yes</td></tr><tr><td>Somalia</td><td>SO</td><td>SOM</td><td>706</td><td>ISO 3166-2:SO</td><td>Yes</td></tr><tr><td>South Africa</td><td>ZA</td><td>ZAF</td><td>710</td><td>ISO 3166-2:ZA</td><td>Yes</td></tr><tr><td>South Georgia and the South Sandwich Islands</td><td>GS</td><td>SGS</td><td>239</td><td>ISO 3166-2:GS</td><td>No</td></tr><tr><td>South Sudan</td><td>SS</td><td>SSD</td><td>728</td><td>ISO 3166-2:SS</td><td>Yes</td></tr><tr><td>Spain</td><td>ES</td><td>ESP</td><td>724</td><td>ISO 3166-2:ES</td><td>Yes</td></tr><tr><td>Sri Lanka</td><td>LK</td><td>LKA</td><td>144</td><td>ISO 3166-2:LK</td><td>Yes</td></tr><tr><td>Sudan</td><td>SD</td><td>SDN</td><td>729</td><td>ISO 3166-2:SD</td><td>Yes</td></tr><tr><td>Suriname</td><td>SR</td><td>SUR</td><td>740</td><td>ISO 3166-2:SR</td><td>Yes</td></tr><tr><td>Svalbard and Jan Mayen</td><td>SJ</td><td>SJM</td><td>744</td><td>ISO 3166-2:SJ</td><td>No</td></tr><tr><td>Swaziland</td><td>SZ</td><td>SWZ</td><td>748</td><td>ISO 3166-2:SZ</td><td>Yes</td></tr><tr><td>Sweden</td><td>SE</td><td>SWE</td><td>752</td><td>ISO 3166-2:SE</td><td>Yes</td></tr><tr><td>Switzerland</td><td>CH</td><td>CHE</td><td>756</td><td>ISO 3166-2:CH</td><td>Yes</td></tr><tr><td>Syrian Arab Republic</td><td>SY</td><td>SYR</td><td>760</td><td>ISO 3166-2:SY</td><td>Yes</td></tr><tr><td>Taiwan Province of China[a]</td><td>TW</td><td>TWN</td><td>158</td><td>ISO 3166-2:TW</td><td>No</td></tr><tr><td>Tajikistan</td><td>TJ</td><td>TJK</td><td>762</td><td>ISO 3166-2:TJ</td><td>Yes</td></tr><tr><td>Tanzania United Republic of</td><td>TZ</td><td>TZA</td><td>834</td><td>ISO 3166-2:TZ</td><td>Yes</td></tr><tr><td>Thailand</td><td>TH</td><td>THA</td><td>764</td><td>ISO 3166-2:TH</td><td>Yes</td></tr><tr><td>Timor-Leste</td><td>TL</td><td>TLS</td><td>626</td><td>ISO 3166-2:TL</td><td>Yes</td></tr><tr><td>Togo</td><td>TG</td><td>TGO</td><td>768</td><td>ISO 3166-2:TG</td><td>Yes</td></tr><tr><td>Tokelau</td><td>TK</td><td>TKL</td><td>772</td><td>ISO 3166-2:TK</td><td>No</td></tr><tr><td>Tonga</td><td>TO</td><td>TON</td><td>776</td><td>ISO 3166-2:TO</td><td>Yes</td></tr><tr><td>Trinidad and Tobago</td><td>TT</td><td>TTO</td><td>780</td><td>ISO 3166-2:TT</td><td>Yes</td></tr><tr><td>Tunisia</td><td>TN</td><td>TUN</td><td>788</td><td>ISO 3166-2:TN</td><td>Yes</td></tr><tr><td>Turkey</td><td>TR</td><td>TUR</td><td>792</td><td>ISO 3166-2:TR</td><td>Yes</td></tr><tr><td>Turkmenistan</td><td>TM</td><td>TKM</td><td>795</td><td>ISO 3166-2:TM</td><td>Yes</td></tr><tr><td>Turks and Caicos Islands</td><td>TC</td><td>TCA</td><td>796</td><td>ISO 3166-2:TC</td><td>No</td></tr><tr><td>Tuvalu</td><td>TV</td><td>TUV</td><td>798</td><td>ISO 3166-2:TV</td><td>Yes</td></tr><tr><td>Uganda</td><td>UG</td><td>UGA</td><td>800</td><td>ISO 3166-2:UG</td><td>Yes</td></tr><tr><td>Ukraine</td><td>UA</td><td>UKR</td><td>804</td><td>ISO 3166-2:UA</td><td>Yes</td></tr><tr><td>United Arab Emirates</td><td>AE</td><td>ARE</td><td>784</td><td>ISO 3166-2:AE</td><td>Yes</td></tr><tr><td>United Kingdom of Great Britain and Northern Ireland</td><td>GB</td><td>GBR</td><td>826</td><td>ISO 3166-2:GB</td><td>Yes</td></tr><tr><td>United States of America</td><td>US</td><td>USA</td><td>840</td><td>ISO 3166-2:US</td><td>Yes</td></tr><tr><td>United States Minor Outlying Islands</td><td>UM</td><td>UMI</td><td>581</td><td>ISO 3166-2:UM</td><td>No</td></tr><tr><td>Uruguay</td><td>UY</td><td>URY</td><td>858</td><td>ISO 3166-2:UY</td><td>Yes</td></tr><tr><td>Uzbekistan</td><td>UZ</td><td>UZB</td><td>860</td><td>ISO 3166-2:UZ</td><td>Yes</td></tr><tr><td>Vanuatu</td><td>VU</td><td>VUT</td><td>548</td><td>ISO 3166-2:VU</td><td>Yes</td></tr><tr><td>Venezuela (Bolivarian Republic of)</td><td>VE</td><td>VEN</td><td>862</td><td>ISO 3166-2:VE</td><td>Yes</td></tr><tr><td>Viet Nam</td><td>VN</td><td>VNM</td><td>704</td><td>ISO 3166-2:VN</td><td>Yes</td></tr><tr><td>Virgin Islands (British)</td><td>VG</td><td>VGB</td><td>092</td><td>ISO 3166-2:VG</td><td>No</td></tr><tr><td>Virgin Islands (U.S.)</td><td>VI</td><td>VIR</td><td>850</td><td>ISO 3166-2:VI</td><td>No</td></tr><tr><td>Wallis and Futuna</td><td>WF</td><td>WLF</td><td>876</td><td>ISO 3166-2:WF</td><td>No</td></tr><tr><td>Western Sahara</td><td>EH</td><td>ESH</td><td>732</td><td>ISO 3166-2:EH</td><td>No</td></tr><tr><td>Yemen</td><td>YE</td><td>YEM</td><td>887</td><td>ISO 3166-2:YE</td><td>Yes</td></tr><tr><td>Zambia</td><td>ZM</td><td>ZMB</td><td>894</td><td>ISO 3166-2:ZM</td><td>Yes</td></tr><tr><td>Zimbabwe</td><td>ZW</td><td>ZWE</td><td>716</td><td>ISO 3166-2:ZW</td><td>Yes</td></tr></tbody></table></div>"]}}],"execution_count":26},{"cell_type":"markdown","source":["Join the two `DataFrame`s on their two-letter country code."],"metadata":{}},{"cell_type":"code","source":["joinedDF = goeocodingDF.join(countries, goeocodingDF.countryCode2 == countries.alpha2Code)\n\ndisplay(joinedDF, streamName = myStreamName)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>timestamp</th><th>pageURL</th><th>countryCode2</th><th>city</th><th>EnglishShortName</th><th>alpha2Code</th><th>alpha3Code</th><th>numericCode</th><th>ISO31662SubdivisionCode</th><th>independentTerritory</th></tr></thead><tbody><tr><td>2020-04-18T00:03:26.318+0000</td><td>http://en.wikipedia.org/wiki/Ellison_Barber</td><td>US</td><td>New York</td><td>United States of America</td><td>US</td><td>USA</td><td>840</td><td>ISO 3166-2:US</td><td>Yes</td></tr><tr><td>2020-04-18T00:03:28.643+0000</td><td>http://en.wikipedia.org/wiki/ARIA_Music_Awards_of_2014</td><td>AU</td><td>null</td><td>Australia</td><td>AU</td><td>AUS</td><td>036</td><td>ISO 3166-2:AU</td><td>Yes</td></tr><tr><td>2020-04-18T00:03:29.224+0000</td><td>http://en.wikipedia.org/wiki/Special:Log/abusefilter</td><td>US</td><td>Staten Island</td><td>United States of America</td><td>US</td><td>USA</td><td>840</td><td>ISO 3166-2:US</td><td>Yes</td></tr></tbody></table></div>"]}}],"execution_count":28},{"cell_type":"code","source":["# Wait until the stream is ready...\nuntilStreamIsReady(myStreamName)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The stream lesson02_ps is active and ready.\n</div>"]}}],"execution_count":29},{"cell_type":"markdown","source":["Stop the stream"],"metadata":{}},{"cell_type":"code","source":["for s in spark.streams.active:\n  s.stop()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":31},{"cell_type":"markdown","source":["-sandbox\n### Batch and Streaming Writes\n\nThe logic that applies to batch processing can be ported over to streaming often with only minor modifications.  One best practice is to run batch operations as streaming jobs using a single trigger.  By using a checkpoint location, the metadata on which data has already been processed will be maintained so the cluster can be shut down without a loss of information.  This works best on streaming from a directory, where new files that appear are added to the stream.\n\nWrites can be done against always up-to-date parquet files or a Databricks Delta table, which offers ACID compliant transactions on top of parquet.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> For more information on Databricks Delta, see the <a href=\"https://academy.databricks.com/collections/frontpage/products/using-databricks-delta-1-user-1-year\" target=\"_blank\">Databricks Delta course from Databricks Academy</a><br>\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Read more about triggers in the <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#triggers\" target=\"_blank\">Structured Streaming Programming Guide</a>"],"metadata":{}},{"cell_type":"markdown","source":["Confirm that Delta is enabled on your cluster."],"metadata":{}},{"cell_type":"code","source":["spark.sql(\"set spark.databricks.delta.preview.enabled=true\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[42]: DataFrame[key: string, value: string]</div>"]}}],"execution_count":34},{"cell_type":"markdown","source":["Write to a Databricks Delta table and partition by country."],"metadata":{}},{"cell_type":"code","source":["basePath = \"{}/etl3p\".format(getUserhome())\ncheckpointPath = \"{}/joined.checkpoint\".format(basePath)\njoinedPath = \"{}/joined\".format(basePath)\n\n(joinedDF\n  .writeStream                                   # Write the stream\n  .format(\"delta\")                               # Use the delta format\n  .partitionBy(\"countryCode2\")                   # Specify a feature to partition on\n  .option(\"checkpointLocation\", checkpointPath)  # Specify where to log metadata\n  .option(\"path\", joinedPath)                    # Specify the output path\n  .outputMode(\"append\")                          # Append new records to the output path\n  .queryName(myStreamName)                       # The name of the stream\n  .start()                                       # Start the operation\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[43]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7fdde3043898&gt;</div>"]}}],"execution_count":36},{"cell_type":"code","source":["# Wait until the stream is ready...\nuntilStreamIsReady(myStreamName)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The stream lesson02_ps is active and ready.\n</div>"]}}],"execution_count":37},{"cell_type":"markdown","source":["Check to see that the data is there."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import count\n\ncountsDF = (spark.read.format(\"delta\").load(joinedPath)\n  .select(count(\"*\").alias(\"count\"))\n)\n\ndisplay(countsDF)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count</th></tr></thead><tbody><tr><td>7</td></tr></tbody></table></div>"]}}],"execution_count":39},{"cell_type":"markdown","source":["Now stop all streams"],"metadata":{}},{"cell_type":"code","source":["for s in spark.streams.active:\n  s.stop()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":41},{"cell_type":"markdown","source":["### Failure Recovery\n\nTo recover from cluster failure, Spark uses checkpointing for maintaining state.  In our initial command, we accomplished this using `.option(\"checkpointLocation\", checkpointPath)`.  The checkpoint directory is per query and while that query is active, Spark continuously writes metadata of the processed data to this checkpoint directory.  Even if the entire cluster fails, the query can be restarted on a new cluster using the same directory and Spark can use this to start a new query where the failed one left off.  \n\n**This is how Spark ensures end-to-end, exactly-once guarantees and how Spark maintains metadata on what data it has already seen between streaming jobs.**"],"metadata":{}},{"cell_type":"code","source":["display(dbutils.fs.ls(checkpointPath))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/user/tbresee@umich.edu/etl3p/joined.checkpoint/commits/</td><td>commits/</td><td>0</td></tr><tr><td>dbfs:/user/tbresee@umich.edu/etl3p/joined.checkpoint/metadata</td><td>metadata</td><td>45</td></tr><tr><td>dbfs:/user/tbresee@umich.edu/etl3p/joined.checkpoint/offsets/</td><td>offsets/</td><td>0</td></tr><tr><td>dbfs:/user/tbresee@umich.edu/etl3p/joined.checkpoint/sources/</td><td>sources/</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":43},{"cell_type":"markdown","source":["## Exercise 1: Streaming ETL\n\nThis exercise entails reading from a Kafka stream of product orders and writing the results to a Delta table."],"metadata":{}},{"cell_type":"markdown","source":["### Step 1: Create a Streaming DataFrame\n\nCreate the streaming DataFrame `productsDF` using the following specifics:<br><br>\n\n* Server endpoint: `server1.databricks.training:9092`\n* Topic name: `product-orders`\n* Format: `kafka`"],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nproductsDF = (spark\n  .readStream\n  .option(\"kafka.bootstrap.servers\", \"server1.databricks.training:9092\")\n  .option(\"subscribe\", \"product-orders\")\n  .format(\"kafka\")\n  .load()\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":46},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\nfrom pyspark.sql.types import *\n\ndbTest(\"ET3-P-02-01-01\", True, productsDF.schema['key'].dataType in [BinaryType(), StringType()])\ndbTest(\"ET3-P-02-01-02\", True, productsDF.schema['value'].dataType in [BinaryType(), StringType()])\ndbTest(\"ET3-P-02-01-03\", True, productsDF.schema['topic'].dataType == StringType())\ndbTest(\"ET3-P-02-01-04\", True, productsDF.schema['partition'].dataType == IntegerType())\ndbTest(\"ET3-P-02-01-05\", True, productsDF.schema['offset'].dataType == LongType())\ndbTest(\"ET3-P-02-01-06\", True, productsDF.schema['timestamp'].dataType in [TimestampType(), StringType()])\ndbTest(\"ET3-P-02-01-07\", True, productsDF.schema['timestampType'].dataType == IntegerType())\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Tests passed!\n</div>"]}}],"execution_count":47},{"cell_type":"markdown","source":["### Step 2: Defining a Schema\n\nDefine a schema that consists of the following values.  Save it as `productSchema`<br>\n\n| Field            | Type             |\n|:-----------------|:-----------------|\n| `orderID`        | `IntegerType()`  |\n| `productID`      | `IntegerType()`  |\n| `orderTimestamp` | `TimestampType()`|\n| `orderQty`       | `IntegerType()`  |"],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.types import IntegerType, StructField, StructType, TimestampType\n\nproductSchema = StructType([\n  StructField(\"orderID\", IntegerType(), nullable=True),\n  StructField(\"productID\", IntegerType(), nullable=True),\n  StructField(\"orderTimestamp\", TimestampType(), nullable=True),\n  StructField(\"orderQty\", IntegerType(), nullable=True)\n])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":49},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\nfrom pyspark.sql.types import IntegerType, StringType, TimestampType\n\ndbTest(\"ET3-P-02-02-01\", True, productSchema['orderID'].dataType == IntegerType())\ndbTest(\"ET3-P-02-02-02\", True, productSchema['productID'].dataType == IntegerType())\ndbTest(\"ET3-P-02-02-03\", True, productSchema['orderTimestamp'].dataType in [StringType(), TimestampType()])\ndbTest(\"ET3-P-02-02-04\", True, productSchema['orderQty'].dataType == IntegerType())\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Tests passed!\n</div>"]}}],"execution_count":50},{"cell_type":"markdown","source":["### Step 3: Parsing the Kafka Data\n\nParse the `value` column of the Kafka data.  Remember to use the `from_json()` function and apply your schema.  Save the results to `ordersDF`"],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import from_json, col\nfrom pyspark.sql.types import StringType\n\nordersDF = productsDF.select(from_json(col(\"value\").cast(StringType()), productSchema).alias(\"order\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":52},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\nfrom pyspark.sql.types import IntegerType, TimestampType\n\n_df = ordersDF.select(col(\"order.orderID\").alias(\"orderID\"),\n                       col(\"order.productID\").alias(\"productID\"),\n                       col(\"order.orderTimestamp\").alias(\"orderTimestamp\"),\n                       col(\"order.orderQty\").alias(\"orderQty\"))\n\nexpected = {\"orderID\": IntegerType(), \"orderQty\": IntegerType(), \"orderTimestamp\": TimestampType(), \"productID\": IntegerType()}\nsch = {c.name: c.dataType for c in _df.schema}\n\ndbTest(\"ET3-P-02-03-01\", True, sch == expected)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Tests passed!\n</div>"]}}],"execution_count":53},{"cell_type":"markdown","source":["### Step 4: Write to a Delta Table\n\nWrite the results to an always up-to-date Databricks Delta table to the provided path.  The table should have the following columns:<br><br>\n\n1. `orderID`\n1. `productID`\n1. `orderTimestamp`\n1. `orderQty`"],"metadata":{}},{"cell_type":"code","source":["# ANSWER\n\nordersPath = \"{}/orders\".format(basePath)\ncheckpointPath = \"{}/orders.checkpoint\".format(basePath)\n\n(ordersDF\n  .select(\"order.orderID\", \"order.orderQty\", \"order.orderTimestamp\", \"order.productID\")\n  .writeStream\n  .format(\"delta\")\n  .partitionBy(\"productID\")\n  .option(\"checkpointLocation\", checkpointPath)\n  .option(\"path\", ordersPath)\n  .outputMode(\"append\")\n  .queryName(myStreamName)\n  .start()\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[54]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7fdde3350860&gt;</div>"]}}],"execution_count":55},{"cell_type":"code","source":["# Wait until the stream is ready...\nuntilStreamIsReady(myStreamName)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":56},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\nfrom pyspark.sql.types import *\n\nordersPath = \"{}/orders\".format(basePath)\n_df = spark.read.format(\"delta\").load(ordersPath)\n\nexpected = {\"orderID\": IntegerType(), \"orderQty\": IntegerType(), \"orderTimestamp\": TimestampType(), \"productID\": IntegerType()}\nsch = {c.name: c.dataType for c in _df.schema}\n\ndbTest(\"ET3-P-02-04-01\", True, sch == expected)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":["### Step 5: Now Stop the Stream and Delete Files\n\nStop the stream."],"metadata":{}},{"cell_type":"code","source":["for s in spark.streams.active:\n  s.stop()"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":["Recursively delete the files you created (this is a permanent operation)."],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.rm(basePath, True)"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":["## Review\n**Question:** What is the best practice for different versions of ETL jobs?  \n**Answer:** Generally speaking, an ETL solution should have three versions:\n0. *Batch* or the ability to run a periodic workload\n0. *Streaming* or the ability to process incoming data in real time\n0. *Incremental* or the ability to process a specific set of data, especially in the case of job failure.<br>\nIn practice, batch and streaming jobs are oftentimes combined where a batch job is a streaming workload using a single trigger.\n\n**Question:** What are commonly approached as data streams?  \n**Answer:** Apache Kafka and the Azure managed alternative Event Hubs are common data streams.  Additionally, it's common to monitor a directory for incoming files.  When a new file appears, it is brought into the stream for processing.\n\n**Question:** How does Spark ensure exactly-once data delivery and maintain metadata on a stream?  \n**Answer:** Checkpoints give Spark this fault tolerance through the ability to maintain state off of the cluster.\n\n**Question:** How does the Spark approach to streaming integrate with other Spark features?  \n**Answer:** Spark Streaming uses the same DataFrame API, allowing easy integration with other Spark functionality."],"metadata":{}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Cleanup<br>\n\nRun the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Cleanup\""],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":["## Next Steps\n\nStart the next lesson, [Runnable Notebooks]($./ETL3 03 - Runnable Notebooks )."],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n**Q:** Where can I find out more information on streaming ETL jobs?  \n**A:** Check out the Databricks blog post <a href=\"https://databricks.com/blog/2017/01/19/real-time-streaming-etl-structured-streaming-apache-spark-2-1.html\" target=\"_blank\">Real-time Streaming ETL with Structured Streaming in Apache Spark 2.1</a>\n\n**Q:** Where can I get more information on integrating Streaming and Kafka?  \n**A:** Check out the <a href=\"https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html\" target=\"_blank\">Structured Streaming + Kafka Integration Guide</a>\n\n**Q:** Where can I see a case study on an IoT pipeline using Spark Streaming?  \n**A:** Check out the Databricks blog post <a href=\"https://databricks.com/blog/2017/04/26/processing-data-in-apache-kafka-with-structured-streaming-in-apache-spark-2-2.html\" target=\"_blank\">Processing Data in Apache Kafka with Structured Streaming in Apache Spark 2.2</a>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"ETL3 02 - Streaming ETL","notebookId":1055298992984024},"nbformat":4,"nbformat_minor":0}
