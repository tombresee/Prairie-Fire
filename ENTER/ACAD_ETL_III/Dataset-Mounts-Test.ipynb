{"cells":[{"cell_type":"markdown","source":["d\n# Dataset-Mounts-Test\nThe purpose of this notebook is to faciliate testing of our systems."],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(\"com.databricks.training.module-name\", \"dataset-mounts-test\")"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["%run ./Dataset-Mounts"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%scala\n\nval testStart = System.currentTimeMillis\nval mountPointBase = \"/mnt/training-test\"\n\ndef unmount(mountPoint:String):Unit = {\n  try {\n    dbutils.fs.unmount(mountPoint)\n  } catch {\n    case e:Exception => println(s\"Not mounted: $mountPoint\")\n  }\n}\n\ndef testRegion(regionType:String, regionName:String, mapper: (String) => (String,Map[String,String])):Unit = {\n  val start = System.currentTimeMillis\n  \n  val (source, extraConfigs) = mapper(regionName)\n  val mountPoint = s\"$mountPointBase-${regionType.toLowerCase}-$regionName\"\n  println(s\"\"\"\\nTesting the $regionType region $regionName ($mountPoint)\"\"\")\n\n  mountSource(true, false, mountPoint, source, extraConfigs)\n  Thread.sleep(5*1000) // give it a second\n  validateDatasets(mountPoint)\n  \n  val duration = (System.currentTimeMillis - start) / 1000.0\n  println(f\"...all tests passed in $duration%1.2f seconds!\")\n}\n\ndef validateDataset(mountPoint:String, target:String):Unit = {\n  val map = scala.collection.mutable.Map[String,(Long,Long)]()\n  for (file <- dbutils.fs.ls(s\"/mnt/training/$target\")) {\n    map.put(file.name, (file.size, -1L))\n  }\n\n  val path = s\"$mountPoint/$target\"\n  for (file <- dbutils.fs.ls(path)) {\n      if (map.contains(file.name)) {\n        val (sizes, _) = map(file.name)\n        map.put(file.name, (sizes, file.size))\n      } else {\n        map.put(file.name, (-1, file.size))\n      }\n  }\n  \n  var errors = \"\"\n  for (key <- map.keySet) {\n    val (sizeA, sizeB) = map(key)\n    if (sizeA == sizeB) {\n      // Everything matches up... no issue here.\n    } else if (sizeA == -1) {\n      if (!key.endsWith(\"_$folder$\"))\n        errors += s\"Extra file: $path$key\\n\"\n    } else if (sizeB == -1) {\n      errors += s\"Missing file: $path$key\\n\"\n    }\n  }\n  \n  errors = errors.trim()\n  if (errors != \"\") {\n    println(errors)\n    throw new IllegalStateException(s\"Errors were found while processing $path\")\n  }\n}\n\n\ndef validateDatasets(mountPoint:String) {\n  val paths = List(\n    \"\",\n    \"301/\",\n    \"Chicago-Crimes-2018.csv\",\n    \"City-Data.parquet/\",\n    \"EDGAR-Log-20170329/\",\n    \"UbiqLog4UCI/\",\n    \"_META/\",\n    \"adventure-works/\",\n    \"airbnb/\",\n    \"airbnb-sf-listings.csv\",\n    \"asa/\",\n    \"auto-mpg.csv\",\n    \"bigrams/\",\n    \"bikeSharing/\",\n    \"bostonhousing/\",\n    \"cancer/\",\n    \"countries/\",\n    \"crime-data-2016/\",\n    \"data/\",\n    \"data-cleansing/\",\n    \"databricks-blog.json\",\n    \"databricks-datasets/\",\n    \"dataframes/\",\n    \"day-of-week/\",\n    \"definitive-guide/\",\n    \"dl/\",\n    \"gaming_data/\",\n    \"global-sales/\",\n    \"graphx-demo/\",\n    \"initech/\",\n    \"ip-geocode.parquet/\",\n    \"iris/\",\n    \"mini_newsgroups/\",\n    \"mnist/\",\n    \"movie-reviews/\",\n    \"movielens/\",\n    \"movies/\",\n    \"online_retail/\",\n    \"philadelphia-crime-data-2015-ytd.csv\",\n    \"purchases.txt\",\n    \"sensor-data/\",\n    \"ssn/\",\n    \"stopwords\",\n    \"structured-streaming/\",\n    \"test.log\",\n    \"tom-sawyer/\",\n    \"tweets.txt\",\n    \"twitter/\",\n    \"wash_dc_crime_incidents_2013.csv\",\n    \"wash_dc_crime_incidents_2015-10-03-to-2016-10-02.csv\",\n    \"weather/\",\n    \"wikipedia/\",\n    \"wine.parquet/\",\n    \"word-game-dict.txt\",\n    \"zip3state.csv\",\n    \"zips.json\"\n  )\n  for (path <- paths) {\n    validateDataset(mountPoint, path)\n  }\n}"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%scala\nval awsRegions=List(\n  \"us-west-2\",\n  \"ap-northeast-1\",\n  \"ap-northeast-2\",\n  \"ap-south-1\",\n  \"ap-southeast-1\",\n  \"ap-southeast-2\",\n  \"ca-central-1\",\n  \"eu-central-1\",\n  \"eu-west-1\",\n  \"eu-west-2\",\n  \"eu-west-3\",\n  \"sa-east-1\",\n  \"us-east-1\",\n  \"us-east-2\"\n).map(_.toLowerCase())\n\nval azureRegions=List(\n \"AustraliaCentral\",\n \"AustraliaCentral2\",\n \"AustraliaEast\",\n \"AustraliaSoutheast\",\n \"CanadaCentral\",\n \"CanadaEast\",\n \"CentralIndia\",\n \"CentralUS\",\n \"EastAsia\",\n \"EastUS\",\n \"EastUS2\",\n \"JapanEast\",\n \"JapanWest\",\n \"NorthCentralUS\",\n \"NorthCentralUS\",\n \"NorthEurope\",\n \"SouthCentralUS\",\n \"SouthCentralUS\",\n \"SouthIndia\",\n \"SoutheastAsia\",\n \"UKSouth\",\n \"UKWest\",\n \"WestCentralUS\",  // Azure Databricks isn't available in region, but we historically copied data here anyway.\n \"WestEurope\",\n \"WestIndia\",\n \"WestUS\",\n \"WestUS2\"\n).map(_.toLowerCase())"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["%scala\nfor (region <- awsRegions) {\n  testRegion(\"AWS\", region, getAwsMapping _)\n}"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["%python\nfor mount in (mount[0] for mount in dbutils.fs.mounts() if \"training-test\" in mount[0]):\n  dbutils.fs.unmount(mount)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%scala\nfor (region <- azureRegions) {\n  testRegion(\"Azure\", region, getAzureMapping _)\n}"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["%scala\nprintln(f\"...all tests passed in ${(System.currentTimeMillis - testStart) / 1000.0 / 60.0}%1.2f minutes!\")"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["%python\nfor mount in (mount[0] for mount in dbutils.fs.mounts() if \"training-test\" in mount[0]):\n  dbutils.fs.unmount(mount)\n"],"metadata":{},"outputs":[],"execution_count":10}],"metadata":{"name":"Dataset-Mounts-Test","notebookId":1055298992984222},"nbformat":4,"nbformat_minor":0}
