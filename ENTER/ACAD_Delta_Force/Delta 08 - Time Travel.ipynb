{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# Use Databricks Delta Time Travel to View an Older Snapshot of Data\nDatabricks&reg; Delta Time Travel allows you to work with older snapshots of data.\n\n## In this lesson you:\n0. Stream power plant data to a Databricks Delta table\n0. Look at summary statistics of current data set\n0. Rewind to an older version of the data\n0. Look at summary statistics of older data set\n\n## Audience\n* Primary Audience: Data Engineers, Data Scientists\n* Secondary Audience: Data Analysts\n\n## Prerequisites\n* Web browser: **Chrome**\n* A cluster configured with **8 cores** and **DBR 6.2**\n* Suggested Courses from <a href=\"https://academy.databricks.com/\" target=\"_blank\">Databricks Academy</a>:\n  - ETL Part 1\n  - Spark-SQL\n  - Structured Streaming\n\n## Datasets Used\nA powerplant dataset found in\n`/mnt/training/power-plant/streamed.parquet`.\n\nThe schema definition is:\n\n- AT = Atmospheric Temperature [1.81-37.11]Â°C\n- V = Exhaust Vaccum Speed [25.36-81.56] cm Hg\n- AP = Atmospheric Pressure in [992.89-1033.30] milibar\n- RH = Relative Humidity [0-100]%\n- PE = Power Output [420.26-495.76] MW\n\nPE is the label or target. This is the value we are trying to predict given the measurements.\n\n*Reference [UCI Machine Learning Repository Combined Cycle Power Plant Data Set](https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant)*"],"metadata":{}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Setup\n\nFor each lesson to execute correctly, please make sure to run the **`Classroom-Setup`** cell at the<br/>\nstart of each lesson (see the next cell) and the **`Classroom-Cleanup`** cell at the end of each lesson."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["## Databricks Delta Time Travel\n\nThe Databricks Delta log has a list of what files are valid for each read / write operation.\n\nBy referencing this list, a request can be made for the data at a specific point in time. \n\nThis is similar to the concept of code Revision histories.\n\nExamples of Time Travel use cases are:\n* Re-creating analyses, reports, or outputs (for example, the output of a machine learning model). \n  * This could be useful for debugging or auditing, especially in regulated industries.\n* Writing complex temporal queries.\n* Fixing mistakes in your data.\n* Providing snapshot isolation for a set of queries for fast changing tables."],"metadata":{}},{"cell_type":"markdown","source":["## Slow Stream of Files\n\nOur stream source is a repository of many small files."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, DoubleType\nspark.conf.set(\"spark.sql.shuffle.partitions\", 8)\n\ndataPath = \"/mnt/training/power-plant/streamed.parquet\"\n\ndataSchema = StructType([\n  StructField(\"AT\", DoubleType(), True),\n  StructField(\"V\", DoubleType(), True),\n  StructField(\"AP\", DoubleType(), True),\n  StructField(\"RH\", DoubleType(), True),\n  StructField(\"PE\", DoubleType(), True)\n])\n\ninitialDF = (spark\n  .readStream                        # Returns DataStreamReader\n  .option(\"maxFilesPerTrigger\", 1)   # Force processing of only 1 file per trigger \n  .schema(dataSchema)                # Required for all streaming DataFrames\n  .parquet(dataPath) \n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["## Append to a Databricks Delta Table\n\nUse this to create `powerTable`."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import TimestampType\n\nwritePath      = workingDir + \"/output.parquet\"    # A subdirectory for our output\ncheckpointPath = workingDir + \"/output.checkpoint\" # A subdirectory for our checkpoint & W-A logs\n\npowerTable = \"powerTable\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["And to help us manage our streams better, we will make use of **`untilStreamIsReady()`**, **`stopAllStreams()`** and define the following, **`myStreamName`**:"],"metadata":{}},{"cell_type":"code","source":["myStreamName = \"lesson08_ps\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["##Introducing Time Travel\n\nDatabricks Delta time travel allows you to query an older snapshot of a table.\n\nHere, we introduce a new option to Databricks Delta.\n\n`.option(\"timestampAsOf\", now)` \n\nWhere `now` is the current timestamp, that must be a STRING that can be cast to a Timestamp.\n\nThere is an alternate notation as well \n\n`.option(\"versionAsOf\", version)`\n\nMore details are described in the <a href=\"https://docs.databricks.com/delta/delta-batch.html#deltatimetravel\" target=\"_blank\">official documentation</a>."],"metadata":{}},{"cell_type":"code","source":["import datetime\nnow = datetime.datetime.now()\n\nstreamingQuery = (initialDF                     # Start with our \"streaming\" DataFrame\n  .writeStream                                  # Get the DataStreamWriter\n  .trigger(processingTime=\"3 seconds\")          # Configure for a 3-second micro-batch\n  .queryName(myStreamName)                       # Specify Query Name\n  .format(\"delta\")                              # Specify the sink type, a Parquet file\n  .option(\"timestampAsOf\", now)                 # Timestamp the stream in the form of string that can be converted to TimeStamp\n  .outputMode(\"append\")                         # Write only new data to the \"file\"\n  .option(\"checkpointLocation\", checkpointPath) # Specify the location of checkpoint files & W-A logs\n  .table(powerTable)\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"code","source":["# Wait until the stream is ready before proceeding\nuntilStreamIsReady(myStreamName)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The stream lesson08_ps is active and ready.\n</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["-sandbox\n## Retention Period and Table Properties\n\nYou configure retention periods using `ALTER TABLE` syntax with the following table properties:\n\n* `delta.logRetentionDuration \"interval interval-string\" `\n  * Configure how long you can go back in time. Default is interval 30 days.\n\n* `delta.deletedFileRetentionDuration = \"interval interval-string\" `\n  * Configure how long stale data files are kept around before being deleted with VACUUM. Default is interval 1 week.\n  \n* `interval-string` is in the form `30 days` or `1 week`\n\nFor full access to 30 days of historical data, set `delta.deletedFileRetentionDuration = \"interval 30 days\" ` on your table. \n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Using a large number of days may cause your storage costs to go way up."],"metadata":{}},{"cell_type":"code","source":["spark.sql(f\"\"\"ALTER TABLE {powerTable} SET TBLPROPERTIES (delta.deletedFileRetentionDuration=\"interval 10 days\") \"\"\")\ntblPropDF = spark.sql(f\"SHOW TBLPROPERTIES {powerTable}\")\ndisplay(tblPropDF)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>key</th><th>value</th></tr></thead><tbody><tr><td>delta.deletedFileRetentionDuration</td><td>interval 10 days</td></tr></tbody></table></div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["-sandbox\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Run this cell multiple times to show that the data is changing."],"metadata":{}},{"cell_type":"code","source":["countDF = spark.sql(f\"SELECT count(*) FROM {powerTable}\")\ndisplay(countDF)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(1)</th></tr></thead><tbody><tr><td>72</td></tr></tbody></table></div>"]}}],"execution_count":18},{"cell_type":"code","source":["historyDF = spark.sql(f\"SELECT timestamp FROM (DESCRIBE HISTORY {powerTable}) ORDER BY timestamp\")\ndisplay(historyDF)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>timestamp</th></tr></thead><tbody><tr><td>2020-04-17T21:33:48.000+0000</td></tr><tr><td>2020-04-17T21:33:58.000+0000</td></tr><tr><td>2020-04-17T21:34:10.000+0000</td></tr><tr><td>2020-04-17T21:34:26.000+0000</td></tr><tr><td>2020-04-17T21:34:46.000+0000</td></tr><tr><td>2020-04-17T21:35:09.000+0000</td></tr></tbody></table></div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["-sandbox\n\nLet's rewind back to almost the beginning (where we had just a handful of rows), let's say the 2nd write.\n\n<div><img src=\"https://files.training.databricks.com/images/eLearning/Delta/second-write.png\" style=\"height: 250px\"/></div><br/>"],"metadata":{}},{"cell_type":"code","source":["# List timestamps of when table writes occurred\nhistoryDF = spark.sql(f\"SELECT timestamp FROM (DESCRIBE HISTORY {powerTable}) ORDER BY timestamp\")\n\n# Pick out 2nd write\noldTimestamp = historyDF.take(2)[-1].timestamp\n\n# Re-build the DataFrame as it was in the 2nd write\nrewoundDF = spark.sql(f\"SELECT * FROM {powerTable} TIMESTAMP AS OF '{oldTimestamp}'\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["We had this many (few) rows back then."],"metadata":{}},{"cell_type":"code","source":["rewoundDF.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[47]: 24</div>"]}}],"execution_count":23},{"cell_type":"markdown","source":["## Clean Up"],"metadata":{}},{"cell_type":"markdown","source":["Stop all remaining streams."],"metadata":{}},{"cell_type":"code","source":["stopAllStreams()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Stopping the stream lesson08_ps.\nThe stream lesson08_ps was stopped.\n</div>"]}}],"execution_count":26},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Cleanup<br>\n\nRun the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Cleanup\""],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> All done!</h2>\n\nThank you for your participation!"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"Delta 08 - Time Travel","notebookId":2761344356637980},"nbformat":4,"nbformat_minor":0}
