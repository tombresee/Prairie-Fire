{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# Databricks Delta Batch Operations - Upsert\n\nDatabricks&reg; Delta allows you to read, write and query data in data lakes in an efficient manner.\n\n## In this lesson you:\n* Use Databricks Delta to UPSERT data into existing Databricks Delta tables\n\n## Audience\n* Primary Audience: Data Engineers \n* Secondary Audience: Data Analysts and Data Scientists\n\n## Prerequisites\n* Web browser: **Chrome**\n* A cluster configured with **8 cores** and **DBR 6.2**\n* Suggested Courses from <a href=\"https://academy.databricks.com/\" target=\"_blank\">Databricks Academy</a>:\n  - ETL Part 1\n  - Spark-SQL\n\n## Datasets Used\nWe will use online retail datasets from\n* `/mnt/training/online_retail` in the demo part and\n* `/mnt/training/structured-streaming/events/` in the exercises"],"metadata":{}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Setup\n\nFor each lesson to execute correctly, please make sure to run the **`Classroom-Setup`** cell at the<br/>\nstart of each lesson (see the next cell) and the **`Classroom-Cleanup`** cell at the end of each lesson."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["<iframe  \nsrc=\"//fast.wistia.net/embed/iframe/lofgyqo0bu?videoFoam=true\"\nstyle=\"border:1px solid #1cb1c2;\"\nallowtransparency=\"true\" scrolling=\"no\" class=\"wistia_embed\"\nname=\"wistia_embed\" allowfullscreen mozallowfullscreen webkitallowfullscreen\noallowfullscreen msallowfullscreen width=\"640\" height=\"360\" ></iframe>\n<div>\n<a target=\"_blank\" href=\"https://fast.wistia.net/embed/iframe/lofgyqo0bu?seo=false\">\n  <img alt=\"Opens in new tab\" src=\"https://files.training.databricks.com/static/images/external-link-icon-16x16.png\"/>&nbsp;Watch full-screen.</a>\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["Set up relevant paths."],"metadata":{}},{"cell_type":"code","source":["deltaMiniDataPath = workingDir + \"/customer-data-mini\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["## UPSERT \n\nLiterally means \"UPdate\" and \"inSERT\". It means to atomically either insert a row, or, if the row already exists, UPDATE the row.\n\nIt is also called **MERGE INTO**, which is what the Databricks Delta operation is called.  \n\nAlter the data by changing the values in one of the columns for a specific `CustomerID`.\n\nLet's load the CSV file `/mnt/training/online_retail/outdoor-products/outdoor-products-mini.csv`."],"metadata":{}},{"cell_type":"code","source":["miniDataInputPath = \"/mnt/training/online_retail/outdoor-products/outdoor-products-mini.csv\"\ninputSchema = \"InvoiceNo STRING, StockCode STRING, Description STRING, Quantity INT, InvoiceDate STRING, UnitPrice DOUBLE, CustomerID INT, Country STRING\"\n\nminiDataDF = (spark.read          \n  .option(\"header\", \"true\")\n  .schema(inputSchema)\n  .csv(miniDataInputPath)                            \n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["## UPSERT Using Non-Databricks Delta Pipeline\n\nThis feature is not supported in non-Delta pipelines.\n\nTo UPSERT means to \"UPdate\" and \"inSERT\". In other words, UPSERT is not an atomic operation. It is literally TWO operations. \n\nRunning an UPDATE could invalidate data that is accessed by the subsequent INSERT operation."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n## UPSERT Using Databricks Delta Pipeline\n\nUsing Databricks Delta, however, we can do UPSERTS.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> In this Lesson, we will explicitly create tables as SQL notation works better with UPSERT."],"metadata":{}},{"cell_type":"code","source":["(miniDataDF\n  .write\n  .mode(\"overwrite\")\n  .format(\"delta\")\n  .save(deltaMiniDataPath) \n)\n\nspark.sql(\"\"\"\n    CREATE TABLE IF NOT EXISTS {}.customer_data_delta_mini\n    USING DELTA \n    LOCATION '{}' \n  \"\"\".format(databaseName, deltaMiniDataPath))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[15]: DataFrame[]</div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["List all rows with `CustomerID=20993`."],"metadata":{}},{"cell_type":"code","source":["sqlCmd = \"SELECT * FROM {}.customer_data_delta_mini WHERE CustomerID=20993\".format(databaseName)\ndisplay(spark.sql(sqlCmd))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>InvoiceNo</th><th>StockCode</th><th>Description</th><th>Quantity</th><th>InvoiceDate</th><th>UnitPrice</th><th>CustomerID</th><th>Country</th></tr></thead><tbody><tr><td>536371</td><td>32129</td><td>EverGlow Single</td><td>228</td><td>1/1/18 9:01</td><td>33.85</td><td>20993</td><td>Sierra Leone</td></tr></tbody></table></div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["Form a new DataFrame where `StockCode` is `99999` for `CustomerID=20993`.\n\nCreate a table `customer_data_delta_to_upsert` that contains this data."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import lit, col\ncustomerSpecificDF = (miniDataDF\n  .filter(\"CustomerID=20993\")\n  .withColumn(\"StockCode\", lit(99999))\n )\n\nspark.sql(\"DROP TABLE IF EXISTS {}.customer_data_delta_to_upsert\".format(databaseName))\ncustomerSpecificDF.write.saveAsTable(\"{}.customer_data_delta_to_upsert\".format(databaseName))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["Upsert the new data into `customer_data_delta_mini`.\n\nUpsert is done using the `MERGE INTO` syntax."],"metadata":{}},{"cell_type":"code","source":["spark.sql(\"USE {}\".format(databaseName))\n\nsqlCmd = \"\"\"\n  MERGE INTO customer_data_delta_mini\n  USING customer_data_delta_to_upsert\n  ON customer_data_delta_mini.CustomerID = customer_data_delta_to_upsert.CustomerID\n  WHEN MATCHED THEN\n    UPDATE SET\n      customer_data_delta_mini.StockCode = customer_data_delta_to_upsert.StockCode\n  WHEN NOT MATCHED\n    THEN INSERT (InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, Country)\n    VALUES (\n      customer_data_delta_to_upsert.InvoiceNo,\n      customer_data_delta_to_upsert.StockCode, \n      customer_data_delta_to_upsert.Description, \n      customer_data_delta_to_upsert.Quantity, \n      customer_data_delta_to_upsert.InvoiceDate, \n      customer_data_delta_to_upsert.UnitPrice, \n      customer_data_delta_to_upsert.CustomerID, \n      customer_data_delta_to_upsert.Country)\"\"\"\nspark.sql(sqlCmd)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[18]: DataFrame[]</div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["Notice how this data is seamlessly incorporated into `customer_data_delta_mini`."],"metadata":{}},{"cell_type":"code","source":["sqlCmd = \"SELECT * FROM {}.customer_data_delta_mini WHERE CustomerID=20993\".format(databaseName)\ndisplay(spark.sql(sqlCmd))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>InvoiceNo</th><th>StockCode</th><th>Description</th><th>Quantity</th><th>InvoiceDate</th><th>UnitPrice</th><th>CustomerID</th><th>Country</th></tr></thead><tbody><tr><td>536371</td><td>99999</td><td>EverGlow Single</td><td>228</td><td>1/1/18 9:01</td><td>33.85</td><td>20993</td><td>Sierra Leone</td></tr></tbody></table></div>"]}}],"execution_count":20},{"cell_type":"markdown","source":["# LAB"],"metadata":{}},{"cell_type":"markdown","source":["## Step 1\n\nWrite base data to `deltaIotPath`.\n\nWe do this for you, so just run the cell below."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import expr, col, from_unixtime, to_date\njsonSchema = \"action string, time long\"\nstreamingEventPath = \"/mnt/training/structured-streaming/events/\"\ndeltaIotPath = workingDir + \"/iot-pipeline\"\n\n(spark.read \n  .schema(jsonSchema)\n  .json(streamingEventPath) \n  .withColumn(\"date\", to_date(from_unixtime(col(\"time\").cast(\"Long\"),\"yyyy-MM-dd\")))\n  .withColumn(\"deviceId\", expr(\"cast(rand(5) * 100 as int)\"))\n  .repartition(200)\n  .write\n  .mode(\"overwrite\")\n  .format(\"delta\")\n  .partitionBy(\"date\")\n  .save(deltaIotPath)\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"markdown","source":["## Step 2\n\nCreate a DataFrame out of the the data sitting in `deltaIotPath`."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\ndeltaIotPath = workingDir + \"/iot-pipeline\"\n\nnewDataDF = spark.sql(\"SELECT * FROM delta.`{}` \".format(deltaIotPath))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":25},{"cell_type":"code","source":["# TEST  - Run this cell to test your solution.\nschema = str(newDataDF.schema)\n\ndbTest(\"assert-1\", True, \"action,StringType\" in schema)\ndbTest(\"assert-2\", True, \"time,LongType\" in schema)\ndbTest(\"assert-3\", True, \"date,DateType\" in schema)\ndbTest(\"assert-4\", True, \"deviceId,IntegerType\" in schema)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Tests passed!\n</div>"]}}],"execution_count":26},{"cell_type":"markdown","source":["-sandbox\n## Step 3\n\nCreate another DataFrame `newDeviceIdDF`\n* Pick up the 1st row you see that has `action` set to `Open`.\n  - <img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** Use the `limit(1)` method.\n* Change `action` to `Close`.\n  - <img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** Use the `lit()` function.\n* We will use the associated `deviceId` in the cells that follow.\n* The DataFrame you construct should only have 1 row."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import col, lit\n\ndevId = (newDataDF\n  .select(\"deviceId\")\n  .filter(col(\"action\") == \"Open\")\n  .limit(1)\n  .first()[0])\n  \nnewDeviceIdDF = (newDataDF\n  .filter(col(\"deviceId\") == devId)\n  .withColumn(\"action\", lit(\"Close\")) \n  .limit(1)) "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":28},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\nactionCount = newDeviceIdDF.filter(col(\"Action\") == \"Close\").count()\n\ndbTest(\"Delta-L4-actionCount\", 1, actionCount)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Tests passed!\n</div>"]}}],"execution_count":29},{"cell_type":"markdown","source":["## Step 4\n\nWrite to a new Databricks Delta table named `iot_data_delta_to_upsert` that contains just our data to be upserted."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nspark.sql(\"DROP TABLE IF EXISTS {}.iot_data_delta_to_upsert\".format(databaseName))\nnewDeviceIdDF.write.saveAsTable(\"{}.iot_data_delta_to_upsert\".format(databaseName))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":31},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\ncount = spark.table(\"{}.iot_data_delta_to_upsert\".format(databaseName)).count()\n\ndbTest(\"Delta-04-demoIotTableHasRow\", True, count > 0)  \n  \nprint(\"Tests passed!\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Tests passed!\n</div>"]}}],"execution_count":32},{"cell_type":"markdown","source":["## Step 5\n\nCreate a Databricks Delta table named `demo_iot_data_delta` that contains just the data from `deltaIotPath`."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nsqlCmd = \"\"\"\n  CREATE TABLE IF NOT EXISTS {}.demo_iot_data_delta\n  USING DELTA\n  LOCATION '{}'\"\"\".format(databaseName, deltaIotPath)\n\nspark.sql(sqlCmd)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[27]: DataFrame[]</div>"]}}],"execution_count":34},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\ntry:\n  tableExists = (spark.table(\"{}.demo_iot_data_delta\".format(databaseName)).count() > 0)\nexcept:\n  tableExists = False\n  \ndbTest(\"Delta-04-demoTableExists\", True, tableExists)  \n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Tests passed!\n</div>"]}}],"execution_count":35},{"cell_type":"markdown","source":["## Step 6\n\nInsert the data `iot_data_delta_to_upsert` into `demo_iot_data_delta`.\n\nYou can adapt the SQL syntax for the upsert from our demo example, above."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nspark.sql(\"USE {}\".format(databaseName))\n\nsqlCmd = \"\"\"\n  MERGE INTO demo_iot_data_delta\n  USING iot_data_delta_to_upsert\n  ON demo_iot_data_delta.deviceId = iot_data_delta_to_upsert.deviceId\n  WHEN MATCHED THEN\n    UPDATE SET\n      demo_iot_data_delta.action = iot_data_delta_to_upsert.action\n  WHEN NOT MATCHED\n    THEN INSERT (action, time, date, deviceId)\n    VALUES (\n      iot_data_delta_to_upsert.action, \n      iot_data_delta_to_upsert.time, \n      iot_data_delta_to_upsert.date, \n      iot_data_delta_to_upsert.deviceId \n  )\"\"\"\n\nspark.sql(sqlCmd)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[29]: DataFrame[]</div>"]}}],"execution_count":37},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\ndevId = newDeviceIdDF.select(\"deviceId\").first()[0]\n\nsqlCmd1 = \"SELECT count(*) as total FROM {}.demo_iot_data_delta WHERE deviceId = {} AND action = 'Open' \".format(databaseName, devId)\ncountOpen = spark.sql(sqlCmd1).first()[0]\n\nsqlCmd2 = \"SELECT count(*) as total FROM {}.demo_iot_data_delta WHERE deviceId = {} AND action = 'Close' \".format(databaseName, devId)\ncountClose = spark.sql(sqlCmd2).first()[0]\n\ndbTest(\"Delta-L4-count\", True, countOpen == 0 and countClose > 0)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["## Step 7\n\nCount the number of items in `demo_iot_data_delta` where \n* `deviceId` is obtained from this query `newDeviceIdDF.select(\"deviceId\").first()[0]` .\n* `action` is `Close`."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nsqlCmd = \"SELECT count(*) as total FROM {}.demo_iot_data_delta WHERE deviceId = {} AND action = 'Close' \".format(databaseName, devId)\ncount = spark.sql(sqlCmd).first()[0]"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\ndbTest(\"Delta-L4-demoiot-count\", True, count > 0)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Cleanup<br>\n\nRun the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Cleanup\""],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["## Summary\nIn this Lesson we:\n* Learned that is not possible to do UPSERTS in the traditional pre-Databricks Delta lake.\n  - UPSERT is essentially two operations in one step \n  - UPdate and inSERT\n* `MERGE INTO` is the SQL expression we use to do UPSERTs.\n* Used Databricks Delta to UPSERT data into existing Databricks Delta tables.\n* Ended up creating tables explicitly because it is easier to work with SQL syntax."],"metadata":{}},{"cell_type":"markdown","source":["## Review Questions\n\n**Q:** What does it mean to UPSERT?<br>\n**A:** To UPSERT is to either INSERT a row, or if the row already exists, UPDATE the row.\n\n**Q:** What happens if you try to UPSERT in a parquet-based data set?<br>\n**A:** That's not possible due to the schema-on-read paradigm, you will get an error until you repair the table.\n\n**Q:** How to you perform UPSERT in a Databricks Delta dataset?<br>\n**A:** Using the `MERGE INTO my-table USING data-to-upsert`.\n\n**Q:** What is the caveat to `USING data-to-upsert`?<br>\n**A:** Your source data has ALL the data you want to replace: in other words, you create a new dataframe that has the source data you want to replace in the Databricks Delta table."],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n* <a href=\"https://docs.databricks.com/delta/delta-batch.html#\" target=\"_blank\">Table Batch Read and Writes</a>"],"metadata":{}},{"cell_type":"markdown","source":["## Next Steps\n\nStart the next lesson, [Streaming]($./Delta 05 - Streaming)."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"Delta 04 - Upsert","notebookId":2761344356637778},"nbformat":4,"nbformat_minor":0}
