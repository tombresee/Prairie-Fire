{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n# Databricks Delta Streaming\nDatabricks&reg; Delta allows you to work with streaming data.\n\n## In this lesson you:\n* Read and write streaming data into a data lake\n\n## Audience\n* Primary Audience: Data Engineers \n* Secondary Audience: Data Analyst sand Data Scientists\n\n## Prerequisites\n* Web browser: **Chrome**\n* A cluster configured with **8 cores** and **DBR 6.2**\n* Suggested Courses from <a href=\"https://academy.databricks.com/\" target=\"_blank\">Databricks Academy</a>:\n  - ETL Part 1\n  - Spark-SQL\n  - Structured Streaming\n\n## Datasets Used\nData from \n`/mnt/training/definitive-guide/data/activity-data`\ncontains smartphone accelerometer samples from all devices and users. \n\nThe file consists of the following columns:\n\n| Field           | Description                  |\n|-----------------|------------------------------|\n| `Arrival_Time`  | when record came in          |\n| `Creation_Time` | when record was created      |\n| `Index`         | unique identifier of event   |\n| `x`             | acceleration in x-dir        |\n| `y`             | acceleration in y-dir        |\n| `z`             | acceleration in z-dir        |\n| `User`          | unique user identifier       |\n| `Model`         | i.e Nexus4                   |\n| `Device`        | type of Model                |\n| `gt`            | ground truth                 |\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> The last column, `gt`, \"ground truth\" means\n* What the person was ACTUALLY doing when the measurement was taken, in this case, walking or going up stairs, etc..\n* Wikipedia: <a href=\"https://en.wikipedia.org/wiki/Ground_truth\" target=\"_blank\">Ground Truth</a>\n\n## CAUTION\n* Streaming jobs will quickly exhaust your disk quota, if you are using Community Edition. If you find yourself running out of disk space, \n  you can [mount your own S3 bucket](https://docs.databricks.com/spark/latest/data-sources/aws/amazon-s3.html#mount-an-s3-bucket) and change\n  your streaming paths accordingly."],"metadata":{}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Setup\n\nFor each lesson to execute correctly, please make sure to run the **`Classroom-Setup`** cell at the<br/>\nstart of each lesson (see the next cell) and the **`Classroom-Cleanup`** cell at the end of each lesson."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["<iframe  \nsrc=\"//fast.wistia.net/embed/iframe/cpilwjdr38?videoFoam=true\"\nstyle=\"border:1px solid #1cb1c2;\"\nallowtransparency=\"true\" scrolling=\"no\" class=\"wistia_embed\"\nname=\"wistia_embed\" allowfullscreen mozallowfullscreen webkitallowfullscreen\noallowfullscreen msallowfullscreen width=\"640\" height=\"360\" ></iframe>\n<div>\n<a target=\"_blank\" href=\"https://fast.wistia.net/embed/iframe/cpilwjdr38?seo=false\">\n  <img alt=\"Opens in new tab\" src=\"https://files.training.databricks.com/static/images/external-link-icon-16x16.png\"/>&nbsp;Watch full-screen.</a>\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["Set up relevant paths."],"metadata":{}},{"cell_type":"code","source":["dataPath = \"dbfs:/mnt/training/definitive-guide/data/activity-data\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["\nprint(dataPath)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">dbfs:/mnt/training/definitive-guide/data/activity-data\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["-sandbox\n##Streaming Concepts\n\n<b>Stream processing</b> is where you continuously incorporate new data into a data lake and compute results.\n\nThe data is coming in faster than it can be consumed.\n\n<div><img src=\"https://files.training.databricks.com/images/eLearning/Delta/firehose.jpeg\" style=\"height: 200px\"/></div><br/>\n\nTreat a <b>stream</b> of data as a table to which data is continously appended. \n\nIn this course we are assuming Databricks Structured Streaming, which uses the DataFrame API. \n\nThere are other kinds of streaming systems.\n\n<div><img src=\"https://files.training.databricks.com/images/eLearning/Delta/stream2rows.png\" style=\"height: 300px\"/></div><br/>\n\nExamples are bank card transactions, Internet of Things (IoT) device data, and video game play events. \n\nData coming from a stream is typically not ordered in any way.\n\nA streaming system consists of \n* <b>Input source</b> such as Kafka, Azure Event Hub, files on a distributed system or TCP-IP sockets\n* <b>Sinks</b> such as Kafka, Azure Event Hub, various file formats, `foreach` sinks, console sinks or memory sinks\n\n### Streaming and Databricks Delta\n\nIn streaming, the problems of traditional data pipelines are exacerbated. \n\nSpecifically, with frequent meta data refreshes, table repairs and accumulation of small files on a secondly- or minutely-basis!\n\nMany small files result because data (may be) streamed in at low volumes with short triggers.\n\nDatabricks Delta is uniquely designed to address these needs."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### READ Stream using Databricks Delta\n\nThe `readStream` method is similar to a transformation that outputs a DataFrame with specific schema specified by `.schema()`. \n\nEach line of the streaming data becomes a row in the DataFrame once `writeStream` is invoked.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> In this lesson, we limit flow of stream to one file per trigger with `option(\"maxFilesPerTrigger\", 1)` so that you do not exceed file quotas you may have on your end. The default value is 1000.\n\nNotice that nothing happens until you engage an action, i.e. a `writeStream` operation a few cells down.\n\nDo some data normalization as well:\n* Convert `Arrival_Time` to `timestamp` format.\n* Rename `Index` to `User_ID`."],"metadata":{}},{"cell_type":"code","source":["\nstatic = spark.read.json(dataPath)\ndataSchema = static.schema\n\ndeltaStreamWithTimestampDF = (spark\n  .readStream\n  .option(\"maxFilesPerTrigger\", 1)\n  .schema(dataSchema)\n  .json(dataPath)\n  .withColumnRenamed('Index', 'User_ID')\n  .selectExpr(\"*\",\"cast(cast(Arrival_Time as double)/1000 as timestamp) as event_time\")\n)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["-sandbox\n### WRITE Stream using Databricks Delta\n\n#### General Notation\nUse this format to write a streaming job to a Databricks Delta table.\n\n> `(myDF` <br>\n  `.writeStream` <br>\n  `.format(\"delta\")` <br>\n  `.option(\"checkpointLocation\", somePath)` <br>\n  `.outputMode(\"append\")` <br>\n  `.table(\"my_table\")` or `.start(path)` <br>\n`)`\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> If you use the `.table()` notation, it will write output to a default location. \n* This would be in parquet files under `/user/hive/warehouse/default.db/my_table`\n\nIn this course, we want everyone to write data to their own directory; so, instead, we use the `.start()` notation.\n\n#### Output Modes\nNotice, besides the \"obvious\" parameters, specify `outputMode`, which can take on these values\n* `append`: add only new records to output sink\n* `complete`: rewrite full output - applicable to aggregations operations\n* `update`: update changed records in place\n\n#### Checkpointing\n\nWhen defining a Delta streaming query, one of the options that you need to specify is the location of a checkpoint directory.\n\n`.writeStream.format(\"delta\").option(\"checkpointLocation\", <path-to-checkpoint-directory>) ...`\n\nThis is actually a structured streaming feature. It stores the current state of your streaming job.\n\nShould your streaming job stop for some reason and you restart it, it will continue from where it left off.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> If you do not have a checkpoint directory, when the streaming job stops, you lose all state around your streaming job and upon restart, you start from scratch.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Also note that every streaming job should have its own checkpoint directory: no sharing."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Let's Do Some Streaming\n\nIn the cell below, we write streaming query to a Databricks Delta table. \n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Notice how we do not need to specify a schema: it is inferred from the data!"],"metadata":{}},{"cell_type":"markdown","source":["And to help us manage our streams better, we will make use of **`untilStreamIsReady()`**, **`stopAllStreams()`** and define the following, **`myStreamName`**:"],"metadata":{}},{"cell_type":"code","source":["myStreamName = \"lesson05_ps\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"code","source":["writePath =      workingDir + \"/output.delta\"\ncheckpointPath = workingDir + \"/output.checkpoint\"\n\ndeltaStreamingQuery = (deltaStreamWithTimestampDF\n  .writeStream\n  .format(\"delta\")\n  .option(\"checkpointLocation\", checkpointPath)\n  .outputMode(\"append\")\n  .queryName(myStreamName)\n  .start(writePath)\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["See list of active streams."],"metadata":{}},{"cell_type":"code","source":["for s in spark.streams.active:\n  print(\"{}: {}\".format(s.name, s.id))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">lesson05_ps: fddc086e-f9e9-445e-95e3-1b937cb7a8e5\n</div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["Wait until stream is done initializing..."],"metadata":{}},{"cell_type":"code","source":["untilStreamIsReady(myStreamName)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The stream lesson05_ps is active and ready.\n</div>"]}}],"execution_count":21},{"cell_type":"code","source":["stopAllStreams()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Stopping the stream lesson05_ps.\nThe stream lesson05_ps was stopped.\n</div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["# LAB"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n## Step 1: Table-to-Table Stream\n\nHere we read a stream of data from from `writePath` and write another stream to `activityPath`.\n\nThe data consists of a grouped count of `gt` events.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Make sure the stream using `deltaStreamingQuery` is still running!\n\nTo perform an aggregate operation, what kind of `outputMode` should you use?"],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nactivityPath   = workingDir + \"/activityCount.delta\"\ncheckpointPath = workingDir + \"/activityCount.checkpoint\"\n\nactivityCountsQuery = (spark.readStream\n  .format(\"delta\")\n  .load(str(writePath))   \n  .groupBy(\"gt\")\n  .count()\n  .writeStream\n  .format(\"delta\")\n  .option(\"checkpointLocation\", checkpointPath)\n  .outputMode(\"complete\")\n  .queryName(myStreamName)\n  .start(activityPath)\n)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":25},{"cell_type":"markdown","source":["Wait until stream is done initializing..."],"metadata":{}},{"cell_type":"code","source":["untilStreamIsReady(myStreamName)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The stream lesson05_ps is active and ready.\n</div>"]}}],"execution_count":27},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\nactivityQueryTruth = spark.streams.get(activityCountsQuery.id).isActive\n\ndbTest(\"Delta-05-activityCountsQuery\", True, activityQueryTruth)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Tests passed!\n</div>"]}}],"execution_count":28},{"cell_type":"code","source":["stopAllStreams()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Stopping the stream lesson05_ps.\nThe stream lesson05_ps was stopped.\n</div>"]}}],"execution_count":29},{"cell_type":"markdown","source":["-sandbox\n## Step 2\n\nPlot the occurrence of all events grouped by `gt`.\n\nUnder <b>Plot Options</b>, use the following:\n* <b>Series Groupings:</b> `gt`\n* <b>Values:</b> `count`\n\nIn <b>Display type</b>, use <b>Bar Chart</b> and click <b>Apply</b>.\n\n\n<div><img src=\"https://files.training.databricks.com/images/eLearning/Delta/ch5-plot-options.png\" style=\"height: 300px;\"/></div><br/>\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> In the cell below, we use the `withWatermark` and `window` methods, which aren't covered in this course. \n\nTo learn more about watermarking, please see <a href=\"https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html\" target=\"_blank\">Event-time Aggregation and Watermarking in Apache Sparkâ€™s Structured Streaming</a>."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import hour, window, col\n\ncountsDF = (deltaStreamWithTimestampDF      \n  .withWatermark(\"event_time\", \"180 minutes\")\n  .groupBy(window(\"event_time\", \"60 minute\"), \"gt\")\n  .count()\n)\n\ndisplay(countsDF.withColumn('hour',hour(col('window.start'))), streamName = myStreamName)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>window</th><th>gt</th><th>count</th><th>hour</th></tr></thead><tbody><tr><td>List(2015-02-23T10:00:00.000+0000, 2015-02-23T11:00:00.000+0000)</td><td>walk</td><td>9513</td><td>10</td></tr><tr><td>List(2015-02-23T10:00:00.000+0000, 2015-02-23T11:00:00.000+0000)</td><td>stairsup</td><td>5790</td><td>10</td></tr><tr><td>List(2015-02-24T13:00:00.000+0000, 2015-02-24T14:00:00.000+0000)</td><td>sit</td><td>6552</td><td>13</td></tr><tr><td>List(2015-02-24T13:00:00.000+0000, 2015-02-24T14:00:00.000+0000)</td><td>null</td><td>7272</td><td>13</td></tr><tr><td>List(2015-02-23T14:00:00.000+0000, 2015-02-23T15:00:00.000+0000)</td><td>stairsdown</td><td>6384</td><td>14</td></tr><tr><td>List(2015-02-24T13:00:00.000+0000, 2015-02-24T14:00:00.000+0000)</td><td>walk</td><td>9726</td><td>13</td></tr><tr><td>List(2015-02-23T10:00:00.000+0000, 2015-02-23T11:00:00.000+0000)</td><td>bike</td><td>6672</td><td>10</td></tr><tr><td>List(2015-02-24T14:00:00.000+0000, 2015-02-24T15:00:00.000+0000)</td><td>bike</td><td>18281</td><td>14</td></tr><tr><td>List(2015-02-24T14:00:00.000+0000, 2015-02-24T15:00:00.000+0000)</td><td>walk</td><td>9480</td><td>14</td></tr><tr><td>List(2015-02-23T13:00:00.000+0000, 2015-02-23T14:00:00.000+0000)</td><td>stairsdown</td><td>6271</td><td>13</td></tr><tr><td>List(2015-02-23T14:00:00.000+0000, 2015-02-23T15:00:00.000+0000)</td><td>walk</td><td>2688</td><td>14</td></tr><tr><td>List(2015-02-23T13:00:00.000+0000, 2015-02-23T14:00:00.000+0000)</td><td>walk</td><td>12264</td><td>13</td></tr><tr><td>List(2015-02-23T12:00:00.000+0000, 2015-02-23T13:00:00.000+0000)</td><td>bike</td><td>5178</td><td>12</td></tr><tr><td>List(2015-02-24T12:00:00.000+0000, 2015-02-24T13:00:00.000+0000)</td><td>stairsdown</td><td>7718</td><td>12</td></tr><tr><td>List(2015-02-24T12:00:00.000+0000, 2015-02-24T13:00:00.000+0000)</td><td>stairsup</td><td>9508</td><td>12</td></tr><tr><td>List(2015-02-23T14:00:00.000+0000, 2015-02-23T15:00:00.000+0000)</td><td>stairsup</td><td>6900</td><td>14</td></tr><tr><td>List(2015-02-24T11:00:00.000+0000, 2015-02-24T12:00:00.000+0000)</td><td>stairsdown</td><td>6896</td><td>11</td></tr><tr><td>List(2015-02-24T13:00:00.000+0000, 2015-02-24T14:00:00.000+0000)</td><td>stairsdown</td><td>10934</td><td>13</td></tr><tr><td>List(2015-02-24T11:00:00.000+0000, 2015-02-24T12:00:00.000+0000)</td><td>stairsup</td><td>7228</td><td>11</td></tr><tr><td>List(2015-02-24T11:00:00.000+0000, 2015-02-24T12:00:00.000+0000)</td><td>bike</td><td>7938</td><td>11</td></tr><tr><td>List(2015-02-24T14:00:00.000+0000, 2015-02-24T15:00:00.000+0000)</td><td>stairsup</td><td>6557</td><td>14</td></tr><tr><td>List(2015-02-23T12:00:00.000+0000, 2015-02-23T13:00:00.000+0000)</td><td>walk</td><td>8232</td><td>12</td></tr><tr><td>List(2015-02-24T12:00:00.000+0000, 2015-02-24T13:00:00.000+0000)</td><td>walk</td><td>18387</td><td>12</td></tr><tr><td>List(2015-02-23T12:00:00.000+0000, 2015-02-23T13:00:00.000+0000)</td><td>stand</td><td>6954</td><td>12</td></tr><tr><td>List(2015-02-24T12:00:00.000+0000, 2015-02-24T13:00:00.000+0000)</td><td>sit</td><td>15684</td><td>12</td></tr><tr><td>List(2015-02-23T10:00:00.000+0000, 2015-02-23T11:00:00.000+0000)</td><td>null</td><td>10503</td><td>10</td></tr><tr><td>List(2015-02-23T10:00:00.000+0000, 2015-02-23T11:00:00.000+0000)</td><td>sit</td><td>8646</td><td>10</td></tr><tr><td>List(2015-02-24T11:00:00.000+0000, 2015-02-24T12:00:00.000+0000)</td><td>stand</td><td>13780</td><td>11</td></tr><tr><td>List(2015-02-24T14:00:00.000+0000, 2015-02-24T15:00:00.000+0000)</td><td>stand</td><td>8058</td><td>14</td></tr><tr><td>List(2015-02-24T12:00:00.000+0000, 2015-02-24T13:00:00.000+0000)</td><td>bike</td><td>8094</td><td>12</td></tr><tr><td>List(2015-02-23T13:00:00.000+0000, 2015-02-23T14:00:00.000+0000)</td><td>stand</td><td>14850</td><td>13</td></tr><tr><td>List(2015-02-24T13:00:00.000+0000, 2015-02-24T14:00:00.000+0000)</td><td>stand</td><td>6648</td><td>13</td></tr><tr><td>List(2015-02-24T12:00:00.000+0000, 2015-02-24T13:00:00.000+0000)</td><td>null</td><td>10611</td><td>12</td></tr><tr><td>List(2015-02-23T13:00:00.000+0000, 2015-02-23T14:00:00.000+0000)</td><td>bike</td><td>6012</td><td>13</td></tr><tr><td>List(2015-02-23T12:00:00.000+0000, 2015-02-23T13:00:00.000+0000)</td><td>stairsdown</td><td>6036</td><td>12</td></tr><tr><td>List(2015-02-24T13:00:00.000+0000, 2015-02-24T14:00:00.000+0000)</td><td>stairsup</td><td>12868</td><td>13</td></tr><tr><td>List(2015-02-24T14:00:00.000+0000, 2015-02-24T15:00:00.000+0000)</td><td>sit</td><td>8364</td><td>14</td></tr><tr><td>List(2015-02-24T12:00:00.000+0000, 2015-02-24T13:00:00.000+0000)</td><td>stand</td><td>10094</td><td>12</td></tr><tr><td>List(2015-02-23T12:00:00.000+0000, 2015-02-23T13:00:00.000+0000)</td><td>null</td><td>4668</td><td>12</td></tr><tr><td>List(2015-02-23T13:00:00.000+0000, 2015-02-23T14:00:00.000+0000)</td><td>stairsup</td><td>6485</td><td>13</td></tr><tr><td>List(2015-02-24T11:00:00.000+0000, 2015-02-24T12:00:00.000+0000)</td><td>sit</td><td>8780</td><td>11</td></tr><tr><td>List(2015-02-24T11:00:00.000+0000, 2015-02-24T12:00:00.000+0000)</td><td>walk</td><td>9246</td><td>11</td></tr><tr><td>List(2015-02-23T12:00:00.000+0000, 2015-02-23T13:00:00.000+0000)</td><td>stairsup</td><td>7374</td><td>12</td></tr><tr><td>List(2015-02-24T14:00:00.000+0000, 2015-02-24T15:00:00.000+0000)</td><td>stairsdown</td><td>5995</td><td>14</td></tr><tr><td>List(2015-02-23T14:00:00.000+0000, 2015-02-23T15:00:00.000+0000)</td><td>null</td><td>1776</td><td>14</td></tr><tr><td>List(2015-02-24T11:00:00.000+0000, 2015-02-24T12:00:00.000+0000)</td><td>null</td><td>7558</td><td>11</td></tr><tr><td>List(2015-02-23T13:00:00.000+0000, 2015-02-23T14:00:00.000+0000)</td><td>null</td><td>7045</td><td>13</td></tr><tr><td>List(2015-02-23T14:00:00.000+0000, 2015-02-23T15:00:00.000+0000)</td><td>bike</td><td>6252</td><td>14</td></tr><tr><td>List(2015-02-23T10:00:00.000+0000, 2015-02-23T11:00:00.000+0000)</td><td>stairsdown</td><td>5958</td><td>10</td></tr><tr><td>List(2015-02-23T13:00:00.000+0000, 2015-02-23T14:00:00.000+0000)</td><td>sit</td><td>17981</td><td>13</td></tr><tr><td>List(2015-02-24T14:00:00.000+0000, 2015-02-24T15:00:00.000+0000)</td><td>null</td><td>13255</td><td>14</td></tr><tr><td>List(2015-02-23T10:00:00.000+0000, 2015-02-23T11:00:00.000+0000)</td><td>stand</td><td>7925</td><td>10</td></tr><tr><td>List(2015-02-24T13:00:00.000+0000, 2015-02-24T14:00:00.000+0000)</td><td>bike</td><td>6354</td><td>13</td></tr><tr><td>List(2015-02-23T12:00:00.000+0000, 2015-02-23T13:00:00.000+0000)</td><td>sit</td><td>7848</td><td>12</td></tr></tbody></table></div>"]}}],"execution_count":31},{"cell_type":"code","source":["untilStreamIsReady(myStreamName)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The stream lesson05_ps is active and ready.\n</div>"]}}],"execution_count":32},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\nschemaStr = str(countsDF.schema)\n\ndbTest(\"Assertion #1\", 3, len(countsDF.columns))\ndbTest(\"Assertion #2\", True, \"(gt,StringType,true)\" in schemaStr) \ndbTest(\"Assertion #3\", True, \"(count,LongType,false)\" in schemaStr) \n\ndbTest(\"Assertion #5\", True, \"window,StructType\" in schemaStr)\ndbTest(\"Assertion #6\", True, \"(start,TimestampType,true)\" in schemaStr) \ndbTest(\"Assertion #7\", True, \"(end,TimestampType,true)\" in schemaStr) \n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Tests passed!\n</div>"]}}],"execution_count":33},{"cell_type":"code","source":["stopAllStreams()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Stopping the stream lesson05_ps.\nThe stream lesson05_ps was stopped.\n</div>"]}}],"execution_count":34},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Cleanup<br>\n\nRun the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Cleanup\""],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["## Summary\n\nIn this lesson, we:\n* Treated a stream of data as a table to which data is continously appended. \n* Learned we could write Databricks Delta output to a directory or directly to a table.\n* Used Databricks Delta's `complete` mode with aggregation queries.\n* Saw that `display` will produce LIVE graphs if we feed it a streaming DataFrame"],"metadata":{}},{"cell_type":"markdown","source":["## Review Questions\n**Q:** Why is Databricks Delta so important for a data lake that incorporates streaming data?<br>\n**A:** Frequent meta data refreshes, table repairs and accumulation of small files on a secondly- or minutely-basis!\n\n**Q:** What happens if you shut off your stream before it has fully initialized and started and you try to `CREATE TABLE .. USING DELTA` ? <br>\n**A:** You will get this: `Error in SQL statement: AnalysisException: The user specified schema is empty;`.\n\n**Q:** When you do a write stream command, what does this option do `outputMode(\"append\")` ?<br>\n**A:** This option takes on the following values and their respective meanings:\n* <b>append</b>: add only new records to output sink\n* <b>complete</b>: rewrite full output - applicable to aggregations operations\n* <b>update</b>: update changed records in place\n\n**Q:** What happens if you do not specify `option(\"checkpointLocation\", pointer-to-checkpoint directory)`?<br>\n**A:** When the streaming job stops, you lose all state around your streaming job and upon restart, you start from scratch.\n\n**Q:** How do you view the list of active streams?<br>\n**A:** Invoke `spark.streams.active`.\n\n**Q:** How do you verify whether `streamingQuery` is running (boolean output)?<br>\n**A:** Invoke `spark.streams.get(streamingQuery.id).isActive`."],"metadata":{}},{"cell_type":"markdown","source":["## Next Steps\n\nStart the next lesson, [Optimization]($./Delta 06 - Optimization)."],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n* <a href=\"https://docs.databricks.com/delta/delta-streaming.html#as-a-sink\" target=\"_blank\">Delta Streaming Write Notation</a>\n* <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#\" target=\"_blank\">Structured Streaming Programming Guide</a>\n* <a href=\"https://www.youtube.com/watch?v=rl8dIzTpxrI\" target=\"_blank\">A Deep Dive into Structured Streaming</a> by Tagatha Das. This is an excellent video describing how Structured Streaming works."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"Delta 05 - Streaming","notebookId":2761344356637881},"nbformat":4,"nbformat_minor":0}
