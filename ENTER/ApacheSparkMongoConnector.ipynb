{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrating Apache Spark interop with MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details: \n",
    "*  Author:  Tom Bresee\n",
    "*  Date:  March 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is this notebook ? \n",
    "> Demonstration of using Apache Spark to interoperate with MongoDB via the purpose-built connector.  \n",
    "See https://www.mongodb.com/products/spark-connector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An html copy of this notebook is available [here](https://htmlpreview.github.io/?https://github.com/tombresee/Prairie-Fire/blob/master/ENTER/ApacheSparkMongoConnector.html) for ease of viewing, feel free to check it out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The MongoDB Connector for Apache Spark exposes all of Sparkâ€™s libraries, including Scala, Java, Python and R. MongoDB data is materialized as DataFrames and Datasets for analysis with machine learning, graph, streaming, and SQL APIs.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/tombresee/Prairie-Fire/raw/master/ENTER/spark-connector-diagram.png\" width=\"500\">   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```python\n",
    "\n",
    "Python 3.7.3 | packaged by conda-forge | (default, Jul  1 2019, 21:52:21) \n",
    "[GCC 7.3.0] :: Anaconda, Inc. on linux\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    "20/03/16 22:53:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "20/03/16 22:53:19 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.5\n",
    "      /_/\n",
    "\n",
    "Using Python version 3.7.3 (default, Jul  1 2019 21:52:21)\n",
    "SparkSession available as 'spark'.\n",
    ">>> \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# The Largest College Football Stadiums - Ranked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are we showing here ? \n",
    "1.  We have some raw stadium capacity data (in .csv form)\n",
    "2.  We will initiate a Spark session, specifically with the **MongoDB Spark connector**\n",
    "3.  We will use Spark to read in the raw stadium data to a DataFrame, and then export that data **directly** to a local mongodb instance (also specifying the new database name and collection name) \n",
    "4.  Then, just to demonstrate how, we will read the mongodb information directly from the mongo database (via the mongo spark connector) *back* in to a new DataFrame and **print** out the dataframe results. This will effectively be the mongo collection data.  \n",
    "5. Thus we demonstrate how to both write and read from a mongo database via the connector \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "\n",
    "# The file we will execute (stadium_analysis.py)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"Connector-Mongodb-Apache Spark\").getOrCreate()\n",
    "\n",
    "    #logger = spark._jvm.org.apache.log4j\n",
    "    #logger.LogManager.getRootLogger().setLevel(logger.Level.FATAL)\n",
    "\n",
    "\n",
    "    # Pull in some raw stadium data\n",
    "    stadiums = spark.read.csv(\"/root/stadium_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "    # WRITE this data in mongo format to the mondoDB\n",
    "    # .format(\"mongo\")\n",
    "    stadiums.write.format(\"mongo\").mode(\"overwrite\").save()\n",
    "\n",
    "\n",
    "    # print the df schema\n",
    "    print(\"Stadium Data Schema:\")\n",
    "    stadiums.printSchema()\n",
    "\n",
    "\n",
    "    # OFFICIALLY READING from the MongoDB collection, **VIA** the spark connector\n",
    "    # i.e. using the enhanced spark connector built for mongo to stream it faster\n",
    "    # effectively:  assigning the collection to a DataFrame with spark.read() \n",
    "    # from within the pyspark shell    \n",
    "    df = spark.read.format(\"mongo\").load()\n",
    "\n",
    "\n",
    "    # SQL\n",
    "    df.registerTempTable(\"stadium_temp\")\n",
    "    data = spark.sql(\"SELECT * FROM stadium_temp\")\n",
    "    print(\"\\n\\n*******************************************************************\")\n",
    "    print(\"\")\n",
    "    print(\"\\nRanking of Largest College Football Stadium Sizes, in the USA:\")\n",
    "    data.show(20,False)\n",
    "    print(\"\")\n",
    "    print(\"\\n\\n*******************************************************************\")\n",
    "\n",
    "    \n",
    "    ```\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "\n",
    "\n",
    "# We will issue these commands to execute our file:\n",
    "\n",
    "\n",
    "./bin/spark-submit --master \"local[*]\"  \\\n",
    "--conf \"spark.mongodb.input.uri=mongodb://127.0.0.1/stadium_db.stadium_collection?readPreference=primaryPreferred\" \\\n",
    "--conf \"spark.mongodb.output.uri=mongodb://127.0.0.1/stadium_db.stadium_collection\" \\\n",
    "--packages org.mongodb.spark:mongo-spark-connector_2.11:2.2.7 \\\n",
    "/root/stadium_analysis.py\n",
    "\n",
    "        \n",
    "# mongodb input and output: \n",
    "# --conf \"spark.mongodb.input.uri=mongodb://127.0.0.1/stadium_db.stadium_collection?readPreference=primaryPreferred\"\n",
    "# --conf \"spark.mongodb.output.uri=mongodb://127.0.0.1/stadium_db.stadium_collection\" \\\n",
    "# format:  mongodb://<host>/<mongo database name>.<mongo collection name>\n",
    "#\n",
    "# --packages org.mongodb.spark:mongo-spark-connector_2.11:2.2.7 \n",
    "#  this is absolutely critical and MUST match your apache spark version\n",
    "#  if you have the wrong connector variable it won't work in any form... \n",
    "        \n",
    "``` \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/tombresee/Prairie-Fire/raw/master/ENTER/stadium_result.png\" width=\"700\">   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prove it:  Let's look at the MongoDB and see what we created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# go into mongodb\n",
    "\n",
    "> use stadium_db   # specify the database to use \n",
    "switched to db stadium_db\n",
    "\n",
    "\n",
    "> show collections  \n",
    "stadium_collection  # this is the collection we created with Spark \n",
    "\n",
    "\n",
    "> db.stadium_collection.find()  # show all the data in the mongo table (collection) \n",
    "{ \"_id\" : ObjectId(\"5e7023ba17d43a604c556d13\"), \"RANK\" : 1, \"SCHOOL\" : \"Michigan\", \"STADIUM\" : \"Michigan Stadium (Ann Arbor, Mich.)\", \"CAPACITY\" : \"107,601\" }\n",
    "{ \"_id\" : ObjectId(\"5e7023ba17d43a604c556d14\"), \"RANK\" : 2, \"SCHOOL\" : \"Penn State\", \"STADIUM\" : \"Beaver Stadium (University Park, Pa.)\", \"CAPACITY\" : \"106,572\" }\n",
    "{ \"_id\" : ObjectId(\"5e7023ba17d43a604c556d15\"), \"RANK\" : 3, \"SCHOOL\" : \"Texas A&M\", \"STADIUM\" : \"Kyle Field (College Station, Texas)\", \"CAPACITY\" : \"102,733\" }\n",
    "{ \"_id\" : ObjectId(\"5e7023ba17d43a604c556d16\"), \"RANK\" : 4, \"SCHOOL\" : \"Tennessee\", \"STADIUM\" : \"Neyland Stadium (Knoxville, Tenn.)\", \"CAPACITY\" : \"102,455\" }\n",
    "{ \"_id\" : ObjectId(\"5e7023ba17d43a604c556d17\"), \"RANK\" : 5, \"SCHOOL\" : \"LSU\", \"STADIUM\" : \"Tiger Stadium (Baton Rouge, La.)\", \"CAPACITY\" : \"102,321\" }\n",
    "{ \"_id\" : ObjectId(\"5e7023ba17d43a604c556d18\"), \"RANK\" : 6, \"SCHOOL\" : \"Ohio State\", \"STADIUM\" : \"Ohio Stadium (Columbus, Ohio)\", \"CAPACITY\" : \"102,082\" }\n",
    "{ \"_id\" : ObjectId(\"5e7023ba17d43a604c556d19\"), \"RANK\" : 7, \"SCHOOL\" : \"Alabama\", \"STADIUM\" : \"Bryant-Denny Stadium (Tuscaloosa, Ala.)\", \"CAPACITY\" : \"101,821\" }\n",
    "{ \"_id\" : ObjectId(\"5e7023ba17d43a604c556d1a\"), \"RANK\" : 8, \"SCHOOL\" : \"Texas\", \"STADIUM\" : \"Darrell K Royal-Texas Memorial Stadium (Austin, Texas)\", \"CAPACITY\" : \"100,119\" }\n",
    "{ \"_id\" : ObjectId(\"5e7023ba17d43a604c556d1b\"), \"RANK\" : 9, \"SCHOOL\" : \"Georgia\", \"STADIUM\" : \"Sanford Stadium (Athens, Ga.)\", \"CAPACITY\" : \"92,746\" }\n",
    "{ \"_id\" : ObjectId(\"5e7023ba17d43a604c556d1c\"), \"RANK\" : 10, \"SCHOOL\" : \"UCLA\", \"STADIUM\" : \"Rose Bowl (Pasadena, Calif.)\", \"CAPACITY\" : \"90,888\" }\n",
    "{ \"_id\" : ObjectId(\"5e7023ba17d43a604c556d1d\"), \"RANK\" : 11, \"SCHOOL\" : \"Florida\", \"STADIUM\" : \"Ben Hill Griffin Stadium (Gainesville, Fla.)\", \"CAPACITY\" : \"88,548\" }\n",
    "{ \"_id\" : ObjectId(\"5e7023ba17d43a604c556d1e\"), \"RANK\" : 12, \"SCHOOL\" : \"Auburn\", \"STADIUM\" : \"Jordan-Hare Stadium (Auburn, Ala.)\", \"CAPACITY\" : \"87,451\" }\n",
    "{ \"_id\" : ObjectId(\"5e7023ba17d43a604c556d1f\"), \"RANK\" : 13, \"SCHOOL\" : \"Oklahoma\", \"STADIUM\" : \"Gaylord Family Oklahoma Memorial Stadium (Norman, Okla.)\", \"CAPACITY\" : \"86,112\" }\n",
    "{ \"_id\" : ObjectId(\"5e7023ba17d43a604c556d20\"), \"RANK\" : 14, \"SCHOOL\" : \"Nebraska\", \"STADIUM\" : \"Memorial Stadium (Lincoln, Neb.)\", \"CAPACITY\" : \"85,458\" }\n",
    "{ \"_id\" : ObjectId(\"5e7023ba17d43a604c556d21\"), \"RANK\" : 15, \"SCHOOL\" : \"Clemson\", \"STADIUM\" : \"Frank Howard Field at Clemson Memorial Stadium (Clemson, S.C.)\", \"CAPACITY\" : \"81,500\" }\n",
    "{ \"_id\" : ObjectId(\"5e7023ba17d43a604c556d22\"), \"RANK\" : 16, \"SCHOOL\" : \"Notre Dame\", \"STADIUM\" : \"Notre Dame Stadium (South Bend, Ind.)\", \"CAPACITY\" : \"80,795\" }\n",
    "{ \"_id\" : ObjectId(\"5e7023ba17d43a604c556d23\"), \"RANK\" : 17, \"SCHOOL\" : \"Wisconsin\", \"STADIUM\" : \"Camp Randall Stadium (Madison, Wisc.)\", \"CAPACITY\" : \"80,321\" }\n",
    "{ \"_id\" : ObjectId(\"5e7023ba17d43a604c556d24\"), \"RANK\" : 18, \"SCHOOL\" : \"South Carolina\", \"STADIUM\" : \"Williams-Brice Stadium (Columbia, S.C.)\", \"CAPACITY\" : \"80,250\" }\n",
    "{ \"_id\" : ObjectId(\"5e7023ba17d43a604c556d25\"), \"RANK\" : 19, \"SCHOOL\" : \"Florida State\", \"STADIUM\" : \"Bobby Bowden Field at Doak Campbell Stadium (Tallahassee, Fla.)\", \"CAPACITY\" : \"79,560\" }\n",
    "{ \"_id\" : ObjectId(\"5e7023ba17d43a604c556d26\"), \"RANK\" : 20, \"SCHOOL\" : \"Southern Cal.\", \"STADIUM\" : \"United Airlines Field at Los Angeles Memorial Coliseum (Los Angeles)\", \"CAPACITY\" : \"77,500\" }\n",
    "\n",
    "\n",
    "> db.stadium_collection.find().pretty()  # show table data in json form... \n",
    "{\n",
    "        \"_id\" : ObjectId(\"5e7023ba17d43a604c556d13\"),\n",
    "        \"RANK\" : 1,\n",
    "        \"SCHOOL\" : \"Michigan\",\n",
    "        \"STADIUM\" : \"Michigan Stadium (Ann Arbor, Mich.)\",\n",
    "        \"CAPACITY\" : \"107,601\"\n",
    "}\n",
    "{\n",
    "        \"_id\" : ObjectId(\"5e7023ba17d43a604c556d14\"),\n",
    "        \"RANK\" : 2,\n",
    "        \"SCHOOL\" : \"Penn State\",\n",
    "        \"STADIUM\" : \"Beaver Stadium (University Park, Pa.)\",\n",
    "        \"CAPACITY\" : \"106,572\"\n",
    "}\n",
    "{\n",
    "        \"_id\" : ObjectId(\"5e7023ba17d43a604c556d15\"),\n",
    "        \"RANK\" : 3,\n",
    "        \"SCHOOL\" : \"Texas A&M\",\n",
    "        \"STADIUM\" : \"Kyle Field (College Station, Texas)\",\n",
    "        \"CAPACITY\" : \"102,733\"\n",
    "}\n",
    "{\n",
    "        \"_id\" : ObjectId(\"5e7023ba17d43a604c556d16\"),\n",
    "        \"RANK\" : 4,\n",
    "        \"SCHOOL\" : \"Tennessee\",\n",
    "        \"STADIUM\" : \"Neyland Stadium (Knoxville, Tenn.)\",\n",
    "        \"CAPACITY\" : \"102,455\"\n",
    "}\n",
    "{\n",
    "        \"_id\" : ObjectId(\"5e7023ba17d43a604c556d17\"),\n",
    "        \"RANK\" : 5,\n",
    "        \"SCHOOL\" : \"LSU\",\n",
    "        \"STADIUM\" : \"Tiger Stadium (Baton Rouge, La.)\",\n",
    "        \"CAPACITY\" : \"102,321\"\n",
    "}\n",
    "{\n",
    "        \"_id\" : ObjectId(\"5e7023ba17d43a604c556d18\"),\n",
    "        \"RANK\" : 6,\n",
    "        \"SCHOOL\" : \"Ohio State\",\n",
    "        \"STADIUM\" : \"Ohio Stadium (Columbus, Ohio)\",\n",
    "        \"CAPACITY\" : \"102,082\"\n",
    "}\n",
    "{\n",
    "        \"_id\" : ObjectId(\"5e7023ba17d43a604c556d19\"),\n",
    "        \"RANK\" : 7,\n",
    "        \"SCHOOL\" : \"Alabama\",\n",
    "        \"STADIUM\" : \"Bryant-Denny Stadium (Tuscaloosa, Ala.)\",\n",
    "        \"CAPACITY\" : \"101,821\"\n",
    "}\n",
    "{\n",
    "        \"_id\" : ObjectId(\"5e7023ba17d43a604c556d1a\"),\n",
    "        \"RANK\" : 8,\n",
    "        \"SCHOOL\" : \"Texas\",\n",
    "        \"STADIUM\" : \"Darrell K Royal-Texas Memorial Stadium (Austin, Texas)\",\n",
    "        \"CAPACITY\" : \"100,119\"\n",
    "}\n",
    "{\n",
    "        \"_id\" : ObjectId(\"5e7023ba17d43a604c556d1b\"),\n",
    "        \"RANK\" : 9,\n",
    "        \"SCHOOL\" : \"Georgia\",\n",
    "        \"STADIUM\" : \"Sanford Stadium (Athens, Ga.)\",\n",
    "        \"CAPACITY\" : \"92,746\"\n",
    "}\n",
    "{\n",
    "        \"_id\" : ObjectId(\"5e7023ba17d43a604c556d1c\"),\n",
    "        \"RANK\" : 10,\n",
    "        \"SCHOOL\" : \"UCLA\",\n",
    "        \"STADIUM\" : \"Rose Bowl (Pasadena, Calif.)\",\n",
    "        \"CAPACITY\" : \"90,888\"\n",
    "}\n",
    "{\n",
    "        \"_id\" : ObjectId(\"5e7023ba17d43a604c556d1d\"),\n",
    "        \"RANK\" : 11,\n",
    "        \"SCHOOL\" : \"Florida\",\n",
    "        \"STADIUM\" : \"Ben Hill Griffin Stadium (Gainesville, Fla.)\",\n",
    "        \"CAPACITY\" : \"88,548\"\n",
    "}\n",
    "{\n",
    "        \"_id\" : ObjectId(\"5e7023ba17d43a604c556d1e\"),\n",
    "        \"RANK\" : 12,\n",
    "        \"SCHOOL\" : \"Auburn\",\n",
    "        \"STADIUM\" : \"Jordan-Hare Stadium (Auburn, Ala.)\",\n",
    "        \"CAPACITY\" : \"87,451\"\n",
    "}\n",
    "{\n",
    "        \"_id\" : ObjectId(\"5e7023ba17d43a604c556d1f\"),\n",
    "        \"RANK\" : 13,\n",
    "        \"SCHOOL\" : \"Oklahoma\",\n",
    "        \"STADIUM\" : \"Gaylord Family Oklahoma Memorial Stadium (Norman, Okla.)\",\n",
    "        \"CAPACITY\" : \"86,112\"\n",
    "}\n",
    "{\n",
    "        \"_id\" : ObjectId(\"5e7023ba17d43a604c556d20\"),\n",
    "        \"RANK\" : 14,\n",
    "        \"SCHOOL\" : \"Nebraska\",\n",
    "        \"STADIUM\" : \"Memorial Stadium (Lincoln, Neb.)\",\n",
    "        \"CAPACITY\" : \"85,458\"\n",
    "}\n",
    "{\n",
    "        \"_id\" : ObjectId(\"5e7023ba17d43a604c556d21\"),\n",
    "        \"RANK\" : 15,\n",
    "        \"SCHOOL\" : \"Clemson\",\n",
    "        \"STADIUM\" : \"Frank Howard Field at Clemson Memorial Stadium (Clemson, S.C.)\",\n",
    "        \"CAPACITY\" : \"81,500\"\n",
    "}\n",
    "{\n",
    "        \"_id\" : ObjectId(\"5e7023ba17d43a604c556d22\"),\n",
    "        \"RANK\" : 16,\n",
    "        \"SCHOOL\" : \"Notre Dame\",\n",
    "        \"STADIUM\" : \"Notre Dame Stadium (South Bend, Ind.)\",\n",
    "        \"CAPACITY\" : \"80,795\"\n",
    "}\n",
    "{\n",
    "        \"_id\" : ObjectId(\"5e7023ba17d43a604c556d23\"),\n",
    "        \"RANK\" : 17,\n",
    "        \"SCHOOL\" : \"Wisconsin\",\n",
    "        \"STADIUM\" : \"Camp Randall Stadium (Madison, Wisc.)\",\n",
    "        \"CAPACITY\" : \"80,321\"\n",
    "}\n",
    "{\n",
    "        \"_id\" : ObjectId(\"5e7023ba17d43a604c556d24\"),\n",
    "        \"RANK\" : 18,\n",
    "        \"SCHOOL\" : \"South Carolina\",\n",
    "        \"STADIUM\" : \"Williams-Brice Stadium (Columbia, S.C.)\",\n",
    "        \"CAPACITY\" : \"80,250\"\n",
    "}\n",
    "{\n",
    "        \"_id\" : ObjectId(\"5e7023ba17d43a604c556d25\"),\n",
    "        \"RANK\" : 19,\n",
    "        \"SCHOOL\" : \"Florida State\",\n",
    "        \"STADIUM\" : \"Bobby Bowden Field at Doak Campbell Stadium (Tallahassee, Fla.)\",\n",
    "        \"CAPACITY\" : \"79,560\"\n",
    "}\n",
    "{\n",
    "        \"_id\" : ObjectId(\"5e7023ba17d43a604c556d26\"),\n",
    "        \"RANK\" : 20,\n",
    "        \"SCHOOL\" : \"Southern Cal.\",\n",
    "        \"STADIUM\" : \"United Airlines Field at Los Angeles Memorial Coliseum (Los Angeles)\",\n",
    "        \"CAPACITY\" : \"77,500\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "> db.stadium_collection.find({}, {RANK:1, SCHOOL:1, _id:0})  # Just list the rank and school columns from the data table\n",
    "{ \"RANK\" : 1, \"SCHOOL\" : \"Michigan\" }\n",
    "{ \"RANK\" : 2, \"SCHOOL\" : \"Penn State\" }\n",
    "{ \"RANK\" : 3, \"SCHOOL\" : \"Texas A&M\" }\n",
    "{ \"RANK\" : 4, \"SCHOOL\" : \"Tennessee\" }\n",
    "{ \"RANK\" : 5, \"SCHOOL\" : \"LSU\" }\n",
    "{ \"RANK\" : 6, \"SCHOOL\" : \"Ohio State\" }\n",
    "{ \"RANK\" : 7, \"SCHOOL\" : \"Alabama\" }\n",
    "{ \"RANK\" : 8, \"SCHOOL\" : \"Texas\" }\n",
    "{ \"RANK\" : 9, \"SCHOOL\" : \"Georgia\" }\n",
    "{ \"RANK\" : 10, \"SCHOOL\" : \"UCLA\" }\n",
    "{ \"RANK\" : 11, \"SCHOOL\" : \"Florida\" }\n",
    "{ \"RANK\" : 12, \"SCHOOL\" : \"Auburn\" }\n",
    "{ \"RANK\" : 13, \"SCHOOL\" : \"Oklahoma\" }\n",
    "{ \"RANK\" : 14, \"SCHOOL\" : \"Nebraska\" }\n",
    "{ \"RANK\" : 15, \"SCHOOL\" : \"Clemson\" }\n",
    "{ \"RANK\" : 16, \"SCHOOL\" : \"Notre Dame\" }\n",
    "{ \"RANK\" : 17, \"SCHOOL\" : \"Wisconsin\" }\n",
    "{ \"RANK\" : 18, \"SCHOOL\" : \"South Carolina\" }\n",
    "{ \"RANK\" : 19, \"SCHOOL\" : \"Florida State\" }\n",
    "{ \"RANK\" : 20, \"SCHOOL\" : \"Southern Cal.\" }\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What's a quick way to  know things worked ?  Remember, whenever you store data in a MongoDB database, it associates (creates) unique ids with each data record (you don't tell it to do that, it just does).  IF you write to a mongo database, and then when you go back and read that data from the table and see a **brand new column called `_id`**, you know you are doing something right...  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Full CLI output: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#./bin/spark-submit --master \"local[*]\"  \\\n",
    ">                      --conf \"spark.mongodb.input.uri=mongodb://127.0.0.1/stadium_db.stadium_collection?readPreference=primaryPreferred\" \\\n",
    ">                      --conf \"spark.mongodb.output.uri=mongodb://127.0.0.1/stadium_db.stadium_collection\" \\\n",
    ">                      --packages org.mongodb.spark:mongo-spark-connector_2.11:2.2.7 \\\n",
    ">                      /root/stadium_analysis.py\n",
    "\n",
    "The jars for the packages stored in: /root/.ivy2/jars\n",
    ":: loading settings :: url = jar:file:/opt/spark-2.4.5-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
    "org.mongodb.spark#mongo-spark-connector_2.11 added as a dependency\n",
    ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1572bebb-4d85-43f8-a3a5-7d8c19073faf;1.0\n",
    "        confs: [default]\n",
    "        found org.mongodb.spark#mongo-spark-connector_2.11;2.2.7 in central\n",
    "        found org.mongodb#mongo-java-driver;3.10.2 in central\n",
    "        [3.10.2] org.mongodb#mongo-java-driver;[3.10,3.11)\n",
    ":: resolution report :: resolve 2024ms :: artifacts dl 4ms\n",
    "        :: modules in use:\n",
    "        org.mongodb#mongo-java-driver;3.10.2 from central in [default]\n",
    "        org.mongodb.spark#mongo-spark-connector_2.11;2.2.7 from central in [default]\n",
    "        ---------------------------------------------------------------------\n",
    "        |                  |            modules            ||   artifacts   |\n",
    "        |       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
    "        ---------------------------------------------------------------------\n",
    "        |      default     |   2   |   1   |   0   |   0   ||   2   |   0   |\n",
    "        ---------------------------------------------------------------------\n",
    ":: retrieving :: org.apache.spark#spark-submit-parent-1572bebb-4d85-43f8-a3a5-7d8c19073faf\n",
    "        confs: [default]\n",
    "        0 artifacts copied, 2 already retrieved (0kB/5ms)\n",
    "20/03/16 17:51:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
    "20/03/16 17:51:29 INFO SparkContext: Running Spark version 2.4.5\n",
    "20/03/16 17:51:29 INFO SparkContext: Submitted application: Connector-Mongodb-Apache Spark\n",
    "20/03/16 17:51:29 INFO SecurityManager: Changing view acls to: root\n",
    "20/03/16 17:51:29 INFO SecurityManager: Changing modify acls to: root\n",
    "20/03/16 17:51:29 INFO SecurityManager: Changing view acls groups to:\n",
    "20/03/16 17:51:29 INFO SecurityManager: Changing modify acls groups to:\n",
    "20/03/16 17:51:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
    "20/03/16 17:51:30 INFO Utils: Successfully started service 'sparkDriver' on port 39687.\n",
    "20/03/16 17:51:30 INFO SparkEnv: Registering MapOutputTracker\n",
    "20/03/16 17:51:30 INFO SparkEnv: Registering BlockManagerMaster\n",
    "20/03/16 17:51:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
    "20/03/16 17:51:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
    "20/03/16 17:51:30 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-614c947a-3823-44a7-aaed-cfbcf39ecd3d\n",
    "20/03/16 17:51:30 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
    "20/03/16 17:51:30 INFO SparkEnv: Registering OutputCommitCoordinator\n",
    "20/03/16 17:51:30 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
    "20/03/16 17:51:30 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://animal-mother.mgmt:4040\n",
    "20/03/16 17:51:30 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.11-2.2.7.jar at spark://animal-mother.mgmt:39687/jars/org.mongodb.spark_mongo-spark-connector_2.11-2.2.7.jar with timestamp 1584406290461\n",
    "20/03/16 17:51:30 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.mongodb_mongo-java-driver-3.10.2.jar at spark://animal-mother.mgmt:39687/jars/org.mongodb_mongo-java-driver-3.10.2.jar with timestamp 1584406290462\n",
    "20/03/16 17:51:30 INFO SparkContext: Added file file:///root/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.11-2.2.7.jar at file:///root/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.11-2.2.7.jar with timestamp 1584406290479\n",
    "20/03/16 17:51:30 INFO Utils: Copying /root/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.11-2.2.7.jar to /tmp/spark-05121f50-5990-4238-9553-51ff7b78a6ab/userFiles-08642a1c-3bef-4057-991b-98b364f8872c/org.mongodb.spark_mongo-spark-connector_2.11-2.2.7.jar\n",
    "20/03/16 17:51:30 INFO SparkContext: Added file file:///root/.ivy2/jars/org.mongodb_mongo-java-driver-3.10.2.jar at file:///root/.ivy2/jars/org.mongodb_mongo-java-driver-3.10.2.jar with timestamp 1584406290495\n",
    "20/03/16 17:51:30 INFO Utils: Copying /root/.ivy2/jars/org.mongodb_mongo-java-driver-3.10.2.jar to /tmp/spark-05121f50-5990-4238-9553-51ff7b78a6ab/userFiles-08642a1c-3bef-4057-991b-98b364f8872c/org.mongodb_mongo-java-driver-3.10.2.jar\n",
    "20/03/16 17:51:30 INFO Executor: Starting executor ID driver on host localhost\n",
    "20/03/16 17:51:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41405.\n",
    "20/03/16 17:51:30 INFO NettyBlockTransferService: Server created on animal-mother.mgmt:41405\n",
    "20/03/16 17:51:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
    "20/03/16 17:51:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, animal-mother.mgmt, 41405, None)\n",
    "20/03/16 17:51:30 INFO BlockManagerMasterEndpoint: Registering block manager animal-mother.mgmt:41405 with 366.3 MB RAM, BlockManagerId(driver, animal-mother.mgmt, 41405, None)\n",
    "20/03/16 17:51:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, animal-mother.mgmt, 41405, None)\n",
    "20/03/16 17:51:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, animal-mother.mgmt, 41405, None)\n",
    "20/03/16 17:51:30 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/spark-2.4.5-bin-hadoop2.7/spark-warehouse/').\n",
    "20/03/16 17:51:30 INFO SharedState: Warehouse path is 'file:/opt/spark-2.4.5-bin-hadoop2.7/spark-warehouse/'.\n",
    "20/03/16 17:51:31 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\n",
    "20/03/16 17:51:31 INFO InMemoryFileIndex: It took 35 ms to list leaf files for 1 paths.\n",
    "20/03/16 17:51:31 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.\n",
    "20/03/16 17:51:33 INFO FileSourceStrategy: Pruning directories with:\n",
    "20/03/16 17:51:33 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
    "20/03/16 17:51:33 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
    "20/03/16 17:51:33 INFO FileSourceScanExec: Pushed Filters:\n",
    "20/03/16 17:51:33 INFO CodeGenerator: Code generated in 191.762826 ms\n",
    "20/03/16 17:51:34 INFO CodeGenerator: Code generated in 18.763374 ms\n",
    "20/03/16 17:51:34 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 284.0 KB, free 366.0 MB)\n",
    "20/03/16 17:51:34 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.9 KB, free 366.0 MB)\n",
    "20/03/16 17:51:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on animal-mother.mgmt:41405 (size: 23.9 KB, free: 366.3 MB)\n",
    "20/03/16 17:51:34 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0\n",
    "20/03/16 17:51:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
    "20/03/16 17:51:34 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
    "20/03/16 17:51:34 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
    "20/03/16 17:51:34 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)\n",
    "20/03/16 17:51:34 INFO DAGScheduler: Parents of final stage: List()\n",
    "20/03/16 17:51:34 INFO DAGScheduler: Missing parents: List()\n",
    "20/03/16 17:51:34 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
    "20/03/16 17:51:34 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.9 KB, free 366.0 MB)\n",
    "20/03/16 17:51:34 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KB, free 366.0 MB)\n",
    "20/03/16 17:51:34 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on animal-mother.mgmt:41405 (size: 4.6 KB, free: 366.3 MB)\n",
    "20/03/16 17:51:34 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1163\n",
    "20/03/16 17:51:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
    "20/03/16 17:51:34 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\n",
    "20/03/16 17:51:34 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 8245 bytes)\n",
    "20/03/16 17:51:34 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
    "20/03/16 17:51:34 INFO Executor: Fetching file:///root/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.11-2.2.7.jar with timestamp 1584406290479\n",
    "20/03/16 17:51:34 INFO Utils: /root/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.11-2.2.7.jar has been previously copied to /tmp/spark-05121f50-5990-4238-9553-51ff7b78a6ab/userFiles-08642a1c-3bef-4057-991b-98b364f8872c/org.mongodb.spark_mongo-spark-connector_2.11-2.2.7.jar\n",
    "20/03/16 17:51:34 INFO Executor: Fetching file:///root/.ivy2/jars/org.mongodb_mongo-java-driver-3.10.2.jar with timestamp 1584406290495\n",
    "20/03/16 17:51:34 INFO Utils: /root/.ivy2/jars/org.mongodb_mongo-java-driver-3.10.2.jar has been previously copied to /tmp/spark-05121f50-5990-4238-9553-51ff7b78a6ab/userFiles-08642a1c-3bef-4057-991b-98b364f8872c/org.mongodb_mongo-java-driver-3.10.2.jar\n",
    "20/03/16 17:51:34 INFO Executor: Fetching spark://animal-mother.mgmt:39687/jars/org.mongodb_mongo-java-driver-3.10.2.jar with timestamp 1584406290462\n",
    "20/03/16 17:51:34 INFO TransportClientFactory: Successfully created connection to animal-mother.mgmt/10.94.207.196:39687 after 39 ms (0 ms spent in bootstraps)\n",
    "20/03/16 17:51:34 INFO Utils: Fetching spark://animal-mother.mgmt:39687/jars/org.mongodb_mongo-java-driver-3.10.2.jar to /tmp/spark-05121f50-5990-4238-9553-51ff7b78a6ab/userFiles-08642a1c-3bef-4057-991b-98b364f8872c/fetchFileTemp6763034555078161264.tmp\n",
    "20/03/16 17:51:34 INFO Utils: /tmp/spark-05121f50-5990-4238-9553-51ff7b78a6ab/userFiles-08642a1c-3bef-4057-991b-98b364f8872c/fetchFileTemp6763034555078161264.tmp has been previously copied to /tmp/spark-05121f50-5990-4238-9553-51ff7b78a6ab/userFiles-08642a1c-3bef-4057-991b-98b364f8872c/org.mongodb_mongo-java-driver-3.10.2.jar\n",
    "20/03/16 17:51:34 INFO Executor: Adding file:/tmp/spark-05121f50-5990-4238-9553-51ff7b78a6ab/userFiles-08642a1c-3bef-4057-991b-98b364f8872c/org.mongodb_mongo-java-driver-3.10.2.jar to class loader\n",
    "20/03/16 17:51:34 INFO Executor: Fetching spark://animal-mother.mgmt:39687/jars/org.mongodb.spark_mongo-spark-connector_2.11-2.2.7.jar with timestamp 1584406290461\n",
    "20/03/16 17:51:34 INFO Utils: Fetching spark://animal-mother.mgmt:39687/jars/org.mongodb.spark_mongo-spark-connector_2.11-2.2.7.jar to /tmp/spark-05121f50-5990-4238-9553-51ff7b78a6ab/userFiles-08642a1c-3bef-4057-991b-98b364f8872c/fetchFileTemp8181146042765144332.tmp\n",
    "20/03/16 17:51:34 INFO Utils: /tmp/spark-05121f50-5990-4238-9553-51ff7b78a6ab/userFiles-08642a1c-3bef-4057-991b-98b364f8872c/fetchFileTemp8181146042765144332.tmp has been previously copied to /tmp/spark-05121f50-5990-4238-9553-51ff7b78a6ab/userFiles-08642a1c-3bef-4057-991b-98b364f8872c/org.mongodb.spark_mongo-spark-connector_2.11-2.2.7.jar\n",
    "20/03/16 17:51:34 INFO Executor: Adding file:/tmp/spark-05121f50-5990-4238-9553-51ff7b78a6ab/userFiles-08642a1c-3bef-4057-991b-98b364f8872c/org.mongodb.spark_mongo-spark-connector_2.11-2.2.7.jar to class loader\n",
    "20/03/16 17:51:34 INFO FileScanRDD: Reading File path: file:///root/stadium_data.csv, range: 0-1337, partition values: [empty row]\n",
    "20/03/16 17:51:34 INFO CodeGenerator: Code generated in 11.910103 ms\n",
    "20/03/16 17:51:34 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1334 bytes result sent to driver\n",
    "20/03/16 17:51:34 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 404 ms on localhost (executor driver) (1/1)\n",
    "20/03/16 17:51:34 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool\n",
    "20/03/16 17:51:34 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0.514 s\n",
    "20/03/16 17:51:34 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 0.572299 s\n",
    "20/03/16 17:51:34 INFO FileSourceStrategy: Pruning directories with:\n",
    "20/03/16 17:51:34 INFO FileSourceStrategy: Post-Scan Filters:\n",
    "20/03/16 17:51:34 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
    "20/03/16 17:51:34 INFO FileSourceScanExec: Pushed Filters:\n",
    "20/03/16 17:51:34 INFO CodeGenerator: Code generated in 6.918409 ms\n",
    "20/03/16 17:51:34 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 284.0 KB, free 365.7 MB)\n",
    "20/03/16 17:51:34 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.9 KB, free 365.7 MB)\n",
    "20/03/16 17:51:34 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on animal-mother.mgmt:41405 (size: 23.9 KB, free: 366.2 MB)\n",
    "20/03/16 17:51:34 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0\n",
    "20/03/16 17:51:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
    "20/03/16 17:51:34 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
    "20/03/16 17:51:34 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
    "20/03/16 17:51:34 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)\n",
    "20/03/16 17:51:34 INFO DAGScheduler: Parents of final stage: List()\n",
    "20/03/16 17:51:34 INFO DAGScheduler: Missing parents: List()\n",
    "20/03/16 17:51:34 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
    "20/03/16 17:51:34 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 16.3 KB, free 365.7 MB)\n",
    "20/03/16 17:51:34 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.1 KB, free 365.7 MB)\n",
    "20/03/16 17:51:34 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on animal-mother.mgmt:41405 (size: 9.1 KB, free: 366.2 MB)\n",
    "20/03/16 17:51:34 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1163\n",
    "20/03/16 17:51:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
    "20/03/16 17:51:34 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks\n",
    "20/03/16 17:51:34 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 8245 bytes)\n",
    "20/03/16 17:51:34 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
    "20/03/16 17:51:35 INFO FileScanRDD: Reading File path: file:///root/stadium_data.csv, range: 0-1337, partition values: [empty row]\n",
    "20/03/16 17:51:35 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1534 bytes result sent to driver\n",
    "20/03/16 17:51:35 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 45 ms on localhost (executor driver) (1/1)\n",
    "20/03/16 17:51:35 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool\n",
    "20/03/16 17:51:35 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 0.059 s\n",
    "20/03/16 17:51:35 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 0.066280 s\n",
    "20/03/16 17:51:35 INFO cluster: Cluster created with settings {hosts=[127.0.0.1:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms', maxWaitQueueSize=500}\n",
    "20/03/16 17:51:35 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
    "20/03/16 17:51:35 INFO connection: Opened connection [connectionId{localValue:1, serverValue:137}] to 127.0.0.1:27017\n",
    "20/03/16 17:51:35 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=127.0.0.1:27017, type=STANDALONE, state=CONNECTED, ok=true, version=ServerVersion{versionList=[4, 2, 3]}, minWireVersion=0, maxWireVersion=8, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=4572629}\n",
    "20/03/16 17:51:35 INFO MongoClientCache: Creating MongoClient: [127.0.0.1:27017]\n",
    "20/03/16 17:51:35 INFO connection: Opened connection [connectionId{localValue:2, serverValue:138}] to 127.0.0.1:27017\n",
    "20/03/16 17:51:35 INFO FileSourceStrategy: Pruning directories with:\n",
    "20/03/16 17:51:35 INFO FileSourceStrategy: Post-Scan Filters:\n",
    "20/03/16 17:51:35 INFO FileSourceStrategy: Output Data Schema: struct<RANK: int, SCHOOL: string, STADIUM: string, CAPACITY: string ... 2 more fields>\n",
    "20/03/16 17:51:35 INFO FileSourceScanExec: Pushed Filters:\n",
    "20/03/16 17:51:35 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 284.0 KB, free 365.4 MB)\n",
    "20/03/16 17:51:35 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 23.9 KB, free 365.4 MB)\n",
    "20/03/16 17:51:35 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on animal-mother.mgmt:41405 (size: 23.9 KB, free: 366.2 MB)\n",
    "20/03/16 17:51:35 INFO SparkContext: Created broadcast 4 from rdd at MongoSpark.scala:154\n",
    "20/03/16 17:51:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
    "20/03/16 17:51:35 INFO SparkContext: Starting job: foreachPartition at MongoSpark.scala:117\n",
    "20/03/16 17:51:35 INFO DAGScheduler: Got job 2 (foreachPartition at MongoSpark.scala:117) with 1 output partitions\n",
    "20/03/16 17:51:35 INFO DAGScheduler: Final stage: ResultStage 2 (foreachPartition at MongoSpark.scala:117)\n",
    "20/03/16 17:51:35 INFO DAGScheduler: Parents of final stage: List()\n",
    "20/03/16 17:51:35 INFO DAGScheduler: Missing parents: List()\n",
    "20/03/16 17:51:35 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[15] at map at MongoSpark.scala:154), which has no missing parents\n",
    "20/03/16 17:51:35 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 18.1 KB, free 365.3 MB)\n",
    "20/03/16 17:51:35 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 9.9 KB, free 365.3 MB)\n",
    "20/03/16 17:51:35 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on animal-mother.mgmt:41405 (size: 9.9 KB, free: 366.2 MB)\n",
    "20/03/16 17:51:35 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1163\n",
    "20/03/16 17:51:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[15] at map at MongoSpark.scala:154) (first 15 tasks are for partitions Vector(0))\n",
    "20/03/16 17:51:35 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks\n",
    "20/03/16 17:51:35 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 8245 bytes)\n",
    "20/03/16 17:51:35 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
    "20/03/16 17:51:35 INFO CodeGenerator: Code generated in 14.184878 ms\n",
    "20/03/16 17:51:35 INFO FileScanRDD: Reading File path: file:///root/stadium_data.csv, range: 0-1337, partition values: [empty row]\n",
    "20/03/16 17:51:35 INFO CodeGenerator: Code generated in 10.456978 ms\n",
    "20/03/16 17:51:35 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1437 bytes result sent to driver\n",
    "20/03/16 17:51:35 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 133 ms on localhost (executor driver) (1/1)\n",
    "20/03/16 17:51:35 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool\n",
    "20/03/16 17:51:35 INFO DAGScheduler: ResultStage 2 (foreachPartition at MongoSpark.scala:117) finished in 0.152 s\n",
    "20/03/16 17:51:35 INFO DAGScheduler: Job 2 finished: foreachPartition at MongoSpark.scala:117, took 0.157083 s\n",
    "20/03/16 17:51:35 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 256.0 B, free 365.3 MB)\n",
    "20/03/16 17:51:35 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 411.0 B, free 365.3 MB)\n",
    "20/03/16 17:51:35 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on animal-mother.mgmt:41405 (size: 411.0 B, free: 366.2 MB)\n",
    "20/03/16 17:51:35 INFO SparkContext: Created broadcast 6 from broadcast at MongoSpark.scala:543\n",
    "Schema:\n",
    "root\n",
    " |-- RANK: integer (nullable = true)\n",
    " |-- SCHOOL: string (nullable = true)\n",
    " |-- STADIUM: string (nullable = true)\n",
    " |-- CAPACITY: string (nullable = true)\n",
    "\n",
    "20/03/16 17:51:35 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 320.0 B, free 365.3 MB)\n",
    "20/03/16 17:51:35 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 441.0 B, free 365.3 MB)\n",
    "20/03/16 17:51:35 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on animal-mother.mgmt:41405 (size: 441.0 B, free: 366.2 MB)\n",
    "20/03/16 17:51:35 INFO SparkContext: Created broadcast 7 from broadcast at MongoSpark.scala:543\n",
    "20/03/16 17:51:35 INFO cluster: Cluster created with settings {hosts=[127.0.0.1:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms', maxWaitQueueSize=500}\n",
    "20/03/16 17:51:35 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
    "20/03/16 17:51:35 INFO connection: Opened connection [connectionId{localValue:3, serverValue:139}] to 127.0.0.1:27017\n",
    "20/03/16 17:51:35 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=127.0.0.1:27017, type=STANDALONE, state=CONNECTED, ok=true, version=ServerVersion{versionList=[4, 2, 3]}, minWireVersion=0, maxWireVersion=8, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=913141}\n",
    "20/03/16 17:51:35 INFO MongoClientCache: Creating MongoClient: [127.0.0.1:27017]\n",
    "20/03/16 17:51:35 INFO connection: Opened connection [connectionId{localValue:4, serverValue:140}] to 127.0.0.1:27017\n",
    "20/03/16 17:51:35 INFO SparkContext: Starting job: treeAggregate at MongoInferSchema.scala:88\n",
    "20/03/16 17:51:35 INFO DAGScheduler: Got job 3 (treeAggregate at MongoInferSchema.scala:88) with 1 output partitions\n",
    "20/03/16 17:51:35 INFO DAGScheduler: Final stage: ResultStage 3 (treeAggregate at MongoInferSchema.scala:88)\n",
    "20/03/16 17:51:35 INFO DAGScheduler: Parents of final stage: List()\n",
    "20/03/16 17:51:35 INFO DAGScheduler: Missing parents: List()\n",
    "20/03/16 17:51:35 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[23] at treeAggregate at MongoInferSchema.scala:88), which has no missing parents\n",
    "20/03/16 17:51:35 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 6.2 KB, free 365.3 MB)\n",
    "20/03/16 17:51:35 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.3 KB, free 365.3 MB)\n",
    "20/03/16 17:51:35 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on animal-mother.mgmt:41405 (size: 3.3 KB, free: 366.2 MB)\n",
    "20/03/16 17:51:35 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1163\n",
    "20/03/16 17:51:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[23] at treeAggregate at MongoInferSchema.scala:88) (first 15 tasks are for partitions Vector(0))\n",
    "20/03/16 17:51:35 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks\n",
    "20/03/16 17:51:35 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 8001 bytes)\n",
    "20/03/16 17:51:35 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
    "20/03/16 17:51:35 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1605 bytes result sent to driver\n",
    "20/03/16 17:51:35 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 70 ms on localhost (executor driver) (1/1)\n",
    "20/03/16 17:51:35 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool\n",
    "20/03/16 17:51:35 INFO DAGScheduler: ResultStage 3 (treeAggregate at MongoInferSchema.scala:88) finished in 0.082 s\n",
    "20/03/16 17:51:35 INFO DAGScheduler: Job 3 finished: treeAggregate at MongoInferSchema.scala:88, took 0.089431 s\n",
    "\n",
    "\n",
    "*******************************************************************\n",
    "\n",
    "\n",
    "Ranking of Largest College Football Stadium Sizes, in the USA:\n",
    "20/03/16 17:51:36 INFO MongoRelation: requiredColumns: RANK, SCHOOL, _id, CAPACITY, STADIUM, filters:\n",
    "20/03/16 17:51:36 INFO CodeGenerator: Code generated in 13.305146 ms\n",
    "20/03/16 17:51:36 INFO CodeGenerator: Code generated in 18.310045 ms\n",
    "20/03/16 17:51:36 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
    "20/03/16 17:51:36 INFO DAGScheduler: Got job 4 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
    "20/03/16 17:51:36 INFO DAGScheduler: Final stage: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0)\n",
    "20/03/16 17:51:36 INFO DAGScheduler: Parents of final stage: List()\n",
    "20/03/16 17:51:36 INFO DAGScheduler: Missing parents: List()\n",
    "20/03/16 17:51:36 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[29] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
    "20/03/16 17:51:36 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 11.8 KB, free 365.3 MB)\n",
    "20/03/16 17:51:36 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 5.7 KB, free 365.3 MB)\n",
    "20/03/16 17:51:36 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on animal-mother.mgmt:41405 (size: 5.7 KB, free: 366.2 MB)\n",
    "20/03/16 17:51:36 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1163\n",
    "20/03/16 17:51:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[29] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
    "20/03/16 17:51:36 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks\n",
    "20/03/16 17:51:36 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 8001 bytes)\n",
    "20/03/16 17:51:36 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
    "20/03/16 17:51:36 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2673 bytes result sent to driver\n",
    "20/03/16 17:51:36 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 23 ms on localhost (executor driver) (1/1)\n",
    "20/03/16 17:51:36 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool\n",
    "20/03/16 17:51:36 INFO DAGScheduler: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0) finished in 0.034 s\n",
    "20/03/16 17:51:36 INFO DAGScheduler: Job 4 finished: showString at NativeMethodAccessorImpl.java:0, took 0.040323 s\n",
    "+--------+----+--------------+--------------------------------------------------------------------+--------------------------+\n",
    "|CAPACITY|RANK|SCHOOL        |STADIUM                                                             |_id                       |\n",
    "+--------+----+--------------+--------------------------------------------------------------------+--------------------------+\n",
    "|107,601 |1   |Michigan      |Michigan Stadium (Ann Arbor, Mich.)                                 |[5e701f174dd58907d6a06630]|\n",
    "|106,572 |2   |Penn State    |Beaver Stadium (University Park, Pa.)                               |[5e701f174dd58907d6a06631]|\n",
    "|102,733 |3   |Texas A&M     |Kyle Field (College Station, Texas)                                 |[5e701f174dd58907d6a06632]|\n",
    "|102,455 |4   |Tennessee     |Neyland Stadium (Knoxville, Tenn.)                                  |[5e701f174dd58907d6a06633]|\n",
    "|102,321 |5   |LSU           |Tiger Stadium (Baton Rouge, La.)                                    |[5e701f174dd58907d6a06634]|\n",
    "|102,082 |6   |Ohio State    |Ohio Stadium (Columbus, Ohio)                                       |[5e701f174dd58907d6a06635]|\n",
    "|101,821 |7   |Alabama       |Bryant-Denny Stadium (Tuscaloosa, Ala.)                             |[5e701f174dd58907d6a06636]|\n",
    "|100,119 |8   |Texas         |Darrell K Royal-Texas Memorial Stadium (Austin, Texas)              |[5e701f174dd58907d6a06637]|\n",
    "|92,746  |9   |Georgia       |Sanford Stadium (Athens, Ga.)                                       |[5e701f174dd58907d6a06638]|\n",
    "|90,888  |10  |UCLA          |Rose Bowl (Pasadena, Calif.)                                        |[5e701f174dd58907d6a06639]|\n",
    "|88,548  |11  |Florida       |Ben Hill Griffin Stadium (Gainesville, Fla.)                        |[5e701f174dd58907d6a0663a]|\n",
    "|87,451  |12  |Auburn        |Jordan-Hare Stadium (Auburn, Ala.)                                  |[5e701f174dd58907d6a0663b]|\n",
    "|86,112  |13  |Oklahoma      |Gaylord Family Oklahoma Memorial Stadium (Norman, Okla.)            |[5e701f174dd58907d6a0663c]|\n",
    "|85,458  |14  |Nebraska      |Memorial Stadium (Lincoln, Neb.)                                    |[5e701f174dd58907d6a0663d]|\n",
    "|81,500  |15  |Clemson       |Frank Howard Field at Clemson Memorial Stadium (Clemson, S.C.)      |[5e701f174dd58907d6a0663e]|\n",
    "|80,795  |16  |Notre Dame    |Notre Dame Stadium (South Bend, Ind.)                               |[5e701f174dd58907d6a0663f]|\n",
    "|80,321  |17  |Wisconsin     |Camp Randall Stadium (Madison, Wisc.)                               |[5e701f174dd58907d6a06640]|\n",
    "|80,250  |18  |South Carolina|Williams-Brice Stadium (Columbia, S.C.)                             |[5e701f174dd58907d6a06641]|\n",
    "|79,560  |19  |Florida State |Bobby Bowden Field at Doak Campbell Stadium (Tallahassee, Fla.)     |[5e701f174dd58907d6a06642]|\n",
    "|77,500  |20  |Southern Cal. |United Airlines Field at Los Angeles Memorial Coliseum (Los Angeles)|[5e701f174dd58907d6a06643]|\n",
    "+--------+----+--------------+--------------------------------------------------------------------+--------------------------+\n",
    "\n",
    "*******************************************************************\n",
    "20/03/16 17:51:36 INFO MongoClientCache: Closing MongoClient: [127.0.0.1:27017]\n",
    "20/03/16 17:51:36 INFO SparkContext: Invoking stop() from shutdown hook\n",
    "20/03/16 17:51:36 INFO connection: Closed connection [connectionId{localValue:2, serverValue:138}] to 127.0.0.1:27017 because the pool has been closed.\n",
    "20/03/16 17:51:36 INFO MongoClientCache: Closing MongoClient: [127.0.0.1:27017]\n",
    "20/03/16 17:51:36 INFO connection: Closed connection [connectionId{localValue:4, serverValue:140}] to 127.0.0.1:27017 because the pool has been closed.\n",
    "20/03/16 17:51:36 INFO SparkUI: Stopped Spark web UI at http://animal-mother.mgmt:4040\n",
    "20/03/16 17:51:36 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
    "\n",
    "            ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another core approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark Mongo Connector - Demo\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/database.collection\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/database.collection\") \\\n",
    "    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.11:2.4.5') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "df = spark.read.format(\"mongo\").load()\n",
    "# df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n",
    "df.printSchema()\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
