{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# Packaging ML Projects\n\nMachine learning projects need to produce both reusable code and reproducible results.  This lesson examines creating, organizing, and packaging machine learning projects with a focus on reproducibility and collaborating with a team.\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n - Introduce organizing code into projects\n - Package a basic project with parameters and an environment\n - Run a basic project locally and remotely using Github\n\n## Prerequisites\n- Web browser: Chrome\n- A cluster configured with **8 cores** and **DBR 6.2**"],"metadata":{}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Setup\n\nFor each lesson to execute correctly, please make sure to run the **`Classroom-Setup`** cell at the<br/>\nstart of each lesson (see the next cell) and the **`Classroom-Cleanup`** cell at the end of each lesson."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["<iframe  \nsrc=\"//fast.wistia.net/embed/iframe/2q71x71wrc?videoFoam=true\"\nstyle=\"border:1px solid #1cb1c2;\"\nallowtransparency=\"true\" scrolling=\"no\" class=\"wistia_embed\"\nname=\"wistia_embed\" allowfullscreen mozallowfullscreen webkitallowfullscreen\noallowfullscreen msallowfullscreen width=\"640\" height=\"360\" ></iframe>\n<div>\n<a target=\"_blank\" href=\"https://fast.wistia.net/embed/iframe/2q71x71wrc?seo=false\">\n  <img alt=\"Opens in new tab\" src=\"https://files.training.databricks.com/static/images/external-link-icon-16x16.png\"/>&nbsp;Watch full-screen.</a>\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### The Case for Packaging\n\nThere are a number of different reasons why teams need to package their machine learning projects:<br><br>\n\n1. Projects have various library dependencies so shipping a machine learning solution involves the environment in which it was built.  MLflow allows for this environment to be a conda environment or docker container.  This means that teams can easily share and publish their code for others to use.\n2. Machine learning projects become increasingly complex as time goes on.  This includes ETL and featurization steps, machine learning models used for pre-processing, and finally the model training itself.\n3. Each component of a machine learning pipeline needs to allow for tracing its lineage.  If there's a failure at some point, tracing the full end-to-end lineage of a model allows for easier debugging.\n\n**ML Projects is a specification for how to organize code in a project.**  The heart of this is an **MLproject file,** a YAML specification for the components of an ML project.  This allows for more complex workflows since a project can execute another project, allowing for encapsulation of each stage of a more complex machine learning architecture.  This means that teams can collaborate more easily using this architecture.\n\n<div><img src=\"https://files.training.databricks.com/images/eLearning/ML-Part-4/mlflow-project.png\" style=\"height: 400px; margin: 20px\"/></div>\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> See <a href=\"https://github.com/mlflow/mlflow-example\" target=\"_blank\">this example project backed by GitHub</a> for an example of how to integrate MLflow with source control."],"metadata":{}},{"cell_type":"markdown","source":["### Packaging a Simple Project\n\nFirst we're going to create a simple MLflow project consisting of the following elements:<br><br>\n\n1. MLProject file\n2. Conda environment\n3. Basic machine learning script\n\nWe're going to want to be able to pass parameters into this code so that we can try different hyperparameter options."],"metadata":{}},{"cell_type":"markdown","source":["Create a new experiment for this exercise.  Navigate to the UI in another tab."],"metadata":{}},{"cell_type":"code","source":["experimentPath = \"/Users/\" + username + \"/experiment-SPL3\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["import mlflow\n\nmlflow.set_experiment(experimentPath)\nmlflow_client = mlflow.tracking.MlflowClient()\nexperimentID = mlflow_client.get_experiment_by_name(name=experimentPath).experiment_id\n\nprint(f\"The experiment can be found at the path `{experimentPath}` and has an experiment_id of `{experimentID}`\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">INFO: &#39;/Users/tbresee@umich.edu/experiment-SPL3&#39; does not exist. Creating a new experiment\nThe experiment can be found at the path `/Users/tbresee@umich.edu/experiment-SPL3` and has an experiment_id of `1243748146715256`\n</div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["-sandbox\nFirst, examine the code we're going to run.  This looks similar to what we ran in the last lesson with the addition of decorators from the `click` library.  This allows us to parameterize our code.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> We'll uncomment out the `__main__` block when we save this code as a Python file.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Check out the <a href=\"https://click.palletsprojects.com/en/7.x/\" target=\"_blank\">`click` docs here.</a>"],"metadata":{}},{"cell_type":"code","source":["import click\nimport mlflow.sklearn\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\n@click.command()\n@click.option(\"--data_path\", default=\"/dbfs/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.csv\", type=str)\n@click.option(\"--n_estimators\", default=10, type=int)\n@click.option(\"--max_depth\", default=20, type=int)\n@click.option(\"--max_features\", default=\"auto\", type=str)\ndef mlflow_rf(data_path, n_estimators, max_depth, max_features):\n\n  with mlflow.start_run() as run:\n    # Import the data\n    df = pd.read_csv(data_path)\n    X_train, X_test, y_train, y_test = train_test_split(df.drop([\"price\"], axis=1), df[[\"price\"]].values.ravel(), random_state=42)\n    \n    # Create model, train it, and create predictions\n    rf = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features)\n    rf.fit(X_train, y_train)\n    predictions = rf.predict(X_test)\n\n    # Log model\n    mlflow.sklearn.log_model(rf, \"random-forest-model\")\n    \n    # Log params\n    mlflow.log_param(\"n_estimators\", n_estimators)\n    mlflow.log_param(\"max_depth\", max_depth)\n    mlflow.log_param(\"max_features\", max_features)\n\n    # Log metrics\n    mlflow.log_metric(\"mse\", mean_squared_error(y_test, predictions))\n    mlflow.log_metric(\"mae\", mean_absolute_error(y_test, predictions))  \n    mlflow.log_metric(\"r2\", r2_score(y_test, predictions))  \n\n# if __name__ == \"__main__\":\n#   mlflow_rf() # Note that this does not need arguments thanks to click"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["Test that it works using the `click` `CliRunner`, which will execute the code in the same way we expect to."],"metadata":{}},{"cell_type":"code","source":["from click.testing import CliRunner\n\nrunner = CliRunner()\nresult = runner.invoke(mlflow_rf, ['--n_estimators', 10, '--max_depth', 20], catch_exceptions=True)\n\nassert result.exit_code == 0, \"Code failed\" # Check to see that it worked\n\nprint(\"Success!\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Success!\n</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["Now create a directory to hold our project files.  This will be a unique directory for this lesson."],"metadata":{}},{"cell_type":"code","source":["# Adust our working directory from what DBFS sees to what python actually sees\nworking_path = workingDir.replace(\"dbfs:\", \"/dbfs\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["-sandbox\nCreate the `MLproject` file.  This is the heart of an MLflow project.  It includes pointers to the conda environment and a `main` entry point, which is backed by the file `train.py`.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Any `.py` or `.sh` file can be an entry point."],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.put(f\"{workingDir}/MLproject\", \n'''\nname: Lesson-3-Model-Training\n\nconda_env: conda.yaml\n\nentry_points:\n  main:\n    parameters:\n      data_path: {type: str, default: \"/dbfs/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.csv\"}\n      n_estimators: {type: int, default: 10}\n      max_depth: {type: int, default: 20}\n      max_features: {type: str, default: \"auto\"}\n    command: \"python train.py --data_path {data_path} --n_estimators {n_estimators} --max_depth {max_depth} --max_features {max_features}\"\n'''.strip())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Wrote 471 bytes.\nOut[18]: True</div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["-sandbox\nCreate the conda environment.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> You can also dynamically view and use a package version by calling `.__version__` on the package."],"metadata":{}},{"cell_type":"code","source":["import cloudpickle, numpy, pandas, sklearn\n\nfile_contents = f\"\"\"\nname: Lesson-03\nchannels:\n  - defaults\ndependencies:\n  - cloudpickle={cloudpickle.__version__}\n  - numpy={numpy.__version__}\n  - pandas={pandas.__version__}\n  - scikit-learn={sklearn.__version__}\n  - pip:\n    - mlflow=={mlflow.__version__}\n\"\"\".strip()\n\ndbutils.fs.put(f\"{workingDir}/conda.yaml\", file_contents, overwrite=True)\n\nprint(file_contents)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Wrote 162 bytes.\nname: Lesson-03\nchannels:\n  - defaults\ndependencies:\n  - cloudpickle=1.3.0\n  - numpy=1.16.2\n  - pandas=0.24.2\n  - scikit-learn=0.20.3\n  - pip:\n    - mlflow==1.8.0\n</div>"]}}],"execution_count":20},{"cell_type":"markdown","source":["Now create the code itself.  This is the same as above except for with the `__main__` is included.  Note how there are no arguments passed into `mlflow_rf()` on the final line.  `click` is handling the arguments for us."],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.put(f\"{workingDir}/train.py\", \n'''\nimport click\nimport mlflow.sklearn\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\n@click.command()\n@click.option(\"--data_path\", default=\"/dbfs/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.csv\", type=str)\n@click.option(\"--n_estimators\", default=10, type=int)\n@click.option(\"--max_depth\", default=20, type=int)\n@click.option(\"--max_features\", default=\"auto\", type=str)\ndef mlflow_rf(data_path, n_estimators, max_depth, max_features):\n\n  with mlflow.start_run() as run:\n    # Import the data\n    df = pd.read_csv(data_path)\n    X_train, X_test, y_train, y_test = train_test_split(df.drop([\"price\"], axis=1), df[[\"price\"]].values.ravel(), random_state=42)\n    \n    # Create model, train it, and create predictions\n    rf = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features)\n    rf.fit(X_train, y_train)\n    predictions = rf.predict(X_test)\n\n    # Log model\n    mlflow.sklearn.log_model(rf, \"random-forest-model\")\n    \n    # Log params\n    mlflow.log_param(\"n_estimators\", n_estimators)\n    mlflow.log_param(\"max_depth\", max_depth)\n    mlflow.log_param(\"max_features\", max_features)\n\n    # Log metrics\n    mlflow.log_metric(\"mse\", mean_squared_error(y_test, predictions))\n    mlflow.log_metric(\"mae\", mean_absolute_error(y_test, predictions))  \n    mlflow.log_metric(\"r2\", r2_score(y_test, predictions))  \n\nif __name__ == \"__main__\":\n  mlflow_rf() # Note that this does not need arguments thanks to click\n'''.strip(), True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Wrote 1610 bytes.\nOut[20]: True</div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["To summarize, you now have three files: `MLproject`, `conda.yaml`, and `train.py`"],"metadata":{}},{"cell_type":"code","source":["display( dbutils.fs.ls(workingDir) )"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/user/tbresee@umich.edu/mlflow/03_packaging_ml_projects_psp/MLproject</td><td>MLproject</td><td>471</td></tr><tr><td>dbfs:/user/tbresee@umich.edu/mlflow/03_packaging_ml_projects_psp/conda.yaml</td><td>conda.yaml</td><td>162</td></tr><tr><td>dbfs:/user/tbresee@umich.edu/mlflow/03_packaging_ml_projects_psp/train.py</td><td>train.py</td><td>1610</td></tr></tbody></table></div>"]}}],"execution_count":24},{"cell_type":"markdown","source":["-sandbox\n### Running Projects\n\nNow you have the three files we need to run the project, we can trigger the run.  We'll do this in a few different ways:<br><br>\n\n1. On the driver node of our Spark cluster\n2. On a new Spark cluster submitted as a job\n3. Using files backed by GitHub\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> This currently relies on environment variables.  [See the setup script for details.]($./Includes/MLflow)"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\nNow run the experiment.  This command will execute against the driver node of a Spark cluster, though it could be running locally or on a different remote VM.\n\nFirst set the experiment using the `experimentPath` defined earlier.  Prepend `/dbfs` to the file path, which allows the cluster's file system to access DBFS.  Then, pass your parameters.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> This will take a few minutes to build the environment for the first time.  Subsequent runs are faster since `mlflow` can reuse the same environment after it has been built."],"metadata":{}},{"cell_type":"code","source":["import mlflow\n\nmlflow.projects.run(uri=working_path,\n  parameters={\n    \"data_path\": \"/dbfs/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.csv\",\n    \"n_estimators\": 10,\n    \"max_depth\": 20,\n    \"max_features\": \"auto\"\n})"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[22]: &lt;mlflow.projects.submitted_run.LocalSubmittedRun at 0x7f011ae5beb8&gt;</div>"]}}],"execution_count":27},{"cell_type":"markdown","source":["Check the run in the UI.  Notice that you can see the run command.  **This is very helpful in debugging.**\n\nNow that it's working, experiment with other parameters."],"metadata":{}},{"cell_type":"code","source":["mlflow.projects.run(working_path,\n  parameters={\n    \"data_path\": \"/dbfs/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.csv\",\n    \"n_estimators\": 1000,\n    \"max_depth\": 10,\n    \"max_features\": \"log2\"\n})"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[26]: &lt;mlflow.projects.submitted_run.LocalSubmittedRun at 0x7f011abb4ef0&gt;</div>"]}}],"execution_count":29},{"cell_type":"markdown","source":["How did the new model do?"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\nNow try executing this code against a new Databricks cluster.  This needs cluster specifications in order for Databricks to know what kind of cluster to use.  Uncomment the following to run it.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/>  <a href=\"https://docs.databricks.com/api/latest/clusters.html\" target=\"_blank\">See the clusters API docs</a> to see how to define cluster specifications."],"metadata":{}},{"cell_type":"code","source":["### issue with   backend_config=clusterspecs ? "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":32},{"cell_type":"code","source":["clusterspecs = {\n    \"num_workers\": 2,\n    \"spark_version\": \"5.3.x-cpu-ml-scala2.11\",\n    \"node_type_id\": \"Standard_DS3_v2\",\n    \"driver_node_type_id\": \"Standard_DS3_v2\",\n}\n\nmlflow.projects.run(\n  uri=working_path,\n  parameters={\n    \"data_path\": \"/dbfs/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.csv\",\n    \"n_estimators\": 1500,\n    \"max_depth\": 5,\n    \"max_features\": \"sqrt\"\n},\n  backend=\"databricks\",\n  backend_config=clusterspecs\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">MlflowException</span>                           Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1243748146714023&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     15</span> },\n<span class=\"ansi-green-intense-fg ansi-bold\">     16</span>   backend<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#34;databricks&#34;</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-fg\">---&gt; 17</span><span class=\"ansi-red-fg\">   </span>backend_config<span class=\"ansi-blue-fg\">=</span>clusterspecs\n<span class=\"ansi-green-intense-fg ansi-bold\">     18</span> )\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/mlflow/projects/__init__.py</span> in <span class=\"ansi-cyan-fg\">run</span><span class=\"ansi-blue-fg\">(uri, entry_point, version, parameters, docker_args, experiment_name, experiment_id, backend, backend_config, use_conda, storage_dir, synchronous, run_id)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    289</span>         parameters<span class=\"ansi-blue-fg\">=</span>parameters<span class=\"ansi-blue-fg\">,</span> docker_args<span class=\"ansi-blue-fg\">=</span>docker_args<span class=\"ansi-blue-fg\">,</span> backend<span class=\"ansi-blue-fg\">=</span>backend<span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    290</span>         backend_config<span class=\"ansi-blue-fg\">=</span>cluster_spec_dict<span class=\"ansi-blue-fg\">,</span> use_conda<span class=\"ansi-blue-fg\">=</span>use_conda<span class=\"ansi-blue-fg\">,</span> storage_dir<span class=\"ansi-blue-fg\">=</span>storage_dir<span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-fg\">--&gt; 291</span><span class=\"ansi-red-fg\">         synchronous=synchronous, run_id=run_id)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    292</span>     <span class=\"ansi-green-fg\">if</span> synchronous<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    293</span>         _wait_for<span class=\"ansi-blue-fg\">(</span>submitted_run_obj<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/mlflow/projects/__init__.py</span> in <span class=\"ansi-cyan-fg\">_run</span><span class=\"ansi-blue-fg\">(uri, experiment_id, entry_point, version, parameters, docker_args, backend, backend_config, use_conda, storage_dir, synchronous, run_id)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    136</span>             remote_run<span class=\"ansi-blue-fg\">=</span>active_run<span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    137</span>             uri<span class=\"ansi-blue-fg\">=</span>uri<span class=\"ansi-blue-fg\">,</span> entry_point<span class=\"ansi-blue-fg\">=</span>entry_point<span class=\"ansi-blue-fg\">,</span> work_dir<span class=\"ansi-blue-fg\">=</span>work_dir<span class=\"ansi-blue-fg\">,</span> parameters<span class=\"ansi-blue-fg\">=</span>parameters<span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-fg\">--&gt; 138</span><span class=\"ansi-red-fg\">             experiment_id=experiment_id, cluster_spec=backend_config)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    139</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    140</span>     <span class=\"ansi-green-fg\">elif</span> backend <span class=\"ansi-blue-fg\">==</span> <span class=\"ansi-blue-fg\">&#34;local&#34;</span> <span class=\"ansi-green-fg\">or</span> backend <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/mlflow/projects/databricks.py</span> in <span class=\"ansi-cyan-fg\">run_databricks</span><span class=\"ansi-blue-fg\">(remote_run, uri, entry_point, work_dir, parameters, experiment_id, cluster_spec)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    286</span>     db_job_runner <span class=\"ansi-blue-fg\">=</span> DatabricksJobRunner<span class=\"ansi-blue-fg\">(</span>databricks_profile<span class=\"ansi-blue-fg\">=</span>profile<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    287</span>     db_run_id = db_job_runner.run_databricks(\n<span class=\"ansi-green-fg\">--&gt; 288</span><span class=\"ansi-red-fg\">         uri, entry_point, work_dir, parameters, experiment_id, cluster_spec, run_id)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    289</span>     submitted_run <span class=\"ansi-blue-fg\">=</span> DatabricksSubmittedRun<span class=\"ansi-blue-fg\">(</span>db_run_id<span class=\"ansi-blue-fg\">,</span> run_id<span class=\"ansi-blue-fg\">,</span> db_job_runner<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    290</span>     submitted_run<span class=\"ansi-blue-fg\">.</span>_print_description_and_log_tags<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/mlflow/projects/databricks.py</span> in <span class=\"ansi-cyan-fg\">run_databricks</span><span class=\"ansi-blue-fg\">(self, uri, entry_point, work_dir, parameters, experiment_id, cluster_spec, run_id)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    196</span>         <span class=\"ansi-red-fg\"># Launch run on Databricks</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    197</span>         command <span class=\"ansi-blue-fg\">=</span> _get_databricks_run_cmd<span class=\"ansi-blue-fg\">(</span>dbfs_fuse_uri<span class=\"ansi-blue-fg\">,</span> run_id<span class=\"ansi-blue-fg\">,</span> entry_point<span class=\"ansi-blue-fg\">,</span> parameters<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">--&gt; 198</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_run_shell_command_job<span class=\"ansi-blue-fg\">(</span>uri<span class=\"ansi-blue-fg\">,</span> command<span class=\"ansi-blue-fg\">,</span> env_vars<span class=\"ansi-blue-fg\">,</span> cluster_spec<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    199</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    200</span>     <span class=\"ansi-green-fg\">def</span> _get_status<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> databricks_run_id<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/mlflow/projects/databricks.py</span> in <span class=\"ansi-cyan-fg\">_run_shell_command_job</span><span class=\"ansi-blue-fg\">(self, project_uri, command, env_vars, cluster_spec)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    181</span>             <span class=\"ansi-blue-fg\">&#34;libraries&#34;</span><span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">{</span><span class=\"ansi-blue-fg\">&#34;pypi&#34;</span><span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">{</span><span class=\"ansi-blue-fg\">&#34;package&#34;</span><span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">&#34;&#39;mlflow&lt;=%s&#39;&#34;</span> <span class=\"ansi-blue-fg\">%</span> VERSION<span class=\"ansi-blue-fg\">}</span><span class=\"ansi-blue-fg\">}</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    182</span>         }\n<span class=\"ansi-green-fg\">--&gt; 183</span><span class=\"ansi-red-fg\">         </span>run_submit_res <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jobs_runs_submit<span class=\"ansi-blue-fg\">(</span>req_body_json<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    184</span>         databricks_run_id <span class=\"ansi-blue-fg\">=</span> run_submit_res<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;run_id&#34;</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    185</span>         <span class=\"ansi-green-fg\">return</span> databricks_run_id\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/mlflow/projects/databricks.py</span> in <span class=\"ansi-cyan-fg\">_jobs_runs_submit</span><span class=\"ansi-blue-fg\">(self, req_body)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     78</span>     <span class=\"ansi-green-fg\">def</span> _jobs_runs_submit<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> req_body<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     79</span>         response = self._databricks_api_request(\n<span class=\"ansi-green-fg\">---&gt; 80</span><span class=\"ansi-red-fg\">             endpoint=&#34;/api/2.0/jobs/runs/submit&#34;, method=&#34;POST&#34;, json=req_body)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">     81</span>         <span class=\"ansi-green-fg\">return</span> json<span class=\"ansi-blue-fg\">.</span>loads<span class=\"ansi-blue-fg\">(</span>response<span class=\"ansi-blue-fg\">.</span>text<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     82</span> \n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/mlflow/projects/databricks.py</span> in <span class=\"ansi-cyan-fg\">_databricks_api_request</span><span class=\"ansi-blue-fg\">(self, endpoint, method, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     74</span>         host_creds <span class=\"ansi-blue-fg\">=</span> databricks_utils<span class=\"ansi-blue-fg\">.</span>get_databricks_host_creds<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>databricks_profile<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     75</span>         return rest_utils.http_request_safe(\n<span class=\"ansi-green-fg\">---&gt; 76</span><span class=\"ansi-red-fg\">             host_creds=host_creds, endpoint=endpoint, method=method, **kwargs)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">     77</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     78</span>     <span class=\"ansi-green-fg\">def</span> _jobs_runs_submit<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> req_body<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/mlflow/utils/rest_utils.py</span> in <span class=\"ansi-cyan-fg\">http_request_safe</span><span class=\"ansi-blue-fg\">(host_creds, endpoint, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     93</span>     Wrapper around<span class=\"ansi-red-fg\"> </span><span class=\"ansi-red-fg\">`</span><span class=\"ansi-red-fg\">`</span>http_request<span class=\"ansi-red-fg\">`</span><span class=\"ansi-red-fg\">`</span> that also verifies that the request succeeds <span class=\"ansi-green-fg\">with</span> code <span class=\"ansi-cyan-fg\">200.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     94</span>     &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">---&gt; 95</span><span class=\"ansi-red-fg\">     </span>response <span class=\"ansi-blue-fg\">=</span> http_request<span class=\"ansi-blue-fg\">(</span>host_creds<span class=\"ansi-blue-fg\">=</span>host_creds<span class=\"ansi-blue-fg\">,</span> endpoint<span class=\"ansi-blue-fg\">=</span>endpoint<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kwargs<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     96</span>     <span class=\"ansi-green-fg\">return</span> verify_rest_response<span class=\"ansi-blue-fg\">(</span>response<span class=\"ansi-blue-fg\">,</span> endpoint<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     97</span> \n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/mlflow/utils/rest_utils.py</span> in <span class=\"ansi-cyan-fg\">http_request</span><span class=\"ansi-blue-fg\">(host_creds, endpoint, retries, retry_interval, max_rate_limit_interval, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     78</span>             time<span class=\"ansi-blue-fg\">.</span>sleep<span class=\"ansi-blue-fg\">(</span>retry_interval<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     79</span>     raise MlflowException(&#34;API request to %s failed to return code 200 after %s tries&#34; %\n<span class=\"ansi-green-fg\">---&gt; 80</span><span class=\"ansi-red-fg\">                           (url, retries))\n</span><span class=\"ansi-green-intense-fg ansi-bold\">     81</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     82</span> \n\n<span class=\"ansi-red-fg\">MlflowException</span>: API request to https://community.cloud.databricks.com/api/2.0/jobs/runs/submit failed to return code 200 after 3 tries</div>"]}}],"execution_count":33},{"cell_type":"markdown","source":["Finally, run this example, which is <a href=\"https://github.com/mlflow/mlflow-example\" target=\"_blank\">a project backed by GitHub.</a>"],"metadata":{}},{"cell_type":"code","source":["mlflow.run(\n  uri=\"https://github.com/mlflow/mlflow-example\",\n  parameters={'alpha':0.4}\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[28]: &lt;mlflow.projects.submitted_run.LocalSubmittedRun at 0x7f011a403cc0&gt;</div>"]}}],"execution_count":35},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["## Review\n\n**Question:** Why is packaging important?  \n**Answer:** Packaging not only manages your code but the environment in which it was run.  This environment can be a Conda or Docker environment.  This ensures that you have reproducible code and models that can be used in a number of downstream environments.\n\n**Question:** What are the core components of MLflow projects?  \n**Answer:** An MLmodel specifies the project components using YAML.  The environment file contains specifics about the environment.  The code itself contains the steps to create a model or process data.\n\n**Question:** What code can I run and where can I run it?  \n**Answer:** Arbitrary code can be run in any number of different languages.  It can be run locally or remotely, whether on a remote VM, Spark cluster, or submitted as a Databricks job."],"metadata":{}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Cleanup<br>\n\nRun the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Cleanup\""],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Next Steps\n\nStart the labs for this lesson, [Packaging ML Projects Lab]($./Labs/03-Lab)"],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n**Q:** Where can I find out more information on MLflow Projects?  \n**A:** Check out the <a href=\"https://www.mlflow.org/docs/latest/projects.html\" target=\"_blank\">MLflow docs</a>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"03-Packaging-ML-Projects","notebookId":1243748146713991},"nbformat":4,"nbformat_minor":0}
