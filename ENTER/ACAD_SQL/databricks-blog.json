{"status": "publish", "description": null, "creator": "roy", "link": "https://databricks.com/blog/2014/04/10/mapr-integrates-spark-stack.html", "authors": ["Tomer Shiran (VP of Product Management at MapR)"], "id": 33, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2014-04-10", "tz": "UTC", "createdOn": "2014-04-10"}, "title": "MapR Integrates the Complete Apache Spark Stack", "slug": "mapr-integrates-spark-stack", "content": "<div class=\"post-meta\">This post is guest authored by our friends at MapR, announcing our new partnership to provide enterprise support for Apache Spark as part of MapR's Distribution of Hadoop.</div>\n\n<hr />\n\nWith over 500 paying customers, my team and I have the opportunity to talk to many organizations that are leveraging Hadoop in production to extract value from big data. One of the most common topics raised by our customers in recent months is Apache Spark. Some customers just want to learn more about the advantages of this technology and the use cases that it addresses, while others are already running it in production with the MapR Distribution. These customers range from the world\u2019s largest cable telcos and retailers to Silicon Valley startups such as Quantifind, which recently talked about its use of Spark on MapR in an <a href=\"http://www.datameer.com/ceoblog/big-data-brews-with-erich-nachbar/\" target=\"_blank\">interview</a> with Stefan Groschupf, CEO of Datameer.\n\nToday, I am happy to <a href=\"http://www.businesswire.com/news/home/20140410005101/en/MapR-Adds-Complete-Apache-Spark-Stack-Distribution#.U0a0G61dXKI\" target=\"_blank\">announce</a> and share with you the beginning of our journey with Databricks, and the addition of the complete Spark stack to the MapR Distribution for Apache Hadoop. We are now the only Hadoop distribution to support the complete Spark stack, including Spark, Spark Streaming (stream processing), Shark (Hive on Spark), MLLib (machine learning) and GraphX (graph processing). This is a testament to our commitment to open source and to providing our customers with maximum flexibility to pick and choose the right tool for the job.\n<h2 id=\"why-spark\">Why Spark?</h2>\nOne of the challenges organizations face when adopting Hadoop is a shortage of developers who have experience building Hadoop applications. Our professional services organization has helped dozens of companies with the development and deployment of Hadoop applications, and our training department has trained countless engineers. Organizations are hungry for solutions that make it easier to develop Hadoop applications while increasing developer productivity, and Spark fits this bill. Spark jobs can require as little as 1/5th of code. Spark provides a simple programming abstraction allowing developers to design applications as operations on data collections (known as RDDs, or Resilient Distributed Datasets). Developers can build these applications in multiple programming languages, including Java, Scala and Python, and the same code can be reused across batch, interactive and streaming applications.\n\nIn addition to making developers happier and more productive, Spark provides significant benefits with respect to end-to-end application performance. To this end, Spark provides a general-purpose execution framework with in-memory pipelining. For many applications, this results in a 5-100x performance improvement, because some or all steps can execute in memory without unnecessarily writing to and reading from disk. The performance advantage of the Spark engine, combined with the industry-leading performance of the MapR Distribution, provides customers with the highest-performance platform for big data applications.\n<h2 id=\"why-databricks\">Why Databricks?</h2>\nDatabricks was founded by the creators of Apache Spark, and is currently the driving force behind the project. When we decided to add the Spark stack to our distribution and double down on our involvement in the Spark community, a strategic partnership with Databricks was a no-brainer. This partnership will benefit MapR customers who are interested in 24x7 support for Spark or any of the other projects in the stack, including Spark Streaming, Shark, MLLib and GraphX (with several other projects coming soon). In addition, MapR will be working closely with Databricks to drive the Spark roadmap and accelerate the development of new features, benefiting both MapR customers and the broader community.\n\nWe are very excited about the upcoming Apache Spark 1.0 release, expected later this month. We are looking forward to a great journey with Databricks and the other members of the Spark community.\n\n<a href=\"http://w.on24.com/r.htm?e=780379&amp;s=1&amp;k=A6FB573A31B1DD9D8AB6294A101383ED&amp;partnerref=MapR\" target=\"_blank\">Register for an upcoming joint webinar</a> to learn more about the benefits of the complete Spark stack on MapR."}
{"status": "publish", "description": null, "creator": "tdas", "link": "https://databricks.com/blog/2014/04/09/spark-0_9_1-released.html", "authors": ["Tathagata Das"], "id": 35, "categories": ["Apache Spark", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2014-04-10", "tz": "UTC", "createdOn": "2014-04-10"}, "title": "Apache Spark 0.9.1 Released", "slug": "spark-0_9_1-released", "content": "We are happy to announce the availability of <a href=\"http://spark.apache.org/releases/spark-release-0-9-1.html\" target=\"_blank\">Apache Spark 0.9.1</a>! This is a maintenance release with bug fixes, performance improvements, better stability with YARN and improved parity of the Scala and Python API. We recommend all 0.9.0 users to upgrade to this stable release.\n\nThis is the first release since Spark graduated as a top level Apache project. Contributions to this release came from 37 developers.\n\nVisit the <a href=\"http://spark.apache.org/releases/spark-release-0-9-1.html\" target=\"_blank\">release notes</a> for more information about all the improvements and bug fixes. <a href=\"http://spark.apache.org/downloads.html\" target=\"_blank\">Download</a> it and try it out!"}
{"status": "publish", "description": null, "creator": "roy", "link": "https://databricks.com/blog/2014/03/31/application-spotlight-alpine.html", "authors": ["Steven Hillion"], "id": 37, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2014-04-01", "tz": "UTC", "createdOn": "2014-04-01"}, "title": "Application Spotlight: Alpine Data Labs", "slug": "application-spotlight-alpine", "content": "<div class=\"post-meta\">This post is guest authored by our friends at Alpine Data Labs, part of the 'Application Spotlight' series highlighting innovative applications that are part of the Databricks \"Certified on Apache Spark\" program.</div>\n\n<hr />\n\nEveryone knows how hard it is to recruit engineers and data scientists in Silicon Valley. At <a href=\"http://www.alpinenow.com\" target=\"_blank\">Alpine Data Labs</a>, we think what we\u2019re up to is pretty fun and challenging, but we still have to compete with other start-ups as well as the big internet companies to attract the best talent. One thing that can help is to be able to say that you\u2019re working with the most innovative and powerful technologies.\n\nLast year, I was interviewing a talented engineer with a strong background in machine learning. And he said that the one thing he wanted to do above all was to work with Apache Spark. \u201cWill I get to do that at Alpine?\u201d he asked.\n\nIf it had been even a year earlier, I would have said \u201cSure\u2026at some point.\u201d But in the meantime I\u2019d met several of the members of the <a href=\"https://amplab.cs.berkeley.edu\" target=\"_blank\">AMPLab</a> research team at Berkeley, and been impressed with their mature approach to building a platform and ecosystem. And I\u2019d seen enough companies installing Spark on their dev clusters that it was clear this was a technology to watch. In a remarkably short time, it went from experimental to very real. And now prospects in the Alpine pipeline were asking me if it was on the roadmap. So yes, I told my candidate. \u201cYou\u2019ll be working on Spark from day one.\u201d\n\nLast week, Alpine announced at <a href=\"http://www.cmswire.com/cms/big-data/alpine-turns-big-data-to-a-priceless-asset-fast-gigaomlive-024541.php\" target=\"_blank\">GigaOM</a> that it\u2019s one of the first analytics companies to leverage Spark for building predictive models. We demonstrated the Alpine engine running on <a href=\"http://www.gopivotal.com/big-data/analytics-workbench\" target=\"_blank\">Pivotal\u2019s Analytics Workbench</a>, where it ran an iterative classification algorithm (logistic regression) on 50 million rows in less than 50 seconds.\n\nFurthermore, we were officially certified on Spark by the team at Databricks. It\u2019s been an honor to work with them and the research team at Berkeley. We think their technology will be a serious contender for the leading platform for data science.\n\nSpark is more to us than just speed. It\u2019s really the entire ecosystem that represents such an exciting paradigm for working with data.\n\nStill, the core capability of caching data in memory was our primary consideration, and our iterative algorithms have been shown to speed up by one or even two orders of magnitude (thanks again to that Pivotal cluster).\n\nWe\u2019ve always had this mantra at Alpine: \u201cAvoid multiple passes through the data!\u201d And we\u2019ve designed many of our machine learning algorithms to avoid scanning the data too many times, packing on calculations into each MapReduce job like a waiter piling up plates to try and clear a table in one go. But it\u2019s rare that we can avoid it entirely. With Spark, it\u2019s incredibly satisfying to watch the progress bar zip along as the system re-uses data it\u2019s already seen before.\n\nAnother thing that\u2019s getting our engineers excited is Spark\u2019s <a href=\"http://spark.apache.org/mllib/\" target=\"_blank\">MLLib</a>, the machine-learning library written on top of the Spark runtime. Alpine has long thought that machine learning algorithms should be open source. (I helped to kick off the <a href=\"http://madlib.net/\" target=\"_blank\">MADlib</a> library of analytics functions for databases, and Alpine now uses it extensively.) So we\u2019re now beginning to contribute some of our code back into MLLib. And, moreover, we think MLLib and MLI have the potential to be a more general repository for open-source machine learning.\n\nSo I\u2019ll congratulate the Alpine team for helping to bring the power of Spark to our users, and I\u2019ll also congratulate the Spark team and Databricks for making it possible!"}
{"status": "publish", "description": null, "creator": "michael", "link": "https://databricks.com/blog/2014/03/26/spark-sql-manipulating-structured-data-using-spark-2.html", "authors": ["Michael Armbrust", "Reynold Xin"], "id": 42, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2014-03-27", "tz": "UTC", "createdOn": "2014-03-27"}, "title": "Spark SQL: Manipulating Structured Data Using Apache Spark", "slug": "spark-sql-manipulating-structured-data-using-spark-2", "content": "Building a unified platform for big data analytics has long been the vision of Apache Spark, allowing a single program to perform ETL, MapReduce, and complex analytics. An important aspect of unification that our users have consistently requested is the ability to more easily import data stored in external sources, such as Apache Hive. Today, we are excited to announce <a href=\"https://spark.apache.org/docs/latest/sql-programming-guide.html\">Spark SQL</a>, a new component recently merged into the Spark repository.\n\nSpark SQL brings native support for SQL to Spark and streamlines the process of querying data stored both in RDDs (Spark\u2019s distributed datasets) and in external sources. Spark SQL conveniently blurs the lines between RDDs and relational tables. Unifying these powerful abstractions makes it easy for developers to intermix SQL commands querying external data with complex analytics, all within in a single application. Concretely, Spark SQL will allow developers to:\n<ul>\n \t<li>Import relational data from Parquet files and Hive tables</li>\n \t<li>Run SQL queries over imported data and existing RDDs</li>\n \t<li>Easily write RDDs out to Hive tables or Parquet files</li>\n</ul>\n<h2 id=\"spark-sql-in-action\">Spark SQL In Action</h2>\nNow, let\u2019s take a closer look at how Spark SQL gives developers the power to integrate SQL commands into applications that also take advantage of MLlib, Spark\u2019s machine learning library. Consider an application that needs to predict which users are likely candidates for a service, based on their profile. Often, such an analysis requires joining data from multiple sources. For the purposes of illustration, imagine an application with two tables:\n<ul>\n \t<li>Users(userId INT, name String, email STRING,\nage INT, latitude: DOUBLE, longitude: DOUBLE,\nsubscribed: BOOLEAN)</li>\n \t<li>Events(userId INT, action INT)</li>\n</ul>\nGiven the data stored in in these tables, one might want to build a model that will predict which users are good targets for a new campaign, based on users that are similar.\n\n[scala]\n// Data can easily be extracted from existing sources,\n// such as Apache Hive.\nval trainingDataTable = sql(&quot;&quot;&quot;\n  SELECT e.action\n         u.age,\n         u.latitude,\n         u.logitude\n  FROM Users u\n  JOIN Events e\n  ON u.userId = e.userId&quot;&quot;&quot;)\n\n// Since `sql` returns an RDD, the results of the above\n// query can be easily used in MLlib\nval trainingData = trainingDataTable.map { row =&gt;\n  val features = Array[Double](row(1), row(2), row(3))\n  LabeledPoint(row(0), features)\n}\n\nval model =\n  new LogisticRegressionWithSGD().run(trainingData)\n[/scala]\n\nNow that we have used SQL to join existing data and train a model, we can use this model to predict which users are likely targets.\n\n[scala]\nval allCandidates = sql(&quot;&quot;&quot;\n  SELECT userId,\n         age,\n         latitude,\n         logitude\n  FROM Users\n  WHERE subscribed = FALSE&quot;&quot;&quot;)\n\n// Results of ML algorithms can be used as tables\n// in subsequent SQL statements.\ncase class Score(userId: Int, score: Double)\nval scores = allCandidates.map { row =&gt;\n  val features = Array[Double](row(1), row(2), row(3))\n  Score(row(0), model.predict(features))\n}\nscores.registerAsTable(&quot;Scores&quot;)\n\nval topCandidates = sql(&quot;&quot;&quot;\n  SELECT u.name, u.email\n  FROM Scores s\n    JOIN Users u ON s.userId = u.userId\n  ORDER BY score DESC\n  LIMIT 100&quot;&quot;&quot;)\n\n// Send emails to top candidates to promote the service.\n[/scala]\n\nIn this example, Spark SQL made it easy to extract and join the various datasets preparing them for the machine learning algorithm. Since the results of Spark SQL are also stored in RDDs, interfacing with other Spark libraries is trivial. Furthermore, Spark SQL allows developers to close the loop, by making it easy to manipulate and join the output of these algorithms, producing the desired final result.\n\nTo summarize, the unified Spark platform gives developers the power to choose the right tool for the right job, without having to juggle multiple systems. If you would like to see more concrete examples of using Spark SQL please check out the <a href=\"http://spark.apache.org/docs/1.0.0/sql-programming-guide.html\">programming guide</a>.\n<h2 id=\"optimizing-with-catalyst\">Optimizing with Catalyst</h2>\nIn addition to providing new ways to interact with data, Spark SQL also brings a powerful new optimization framework called Catalyst. Using Catalyst, Spark can automatically transform SQL queries so that they execute more efficiently. The Catalyst framework allows the developers behind Spark SQL to rapidly add new optimizations, enabling us to build a faster system more quickly. In one recent example, we found an inefficiency in Hive group-bys that took an experienced developer an entire weekend and over 250 lines of code to fix; we were then able to make the same fix in Catalyst in only a few lines of code.\n<h2 id=\"future-of-shark\">Future of Shark</h2>\nThe natural question that arises is about the future of Shark. Shark was among the first systems that delivered up to 100X speedup over Hive. It builds on the Apache Hive codebase and achieves performance improvements by swapping out the physical execution engine part of Hive. While this approach enables Shark users to speed up their Hive queries without modification to their existing warehouses, Shark inherits the large, complicated code base from Hive that makes it hard to optimize and maintain. As Spark SQL matures, Shark will transition to using Spark SQL for query optimization and physical execution so that users can benefit from the ongoing optimization efforts within Spark SQL.\n\nIn short, we will continue to invest in Shark and make it an excellent drop-in replacement for Apache Hive. It will take advantage of the new Spark SQL component, and will provide features that complement it, such as Hive compatibility and the standalone SharkServer, which allows external tools to connect queries through JDBC/ODBC.\n<h2 id=\"whats-next\">What\u2019s next</h2>\nSpark SQL will be included in Spark 1.0 as an alpha component. However, this is only the beginning of better support for relational data in Spark, and this post only scratches the surface of Catalyst. Look for future blog posts on the following topics:\n<ul>\n \t<li>Generating custom bytecode to speed up expression evaluation</li>\n \t<li>Reading and writing data using other formats and systems, include Avro and HBase</li>\n \t<li>API support for using Spark SQL in Python and Java</li>\n</ul>"}
{"status": "publish", "description": null, "creator": "patrick", "link": "https://databricks.com/blog/2014/02/03/release-0_9_0.html", "authors": ["Patrick Wendell"], "id": 58, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2014-02-04", "tz": "UTC", "createdOn": "2014-02-04"}, "title": "Apache Spark 0.9.0 Released", "slug": "release-0_9_0", "content": "Our goal with Apache Spark is very simple: provide the best platform for computation on big data. We do this through both a powerful core engine and rich libraries for useful analytics tasks. Today, we are excited to announce the release of Apache Spark 0.9.0. This major release extends Spark\u2019s libraries and further improves its performance and usability. Apache Spark 0.9.0 is the largest release to date, with work from 83 contributors, who submitted over 300 patches.\n\nApache Spark 0.9 features significant extensions to the set of standard analytical libraries packaged with Spark. The release introduces GraphX, a library for graph computation that comes with implementations of several standard algorithms, such as PageRank. Spark\u2019s machine learning library (MLlib) has been extended to support Python, using the NumPy numerical library. A Naive Bayes Classifier has also been added to MLlib. Finally, Spark Streaming, which supports near-real-time continuous computation, has added a simplified high-availability mode and several significant optimizations.\n\nIn addition to higher-level libraries, Spark 0.9 features improvements to the core computation engine. Spark now now automatically spills reduce output to disk, increasing the stability of workloads with very large aggregations. Support for Spark in YARN mode has been hardened and improved. The standalone mode has added automatic supervision of applications and better support for sharing clusters amongst several users. Finally, we\u2019ve focused on stabilizing API\u2019s ahead of Apache Spark\u2019s 1.0 release to make things easy for developers writing Spark applications. This includes upgrading to Scala 2.10, allowing applications written in Scala to use newer libraries.\n\nApache Spark 0.9.0 can be <a href=\"http://spark.incubator.apache.org/downloads.html\">downloaded directly</a> from the Apache Spark website. It will also be available to CDH users via a Cloudera parcel, which can automatically install Spark on existing CDH clusters. For a more detailed explanation of the features in this release, head on over to the <a href=\"http://spark.incubator.apache.org/releases/spark-release-0-9-0.html\">official release notes</a>. Enjoy the newest release of Spark!"}
{"status": "publish", "description": null, "creator": "ali", "link": "https://databricks.com/blog/2014/01/01/simr.html", "authors": ["Ali Ghodsi", "Ahir Reddy"], "id": 65, "categories": ["Apache Spark", "Ecosystem", "Engineering Blog"], "dates": {"publishedOn": "2014-01-02", "tz": "UTC", "createdOn": "2014-01-02"}, "title": "Apache Spark In MapReduce (SIMR)", "slug": "simr", "content": "Apache Hadoop integration has always been a key goal of Apache Spark and <a href=\"http://hortonworks.com/wp-content/uploads/2013/06/YARN.png\">YARN</a> users have long been able to run <a href=\"http://spark.incubator.apache.org/docs/latest/running-on-yarn.html\">Spark on YARN</a>. However, up to now, it has been relatively hard to run Spark on Hadoop MapReduce v1 clusters, i.e. clusters that do not have YARN installed. Typically, users would have to get permission to install Spark/Scala on some subset of the machines, a process that could be time consuming. Enter <a href=\"http://databricks.github.io/simr/\">SIMR (Spark In MapReduce)</a>, which has been released in conjunction with <a href=\"https://databricks.com/blog/2013/12/19/release-0_8_1.html\">Apache Spark 0.8.1</a>.\n\nSIMR allows anyone with access to a Hadoop MapReduce v1 cluster to run Spark out of the box. A user can run Spark directly on top of Hadoop MapReduce v1 without any administrative rights, and without having Spark or Scala installed on any of the nodes. The only requirements are HDFS access and MapReduce v1. SIMR is open sourced under the <a href=\"http://apache.org/licenses/LICENSE-2.0.html\">Apache license</a> and was jointly developed by Databricks and UC Berkeley <a href=\"http://amplab.cs.berkeley.edu\">AMPLab</a>.\n\nThe basic idea is that a user can download the package of SIMR (<a href=\"http://databricks.github.io/simr/#download\">3 files</a>) that matches their Hadoop cluster and immediately start using Spark. SIMR includes the interactive Spark shell, and allows users to use the shell backed by the computational power of the cluster. It is a simple as <code>./simr --shell</code>:\n\n&nbsp;\n\n<img src=\"https://databricks.com/wp-content/uploads/2014/01/simrshell.png\" alt=\"simrshell\" width=\"90%\" />\n\nRunning a Spark program simply requires bundling it along with its dependencies into a jar and launching the job through SIMR. SIMR uses the following command line syntax for running jobs:\n\n<pre>./simr jar_file main_class parameters</pre>\n\nSIMR simply launches a MapReduce job with the desired number of map slots, and ensures that Spark/Scala and your job gets shipped to all those nodes. It then designates one of the mappers as the master and runs the Spark driver inside it. On the rest of the mappers it launches Spark executors that will execute tasks on behalf of the driver. Voila, your Spark job is running inside MapReduce on the cluster.\n\nSIMR lets users interact with the driver program. For example, users can type into the Spark shell and see its output interactively. The way this works is that SIMR runs a relay server on the master mapper and a relay client on the machine that launched SIMR. Any input from the client and output from the driver are relayed back and forth between the client and the master mapper.\n\nTo make all this work, SIMR makes extensive use of HDFS. The master mapper is elected using leader election by writing to HDFS and picking the mapper that firsts gets to write to HDFS. Similarly, the executors launched inside the rest of the mappers find out the driver\u2019s URL by reading it from a specific file from HDFS. Thus, SIMR uses MapReduce and HDFS in place of a cluster manager.\n\n<img src=\"https://databricks.com/wp-content/uploads/2014/01/simr-arch.png\" alt=\"simr-arch\" width=\"90%\" />\n\nSIMR is not intended to be used in production mode, but rather to enable users to explore and use Spark before running it on a proper resource manager, such as YARN, Mesos, or Standalone Mode. MapReduce 2 (YARN) users can of course use the existing <a href=\"http://spark.incubator.apache.org/docs/latest/running-on-yarn.html\">Spark on YARN</a> solution, or explore other <a href=\"http://spark.incubator.apache.org/docs/latest/index.html#launching-on-a-cluster\">Spark deployment options</a>.\n\nWe hope SIMR will enable users to try out Spark without any heavy operational burden. Towards this goal, we have pre-built several SIMR binaries for different versions of Hadoop. Please go ahead and try it and let us know if you have any feedback.\n\nSIMR resources:\n<ul>\n \t<li><a href=\"http://databricks.github.io/simr\">Homepage</a></li>\n \t<li><a href=\"http://databricks.github.io/simr#download\">Download</a></li>\n \t<li><a href=\"https://github.com/databricks/simr\">Source code</a></li>\n</ul>"}
{"status": "publish", "description": null, "creator": "roy", "link": "https://databricks.com/blog/2014/03/25/sharethrough-and-spark-streaming.html", "authors": ["Russell Cardullo (Data Infrastructure Engineer at Sharethrough)", "Michael Ruggiero (Data Infrastructure Engineer at Sharethrough)"], "id": 2409, "categories": ["Company Blog", "Customers"], "dates": {"publishedOn": "2014-03-26", "tz": "UTC", "createdOn": "2014-03-26"}, "title": "Sharethrough Uses Apache Spark Streaming to Optimize Bidding in Real Time", "slug": "sharethrough-and-spark-streaming", "content": "<div class=\"post-meta\">We're very happy to see our friends at Cloudera continue to get the word out about Apache Spark, and their latest blog post is a great example of how users are putting Spark Streaming to use to solve complex problems in real time. Thanks to Russell Cardullo and Michael Ruggiero, Data Infrastructure Engineers at <a href=\"http://engineering.sharethrough.com/\">Sharethrough</a>, for this <a href=\"http://blog.cloudera.com/blog/2014/03/letting-it-flow-with-spark-streaming/\">guest post on Cloudera's blog</a>, which we've cross-posted below</div>\n\n<hr />\n\nAt Sharethrough, which offers an advertising exchange for delivering in-feed ads, we\u2019ve been running on CDH for the past three years (after migrating from Amazon EMR), primarily for ETL. With the launch of our exchange platform in early 2013 and our desire to optimize content distribution in real time, our needs changed, yet CDH remains an important part of our infrastructure.\n\nIn mid-2013, we began to examine stream-based approaches to accessing click-stream data from our pipeline. We asked ourselves: Rather than \u201cwarm up our cold path\u201d by running those larger batches more frequently, can we give our developers a programmatic model and framework optimized for incremental, small batch processing, yet continue to rely on the Cloudera platform? Ideally, our engineering team focuses on the data itself, rather than worrying about details like consistency of state across the pipeline or fault recovery.\n<h2>Spark (and Spark Streaming)</h2>\n<a href=\"http://spark.incubator.apache.org/\">Apache Spark</a>\u00a0is a fast and general framework for large-scale data processing, with a programming model that supports building applications that would be more complex or less feasible using conventional MapReduce. (Spark ships inside Cloudera Enterprise 5, and is already supported for use with CDH 4.4 and later.) With an in-memory persistent storage abstraction, Spark supports complete MapReduce functionality without the long execution times required by things like data replication, disk I/O, and serialization.\n\nBecause <a href=\"http://spark.incubator.apache.org/streaming/\">Spark Streaming</a> shares the same API as Spark\u2019s batch and interactive modes, we now use Spark Streaming to aggregate business-critical data in real time. A consistent API means that we can develop and test locally in the less complex batch mode and have that job work seamlessly in production streaming. For example, we can now optimize bidding in real time, using the entire dataset for that campaign without waiting for our less frequently run ETL flows to complete. We are also able to perform real-time experiments and measure results as they come in.\n<h2>Before and After</h2>\nOur batch-processing system looks like this:\n<ol>\n \t<li>Apache Flume writes out files based on optimal HDFS block size (64MB) to hourly buckets.</li>\n \t<li>MapReduce (Scalding) jobs are scheduled N times per day.</li>\n \t<li>Apache Sqoop moves results into the data warehouse.</li>\n \t<li>Latency is ~1 hour behind, plus Hadoop processing time.</li>\n</ol>\n<img class=\"alignleft size-full wp-image-152\" src=\"https://databricks.com/wp-content/uploads/2014/03/spark-streaming11.png\" alt=\"spark-streaming11\" width=\"450px\" />\n\n<strong>Sharethrough\u2019s former batch-processing dataflow</strong>\n\nFor our particular use case, this batch-processing workflow wouldn\u2019t provide access to performance data while the results of those calculations would still be valuable. For example, knowing that a client\u2019s optimized content performance is 4.2 percent an hour after their daily budget is spent, means our advertisers aren\u2019t getting their money\u2019s worth, and our publishers aren\u2019t seeing the fill they need. Even when the batch jobs take minutes, a spike in traffic could slow down a given batch job, causing it to \u201cbump into\u201d newly launched jobs.\n\nFor these use cases, a streaming dataflow is the viable solution:\n<ol>\n \t<li>Flume writes out clickstream data to HDFS.</li>\n \t<li>Spark reads from HDFS at batch sizes of five seconds.</li>\n \t<li>Output to a key-value store, updating our predictive modeling.</li>\n</ol>\n<img class=\"alignleft size-full wp-image-152\" src=\"https://databricks.com/wp-content/uploads/2014/03/spark-streaming21.png\" alt=\"spark-streaming11\" width=\"450px\" />\n\n<strong>Sharethrough\u2019s new Spark Streaming-based dataflow</strong>\n\nIn this new model, our latency is only Spark processing time and the time it takes Flume to transmit files to HDFS\u037e in practice, this works out to be about five seconds.\n<h2>On the Journey</h2>\nWhen we began using Spark Streaming, we shipped quickly with minimal fuss. To get the most out of our new streaming jobs, we quickly adjusted to the Spark programming model.\n\nHere are some things we discovered along the way:\n<ul>\n \t<li>The profile of a 24 x 7 streaming app is different than an hourly batch job \u2014 you may need finer-grained alerting and more patience with repeated errors. And with a streaming application, good exception handling is your friend. (Be prepared to answer questions like: \u201cWhat if the Spark receiver is unavailable? Should the application retry? Should it forget data that was lost? Should it alert you?\u201d)</li>\n \t<li>Take time to validate output against the input. A stateful job that, for example, keeps a count of clicks, may return results you didn\u2019t expect in testing.</li>\n \t<li>Confirm that supporting objects are being serialized. The Scala DSL makes it easy to close over a non-serializable variable or reference. In our case, a GeoCoder object was not getting serialized and our app became very slow; it had to return to the driver program for the original, non-distributed object.</li>\n \t<li>The output of your Spark Streaming job is only as reliable as the queue that feeds Spark. If the producing queue drops, say, 1 percent of messages, you may need a periodic reconciliation strategy (such as merging your lossy \u201chot path\u201d with \u201ccold path\u201d persistent data). For these kinds of merges, the monoid abstraction can be helpful when you need certainty that associative calculations (counts, for example) are accurate and reliable. For more on this, see merge-able stores like Twitter\u2019s <a href=\"https://github.com/twitter/storehaus\">Storehaus</a> or Oscar Boykin\u2019s \u201c<a href=\"https://speakerdeck.com/johnynek/algebra-for-analytics\">Algebra for Analytics</a>\u201c.</li>\n</ul>\n<h2>Conclusion</h2>\nSharethrough Engineering intends to do a lot more with Spark Streaming. Our engineers can interactively craft an application, test it in batch, move it into streaming and it just works. We\u2019d encourage others interested in unlocking real-time processing to look at Spark Streaming. Because of the concise Spark API, engineers comfortable with MapReduce can build streaming applications today without having to learn a completely new programming model.\n\nSpark Streaming equips your organization with the kind of insights only available from up-to-the-minute data, either in the form of machine-learning algorithms or real-time dashboards: It\u2019s up to you!"}
{"status": "publish", "description": null, "creator": "matei", "link": "https://databricks.com/blog/2014/03/20/apache-spark-a-delight-for-developers.html", "authors": ["Jai Ranganathan", "Matei Zaharia"], "id": 2410, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2014-03-21", "tz": "UTC", "createdOn": "2014-03-21"}, "title": "Apache Spark: A Delight for Developers", "slug": "apache-spark-a-delight-for-developers", "content": "<div class=\"post-meta\">\n\nThis article was cross-posted in the <a href=\"http://blog.cloudera.com/blog/2014/03/apache-spark-a-delight-for-developers/\">Cloudera developer blog</a>.\n\n</div>\n<a href=\"http://spark.apache.org/\">Apache Spark</a> is well known today for its <a href=\"http://blog.cloudera.com/blog/2013/11/putting-spark-to-use-fast-in-memory-computing-for-your-big-data-applications/\">performance benefits</a> over MapReduce, as well as its <a href=\"http://blog.cloudera.com/blog/2014/03/why-apache-spark-is-a-crossover-hit-for-data-scientists/\">versatility</a>. However, another important benefit \u2014 the elegance of the development experience \u2014 gets less mainstream attention.\n\nIn this post, you\u2019ll learn just a few of the features in Spark that make development purely a pleasure.\n<h2>Language Flexibility</h2>\nSpark natively provides support for a variety of popular development languages. Out of the box, it supports Scala, Java, and Python, with some promising work ongoing <a href=\"http://amplab-extras.github.io/SparkR-pkg/\">to support R</a>.\n\nOne common element among these languages (with the temporary exception of Java, which is due for a major update imminently in the form of Java 8) is that they all provide concise ways to express operations using \u201cclosures\u201d and lambda functions. <a href=\"http://en.wikipedia.org/wiki/Closure_(computer_programming)\">Closures</a> allow users to define functions in-line with the core logic of the application, thereby preserving application flow and making for tight and easy-to-read code:\n\n<strong>Closures in Python with Spark:</strong>\n\n<pre>lines = sc.textFile(...)\nlines.filter(lambda s:\"ERROR\"in s).count()</pre>\n\n<strong>Closures in Scala with Spark:</strong>\n\n<pre>val lines = sc.textFile(...)\nlines.filter(s => s.contains(\"ERROR\")).count()</pre>\n\n<strong>Closures in Java with Spark:</strong>\n\n<pre>JavaRDD<String> lines = sc.textFile(...);\nlines.filter(newFunction<String,Boolean>()  {\n  Boolean call(String s){\n    return s.contains(\"error\");\n  }\n}).count();</pre>\n\nOn the performance front, a lot of work has been done to optimize all three of these languages to run efficiently on the Spark engine. Spark is written in Scala, which runs on the JVM, so Java can run efficiently in the same JVM container. Via the smart use of <a href=\"http://py4j.sourceforge.net/\">Py4J</a>, the overhead of Python accessing memory that is managed in Scala is also minimal.\n<h2>APIs That Match User Goals</h2>\nWhen developing in MapReduce, you are often forced to stitch together basic operations as custom Mapper/Reducer jobs because there are no built-in features to simplify this process. For that reason, many developers turn to the higher-level APIs offered by frameworks like Apache Crunch or Cascading to write their MapReduce jobs.\n\nIn contrast, Spark natively provides a rich and ever-growing library of operators. Spark APIs include functions for:\n\n<ul>\n<li><code>cartesian</code></li>\n<li><code>cogroup</code></li>\n<li><code>collect</code></li>\n<li><code>count</code></li>\n<li><code>countByValue</code></li>\n<li><code>distinct</code></li>\n<li><code>filter</code></li>\n<li><code>flatMap</code></li>\n<li><code>fold</code></li>\n<li><code>groupByKey</code></li>\n<li><code>join</code></li>\n<li><code>map</code></li>\n<li><code>mapPartitions</code></li>\n<li><code>reduce</code></li>\n<li><code>reduceByKey</code></li>\n<li><code>sample</code></li>\n<li><code>sortByKey</code></li>\n<li><code>subtract</code></li>\n<li><code>take</code></li>\n<li><code>union</code></li>\n</ul>\n\nand <a href=\"https://spark.apache.org/docs/0.9.0/api/core/#org.apache.spark.rdd.RDD\">many more.</a> In fact, there are more than 80 operators available out of the box in Spark!\n\nWhile many of these operations often boil down to Map/Reduce equivalent operations, the high-level API matches user intentions closely, allowing you to write much more concise code.\n\nAn important note here is that while scripting frameworks like Apache Pig provide many high-level operators as well, Spark allows you to access these operators in the context of a full programming language \u2014 thus, you can use control statements, functions, and classes as you would in a typical programming environment.\n<h2>Automatic Parallelization of Complex Flows</h2>\nWhen constructing a complex pipeline of MapReduce jobs, the task of correctly parallelizing the sequence of jobs is left to you. Thus, a scheduler tool such as Apache Oozie is often required to carefully construct this sequence.\n\nWith Spark, a whole series of individual tasks is expressed as a single program flow that is lazily evaluated so that the system has a complete picture of the execution graph. This approach allows the core scheduler to correctly map the dependencies across different stages in the application, and automatically parallelize the flow of operators without user intervention.\n\nThis capability also has the property of enabling certain optimizations to the engine while reducing the burden on the application developer. Win, and win again!\n\nFor example, consider the following job:\n\n<pre>rdd1.map(splitlines).filter(\"ERROR\")\nrdd2.map(splitlines).groupBy(key)\nrdd2.join(rdd1, key).take(10)</pre>\n\n<p align=\"center\"><img style=\"width: 90%; height: auto;\" src=\"https://databricks.com/wp-content/uploads/2014/03/spark-devs1.png\" alt=\"spark-devs1\" /></p>\n\nThis simple application expresses a complex flow of six stages. But the actual flow is completely hidden from the user \u2014 the system automatically determines the correct parallelization across stages and constructs the graph correctly. In contrast, alternate engines would require you to manually construct the entire graph as well as indicate the proper parallelization.\n\n<h2>Interactive Shell</h2>\n\nSpark also lets you access your datasets through a simple yet specialized Spark shell for Scala and Python. With the Spark shell, developers and users can get started accessing their data and manipulating datasets without the full effort of writing an end-to-end application. Exploring terabytes of data without compiling a single line of code means you can understand your application flow by literally test-driving your program before you write it up.\n\n<p align=\"center\"><img style=\"width: 90%; height: auto;\" src=\"https://databricks.com/wp-content/uploads/2014/03/spark-devs21.png\" alt=\"spark-devs21\" /></p>\n\nJust open up a shell, type a few commands, and you\u2019re off to the races!\n\n<h2>Performance</h2>\n\nWhile this post has focused on how Spark not only improves performance but also programmability, we should\u2019t ignore one of the best ways to make developers more efficient: performance!\n\nDevelopers often have to run applications many times over the development cycle, working with subsets of data as well as full data sets to repeatedly follow the develop/test/debug cycle. In a Big Data context, each of these cycles can be very onerous, with each test cycle, for example, being hours long.\n\nWhile there are various ways systems to alleviate this problem, one of the best is to simply run your program fast. Thanks to the performance benefits of Spark, the development lifecycle can be materially shortened merely due to the fact that the test/debug cycles are much shorter.\n\nAnd your end-users will love you too!\n\n<p align=\"center\"><img style=\"width: 90%; height: auto;\" src=\"https://databricks.com/wp-content/uploads/2014/03/spark-dev3.png\" alt=\"spark-dev3\" /></p>\n\n<h2>Example: WordCount</h2>\n\nTo give you a sense of the practical impact of these benefits in a concrete example, the following two snippets of code reflect a WordCount implementation in MapReduce versus one in Spark. The difference is self-explanatory:\n\n<strong>WordCount the MapReduce way:</strong>\n\n<pre>public static class WordCountMapClass extends MapReduceBase\n  implements Mapper<LongWritable, Text, Text, IntWritable> {\n  private final static IntWritable one = newIntWritable(1);\n  private Text word = newText();\n  public void map(LongWritable key,Text value,\n                  OutputCollector <Text, IntWritable> output,\n                  Reporter reporter) throws IOException {\n    String line = value.toString();\n    StringTokenizer itr = newStringTokenizer(line);\n    while (itr.hasMoreTokens()) {\n      word.set(itr.nextToken());\n      output.collect(word, one);\n    }\n  }\n}\n\npublic static class WordCountReduce extends MapReduceBase\n  implements Reducer<Text, IntWritable, Text, IntWritable> {\n  public void reduce(Text key,Iterator<IntWritable> values,\n                     OutputCollector<Text,IntWritable> output,\n                     Reporter reporter) throws IOException{\n    int sum = 0;\n    while (values.hasNext()) {\n      sum += values.next().get();\n    }\n    output.collect(key,newIntWritable(sum));\n  }\n}</pre>\n\n<strong>WordCount the Spark way:</strong>\n\n<pre>val spark = newSparkContext(master, appName, home, jars)\nval file = spark.textFile(\"hdfs://...\")\nval counts = file.flatMap(line => line.split(\" \"))\n                 .map(word =>(word,1))\n                 .reduceByKey(_ + _)\ncounts.saveAsTextFile(\"hdfs://...\")</pre>\n\nOne cantankerous data scientist at Cloudera, Uri Laserson, wrote his first PySpark job recently after several years of tussling with raw MapReduce. Two days into Spark, he declared his intent to never write another MapReduce job again.\n\nUri, we got your back, buddy: <a href=\"http://vision.cloudera.com/apache-spark-welcome-to-the-cdh-family/\">Spark will ship inside CDH 5.</a>\n\n<h2>Further Reading</h2>\n\n<ul>\n\t<li><a href=\"http://spark.apache.org/docs/latest/quick-start.html\">Spark Quick Start</a></li>\n\t<li><a href=\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.package\">Spark API for Scala</a></li>\n        <li><a href=\"http://spark.apache.org/docs/latest/api/java/index.html\">Spark API for Java</a></li>\n\t<li><a href=\"http://spark.apache.org/docs/latest/api/python/index.html\">Spark API for Python</a></li>\n</ul>\n\n<em>Jai Ranganathan is Director of Product at Cloudera.</em>\n\n<em>Matei Zaharia is CTO of Databricks.</em>"}
{"status": "publish", "description": null, "creator": "roy", "link": "https://databricks.com/blog/2014/03/18/spark-certification.html", "authors": ["Databricks Press Office"], "id": 2411, "categories": ["Announcements", "Company Blog"], "dates": {"publishedOn": "2014-03-19", "tz": "UTC", "createdOn": "2014-03-19"}, "title": "Databricks announces \"Certified on Apache Spark\" Program", "slug": "spark-certification", "content": "<strong>BERKELEY, Calif. \u2013 March 18, 2014 \u2013</strong> Databricks, the company founded by the creators of Apache Spark that is revolutionizing what enterprises can do with Big Data, today announced the Databricks <a href=\"/certification/\">\u201cCertified on Spark\u201d Program</a> for applications built on top of the Apache Spark platform. This program ensures that certified applications will work with a multitude of commercially supported Spark distributions.\n\n\u201cPioneering application developers that are leveraging the power of Spark have had to choose between two sub-optimal choices: they either have to package Spark platform support with their application or attempt to maintain integration/certification individually with a rapidly increasing set of commercially supported Spark distributions,\u201d said Ion Stoica, Databricks CEO. \u201cThe Databricks \u2018Certified on Spark\u2019 program enables developers to certify solely against the 100% open-source Apache Spark distribution, and ensures interoperability with Apache Spark-compatible distributions. Databricks will handle the task of certifying the compatibility of each commercial Spark distribution with the Apache version and will soon announce the initial set of distributions that meet this criteria.\u201d\n\nThe pioneering partners of the Databricks certification program include Adatao, Alpine Data Labs, and Tresata - advanced analytics application vendors that have been at the forefront in leveraging the power of Spark to deliver faster and deeper insights for their enterprise customers.\n\n\u201cCertified on Spark\u201d also provides multiple benefits for enterprise users including:\n<ul>\n \t<li>Decoupling Spark distribution (and commercial support) from application licensing</li>\n \t<li>Full transparency into which applications are truly designed to work with and leverage the power of Spark</li>\n \t<li>A rapidly increasing set of certified applications to incentivize distribution vendors to maintain compatibility with Apache Spark and avoid forking and fragmentation, ultimately resulting in greater compatibility and shared innovation for users</li>\n</ul>\n\u201cAt Databricks we are fully committed to keeping Spark 100% open source and to bringing it to the widest possible set of users,\u201d said Matei Zaharia, Databricks CTO and the creator of Spark. \u201cThe \u2018Certified on Spark\u2019 program is key to maintaining a healthy and unified Apache Spark platform, and it is an expression of our community focused efforts, such as organizing meetups and the upcoming Spark Summit, and driving the future of open-source development of Spark.\u201d\n\nApplication developers that are interested in applying for the \u201cCertified on Spark\u201d program should visit <a href=\"http://www.databricks.com\">www.databricks.com</a> and select \u201cApply for Certification\u201d. Enterprise users can also visit the Databricks site regularly to see the latest set of certified application vendors and read \u201capplication spotlight\u201d blog articles that deep-dive into specific examples of Spark-powered applications."}
{"status": "publish", "description": null, "creator": "ion", "link": "https://databricks.com/blog/2014/03/02/spark-apache-top-level-project.html", "authors": ["Ion Stoica"], "id": 2412, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2014-03-03", "tz": "UTC", "createdOn": "2014-03-03"}, "title": "Apache Spark Now a Top-level Apache Project", "slug": "spark-apache-top-level-project", "content": "<div class=\"blogContent\">\n\nWe are delighted with the recent <a href=\"https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces50\">announcement</a> of the Apache Software Foundation that <a href=\"http://spark.apache.org\">Apache Spark</a> has become a top-level Apache project. This is a recognition of the fantastic work done by the Spark open source community, which now counts over 140 developers from 30+ companies.\n\nIn short time, Spark has become an increasingly popular solution for numerous big data applications, including machine learning, interactive queries, and stream processing. Spark now is an integral part of the Hadoop ecosystem, with many organizations employing Spark to perform sophisticated processing on their Hadoop data.\n\nAt Databricks we are looking forward to continuing our work with the open source community to accelerate the development and adoption of Apache Spark. Currently employing the lead developers and creators of many of the components of the Spark ecosystem, including Matei Zaharia, the creator of Spark, and Patrick Wendell, the release manager of Spark, we are committed to the success of Apache Spark.\n\n</div>"}
{"status": "publish", "description": null, "creator": "rxin", "link": "https://databricks.com/blog/2014/02/12/big-data-benchmark.html", "authors": ["Ahir Reddy", "Reynold Xin"], "id": 2413, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2014-02-13", "tz": "UTC", "createdOn": "2014-02-13"}, "title": "AMPLab updates the Big Data Benchmark", "slug": "big-data-benchmark", "content": "The AMPLab at UC Berkeley, with help from Databricks, recently released an update to the <a href=\"https://amplab.cs.berkeley.edu/benchmark/\">Big Data Benchmark</a>. This benchmark uses Amazon EC2 to compare performance of five popular SQL query engines in the Big Data ecosystem on common types of queries, which can be reproduced through publicly available scripts and datasets.\n\nIn the past year, the community has invested heavily in performance optimizations of query engines. We are glad to see that all projects have evolved in this area. Although the queries used in the benchmark are simple, we are proud that Shark remains one of the fastest engines for these workloads, and has improved significantly since the last run.\n\nWhile this benchmark reaffirms Shark as a highly performant SQL query engine, we are working hard at Databricks to push the boundaries further. Stay tuned for some exciting news we will share soon with the community.\n<ul>\n\t<li><a href=\"https://amplab.cs.berkeley.edu/benchmark/\">Big Data Benchmark</a></li>\n</ul>\n&nbsp;"}
{"status": "publish", "description": null, "creator": "pat.mcdonough", "link": "https://databricks.com/blog/2014/02/10/strata-santa-clara-2014.html", "authors": ["Pat McDonough"], "id": 2414, "categories": ["Company Blog", "Events"], "dates": {"publishedOn": "2014-02-11", "tz": "UTC", "createdOn": "2014-02-11"}, "title": "Databricks at the O'Reilly Strata Conference 2014", "slug": "strata-santa-clara-2014", "content": "The Databricks team is excited to take part in a number of activities throughout the 2014 O\u2019Reilly Strata Conference in Santa Clara. From hands-on training, to office hours, to several talks (including a keynote), there are plenty of chances for attendees to learn how Apache Spark is bringing ease of use and outstanding performance to your big data. The schedule for the Databricks team includes:\n<ul>\n \t<li><a href=\"http://ampcamp.berkeley.edu/4/\">AMPCamp4</a>, Hosted at Strata</li>\n \t<li><a href=\"http://strataconf.com/strata2014/public/content/office-hours\">Office Hours</a> on Wednesday at 5:45pm</li>\n \t<li><a href=\"http://strataconf.com/strata2014/public/schedule/detail/33057\">How Companies are Using Spark, and Where the Edge in Big Data Will Be</a>, a keynote talk presented by Matei Zaharia on Thursday at 9:15am</li>\n \t<li><a href=\"http://strataconf.com/strata2014/public/schedule/detail/32375\">Querying Petabytes of Data in Seconds with BlinkDB</a>, co-presented by Reynold Xin on Thursday at 1:30pm</li>\n</ul>\n<h2 id=\"about-ampcamp-4\">About AMPCamp 4</h2>\nWe\u2019ll be kicking off the conference on Tuesday by helping to host the fourth iteration of AMPCamp, where some of the newer components of the Berkeley Data Analytics stack will get some time in the spotlight. There are talks on BlinkDB, MLbase, GraphX, and Tachyon in the morning, and hands-on training for BlinkDB, MLbase, Spark, Spark Streaming, GraphX, and Shark in the afternoon.\n<h2 id=\"conference-talks--office-hours\">Conference Talks &amp; Office Hours</h2>\nOn Wednesday afternoon, attendees can come speak directly with a few of the members from our team in Strata\u2019s Office Hours. Office hours provide a chance to meet face-to-face with the Databricks team and to ask any questions about the Spark project and ecosystem.\n\nTo learn the latest about Apache Spark, attendees should listen to talks featuring the Databricks team, including a keynote by our CTO Matei Zaharia. On Thursday morning, Matei will describe how organizations must now compete on the speed and sophistication with which they can draw value from data, and how Apache Spark makes a new class of applications possible.\n<h2 id=\"well-see-you-there\">We\u2019ll See You There!</h2>\nWe\u2019re looking forward to seeing you at the conference!"}
{"status": "publish", "description": null, "creator": "ion", "link": "https://databricks.com/blog/2014/01/21/spark-and-hadoop.html", "authors": ["Ion Stoica"], "id": 2415, "categories": ["Apache Spark", "Ecosystem", "Engineering Blog"], "dates": {"publishedOn": "2014-01-22", "tz": "UTC", "createdOn": "2014-01-22"}, "title": "Apache Spark and Hadoop: Working Together", "slug": "spark-and-hadoop", "content": "We are often asked how does <a href=\"http://spark.incubator.apache.org\">Apache Spark</a> fits in the Hadoop ecosystem, and how one can run Spark in a existing Hadoop cluster. This blog aims to answer these questions.\n\nFirst, Spark is intended to <em>enhance</em>, not replace, the Hadoop stack. From day one, Spark was designed to read and write data from and to HDFS, as well as other storage systems, such as HBase and Amazon\u2019s S3. As such, Hadoop users can enrich their processing capabilities by combining Spark with Hadoop MapReduce, HBase, and other big data frameworks.\n\nSecond, we have constantly focused on making it as easy as possible for <em>every Hadoop user</em> to take advantage of Spark\u2019s capabilities. No matter whether you run Hadoop 1.x or Hadoop 2.0 (YARN), and no matter whether you have administrative privileges to configure the Hadoop cluster or not, there is a way for you to run Spark! In particular, there are three ways to deploy Spark in a Hadoop cluster: standalone, YARN, and SIMR.\n\n<img class=\"aligncenter\" src=\"https://databricks.com/wp-content/uploads/2014/01/SparkHadoop.png\" alt=\"SparkHadoop1.png\" />\n\n<strong>Standalone deployment</strong>: With the standalone deployment one can statically allocate resources on all or a subset of machines in a Hadoop cluster and run Spark side by side with Hadoop MR. The user can then run arbitrary Spark jobs on her HDFS data. Its simplicity makes this the deployment of choice for many Hadoop 1.x users.\n\n<strong><a href=\"https://hadoop.apache.org/docs/current2/hadoop-yarn/hadoop-yarn-site/YARN.html\">Hadoop Yarn</a> deployment</strong>: Hadoop users who have already deployed or are planning to deploy <a href=\"https://hadoop.apache.org/docs/current2/hadoop-yarn/hadoop-yarn-site/YARN.html\">Hadoop Yarn</a> can simply run Spark on YARN without any pre-installation or administrative access required. This allows users to easily integrate Spark in their Hadoop stack and take advantage of the full power of Spark, as well as of other components running on top of Spark.\n\n<strong>Spark In MapReduce (<a href=\"https://databricks.com/blog/2014/01/01/simr.html\">SIMR</a>)</strong>: For the Hadoop users that are not running YARN yet, another option, in addition to the standalone deployment, is to use SIMR to launch Spark jobs inside MapReduce. With SIMR, users can start experimenting with Spark and use its shell within a couple of minutes after downloading it! This tremendously lowers the barrier of deployment, and lets virtually everyone play with Spark.\n<h2 id=\"interoperability-with-other-systems\">Interoperability with other Systems</h2>\nSpark interoperates not only with Hadoop, but with other popular big data technologies as well.\n<ul>\n \t<li><strong><a href=\"http://hive.apache.org/\">Apache Hive</a></strong>: Through <a href=\"https://github.com/amplab/shark/wiki\">Shark</a>, Spark enables Apache Hive users to run their unmodified queries much faster. Hive is a popular data warehouse solution running on top of Hadoop, while Shark is a system that allows the Hive framework to run on top of Spark instead of Hadoop. As a result, Shark can accelerate Hive queries by as much as 100x when the input data fits into memory, and up 10x when the input data is stored on disk.</li>\n \t<li><strong><a href=\"http://aws.amazon.com/\">AWS EC2</a></strong>: Users can easily run Spark (and Shark) on top of Amazon\u2019s EC2 either using the <a href=\"http://spark.incubator.apache.org/docs/latest/ec2-scripts.html\">scripts</a> that come with Spark, or the hosted <a href=\"http://aws.amazon.com/articles/4926593393724923\">versions of Spark and Shark</a> on Amazon\u2019s Elastic MapReduce.</li>\n \t<li><strong><a href=\"http://mesos.apache.org/\">Apache Mesos</a></strong>: Spark runs on top of Mesos, a cluster manager system which provides efficient resource isolation across distributed applications, including <a href=\"http://www.mcs.anl.gov/research/projects/mpi/\">MPI</a> and Hadoop. Mesos enables <em>fine grained</em> sharing which allows a Spark job to dynamically take advantage of the idle resources in the cluster during its execution. This leads to considerable performance improvements, especially for long running Spark jobs.</li>\n</ul>"}
{"status": "publish", "description": null, "creator": "patrick", "link": "https://databricks.com/blog/2013/12/19/release-0_8_1.html", "authors": ["Patrick Wendell"], "id": 2416, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2013-12-20", "tz": "UTC", "createdOn": "2013-12-20"}, "title": "Apache Spark 0.8.1 Released", "slug": "release-0_8_1", "content": "We are happy to announce the release of Apache Spark 0.8.1. In addition to performance and stability improvements, this release adds three new features. First, Spark now supports for the newest versions of YARN (2.2+). Second, the standalone cluster manager supports a high-availability mode in which it can tolerate master failures. Third, shuffles have been optimized to create fewer files, improving shuffle performance drastically in some settings.\n\nIn conjunction with the Apache Spark 0.8.1 release we are separately releasing <a href=\"https://databricks.com/blog/2014/01/01/simr.html\">Spark In MapReduce (SIMR)</a>, which enables seamlessly running Spark on Hadoop MapReduce v1 clusters without requiring the installation of Scala or Spark.\n\nWhile Apache Spark 0.8.1 is a minor release, it includes these larger features for the benefit of Scala 2.9 users. The next major release of Apache Spark, 0.9.0, will be based on Scala 2.10.\n\nThis release was a community effort, featuring contributions from more than 40 developers. For much more information about Apache Spark 0.8.1, head over to the <a href=\"http://spark.incubator.apache.org/releases/spark-release-0-8-1.html\">release notes</a> or <a href=\"http://spark.incubator.apache.org/downloads.html\">download Apache Spark 0.8.1</a> and try it out for yourself."}
{"status": "publish", "description": null, "creator": "andy", "link": "https://databricks.com/blog/2013/12/18/spark-summit-2013-follow-up.html", "authors": ["Andy Konwinski"], "id": 2417, "categories": ["Company Blog", "Customers", "Events"], "dates": {"publishedOn": "2013-12-19", "tz": "UTC", "createdOn": "2013-12-19"}, "title": "Highlights From Spark Summit 2013", "slug": "spark-summit-2013-follow-up", "content": "Earlier this month we held the <a href=\"http://spark-summit.org/2013\">first Spark Summit</a>, a conference to bring the Apache Spark community together. We are excited to share some statistics and highlights from the event.\n<ul>\n \t<li>450 participants from over 180 companies attended</li>\n \t<li>Participants came from 13 countries</li>\n \t<li>Spark training was sold out at 200 participants from 80 companies</li>\n \t<li>20 organizations sponsored the event, including all major Hadoop platform vendors</li>\n \t<li>20 different organizations gave talks</li>\n</ul>\nVideos and slides for all talks are now available on the <a href=\"http://spark-summit.org/2013\">Summit 2013 page</a>.\n\nThe Summit included Keynotes from Databricks, the UC Berkeley AMPLab, and Yahoo, as well as presentations from 18 other companies including Amazon, Red Hat, and Adobe. Talk topics covered a wide range including specialized applications such as mapping and manipulating the brain, product launches, and research projects about future directions for the project. We are very excited to see Spark and related projects come such a long way from research prototypes originally developed in AMPLab at Berkeley to being used in production by startups and large companies alike.\n<h2 id=\"the-state-of-spark-and-where-were-going-next\">The State of Spark, and Where We\u2019re Going Next</h2>\n<iframe style=\"float: left; padding: 10px;\" src=\"//www.youtube.com/embed/nU6vO2EJAb4\" width=\"300\" height=\"173\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\nIn the first keynote of the day, Matei Zaharia, who started the Spark project and is now CTO at Databricks, gave a summary of recent growth in the projec<span style=\"letter-spacing: 3px;\">t</span><a style=\"vertical-align: super; font-size: 60%;\" href=\"#footnote-1\">1</a>, highlighting key contributions from across the community. In particular, Spark recently reached 100 contributors, making it the second-largest open source development community in the Big Data space after Hadoop, and it\u2019s also overtaken Hadoop in the past 6 months. Matei also previewed what is coming up in the Spark development roadmap, including features under development for the upcoming 0.8.1 and 0.9 releases including high availability for the Spark Master in standalone mode, external hashing and sorting, support for Scala 2.10, and more. Finally, Matei discussed features of Spark that differentiate it from other projects, such as the focus on unification of diverse programming models.\n<h2 id=\"spark-and-hadoop\">Spark and Hadoop</h2>\n<iframe style=\"float: right; padding-left: 10px;\" src=\"//www.youtube.com/embed/qs40jiN2iwM\" width=\"300\" height=\"173\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\nIt was exciting to hear from Eric Baldeschwieler, whose background includes leading the Yahoo team that took Hadoop from being a prototype project to what it is today, as well as serving as both CEO and CTO of Hortonworks. In his keynote, Eric presented his view of how Spark is on track to become the \u201clingua franca\u201d for Big Data. He also talked about how Spark updates Hadoop with important features such as effective utilization of RAM, low latency queries, and streaming ingest. Finally, he discussed how the Spark and Hadoop projects relate to each other now and going forward.\n<h2 id=\"spark-training-day\">Spark Training Day</h2>\nOn the sold-out second day, 200 attendees heard 4 talks on using, deploying, and administering Spark, and also participated in hands-on training led by the creators of Spark. Amazon, a sponsor of the Summit, donated EC2 resources so participants could each have their own 6 node Spark cluster to practice using Spark, Spark Streaming and Shark on real Wikipedia and Twitter data. We have made the hands-on exercises <a href=\"http://spark-summit.org/exercises\">available on the summit website</a> for free. Using these, anybody can use their own Amazon AWS account to launch a 6 node Spark cluster and get experience analyzing real data.\n\n<iframe style=\"float: left; padding-right: 10px;\" src=\"//www.youtube.com/embed/zGW8glN-Mo8\" width=\"300\" height=\"173\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\nThe final talk of the training day was given by core Spark developer and Databricks cofounder Patrick Wendell. Patrick\u2019s talk was about Administering Spark and was prepared in response to requests for more advanced technical material as part of the training. In it he dove into the core software components of the project, the different resource managers that Spark runs on, what type of hardware to use when running Spark, how to link against Spark when writing applications, monitoring Spark clusters running in production, and more.\n<h2 id=\"other-interesting-talks\">Other Interesting Talks</h2>\nIn addition to those above, there were many more great talks at the Summit, here are a few more that highligh the diversity of topics covered:\n<ul class=\"talk-list\">\n \t<li><span class=\"talk-title\">Mapping and manipulating the brain at scale</span> (<a href=\"http://spark-summit.org/talk/freeman-mapping-and-manipulating-the-brain-at-scale/\">abstract</a>, <a href=\"http://www.jeremyfreeman.net/share/talks/spark5/\">slides</a>, <a href=\"http://www.youtube.com/watch?v=7mmcEl_1CPw\">video</a>). <span class=\"speaker-info\">Jeremy Freeman, HHMI Janelia Farm Research Campus</span></li>\n \t<li><span class=\"talk-title\">Beyond Word Count \u2013 Productionalizing Spark Streaming</span> (<a href=\"http://spark-summit.org/talk/weald-beyond-word-count-productionalizing-spark-streaming/\">abstract</a>, <a href=\"http://spark-summit.org/wp-content/uploads/2013/10/Productionalizing-Spark-Streaming-Spark-Summit-2013-copy.pdf\">slides</a>, <a href=\"http://www.youtube.com/watch?v=OhpjgaBVUtU\">video</a>). <span class=\"speaker-info\">Ryan Weald, Sharethrough</span></li>\n \t<li><span class=\"talk-title\">Hadoop and Spark Join Forces in Yahoo</span> (<a href=\"http://spark-summit.org/talk/feng-hadoop-and-spark-join-forces-at-yahoo/\">abstract</a>, <a href=\"http://spark-summit.org/wp-content/uploads/2013/10/Feng-Andy-SparkSummit-Keynote.pdf\">slides</a>, <a href=\"http://www.youtube.com/watch?v=GbFtbIepk-s\">video</a>). <span class=\"speaker-info\">Andy Feng, Distinguished Architect, Cloud Services, Yahoo</span></li>\n \t<li><span class=\"talk-title\">Next-Generation Spark Scheduling with Sparrow</span> (<a href=\"http://spark-summit.org/talk/ousterhout-next-generation-spark-scheduling-with-sparrow/\">abstract</a>, <a href=\"http://spark-summit.org/wp-content/uploads/2013/10/Kay_Sparrow_Spark_Summit.pdf\">slides</a>, <a href=\"http://www.youtube.com/watch?v=ayjH_bG-RC0\">video</a>). <span class=\"speaker-info\">Kay Ousterhout, UC Berkeley AMPLab</span></li>\n</ul>\n<h2 id=\"the-upcoming-spark-summit-2014\">The Upcoming Spark Summit 2014</h2>\nThe next Summit will be this Summer 2014, and you can <a href=\"http://spark-summit.org/2014/pre-register\">pre-register now</a>. We look forward to seeing you there!\n<h2 id=\"other-summit-resources\">Other Summit Resources</h2>\n<ul>\n \t<li>Follow the <a href=\"http://twitter.com/spark_summit\">@spark_summit</a> twitter handle</li>\n \t<li>Like the <a href=\"http://facebook.com/ApacheSparkSummit\">Summit Facebook page</a></li>\n \t<li>More <a href=\"https://www.dropbox.com/sc/k6l01hiv4zbhhfg/hhCzeS_U9P\">Summit 2013 photos</a></li>\n \t<li>A look <a href=\"http://strata.oreilly.com/2013/11/behind-the-scenes-of-the-first-spark-summit.html\">behind the scenes look at the first Spark Summit</a></li>\n</ul>\n<h2 id=\"footnotes\">Footnotes</h2>\n<ol>\n \t<li>For more on this, check out <a href=\"/blog/2013/10/27/the-growing-spark-community.html\" name=\"footnote-1\">our recent blog post about the growth of the Spark community</a>.</li>\n</ol>"}
{"status": "publish", "description": null, "creator": "pat.mcdonough", "link": "https://databricks.com/blog/2013/11/21/putting-spark-to-use.html", "authors": ["Pat McDonough"], "id": 2418, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2013-11-22", "tz": "UTC", "createdOn": "2013-11-22"}, "title": "Putting Apache Spark to Use: Fast In-Memory Computing for Your Big Data Applications", "slug": "putting-spark-to-use", "content": "[sidenote]A version of this post appears on the <a href=\"http://blog.cloudera.com/blog/2013/11/putting-spark-to-use-fast-in-memory-computing-for-your-big-data-applications/\">Cloudera Blog</a>.[/sidenote]\n\n<hr/>\n\nApache Hadoop has revolutionized big data processing, enabling users to store and process huge amounts of data at very low costs. MapReduce has proven to be an ideal platform to implement complex batch applications as diverse as sifting through system logs, running ETL, computing web indexes, and powering personal recommendation systems. However, its reliance on persistent storage to provide fault tolerance and its one-pass computation model make MapReduce a poor fit for low-latency applications and iterative computations, such as machine learning and graph algorithms.\n\nApache Spark addresses these limitations by generalizing the MapReduce computation model, while dramatically improving performance and ease of use.\n<h2 id=\"fast-and-easy-big-data-processing-with-spark\">Fast and Easy Big Data Processing with Spark</h2>\nAt its core, Spark provides a general programming model that enables developers to write application by composing arbitrary operators, such as mappers, reducers, joins, group-bys, and filters. This composition makes it easy to express a wide array of computations, including iterative machine learning, streaming, complex queries, and batch.\n\nIn addition, Spark keeps track of the data that each of the operators produces, and enables applications to reliably store this data in memory. This is the key to Spark\u2019s performance, as it allows applications to avoid costly disk accesses. As illustrated in the figure below, this feature enables:\n\n<img style=\"float: left; width: 280px; margin: 15px 5px;\" src=\"/wp-content/uploads/2013/11/using-ram.png\" alt=\"Efficiently Leveraging Memory\" />\n<ul>\n \t<li>Low-latency computations by caching the working dataset in memory and then performing computations at memory speeds, and</li>\n \t<li>Efficient iterative algorithm by having subsequent iterations share data through memory, or repeatedly accessing the same dataset</li>\n</ul>\nSpark\u2019s ease-of-use comes from its general programming model, which does not constrain users to structure their applications into a bunch of map and reduce operations. Spark\u2019s parallel programs look very much like sequential programs, which make them easier to develop and reason about. Finally, Spark allows users to easily combine batch, interactive, and streaming jobs in the same application. As a result, a Spark job can be up to 100x faster and requires writing 2-10x less code than an equivalent Hadoop job.\n<h2 id=\"using-spark-for-advanced-data-analysis-and-data-science\">Using Spark for Advanced Data Analysis and Data Science</h2>\n<h3 id=\"interactive-data-analysis\">Interactive Data Analysis</h3>\nOne of Spark\u2019s most useful features is the interactive shell, bringing Spark\u2019s capabilities to the user immediately \u2013 no IDE and code compilation required. The shell can be used as the primary tool for exploring data interactively, or as means to test portions of an application you\u2019re developing.\n\nThe screenshot to the right shows a Spark Python shell in which the user loads a file and then counts the number of lines that contain \u201cHoliday\u201d.\n\n<img style=\"display: block; width: 480px; margin: 1em auto;\" src=\"/wp-content/uploads/2013/11/spark-python-shell.png\" alt=\"Spark's Python Shell\" />\n\nAs illustrated in this example, Spark can read and write data from and to HDFS. Thus, as soon as Spark is installed, a Hadoop user can immediately start analyzing HDFS data. Then, by caching a dataset in memory, a user can perform a large variety of complex computations interactively!\n\nSpark also provides a Scala shell, and APIs in Java, Scala, and Python for stand-alone applications.\n<h3 id=\"faster-batch\">Faster Batch</h3>\nSome of the earliest deployments of Spark have focused on how to improve performance in existing MapReduce applications. Remember that MapReduce is actually a generic execution framework and is not exclusive to it\u2019s most well-known implementation in core Hadoop. Spark provides MapReduce as well, and because it can efficiently use memory (while using lineage to recover from failure if necessary), some implementations are simply faster in Spark\u2019s MapReduce as compared to Hadoop\u2019s MapReduce right off the bat, before you even get in to leveraging cache for iterative programs.\n\nThe example below illustrates Spark\u2019s implementation of MapReduce\u2019s most famous example, word count. You can see that Spark supports operator chaining. This becomes very useful when doing a bit of pre- or post-processing on your data, such as filtering data prior to running a complex MapReduce job.\n\n<pre>val file = sc.textFile(\"hdfs://.../pagecounts-*.gz\");\nval counts = file.flatMap(line => line.split(\" \"));\n  .map(word => (word, 1))\n  .reduceByKey(_ + _)\ncounts.saveAsTextFile(\"hdfs://.../word-count\");</pre>\n\nSpark\u2019s batch capabilities have been proven in real-world scenarios. A very large Silicon Valley Internet company did a plain-vanilla port of a single MR job implementing feature extraction in a model training pipeline, and saw a 3x speedup.\n<h3 id=\"iterative-algorithms\">Iterative Algorithms</h3>\n<img style=\"float: right; width: 140px; margin: 0 10px;\" src=\"https://databricks.com/wp-content/uploads/2013/11/logistic-regression-performance.png\" alt=\"Logistic Regression Performance\" />\nSpark allow users and applications to explicitly cache a dataset by calling the cache() operation. This means that your applications can now access data from RAM instead of disk, which can dramatically improve the performance of iterative algorithms that access the same dataset repeatedly. This use case covers an important class of applications, as all machine learning and graph algorithms are iterative in nature.\n\nTwo of the world\u2019s largest Internet companies leverage Spark\u2019s efficient iterative execution to provide content recommendations and ad targeting. Machine-learning algorithms such as logistic regression have run 100x faster than previous Hadoop-based implementations (see the plot to the right), while other algorithms such as collaborative filtering or alternating direction method of multipliers have run over 15x faster.\n\nThe following example uses logistic regression to find the best hyperplane that separates two sets of points in a multi-dimensional feature space. Note the cached dataset \u201cpoints\u201d is accessed repeatedly from memory, whereas in MapReduce, each iteration will read data from the disk, which incurs a huge overhead.\n\n<pre>val points = sc.textFile(\"...\").map(parsePoint).cache()\nvar w = Vector.random(D) //current separating plane\nfor (i <- 1 to ITERATIONS) {\n  val gradient = points.map(p =>\n    (1 / (1 + exp(-p.y*(w dot p.x))) - 1) * p.y * p.x)\n    .reduce(_ + _)\n    w -= gradient\n}\nprintln(\"Final separating plane: \" + w)</pre>\n\n<h3 id=\"real-time-stream-processing\">Real-Time Stream Processing</h3>\nWith a low-latency data analysis system at your disposal, it\u2019s natural to extend the engine towards processing live data streams. Spark has an API for working with streams, providing exactly-once semantics and full recovery of stateful operators. It also has the distinct advantage of giving you the same Spark APIs to process your streams, including reuse of your regular Spark application code.\n\nThe code snippet below shows a simple job processing a network stream, filtering for words beginning with a hashtag and performing a word count on every 10 seconds of data. Compare this to the previous word-count example and you\u2019ll see how almost the exact same code is used, but this time processing a live data stream.\n\n<pre> val ssc = new StreamingContext(\n  args(0), \"NetworkHashCount\",\n  Seconds(10), System.getenv(\"SPARK_HOME\"),\n  Seq(System.getenv(\"SPARK_EXAMPLES_JAR\")))\n\nval lines = ssc.socketTextStream(\"localhost\", 9999)\nval words = lines.flatMap(_.split(\" \"))\n  .filter(_.startsWith(\"#\"))\nval wordCounts = words.map(x => (x, 1))\n  .reduceByKey(_ + _)\nwordCounts.print()\nssc.start()</pre>\n\nAlthough the Spark Streaming API was released less than a year ago, users have deployed it in production to provide monitoring and alerting against stateful, aggregated data from system logs, achieving very fast processing with only seconds of latency.\n<h3 id=\"faster-decision-making\">Faster Decision-Making</h3>\nMany companies use big data to make or facilitate user\u2019s decisions in the form of recommendation systems, ad targeting, or predictive analytics. One of the key properties of any decision is latency \u2014 that is, the time it takes to make the decision from the moment the input data is available. Reducing decision latency can significantly increase their effectiveness, and ultimately increase the company\u2019s return on investment. Since many of these decisions are based on complex computations (such as machine learning and statistical algorithms), Spark is an ideal fit to speed up decisions.\n\nNot surprisingly, Spark has been deployed to improve decision quality as well as to reduce latency. Examples range from ad targeting, to improving the quality of video delivery over the Internet.\n<h3 id=\"unified-pipelines\">Unified Pipelines</h3>\nMany of today\u2019s Big Data deployments go beyond MapReduce by integrating other frameworks for streaming, batch, and interactive computation. Users can dramatically reduce the complexity of their data processing pipelines by replacing several systems with Spark.\n\nFor instance, today, many companies use MapReduce to generate reports and answer historical queries, and deploy a separate system for stream processing to follow key metrics in real-time. This approach requires one to maintain and manage two different systems, as well as develop applications for two different computation models. It would also require one to make sure the results provided by the two stacks are consistent (for example, a count computed by the streaming application and the same count computed by MapReduce).\n\nRecently, users have deployed Spark to implement stream processing as well as batch processing for providing historical reports. This not only simplifies deployment and maintenance, but dramatically simplifies application development. For example, maintaining the consistency of real-time and historical metrics is no longer a problem as they are computed using the same code. A final benefit of the unification is improved performance, as there is no need to move the data between different systems: once in-memory, the data can be shared between the streaming computations and historical (or interactive) queries.\n<h3 id=\"your-turn-go-get-started\">Your Turn: Go Get Started</h3>\nSpark is very easy to get started writing powerful Big Data applications. Your existing Hadoop and/or programming skills will have you productively interacting with your data in minutes. Go get started today:\n<ul>\n \t<li><a href=\"http://spark.incubator.apache.org/downloads.html\">Download Spark</a></li>\n \t<li><a href=\"http://spark.incubator.apache.org/docs/latest/quick-start.html\">Quick Start</a></li>\n \t<li><a href=\"http://spark-summit.org\">Spark Summit (2013 Conference Talks and Training)</a></li>\n</ul>"}
{"status": "publish", "description": null, "creator": "ion", "link": "https://databricks.com/blog/2013/10/28/databricks-and-cloudera-partner-to-support-spark.html", "authors": ["Ion Stoica"], "id": 2419, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2013-10-29", "tz": "UTC", "createdOn": "2013-10-29"}, "title": "Databricks and Cloudera Partner to Support Apache Spark", "slug": "databricks-and-cloudera-partner-to-support-spark", "content": "Today, Cloudera announced that it will distribute and support Apache Spark. We are very excited about this announcement, and what it brings to the Spark platform and the open source community. So what does this announcement mean for Spark?\n\nFirst, it validates the maturity of the Spark platform. Started as a research project at UC Berkeley in 2009, Spark is the first general purpose cluster computing engine that can run sophisticated computations at memory speeds on Hadoop clusters. Spark started with the goal of providing efficient support for iterative algorithms (such as machine learning) and interactive queries, workloads not well supported by MapReduce. Since then, Spark has grown to support other applications such as streaming, and has gained rapid industry adoption. Today, Spark is used in production by numerous companies, and it counts on an ever growing open source community with over 90 contributors from 25 companies.\n\nSecond, it will make the Spark platform available to a wide range of enterprise customers both in US and internationally. By being distributed in conjunction with <a href=\"http://www.cloudera.com/content/cloudera/en/products-and-services/cdh.html\">Cloudera\u2019s CDH</a>, Spark will enjoy the same enterprise-grade support as the other components in Cloudera\u2019s stack. Databricks is fully committed to working with Cloudera to guarantee that its customers will have the best possible support. Furthermore, we are looking forward to this partnership to enable new categories of exciting applications, and address unique usage scenarios.\n\nThird, this partnership underlines and strengthens the integration of Spark into the Hadoop ecosystem. Spark has the ability to read Hadoop files, share data with other Hadoop frameworks, and support existing Hadoop workloads, including Hive queries. This integration is beneficial not only for Spark, but for the Hadoop ecosystem as a whole, as Spark brings new capabilities to the Hadoop ecosystem through its ability to run on top of Hadoop YARN. This could give Hadoop users the opportunity to run jobs up to 100x faster than MapReduce, while writing 2-5x less code. For example, a data scientist could leverage Spark\u2019s simple yet powerful API to rapidly develop machine learning algorithms, and then run them at memory speeds on her Hadoop data.\n\nFinally, we want to reiterate our full commitment to open source. The success Spark has enjoyed thus far has only been possible because of a vibrant open source community, who has contributed a continuous stream of new functionality and bug fixes. We believe this partnership will ignite a new wave of growth of our community and accelerate the development of the Apache Spark platform to support an ever growing number of customers.\n\nWe have no doubt that we are just at the beginning of a journey to give users the tools to solve tomorrow\u2019s big data challenges. The next stop in this journey is the <a href=\"http://spark-summit.org\">Spark Summit</a>. Sponsored by leading big data companies and Spark users, including Databricks and Cloudera, this is the first conference that will bring together the Spark community. Come and join us on this journey!"}
{"status": "publish", "description": null, "creator": "matei", "link": "https://databricks.com/blog/2013/10/27/the-growing-spark-community.html", "authors": ["Matei Zaharia"], "id": 2420, "categories": ["Announcements", "Company Blog"], "dates": {"publishedOn": "2013-10-28", "tz": "UTC", "createdOn": "2013-10-28"}, "title": "The Growing Apache Spark Community", "slug": "the-growing-spark-community", "content": "This year has seen unprecedented growth in both the user and contributor communities around <a href=\"http://spark.incubator.apache.org\">Apache Spark</a>. This rapid growth validates the tremendous potential of the platform, and shows the great excitement around it.\n\nWhile Spark started as a research project by a few grad students at UC Berkeley in 2009, today <strong>over 90 developers from 25 companies have contributed to Spark</strong>. This is not counting contributors to Shark (Hive on Spark), of which there are 25. Indeed, out of the many new big data engines created in the past few years, <strong>Spark has the largest development community after Hadoop MapReduce</strong>. We believe that new components in the project, like <a href=\"http://spark.incubator.apache.org/docs/latest/streaming-programming-guide.html\">Spark Streaming</a> and <a href=\"http://spark.incubator.apache.org/docs/latest/mllib-guide.html\">MLlib</a>, will only increase this growth.\n<h2>Growth by Numbers</h2>\nTo give a sense of the growth of the project, the graph below shows the number of contributors to each major Spark release in the past year.\n<div style=\"text-align: center;\"><strong>Past Year Spark Releases</strong>\n<img class=\"aligncenter wp-image-82\" src=\"/wp-content/uploads/2013/10/growth-of-spark-graphic.png\" alt=\"growth-of-spark-graphic\" width=\"507\" height=\"300\" /></div>\nThe number of contributors quadrupled between October 2012 and 2013, and each of our releases has been getting bigger in terms of features. In addition, an increasing number of Spark\u2019s major features have been contributed by the user community. These include Hadoop YARN support in Apache Spark 0.8; metrics collection; new machine learning algorithms and examples; fair scheduling; and column-oriented compression in Shark. At Databricks, we plan to continue working with the open source community to expand Apache Spark.\n\nA final indicator of growth is conferences and events. The <a href=\"http://ampcamp.berkeley.edu\">AMP Camp</a> training camp at Berkeley was sold out with members from over 100 companies attending, while our <a href=\"http://www.meetup.com/spark-users/\">San Francisco user meetup</a> has grown to 1,300 members. We\u2019re excited to continue organizing such events.\n<h2>Bringing the Community Together: The First Spark Summit</h2>\n<img style=\"float: right;\" src=\"/wp-content/uploads/2013/10/Summit-Logo-FINALtr-150x150px.png\" alt=\"Summit-Logo-FINALtr-150x150px\" width=\"150\" height=\"150\" />\nTo celebrate the growth around Spark and bring users and contributors together, we\u2019re excited to host the <a href=\"http://spark-summit.org\">first Spark Summit</a>, on December 2nd and 3rd, 2013. Sponsored by leading big data companies and production users of Spark, this will be the first conference around the Spark stack. In addition to talks from users and developers, the Summit and will include a day of <a href=\"http://www.spark-summit.org/agenda/\">hands-on Spark training</a>. We\u2019re looking forward to your continuous involvement to expand Spark and tackle tomorrow\u2019s big data challenges. So whether you\u2019re a big data veteran or new to Spark, come by to learn how to use it to solve your problems."}
{"status": "publish", "description": null, "creator": "ion", "link": "https://databricks.com/blog/2013/10/27/databricks-and-the-apache-spark-platform.html", "authors": ["Ion Stoica", "Matei Zaharia"], "id": 2421, "categories": ["Announcements", "Company Blog"], "dates": {"publishedOn": "2013-10-27", "tz": "UTC", "createdOn": "2013-10-27"}, "title": "Databricks and the Apache Spark Platform", "slug": "databricks-and-the-apache-spark-platform", "content": "When we announced that the original team behind <a href=\"http://spark.incubator.apache.org\">Apache Spark</a> is starting a company around the project, we got a lot of excited questions. What areas will the company focus on, and what will it mean for the open source project? Today, in our first blog post at Databricks, we\u2019re happy to share some of our goals, and say a little about what we\u2019re doing next with Spark.\n\nTo start with, our mission at Databricks is simple: we want to build the very best computing platform for extracting value from data. Big data is a tremendous opportunity that is still largely untapped, and we\u2019ve been working for the past six years to transform what can be done with it. Going forward, we are fully committed to building out the open source Apache Spark platform to achieve this goal.\n<h2 id=\"how-we-think-about-big-data-speed-and-sophistication\">How We Think about Big Data: Speed and Sophistication</h2>\nIn the past few years, open source technologies like Hadoop have made it dramatically easier to store large volumes of data. This capability is transforming a wide range of industries, from brick-and-mortar enterprises to the web. Over time though, simply collecting big data will not be enough to maintain a competitive edge. The question will be what can you do with this data.\n\nWe believe that two axes will determine how well an organization can draw value from data: <strong>speed</strong> and <strong>sophistication</strong>. By speed, we mean not only the speed at which we compute and return answers, but also the speed of development: how quickly can users take a new idea from the drawing board to a production application? By sophistication, we mean what type of analysis can be done. Today\u2019s big data systems do not support the sophisticated analysis functions in tools like R and Matlab, limiting their scope. Enabling these types of analyses would greatly increase their value.\n\nThrough the Apache Spark project, we\u2019ve been working to address both axes in a way that works seamlessly with the Hadoop stack. Released in 2010, Spark remains the only widely deployed engine for Hadoop to support in-memory computing and general execution graphs, as well as the easiest way to program applications on Hadoop data, with APIs in Scala, Java and Python. Released shortly after, <a href=\"http://shark.cs.berkeley.edu\">Shark</a> was the first system to speed up Hive by 100x, and is the only one of the new \u201cSQL on Hadoop\u201d engines to retain full Hive compatibility (by building directly on Hive) and to support in-memory computation. Looking forward, libraries like <a href=\"http://spark.incubator.apache.org/docs/latest/mllib-guide.html\">MLlib</a> and <a href=\"http://www.meetup.com/spark-users/events/124935592/\">GraphX</a> are making it easy to call sophisticated machine learning and graph algorithms from Spark, while running them at memory speeds. These tools have already given numerous organizations the ability to do faster and richer data analysis, and we hope to bring them to hundreds more.\n<h2 id=\"what-were-working-on\">What We\u2019re Working On</h2>\nAt Databricks, we\u2019re committed to bringing Spark to an ever-wider set of users and greatly increasing its capabilities. Through both the recent <a href=\"http://spark.incubator.apache.org/releases/spark-release-0-8-0.html\">Apache Spark 0.8 release</a> and our ongoing work, we\u2019ve been building out quite a few new features. Expect to see a focus in the following areas:\n<ul>\n \t<li><strong>Deployment:</strong> We want to make Spark effortless to deploy for any user, whether with or without an existing Hadoop cluster. Apache Spark 0.8 made significant strides in this respect with improved support for Mesos, EC2 and Hadoop YARN.</li>\n \t<li><strong>High availability</strong>: One exciting feature that we\u2019ve already merged into Apache Spark 0.8.1 is <a href=\"https://github.com/apache/incubator-spark/pull/19\">high availability for the master node</a>. In general, due to the many users who are running Spark in availability-critical settings (e.g. streaming or user-facing applications), we want to make availability throughout the stack easier.</li>\n \t<li><strong>New features:</strong> Besides these top-level goals, we have an exciting roadmap of features, such as Scala 2.10 support, new machine learning algorithms, graph computation, and updates to <a href=\"http://spark.incubator.apache.org/docs/latest/streaming-programming-guide.html\">Spark Streaming</a>, coming soon.</li>\n</ul>\nMost importantly, we believe that, despite the effort in the past few years, big data processing is still in its infancy, and there is tremendous room for tools that are faster, easier to use, and capable of richer computation. We hope you join us in defining the next generation of big data systems and unlocking the speed and sophistication that we believe is possible for big data."}
{"status": "publish", "description": null, "creator": "arsalan", "link": "https://databricks.com/blog/2014/04/10/partnership-between-databricks-and-mapr.html", "authors": ["Arsalan Tavakoli-Shiraji"], "id": 2461, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2014-04-11", "tz": "UTC", "createdOn": "2014-04-11"}, "title": "Databricks and MapR", "slug": "partnership-between-databricks-and-mapr", "content": "Today, MapR announced that it will distribute and support the Apache Spark platform as part of the MapR Distribution for Hadoop in partnership with Databricks. We\u2019re thrilled to start on this journey with MapR for a multitude of reasons.\n\nOne of our primary goals at Databricks is to drive broad adoption of Spark and ensure everybody who uses it has a fantastic experience. This partnership will enable all of MapR\u2019s enterprise customers, existing and new, to leverage Spark with the backing of the same great enterprise support available for the rest of MapR\u2019s Hadoop Distribution. As Tomer mentioned in his <a href=\"/blog/2014/04/10/MapR-Integrates-Spark-Stack.html\">blog post</a>, Spark is one of the most common topics in discussions with MapR\u2019s existing customers and many are even already running it in production!\n\nA core part of Spark\u2019s value proposition is the ability to easily build a unified end-to-end workflow where critical functions are first class citizens that are seamlessly integrated into the platform. An important part of this workflow is the ability to provide SQL-based interactive queries delivered by Shark, which also serves as the gateway for a wealth of traditional and new SQL-based tools to run on top of Spark. At Databricks, we continue to work on innovating across the entire Spark platform, including Shark, and customers now have an enterprise support option for Shark available.\n\nFinally, we see this partnership as continued validation of Spark\u2019s emergence as the leading open source processing engine in the Big Data community. Spark is the most active project in the Hadoop ecosystem in the past year with over 170 contributors, and we\u2019re heartened to see a rapidly growing attendance at community events pointing to entirely new classes of Spark enterprise use cases. The <a href=\"/certification/\">Databricks \u201cCertified on Spark\u201d</a> program has seen incredible interest from application developers who are leveraging Spark to deliver deeper insights, faster to their customers.\n\nAt Databricks we are fully committed to open source, and we\u2019re excited to partner with MapR - a company with strong support for open source Big Data projects - to together help drive continued growth and innovation of Spark. Join us at the upcoming <a href=\"http://spark-summit.org/2014\" target=\"_blank\">Spark Summit</a>, the largest conference focused on Spark, to learn more!"}
{"status": "publish", "description": null, "creator": "matei", "link": "https://databricks.com/blog/2014/04/14/spark-with-java-8.html", "authors": ["Prashant Sharma", "Matei Zaharia"], "id": 12, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2014-04-15", "tz": "UTC", "createdOn": "2014-04-15"}, "title": "Making Apache Spark Easier to Use in Java with Java 8", "slug": "spark-with-java-8", "content": "One of Apache Spark\u2019s main goals is to make big data applications easier to write. Spark has always had concise APIs in Scala and Python, but its Java API was verbose due to the lack of function expressions. With the addition of <a href=\"http://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html\">lambda expressions</a> in Java 8, we\u2019ve updated Spark\u2019s API to transparently support these expressions, while staying compatible with old versions of Java. This new support will be available in Apache Spark 1.0.\n<h2 id=\"a-few-examples\">A Few Examples</h2>\nThe following examples show how Java 8 makes code more concise. In our first example, we search a log file for lines that contain \u201cerror\u201d, using Spark\u2019s <code>filter</code> and <code>count</code> operations. The code is simple to write, but passing a Function object to <code>filter</code> is clunky:\n<h5 id=\"java-7-search-example\">Java 7 search example:</h5>\n\n<pre>JavaRDD<String> lines = sc.textFile(\"hdfs://log.txt\").filter(\n  new Function<String, Boolean>() {\n    public Boolean call(String s) {\n      return s.contains(\"error\");\n    }\n});\nlong numErrors = lines.count();</pre>\n\n(If you\u2019re new to Spark, <a href=\"http://spark.apache.org/docs/latest/quick-start.html\">JavaRDD</a> is a distributed collection of objects, in this case lines of text in a file. We can apply operations to these objects that will automatically be parallelized across a cluster.)\n\nWith Java 8, we can replace the Function object with an inline function expression, making the code a lot cleaner:\n<h5 id=\"java-8-search-example\">Java 8 search example:</h5>\n\n<pre>JavaRDD<String> lines = sc.textFile(\"hdfs://log.txt\")\n                          .filter(s -> s.contains(\"error\"));\nlong numErrors = lines.count();</pre>\n\nThe gains become even bigger for longer programs. For instance, the program below implements Word Count, by taking a file (read as a collection of lines), splitting each line into multiple words, then counting the words with a reduce function.\n<h5 id=\"java-7-word-count\">Java 7 word count:</h5>\n\n<pre>JavaRDD<String> lines = sc.textFile(\"hdfs://log.txt\");\n\n// Map each line to multiple words\nJavaRDD<String> words = lines.flatMap(\n  new FlatMapFunction<String, String>() {\n    public Iterable<String> call(String line) {\n      return Arrays.asList(line.split(\" \"));\n    }\n});\n\n// Turn the words into (word, 1) pairs\nJavaPairRDD<String, Integer> ones = words.mapToPair(\n  new PairFunction<String, String, Integer>() {\n    public Tuple2<String, Integer> call(String w) {\n      return new Tuple2<String, Integer>(w, 1);\n    }\n});\n\n// Group up and add the pairs by key to produce counts\nJavaPairRDD<String, Integer> counts = ones.reduceByKey(\n  new Function2<Integer, Integer, Integer>() {\n    public Integer call(Integer i1, Integer i2) {\n      return i1 + i2;\n    }\n});\n\ncounts.saveAsTextFile(\"hdfs://counts.txt\");</pre>\n\nWith Java 8, we can write this program in just a few lines:\n<h5 id=\"java-8-word-count\">Java 8 word count:</h5>\n\n<pre>JavaRDD<String> lines = sc.textFile(\"hdfs://log.txt\");\nJavaRDD<String> words =\n    lines.flatMap(line -> Arrays.asList(line.split(\" \")));\nJavaPairRDD<String, Integer> counts =\n    words.mapToPair(w -> new Tuple2<String, Integer>(w, 1))\n         .reduceByKey((x, y) -> x + y);\ncounts.saveAsTextFile(\"hdfs://counts.txt\");</pre>\n\nWe are very excited to offer this functionality, as it opens up the simple, concise programming style that Scala and Python Spark users are familiar with to a much broader set of developers.\n<h2 id=\"availability\">Availability</h2>\nJava 8 lambda support will be available in Apache Spark 1.0, which will be released in early May. Although using this syntax requires Java 8, <em>Apache Spark 1.0 will still support older versions of Java through the old form of the API</em>. Lambda expressions are simply a shorthand for anonymous inner classes, so the same API can be used in any Java version.\n<h2 id=\"learn-more-about-spark\">Learn More About Spark</h2>\nIf you\u2019d like to learn more about Spark, the <a href=\"http://spark.apache.org/docs/latest/index.html\">official documentation</a> can help you get started today in either Java, Scala or Python. Spark is easy to run on your laptop, without any installation other than <a href=\"http://spark.apache.org/downloads.html\">downloading</a> and unzipping a release."}
{"status": "publish", "description": null, "creator": "pat.mcdonough", "link": "https://databricks.com/blog/2014/06/02/databricks-hands-on-technical-workshops.html", "authors": ["Databricks Training Team"], "id": 273, "categories": ["Announcements", "Company Blog", "Events"], "dates": {"publishedOn": "2014-06-02", "tz": "UTC", "createdOn": "2014-06-02"}, "title": "Databricks Announces Apache Spark Training Workshops", "slug": "databricks-hands-on-technical-workshops", "content": "Databricks is excited\u00a0to launch\u00a0its\u00a0training program, starting with\u00a0<a title=\"Spark Training\" href=\"https://databricks.com/training\">a series of hands-on Apache Spark\u00a0workshops</a>\u00a0designed by the creators of Apache Spark.\n\nThe first workshop, <em>Introduction to Apache Spark</em>, establishes\u00a0the fundamentals of\u00a0using Spark for data exploration, analysis, and building big data applications. This\u00a0one day workshop is\u00a0hands-on, covering topics such as: interactively\u00a0working with\u00a0Spark's core APIs, learning\u00a0the key concepts of big data, deploying applications on common Hadoop distributions, and unifying data pipelines\u00a0with\u00a0SQL, Streaming, and Machine Learning.\n\nWorkshops are currently scheduled in New York, San Jose, Austin, and Chicago, with workshops in more cities to come.\u00a0Visit <a title=\"Databricks Training\" href=\"https://databricks.com/training\">Databricks' training page</a>\u00a0to find more information and\u00a0please leave\u00a0feedback there if you'd like to see\u00a0a workshop in your area.\n<ul class=\"content\">\n \t<li>Jun 11 - New York, NY -\u00a0<a href=\"http://apache-spark-nyc.eventbrite.com\" target=\"_blank\">More Info &amp; Registration</a></li>\n \t<li>Jun 23 - San Jose, CA -\u00a0<a href=\"http://spark-san-jose.eventbrite.com/\" target=\"_blank\">More Info &amp; Registration</a></li>\n \t<li>July 2 - San Francisco, CA -\u00a0<a href=\"http://spark-summit.org/2014\">More Info &amp; Registration</a></li>\n \t<li>Jul 28 - Austin, TX -\u00a0<a href=\"https://www.eventbrite.com/e/hands-on-apache-spark-workshop-austin-tickets-11663609169\" target=\"_blank\">More Info &amp; Registration</a></li>\n \t<li>Aug 25 - Chicago, IL -\u00a0<a href=\"http://spark-workshop-chicago.eventbrite.com\" target=\"_blank\">More Info &amp; Registration</a></li>\n</ul>\nDatabricks\u00a0will continue to expand\u00a0the curriculum and\u00a0coverage area in the near future. Check back soon as we announce new workshops, or send feedback to <a href=\"mailto:training-feedback@databricks.com\">training-feedback@databricks.com</a>."}
{"status": "publish", "description": null, "creator": "arsalan", "link": "https://databricks.com/blog/2014/05/22/application-spotlight-atigeo-xpatterns.html", "authors": ["Claudiu Barbura (Sr. Dir. of Engineering at Atigeo LLC)"], "id": 274, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2014-05-23", "tz": "UTC", "createdOn": "2014-05-23"}, "title": "Application Spotlight: Atigeo xPatterns", "slug": "application-spotlight-atigeo-xpatterns", "content": "<div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://atigeo.com\">Atigeo</a> announcing the certification of their xPatterns offering.</div>\n\n<hr />\n\nHere at <a href=\"http://atigeo.com/\">Atigeo</a>, we are always looking for ways to build on, improve, and expand our big data analytics platform, Atigeo xPatterns. More than that, both our development and product management team are focused on big data and on knowing what is right for our customers: data scientists and application developers at companies who are seeking to make the best possible use of their data assets. So we all stay on the lookout for the most useful, advanced, and best-performing set of technologies available.\n\nApache Spark, for us, was a standout: We could see that making a dramatic performance improvement available to our customers and users would mean that xPattern\u2019s analytics, modeling, and machine learning would be more responsive, and that Spark in xPatterns would give our customers an even quicker path from data ingestion to useful insights.\n\nOur development team works on the 2 nearly opposite sides of the world, here in Bellevue, Washington just outside Seattle, and in Timisoara, Romania. These teams stay in nearly constant touch online, on social media, and even in person. So we all shared this vision of xPatterns enhanced with Spark. We found Spark useful across many stages of our xPatterns big data pipeline \u2014 some highlights of which include: ingestion, data transformation, interactive data exploration, and export to NoSQL.\n\nAnd as we made this vision a reality, with infrastructure improvements and corresponding improvements in data modeling, the Strata conference in Santa Clara, California, lay ahead as a goal and a chance to share what we\u2019d learned and built with the wider community.\n\nAnd at Strata, sure enough, xPatterns with Spark \u2014 as well as Shark, Apache Mesos, and Tachyon \u2014 was a big hit with visitors to our booth and on social media.\n\nEven then, we knew there was still more to do: We wanted to provide objective evidence that our integration of Spark would be robust and offer excellent performance across a wide range of Spark distributions, so that users could be sure that the goodness of Spark would be available to them regardless of their choice in Spark distribution. So, we made an enormous investment across our far-flung team for further development, testing, profiling, and logging. After this broad effort, we reached out to Databricks because we saw that their certification program was in line with our vision: to prevent fragmentation and forking in the Spark community and help the Spark ecosystem grow. Additionally it was based on an entirely open process: open source testing tools along with the open source Apache Spark distribution that gave us additional comfort.\n\nSo, it is with both pride \u2014 as well as thanks to the team at Databricks \u2014 that we announce that xPatterns is \u201cCertified on Spark\u201d."}
{"status": "publish", "description": null, "creator": "arsalan", "link": "https://databricks.com/blog/2014/05/23/pivotal-hadoop-integrates-the-full-apache-spark-stack.html", "authors": ["Sarabjeet Chugh (Head of Hadoop Product Management at Pivotal Inc.)"], "id": 297, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2014-05-23", "tz": "UTC", "createdOn": "2014-05-23"}, "title": "Pivotal Hadoop Integrates the Full Apache Spark Stack", "slug": "pivotal-hadoop-integrates-the-full-apache-spark-stack", "content": "<div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://www.gopivotal.com\" target=\"_blank\">Pivotal</a> describing why they\u2019re excited to deliver Apache Spark on their world class Pivotal HD big data analytics platform suite.</div>\n\n<hr />\n\nToday, we are excited to announce the immediate availability of the full Apache Spark stack on Pivotal HD.  We have been impressed with the rapid adoption of Spark as a replacement for Hadoop\u2019s more traditional processing engines as well as its vibrant ecosystem, and are thrilled to make it possible for Pivotal customers to run Apache Spark on Pivotal HD Hadoop.  Just as important is how we\u2019re doing it: Pivotal HD will be part of Databricks\u2019 upcoming certification program \u2013 meaning a commitment to provide compatibility with Apache Spark and support the growing ecosystem of Spark applications.\n\n<h2>PivotalHD and Spark</h2>\nUnlike a multi-vendor patchwork of heterogeneous solutions, Pivotal brings together an integrated full stack of technologies to allow enterprises to create a Business Data Lake. Pivotal HD 2.0.1 consists of a Hadoop distribution that is compatible with Apache Hadoop 2.x, a market-leading SQL on Hadoop query engine in HAWQ, and GemfireXD for in-memory data serving and ultra-low latency transaction processing capabilities. Together these platforms extend Pivotal\u2019s differentiation in both the Hadoop ecosystem and the more established data warehousing markets, meeting the full spectrum of analytics requirements from batch to ultra-low latency.\n \nWith Spark, Pivotal aims to further extend this differentiation by leveraging Spark\u2019s cutting edge capabilities and integrating it with the rest of Pivotal\u2019s world-class platform.  Much has been written about Spark\u2019s benefits, but what really drew us to it were the following characteristics:\n<ul>\n\n\t<li><strong>Speed: </strong>Spark can process HDFS data in-place up to 100x faster than Hadoop MapReduce using it\u2019s in-memory-optimized architecture<p></li>\n\n\t<li><strong>Unification: </strong>Out-of-the-box Spark provides a wide breath of functionality \u2013 including streaming data support, machine learning, and graph computation \u2013 which when combined with PivotalHD give customers a full end-to-end experience<p></li>\n\n\t<li><strong>Ease of use: </strong>Spark enables developers to use Java, Scala, or Python across their entire workflow; additionally it exposes 80+ high-level operators that allow it to have 2-5x less code than similar MapReduce jobs</li>\n\n</ul>\n\nThough the traditional Hadoop processing components such as MapReduce, Pig, and Hive will remain part of PivotalHD, we imagine many customers will begin using Spark instead because of these benefits.\n\n<h2>Pivotal and Open Source</h2>\nOpen source is a critical part of Pivotal\u2019s DNA. Pivotal has been committed to open source software through active involvement in open-source projects such as Tomcat, RabbitMQ, Redis, Hadoop, Cloud Foundry, Spring, Grails, MADlib, Chorus, and Groovy \u2013 Spark will be no different. \n\nOne of the main attractions of Spark for us is the growing community and ecosystem.  With nearly 200 contributors over the past 12 months, it is one of the most active projects in the Apache and Hadoop open-source ecosystem.  Even more exciting is the potential that the ecosystem of applications built on top of Spark holds (something that we\u2019re obviously passionate about at Pivotal); new \u201cpowered-by-spark\u201d applications seem to be emerging daily.\n\nHowever, we\u2019ve seen how quickly this potential can diminished with forking and fragmentation in open source projects before.  That is why we\u2019re excited to join Databricks in their efforts to unify the community.  Pivotal\u2019s distribution of Spark provides full compatibility with Apache Spark, enabling the growing set of \u201cCertified on Spark\u201d applications to run on it out of the box.  Given the benefits for our customers, and the open and transparent nature of the process, this was an easy decision.  This effort is yet another testament of Pivotal\u2019s commitment to open source innovation that brings value to customers. \n\n<h2>Pivotal and Databricks</h2>\nDatabricks was founded by the original team that developed Apache Spark, and is currently the driving force behind the project. When Pivotal decided to certify its distribution with full Apache Spark stack with PivotalHD and increase our involvement in the Spark community, we could think of no better ally than Databricks with whom to embark on this exciting journey. Furthermore, we\u2019re thrilled to join their effort to maintain compatibility across the growing Spark ecosystem.  We\u2019re excited to be making this announcement on the Databricks blog, and look forward to a long and deep relationship with our friends at Databricks.\n\n<h2>Getting Started</h2>\nTry out Pivotal\u2019s Spark bundle on Pivotal HD 2.0.1 by obtaining the Pivotal Spark tarball and quick-start instructions <a href=\"https://support.gopivotal.com/hc/en-us/articles/203271897-Spark-on-Pivotal-Hadoop-2-0-Quick-Start-Guide\" target=\"_blank\">here</a>. The Pivotal HD 2.0.1 release is available for download <a href=\"https://network.gopivotal.com/products/pivotal-hd\" target=\"_blank\">here</a>. We would love to hear from you and welcome the opportunity to engage in a dialog. Please feel free to drop us a note at <a href=\"mailto:spark@gopivotal.com\">spark@gopivotal.com</a> if you have any questions, or if we can be of any help for your intended use case.\n\nAdditionally, make sure you come visit our booth at the upcoming <a href=\"http://www.spark-summit.org/\" target=\"_blank\">Spark Summit</a> to hear more about using Pivotal HD \u2013 now with Spark!"}
{"status": "publish", "description": null, "creator": "patrick", "link": "https://databricks.com/blog/2014/05/30/announcing-spark-1-0.html", "authors": ["Patrick Wendell"], "id": 502, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2014-05-30", "tz": "UTC", "createdOn": "2014-05-30"}, "title": "Announcing Apache Spark 1.0", "slug": "announcing-spark-1-0", "content": "Today, we\u2019re very proud to announce the release of <a title=\"Spark 1.0.0 Release Notes\" href=\"http://spark.apache.org/releases/spark-release-1-0-0.html\">Apache Spark 1.0</a>. Apache Spark 1.0 is a major milestone for the Spark project that brings both numerous new features and strong API compatibility guarantees. The release is also a huge milestone for the Spark developer community: with more than 110 contributors over the past 4 months, it is Spark\u2019s largest release yet, continuing a trend that has quickly made Spark the most active project in the Hadoop ecosystem.\n<h2>New Features</h2>\nWhat features are we most excited about in Apache Spark 1.0? While there are dozens of new features in the release, we\u2019d like to highlight three.\n\n<b>Spark SQL</b>\n\nThe biggest single addition to Apache Spark 1.0 is Spark SQL, a new module that <a title=\"Spark SQL\" href=\"https://databricks.com/blog/2014/03/26/spark-sql-manipulating-structured-data-using-spark-2.html\">we\u2019ve previously blogged about</a>. Spark SQL offers integrated support for SQL queries alongside existing Spark code, making it seamless to write applications that load structured data (from sources like Hive and Parquet) and run advanced analytics or ETL. Spark SQL will also be the backend for future versions of Shark, providing a simpler, more agile, and optimized execution engine.\n\n<b>Management and Deployment</b>\n\nApache Spark 1.0 also includes major improvements to management and deployment. It adds full support for the Hadoop/YARN security model, running seamlessly in secured Hadoop clusters. It also drastically simplifies job submission, allowing users to easily deploy the same application and on a single machine, a Spark cluster, EC2, Mesos, or YARN. Packaging and deploying a Spark app has never been easier!\n\n<b>Java 8 API</b>\n\nSpark\u2019s Java API has been extended to support <a title=\"Making Spark Easier to Use in Java with Java 8\" href=\"https://databricks.com/blog/2014/04/14/spark-with-java-8.html\">Java 8 lambda expressions</a>, allowing much more concise programming for users of Java 8. Spark still supports Java 6 and 7 through the older API.\n<h2>Community Growth</h2>\nWe are especially excited about the continued growth of the Spark community. Apache Spark 1.0 is the work of more than 110 individuals over the past 4 months, the most to ever contribute to a Spark release. More impressively, the rapid growth in the community has now made Spark the most active project in the Hadoop ecosystem by a wide margin, and one of the <a href=\"https://www.ohloh.net/orgs/apache\">most active projects at Apache</a>. This rapid pace of innovation allows us add features, stability improvements, optimizations and fixes at an unprecedented rate.\n\n<center><a href=\"https://databricks.com/wp-content/uploads/2014/05/chart2.png\"><img class=\"alignnone wp-image-516 size-medium\" style=\"margin-top: 5px; margin-bottom: 5px;\" src=\"https://databricks.com/wp-content/uploads/2014/05/chart2-300x228.png\" alt=\"\" width=\"300\" height=\"228\" /></a></center>After the 1.0 release, Spark will target a quarterly cadence for minor releases (1.1, 1.2, 1.3) and will continue to make maintenance releases as-needed to provide stable versions to users.\n<h2>Databricks\u2019 Commitment to Open Source</h2>\nAt Databricks, we are proud to do all our Apache Spark development in the open -- every new feature and improvement we have made to Spark has been open source. Many of our distribution partners are quickly moving to include 1.0 -- for example, Apache Spark 1.0 will appear in CDH 5.1 in June.\n<h2>More Information</h2>\nThis post has only scratched the surface: Apache Spark 1.0 includes dozens of features not mentioned here, including major improvements to MLLib, GraphX, and Spark Streaming. Head over to the official release notes for a longer write-up. Over the next few weeks, we\u2019ll also be writing more blog posts on select new features here.\n\nFinally, if you would like to learn more about Apache Spark or see how it is being used, join us at the <a href=\"http://spark-summit.org/2014\">Spark Summit</a> on June 30th--July 2nd. With over 50 talks from organizations using Spark and a full day of training, the Summit will be the largest Spark community event yet."}
{"status": "publish", "description": null, "creator": "michael", "link": "https://databricks.com/blog/2014/06/02/exciting-performance-improvements-on-the-horizon-for-spark-sql.html", "authors": ["Michael Armbrust", "Zongheng Yang"], "id": 528, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2014-06-02", "tz": "UTC", "createdOn": "2014-06-02"}, "title": "Exciting Performance Improvements on the Horizon for Spark SQL", "slug": "exciting-performance-improvements-on-the-horizon-for-spark-sql", "content": "With <a title=\"Announcing Spark 1.0\" href=\"https://databricks.com/blog/2014/05/30/announcing-spark-1-0.html\">Apache Spark 1.0</a> out the door, we\u2019d like to give a preview of the next major initiatives in the Spark project. Today, the most active component of Spark is <a title=\"Spark SQL: Manipulating Structured Data Using Spark\" href=\"https://databricks.com/blog/2014/03/26/spark-sql-manipulating-structured-data-using-spark-2.html\">Spark SQL</a> - a tightly integrated relational engine that inter-operates with the core Spark API. Spark SQL was released in Spark 1.0, and will provide a lighter weight, agile execution backend for future versions of Shark. In this post, we\u2019d like to highlight some of the ways in which tight integration into Scala and Spark provide us powerful tools to optimize query execution with Spark SQL. This post outlines one of the most exciting features, dynamic code generation, and explains what type of performance boost this feature can offer using queries from a well-known benchmark, TPC-DS. As a baseline, we compare performance against the current Shark release. We\u2019re happy to report that in these cases Spark SQL outperforms Shark,\u00a0sometimes\u00a0dramatically. Note that the following\u00a0tests were run on a development branch of Spark SQL,\u00a0which includes\u00a0several new performance features.\n\n<a href=\"https://databricks.com/wp-content/uploads/2014/06/SharkVsSparkSql.png\"><img class=\"alignnone size-full wp-image-546\" src=\"https://databricks.com/wp-content/uploads/2014/06/SharkVsSparkSql.png\" alt=\"SharkVsSparkSql\" width=\"518\" height=\"371\" /></a><em>While this is only a few queries from the TPC-DS benchmark, we plan to release more comprehensive results in the future.</em>\n\nNow that we have seen where things are headed, let's dive into the technical details of how we plan to get there. \u00a0This is the first in a series of blog posts about optimizations coming\u00a0in Spark SQL.\n<h2>Runtime Bytecode Generation</h2>\nOne of the more expensive operations that needs to be performed by a database is the evaluation of expressions in the query. The memory model of the JVM\u00a0can increase the cost of this evaluation significantly. \u00a0To understand why this is the case, let\u2019s look at a concrete example:\n\n<pre>SELECT a + b FROM table</pre>\n\nIn the above query, there is one expression that is going to be evaluated for each row of the table, <tt>a + b</tt>. \u00a0Typically, this expression would be represented by an expression tree. <a href=\"https://databricks.com/wp-content/uploads/2014/06/ExpressionTree.png\"><img class=\"wp-image-532 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2014/06/ExpressionTree-300x202.png\" alt=\"ExpressionTree\" width=\"200\" height=\"135\" /></a>\n\nEach node of the tree is represented by an\u00a0object\u00a0with an evaluation method that knows how to calculate the result of the expression given an input row. \u00a0In this example, when the evaluate method is called on the Add object, it would in turn call evaluate on each of its children and then compute the sum of the returned values. What is wrong with this approach from a performance perspective? \u00a0In practice, the performance hit comes from several details of this interpreted execution:\n<ul>\n\t<li><strong>Virtual Function Calls</strong> - Each time evaluate is called on a given expression object, \u00a0a <a href=\"http://en.wikipedia.org/wiki/Virtual_function\">virtual function</a> call is made. \u00a0These types of function calls can disrupt pipelining in the processor, slowing down execution. \u00a0The problem worsens when the expressions are very complex, such as those in the TPC-DS benchmark.</li>\n\t<li><strong>Boxing of Primitive Values</strong> - Since evaluate needs to be able to return many different types of values depending on the expression (Integer, String, Float, etc.) it needs to have a generic return type of Object. \u00a0This means that an extra object needs to be allocated for each step of the evaluation. \u00a0While modern JVMs have gotten better at cheaply allocating short-lived objects, this cost can really add up.</li>\n\t<li><strong>Cost of Genericity</strong> - These expression trees need to be able to handle many different data types. \u00a0However, the actual function that is required to add two integers is different from that that is required to add two doubles, for example. \u00a0This genericity means that the evaluation code often requires extra if-statements that branch based on the type of data being processed.</li>\n</ul>\nAltogether, the above issues can lead to a significant increase in query execution time. Fortunately, there is another way! \u00a0In a\u00a0development\u00a0branch of Spark SQL, we have implemented\u00a0a version of our expression evaluator that dynamically generates custom bytecode for each query. \u00a0While such generation may sound like a difficult task, its actually straightforward\u00a0to implement using a new feature in Scala 2.10, <a href=\"http://docs.scala-lang.org/overviews/reflection/overview.html\">runtime reflection</a>. At a high level, bytecode generation means that, instead of using an expression tree to evaluate <tt>a + b</tt>, Spark SQL will create new classes at runtime that have custom code similar to the following:\n\n<pre>val a: Int = inputRow.getInt(0)\nval b: Int = inputRow.getInt(1)\nval result: Int = a + b\nresultRow.setInt(0, result)</pre>\n\nCompared to the interpreted evaluation, the generated code works on primitive values (and thus doesn't allocate any objects) and includes no extra function calls.\n\n<h2>Using Quasiquotes</h2>\n\nWhile Spark SQL is not the only system to perform dynamic generation of code for query execution, the use of Scala reflection greatly simplifies the implementation, making it much easier to extend and improve. \u00a0Key to this functionality a new feature in Scala, known as <a href=\"http://docs.scala-lang.org/overviews/quasiquotes/intro.html\">Quasiquotes</a>. \u00a0Quasiquotes make it easy to build trees of Scala code at runtime, without having to build complex ASTs by hand. To use a quasiquote, you simple prefix a string in Scala with the letter \"q\". \u00a0In doing so, you tell the Scala compiler to treat the contents of the string as code, instead of text. \u00a0You can also used $ variables to splice together different fragments of code. For example, the code to generate the above addition expression could be expressed simply as follows:\n\n<pre>def generateCode(e: Expression): Tree = e match {\n  case Attribute(ordinal) =&>\n    q\"inputRow.getInt($ordinal)\"\n  case Add(left, right) =&>\n    q\"\"\"\n      {\n        val leftResult = ${generateCode(left)}\n        val rightResult = ${generateCode(right)}\n        leftResult + rightResult\n      }\n    \"\"\"\n}</pre>\n\nIn practice, the generated code is a little more complicated, as it also needs to handle null values. \u00a0If you'd like to see the full version of the generated code for our example expression, it is available <a href=\"https://gist.github.com/marmbrus/9efb31d2b5154aea6652\">here</a>.\n\n<h2>Conclusion</h2>\n\nDynamic code generation is\u00a0just the tip of the iceberg, and we have a lot more improvements in store, including: improved Parquet integration, the ability to automatically query semi-structured data (such as JSON), and JDBC access to Spark SQL. \u00a0Stay tuned for more updates on other optimization we are making to Spark SQL!\n\n<span style=\"color: #777777;\">Finally, if you would like to learn more about Spark SQL or see how it is being used, join us at the\u00a0</span><a style=\"color: #428bca;\" href=\"http://spark-summit.org/2014\">Spark Summit</a><span style=\"color: #777777;\">\u00a0on June 30th\u2013July 2nd. With over 50 talks from organizations using Spark and a full day of training, the Summit will be the largest Spark community event yet.</span>"}
{"status": "publish", "description": null, "creator": "arsalan", "link": "https://databricks.com/blog/2014/06/04/microstrategy-certified-on-spark.html", "authors": ["Michael Hiskey (VP at MicroStrategy Inc.)"], "id": 569, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2014-06-04", "tz": "UTC", "createdOn": "2014-06-04"}, "title": "MicroStrategy \"Certified on Apache Spark\"", "slug": "microstrategy-certified-on-spark", "content": "<div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://www.microstrategy.com\" target=\"_blank\">MicroStrategy</a> describing why they're excited to have their platform \"Certified on Apache Spark\".</div>\n\n<hr />\n\n<h2>The Need for Speed</h2>\nOver the past few years, we have seen Hadoop emerge as an effective foundation for many organizations\u2019 big data management frameworks, but as the volume and varieties of data increase, speed continues to be a challenge. More and more of our customers are embracing Big Data, and the value of their investment is dependent on (and limited by) how quickly they can take data to action. We\u2019ve been listening to our clients to understand how we can innovate to stay ahead of the curve to help solve these challenges. Apache Spark grabbed our attention because it addresses many of the limitations of Hadoop\u2019s traditional functionality. Plus, Spark is simply impossible to ignore. The active, growing community of developers and enterprises that have adopted Spark has made this project the hottest open source item from the last year. We want to see the open source community continue to flourish and are excited to see Spark open doors for our customers as the application ecosystem grows and reaches wider audiences.\n<h2>MicroStrategy Analytics Platform Certified on Spark</h2>\nAs a leader in enterprise BI and analytics, MicroStrategy is devoted to helping people maximize the value of their data quickly and easily. With this in mind, we are excited to say that MicroStrategy Analytics PlatformTM is one of the first comprehensive enterprise-grade BI and analytics platforms to be officially certified to work with Apache Spark through the Databricks \u201cCertified on Spark\u201d program. On top of this, MicroStrategy has decided to begin a key partnership with Databricks\u2014the creators, advocates, and community-drivers of Spark. This was a no-brainer for us. We share Databricks\u2019 belief that widespread adoption of Spark will be a huge benefit to the community. Also, this partnership will facilitate close collaboration and deep integration of our technologies in the long run. Microstrategy\u2019s certification with Spark is just the first step of a hopefully long, productive relationship. We are thrilled to be an early adopter of Spark and excited for this journey.\n<h2>MicroStrategy/Spark Advantages</h2>\nRunning Microstrategy on Spark can be boiled down to three significant advantages: speed, ease-of-use, and enhanced advanced analytics. By running MicroStrategy on top of Spark, users will be able to accomplish data analytics projects up to 100x faster. The key is Spark\u2019s in-memory processing, which avoids the constant access to the disk required by MapReduce, a costly and time-consuming exercise. Spark has native support of Java, Scala, and Python and requires far less coding for comparable Hadoop jobs. Also, Shark (Hive on Spark) delivers real-time query capabilities for improved ad-hoc data exploration, making Hadoop reasonably interactive for the first time. Spark\u2019s fully unified platform provides a wide array of advanced analytics capabilities as first-class citizens, including streaming data support, machine learning, and graph computation. These features benefit end-to-end application performance and reduce decision latency\u2014in other words, once data is available, users can uncover insights and act, faster. In effect, users can enhance and fine-tune the performance of their big data framework and as a result get more value from their data at every level.\n\nCustomers will benefit enormously from Spark/MicroStrategy integration, and some are already leading the charge: Yahoo! Taiwan is currently using Spark/Microstrategy configurations in a testing environment. This partnership is an extension of MicroStrategy\u2019s commitment to giving our clients the most comprehensive advanced analytics and business intelligence solutions that not only provide fast visual insights on Big Data, but also carries enterprise-class scalability, reliability, and data governance.\n\nMicroStrategy is committed to solving the biggest data challenges. For more information on MicroStrategy Big Data Solutions, visit our website at:\n\n<a href=\"http://www.microstrategy.com/us/platforms/analytics/big-data-solutions\" target=\"_blank\">http://www.microstrategy.com/us/platforms/analytics/big-data-solutions</a>\n\nAlso visit our blog at:\n<a href=\"http://www.microstrategy.com/us/blog\" target=\"_blank\">http://www.microstrategy.com/us/blog</a>"}
{"status": "publish", "description": null, "creator": "arsalan", "link": "https://databricks.com/blog/2014/06/11/application-spotlight-arimo.html", "authors": ["Christopher Nguyen (CEO &amp; Co-Founder of Adatao)"], "id": 585, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2014-06-11", "tz": "UTC", "createdOn": "2014-06-11"}, "title": "Application Spotlight: Arimo", "slug": "application-spotlight-arimo", "content": "<div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://www.arimo.com\" target=\"_blank\">Arimo</a> describing why and how they bet on Apache Spark.</div>\n\n<hr />\n\nIn early 2012, a group of engineers with background in distributed systems and machine learning came together to form Arimo. We saw a major unsolved problem in the nascent Hadoop ecosystem: it was largely a storage play. Data was sitting passively on HDFS, with very little value being extracted. To be sure, there was MapReduce, Hive, Pig, etc., but value is a strong function of (a) speed of computation, (b) sophistication of logic, and (c) ease of use. While Hadoop ecosystem was being developed well at the substrate, there was enormous opportunities above it left uncaptured.\n\n<strong>On speed:</strong> we had seen data move at-scale and at enormously faster rates in systems like Dremel and PowerDrill at Google. It enabled interactive behavior simply not available to Hadoop users. Without doubt, we knew that interactive speed was necessary, and that in-memory computing was key to the solution. As Cloudera\u2019s Mike Olson has quipped, \u201cWe\u2019re lucky to live in an age where there\u2019s a Google. They live about 5 years in the future, and occasionally send messages back to the rest of us.\u201d Google does indeed \u201clive in the future\u201d, in terms of the demands of scale and the value it is extracting from data.\n\n<strong>On sophistication:</strong> for Arimo, the essential difference between \u201csmall\u201d and \u201cbig\u201d data is whether data is big enough to learn from. For some questions, such as \u201cDoes it hurt to hit my head against a brick wall?\u201d, 100 samples suffice. To classify large images, a million samples aren\u2019t enough. We knew this was the second missing key in Big Data: aggregates and descriptives were necessary but insufficient. The Big-Data world needed the sophistication of machine learning. Big Data needed Big Compute. \u201cPredictive\u201d isn\u2019t just another adjective in a long string of X-analytics; it is the quantum change, separating the value of big from small.\n\nThus Arimo was born as a \u201cBig Data/Machine Learning\u201d company. Our exact product features would be driven by customer conversations, but the core thesis was clear. We wanted to bring \u201cData Intelligence for All\u201d, specifically with the speed and sophistication discussed above.\n\nIf in-memory compute and machine-learning logic were the key to unlocking the value of Big Data, why hadn\u2019t this been solved already in 2012? Because cost/benefit trade-offs matter, in any technology transition. In the chart below, the crossover points happened at different times for different endeavors; it hit critical mass on Wall Street about 2000-2005, at Google c. 2006-2010, and we project for the enterprise world at-large: about now (2013-2015).\n<p style=\"margin-bottom: 10px;\"><img class=\"aligncenter size-full wp-image-62\" src=\"https://databricks.com/wp-content/uploads/2014/06/MemoryCosts.png\" alt=\"MemoryCosts\" width=\"472\" /></p>\n<em>Fig 1. Cross-over points for transitions to in-memory computing</em>\n\nIf this isn\u2019t clearly happening for your organization or industry yet, relax. It will, soon. Because as the latency and bandwidth trend charts below show, the future increasingly favors RAM.\n\n<img src=\"https://databricks.com/wp-content/uploads/2014/06/Latency.png\" alt=\"Latency\" />\n\n<img src=\"https://databricks.com/wp-content/uploads/2014/06/BandwidthTrends.png\" alt=\"BandwidthTrends\" />\n\n<em>Fig 2. The future increasingly favors a shift to RAM</em><!--more-->\n\nAs the Arimo team set out to build this big-compute analytic stack on Hadoop, we wanted our solution to reach all the way to the business users, while also exposing convenient APIs for data engineers and scientists. This required a combination of a great collaborative user interface, solid data-mining and machine-learning logic, backed by a powerful big-compute engine. We did a survey of the in-memory landscape, and found a small number of teams also working in the same direction. But virtually all were either too incremental or too aggressive. Some were developing work-arounds such as caching data between MR iterations, or maintaining a low-level memory cache with no persistent, high-level data structures. Others promoted yet-slow &amp; expensive \u201cvirtualized memory\u201d architectures, still too early for prime time.\n\nThen we came across Spark and the Berkeley AMPLab team. Immediately, we knew they had identified the right problem statements, and made the right architectural decisions for the times. Here are some key design choices correctly made for widespread adoption c. 2012:\n<ol>\n \t<li style=\"margin-bottom: 14px;\"><strong>Data model:</strong> Spark was the only architecture that supported the concept of a high-level, persistent distributed in-memory dataset. All \u201cin-memory\u201d systems are not equivalent. Spark\u2019s RDDs exist independently of any given compute step, allowing for not only speedy iterative algorithms, with high-level data sets readily available to each iteration without delay. Equally importantly, they made long-running interactive memory-speed applications possible.</li>\n \t<li style=\"margin-bottom: 14px;\"><strong>Resiliency by recomputation</strong>: with replication being the other option, Spark made the timely choice to prefer recomputation. Memory had gotten cheaper, but not yet cheap enough for replication to be the order of the day, as it is with HDFS disks.</li>\n \t<li style=\"margin-bottom: 14px;\"><strong>General DAG support</strong>: while it was possible to build dedicated SQL query engines to overcome Hadoop MapReduce\u2019s limitations (and others did choose this path), Spark\u2019s general DAG model meant we could build arbitrary algorithms and applications on it.</li>\n</ol>\nWe were ecstatic. Spark represented years of R&amp;D we didn\u2019t have to spend building an engine before building sophisticated, user-facing applications. When we made the decision to support the AMPLab Spark effort, there were only 1 or 2 others that had made similar commitments. We were seriously betting the company on Spark.\n\nBut thanks to Spark, we were able to move ahead quietly and quickly on Arimo <a href=\"http://adatao.com/pinsights.html\" target=\"_blank\">pInsights</a> and <a href=\"http://arimo.com/panalytics.html\" target=\"_blank\">pAnalytics</a>, iterating on customer feedback while passing our inputs and market data along to the Spark team. We promoted Spark\u2019s goodness in every relevant conversation. By late summer 2013, <a href=\"http://www.databricks.com/\" target=\"_blank\">Databricks</a> was about to be born, further increasing our confidence on the Spark-on-Hadoop ecosystem. There was now going to be an official, commercial entity with an existence predicated on developing the growth of the ecosystem and maintaining its health. And the team at Databricks is doing an excellent job at that stewardship.\n\nToday, Arimo is one of the first applications to be <em><a href=\"https://databricks.com/certification\" target=\"_blank\">Certified on Spark</a></em>. We\u2019re seeing remarkable enterprise adoption speeds for Arimo-on-Spark. The most sophisticated customers tend to be companies that have already deployed Hadoop, who are all too familiar with the failed promises of Big Data. We see immediate excitement in customers the moment they see the Arimo solution: a user-facing analytics application that is interactive, easy-to-use, supports both basic analytics and machine learning, and is actually running in seconds of real time over large Hadoop datasets. Finally, users are truly able to extract data intelligence from data storage. Value creation is no longer just about Big Data. It\u2019s about Big Compute, and Spark has delivered that capability for us.\n\nSpark has made it as a top-level Apache project, going from incubation to graduation in record time. It is also one of Apache\u2019s most active projects with hundreds of contributors. This is because of its superior architecture and timeliness of engineering choices, as discussed above. With that plus appropriate care and feeding, Apache Spark will have a bright future even as it evolves and adapts to changing technology and business drivers.\n\n<hr />\n\n<em>Christopher Nguyen is co-founder and CEO of Arimo. He is a former engineering director of Google Apps, a Stanford PhD who co-founded two enterprise startups with successful exits, and a professor and co-founder of the Computer Engineering program at HKUST. He graduated from U.C. Berkeley summa cum laude. Christopher has extensive experience building technology companies that solve enterprise business challenges. Come hear his talk on <a href=\"https://spark-summit.org/2014/talk/distributed-dataframe-ddf-on-apache-spark-simplifying-big-data-for-the-rest-of-us\" target=\"_blank\">Distributed DataFrames (DDF) on Spark: Simplifying Big Data for the Rest of Us</a> at the <a href=\"http://www.spark-summit.org/\" target=\"_blank\">Spark Summit 2014</a>.</em>"}
{"status": "publish", "description": null, "creator": "scott", "link": "https://databricks.com/blog/2014/06/11/spark-summit-2014-brings-together-apache-spark-community.html", "authors": ["Databricks Press Office"], "id": 609, "categories": ["Company Blog", "Events"], "dates": {"publishedOn": "2014-06-12", "tz": "UTC", "createdOn": "2014-06-12"}, "title": "Spark Summit 2014 Brings Together Apache Spark Community", "slug": "spark-summit-2014-brings-together-apache-spark-community", "content": "<ul>\n \t<li>Three-Day Event in San Francisco Invites Attendees to Gain Insights from the Leading Organizations in Big Data</li>\n \t<li>Keynote Speakers Include Executives from Databricks, Cloudera, MapR, DataStax, Jawbone and More</li>\n \t<li>Spark Summit Features Different Tracks for Applications, Development, Data Science and Research</li>\n</ul>\n&nbsp;\n\nBERKELEY, Calif.--(BUSINESS WIRE)-- Databricks and the sponsors of Spark Summit 2014 today announced the full agenda for the summit, including a host of exciting keynotes and community talks. The event will be held June 30\u2013July 2, 2014, at The Westin St. Francis in San Francisco.\n\nSpark Summit 2014 arrives at an exciting time for the Apache Spark platform, which has become the most active open source project in the Hadoop ecosystem with more than 200 contributors in the past year. Now available in all major Hadoop distributions, Spark has fostered a fast-growing community on the strength of its technical capabilities, which make big data analysis simpler with a unified platform, easy-to-use APIs in Java, Scala and Python, and drastically faster in-memory computation.\n\n\u201cThe Spark Summit presents an exciting opportunity to bring together the fast-growing Spark community and collaborate to push this platform forward,\u201d said Databricks CEO Ion Stoica. \u201cWe look forward to discussing Spark\u2019s impact on the big data landscape and how this rapidly adopted technology will play an instrumental role in shaping the future of big data analysis.\u201d\n\nIn its second year, the Spark Summit will gather enterprise users, developers and data scientists to examine best practices for development and learn how to use the Spark stack in a variety of applications across multiple industries. Attendees can explore various tracks on topics such as applications, development, data science and research. Keynotes will be delivered by Stoica, Databricks Chief Technology Officer Matei Zaharia, Cloudera Chief Strategy Officer Mike Olson, MapR Chief Technology Officer M.C. Srivas and others from DataStax, Jawbone, Howard Hughes Medical Institute and UC Berkeley\u2019s AMPLab.\n\nAdditionally, Databricks will be hosting an introductory level Spark training on the final day of the three-day event, and the Application Spotlight segment will also showcase promising applications that have been \u201cCertified on Spark.\u201d\n\nOther Spark Summit sponsors to date include IBM, UC Berkeley\u2019s AMPLab, Cloudera, Pivotal, Sharethrough, Guavus, Stratio, Red Hat, Ooyala, MapR, DataStax, ClearStory Data, Shopify, SanDisk, Hortonworks, NTT DATA, Yahoo, Platfora, Atigeo and Brain Corporation.\n\nTo view the full agenda of keynotes and 50+ talks, click <a title=\"here\" href=\"http://spark-summit.org/2014/agenda\">here</a>.\n\nTo purchase tickets to the Spark Summit, visit:\n<a title=\"http://spark-summit.org/register\" href=\"http://spark-summit.org/register\">http://spark-summit.org/register</a>\n\nTo register for a live stream of Spark Summit, click <a title=\"here\" href=\"https://spark-summit.org/2014/live-stream\">here</a>.\n\nAbout Databricks\n\nDatabricks (<a href=\"https://databricks.com\">databricks.com</a>) was founded by the creators of Apache Spark, and are using cutting-edge technology based on years of research to build next-generation software for analyzing and extracting value from Big Data. They believe Big Data is a tremendous opportunity that is still largely untapped, and are working to revolutionize what enterprises can do with it. They are venture-backed by Andreessen Horowitz.\n\n&nbsp;\n\nDatabricks Public Relations\nAlex Koritz, 801-461-9795\nalex@methodcommunications.com\nor\nJoshua Heath, 801-461-9794\njoshua@methodcommunications.com"}
{"status": "publish", "description": null, "creator": "arsalan", "link": "https://databricks.com/blog/2014/06/13/application-spotlight-lightbend.html", "authors": ["Dean Wampler (Typesafe)"], "id": 628, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2014-06-13", "tz": "UTC", "createdOn": "2014-06-13"}, "title": "Application Spotlight: Lightbend", "slug": "application-spotlight-lightbend", "content": "<div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://www.lightbend.com\" target=\"_blank\">Lightbend</a> after having their Lightbend Activator Apache Spark templates be \"Certified on Apache Spark\".</div>\n\n<hr />\n\n<h2>Apache Spark and the Lightbend Reactive Platform: A Match Made in Heaven</h2>\nWhen I started working with Hadoop several years ago, it was frustrating to find that writing Hadoop jobs was hard to do. If your problem fits a query model, then <a title=\"Hive\" href=\"http://hive.apache.org\" target=\"_blank\">Hive</a> provides a SQL-based scripting tool. For many common dataflow problems, <a href=\"http://pig.apache.org\" target=\"_blank\">Pig</a> provides useful abstractions, but it isn't a full-fledged, \"Turing-complete\" language. Otherwise, you had to use the low-level <a href=\"http://wiki.apache.org/hadoop/MapReduce\" target=\"_blank\">Hadoop MapReduce</a> API. Some third-party APIs exist that wrap the MapReduce API, such as <a href=\"http://cascading.org\" target=\"_blank\">Cascading</a> and <a href=\"https://github.com/twitter/scalding\" target=\"_blank\">Scalding</a>, but they couldn't fix MapReduce's performance problems.\n<h2>Spark - The New Big Data Compute Engine</h2>\nBut interest in an alternative, <a href=\"http://spark.apache.org\" target=\"_blank\">Apache Spark</a>, was growing. Now, Spark has emerged as the next-generation platform for writing Big Data applications for Hadoop and Mesos clusters.\n\nSpark is replacing the venerable Hadoop MapReduce for several reasons:\n<h4>Performance</h4>\nSpark's <a href=\"http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds\" target=\"_blank\">Resilient Distributed Datasets</a> (RDDs), which are fault-tolerant, distributed collections of data that can be manipulated in parallel. RDDs exploit intelligent, in-memory caching of data that avoids unnecessary round trips to disk, writes followed by reads, which are common in non-trivial MapReduce jobs where map and reduce steps are sequenced together.\n<h4>Natural Data Processing Idioms</h4>\nSpark provides a powerful set of composable building blocks for writing concise, yet powerful queries and dataflows. While the MapReduce API can be used to write a wide-range of computations, translating many algorithms to the API can be very difficult, requiring special expertise. In contrast, the concise Scala, Java, and Python APIs provided by Spark make developers highly productive.\n<h4>Streaming vs. Batch-mode Operations</h4>\nMapReduce only supports batch-mode operations. Increasingly, data teams need more \"real-time\" processing of event streams. Rather than turning to yet another tool for this purpose, Spark lets you writes streaming and batch-mode applications with very similar logic and APIs.\n<h2>What Makes Spark so Successful?</h2>\nPart of Spark's success is due to the foundation it is built upon, components of the <a href=\"http://lightbend.com/platform\" target=\"_blank\">Lightbend Reactive Platform</a>. First, there's <a href=\"http://scala-lang.org\" target=\"_blank\">Scala</a>, the flexible, object-functional language for the JVM. People often ask Matei Zaharia, the creator of Spark and the co-founder of Databricks, why he chose Scala. Here is a <a href=\"http://apache-spark-user-list.1001560.n3.nabble.com/Why-Scala-td6536.html\" target=\"_blank\">recent answer</a> he gave to the question:\n\nQuite a few people ask this question and the answer is pretty simple. When we started Spark, we had two goals \u2014 we wanted to work with the Hadoop ecosystem, which is JVM-based, and we wanted a concise programming interface similar to Microsoft\u2019s <a href=\"http://research.microsoft.com/en-us/projects/dryadlinq/\" target=\"_blank\">DryadLINQ</a> (the first language-integrated big data framework I know of, that begat things like <a href=\"http://pages.cs.wisc.edu/~akella/CS838/F12/838-CloudPapers/FlumeJava.pdf\" target=\"_blank\">FlumeJava</a> and <a href=\"http://crunch.apache.org/\" target=\"_blank\">Crunch</a>). On the JVM, the only language that would offer that kind of API was Scala, due to its ability to capture functions and ship them across the network. Scala\u2019s static typing also made it much easier to control performance compared to, say, Jython or Groovy.\n\nThe second Lightbend component in Spark's foundation is <a href=\"http://akka.io\" target=\"_blank\">Akka</a>, a toolkit and runtime for building highly-concurrent, distributed, and fault tolerant event-driven applications on the JVM.\n\nSpark exploits Akka's distributed, fine-grained, flexible, and dynamic <a href=\"http://akka.io/#actors\" target=\"_blank\">Actor model</a> to build resilient, distributed components for managing and processing data.\n<h2>Lightbend and Databricks, Working Together</h2>\nThe combination of Apache Spark and the Lightbend Reactive Platform, including Scala, Akka, Play, and Slick, gives Enterprise developers a comprehensive suite of tools for building Certified on Spark applications with minimal effort that are highly scalable and resilient.\n\nLightbend will continue to build tools that help make Spark great and Databricks successful. We'll also work to make the developer experience seamless between our tools.\n\nFor starters, I encourage you to check out our growing <a href=\"http://lightbend.com/activator\" target=\"_blank\">Lightbend Activator</a> templates for Spark, especially my introductory <a href=\"http://lightbend.com/activator/template/spark-workshop\" target=\"_blank\">Spark Workshop</a>, which is our first Certified on Spark application."}
{"status": "publish", "description": null, "creator": "arsalan", "link": "https://databricks.com/blog/2014/06/23/application-spotlight-apervi.html", "authors": ["Hari Kodakalla (EVP at Apervi Inc.)"], "id": 643, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2014-06-23", "tz": "UTC", "createdOn": "2014-06-23"}, "title": "Application Spotlight: Apervi", "slug": "application-spotlight-apervi", "content": "<div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://www.apervi.com\" target=\"_blank\">Apervi</a> after having their Conflux Director\u2122 application be \"Certified on Apache Spark\".</div>\n\n<hr />\n\n<h2>Big Data on Steroids with Apache Spark</h2>\nAs big data takes center stage in the new data explosion, Hadoop has emerged as one the leading technologies addressing the challenges in the space. As the data processing needs of enterprises are growing newer technologies like Apache Spark have emerged as significant options that consistently offer expanded capabilities for the big data space. As these enterprise needs are met, so is the increased appetite for faster processing, low latency requirements for high velocity data and an iterative demand for processing where leading technologies like Hadoop fall short of expectations or at times seem cumbersome to implement due to its inherent design.\n\nDelivering on this growing need of enterprises is where Spark plays a very crucial role and is emerging as the platform of choice. Why? Apache Spark offers built in support for in-memory processing, support for HDFS and the ability to work with data using SQL unlike any other open source big data technology and much more, all along offering superior performance gains over traditional choices. Spark thus supercharges the big data processing landscape and is truly a powerful complementary technology stack that reinforces the big data platform for enterprises, offering an expanded ability to do far more than Hadoop alone could deliver upon, thus the term Big Data on steroids with Spark.\n<h2>Apervi Conflux Director\u2122 &amp; Databricks</h2>\nApervi, a leading innovator in the space of data engineering for big data, is completely focused on making it easy for users to leverage the benefits of big data technologies without the complexity associated with the respective technologies (namely Hadoop, Storm, &amp; Spark). In the same spirit Apervi identified Spark very early on as one of the strong offerings capable of catering to the growing needs of enterprises to process diverse forms of data beyond the limitations of MapReduce for superior performance. Apervi started work to build support for Spark within its offering the Conflux Director\u2122, a unified orchestration platform for big data, to quickly expand what users can do with their data without being bound by the processing paradigm limitations, be it batch / real-time. By offering support for Spark standard within Conflux Director\u2122, Apervi is able to deliver the power of Spark readily to its users to build high performance data engineering workflows in an intuitive fashion consistent with its support for other Big Data technologies, all without a steep learning curve.\n<h2>Spark and Conflux for Telecom</h2>\nSpark support in Conflux has enabled one of our customers in the telecom industry to quickly build and test the viability of a real-time promotion targeting program of wireless customers based on location and status in a relatively short period of time. Combining Spark with Conflux for ETL and stream processing has enabled this customer to compress processing windows drastically, realizing swift gains, and conduct rapid prototyping to A/B test promotion effectiveness efficiently.\n<h2>Conflux \u201cCertified on Spark\u201d</h2>\nAs we progressed in our journey of building support for Spark, the introduction of the Databricks \u201cCertified on Spark \u201c program got us really excited. It allowed us to both showcase our close integration with Spark and highlight the need to promote Spark to the user community in a systematic fashion. We are proud and excited to be participating in the \u201cCertified on Spark \u201c program by Databricks because we feel that working with Databricks will help the Spark ecosystem grow, thus encouraging broader adoption of such a fantastic technology to meet the diverse needs of our customers to tame the challenges of big data.\n\nSo, it is with both pride \u2014 as well as thanks to the team at Databricks \u2014 that we announce that Conflux Director\u2122 is now \u201cCertified on Spark\u201d.\n\nApervi will continue to build tools that help make Spark easy to use and support Databricks in its mission. We\u2019ll strive to make the developer experience seamless between our tools and the Spark technology stack thus delivering on the broader promise of enabling enterprises to derive value from Big Data easier and faster."}
{"status": "publish", "description": null, "creator": "arsalan", "link": "https://databricks.com/blog/2014/06/24/application-spotlight-qlik.html", "authors": ["Bill Kehoe (Big Data Architect at Qlik)"], "id": 651, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2014-06-24", "tz": "UTC", "createdOn": "2014-06-24"}, "title": "Application Spotlight: Qlik", "slug": "application-spotlight-qlik", "content": "<div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://www.qlik.com\" target=\"_blank\">Qlik</a> describing how Apache Spark enables the full power of QlikView, recently Certified on Apache Spark, and its Associative Experience feature over the entire HDFS data set.</div>\n\n<hr />\n\n<h2>The Power of Qlik</h2>\nQlik provides software and services that help make understanding data a natural part of how people make decisions. Our product, QlikView, is the leading Business Discovery platform that incorporates a unique, associative experience that empowers business users to follow their own path to formulate and answer questions that lead to better decisions. Traditional, query-based BI tools force users thru pre-defined navigation paths which limit the kinds of questions that can be answered and require costly and time consuming revisions to address evolving business needs. In contrast, when a user selects data items using QlikView, all the fields and charts are immediately updated to reflect the relationships between the selected items and the other data items in the business model.\n\n<img class=\"aligncenter size-full wp-image-62\" src=\"https://databricks.com/wp-content/uploads/2014/06/Associative.png\" width=\"472\" />\n\nUsing QlikView, it is always apparent what data is associated since the selected items are presented with a green background, all related values have a white background and unrelated items are shown with a gray background.\n<h2>Expanding to larger data sets</h2>\nRecognizing that many customers need the power of QlikView to analyze volumes of data too large to load into the QlikView in-memory engine, Qlik expanded its engine by adding Direct Discovery, a hybrid approach that combines both in-memory data with dynamic access to data residing in external, big data sources. With this hybrid model, the QlikView engine dynamically formulates and submits queries for external data whenever a user selects in-memory data values that are associated with external data. With Direct Discovery, users are taking full advantage of the QlikView Associative Experience without having to first ingest the raw data from big data sources into QlikView.\n\nNaturally, the ability to run Direct Discovery applications on Hadoop was one of the very first use cases Qlik anticipated so we\u2019ve worked closely with our Hadoop partners to ensure the fastest and easiest means to access HDFS data. We knew we had existing customers using MapReduce to aggregate HDFS data for loading into QlikView and that these same customers would not only want faster load times but also the option of using Direct Discovery on their pre-aggregated data.\n<h2>Leveraging Apache Spark</h2>\nSince the advent of Direct Discovery, we\u2019ve followed the Apache Spark project noting its obvious performance benefit of using RAM for inter-stage caching rather than the much higher latency, disk-based approach of traditional MapReduce. We\u2019ve been particularly impressed by the way Spark provides a high performance platform not just for SQL but other critical big data needs such as iterative machine learning algorithms and streaming.\n\nQlik is thrilled to participate in the Databricks \u201cCertified on Spark\u201d program because we know that Spark is accelerating our customers\u2019 ability to deliver high performance applications that operate on the full spectrum of their HDFS data. For starters, using Shark, QlikView users are now able to use Direct Discovery to utilize the full power of the QlikView Associative Experience over their entire HDFS data set. Using Spark\u2019s RDD API, customers are able to build very succinct, pipeline-style flows that dramatically reduce the time needed to execute their HDFS data reload tasks for batch loaded data.\n\n<img class=\"aligncenter size-full wp-image-62\" src=\"https://databricks.com/wp-content/uploads/2014/06/Shark.png\" width=\"472\" />\n\nQlik understands that fast, easy access to increasingly large and diverse sources of data is one of the most critical enablers for deriving maximal business value from data. Clearly Spark significantly advances the accessibility of big data and, by certifying QlikView on Apache Spark, Qlik delivers this enhanced data accessibility to its customers while encouraging the widespread adoption of this critical big data platform."}
{"status": "publish", "description": null, "creator": "arsalan", "link": "https://databricks.com/blog/2014/06/26/databricks-launches-certified-spark-distribution-program.html", "authors": ["Databricks Press Office"], "id": 703, "categories": ["Announcements", "Company Blog"], "dates": {"publishedOn": "2014-06-26", "tz": "UTC", "createdOn": "2014-06-26"}, "title": "Databricks Launches \"Certified Apache Spark Distribution\" Program", "slug": "databricks-launches-certified-spark-distribution-program", "content": "<em>Certified distributions maintain compatibility with open source Apache Spark distribution and thus support the growing ecosystem of Apache Spark applications</em>\n\n<hr />\n\n<strong>BERKELEY, Calif. -- June 26, 2014 --</strong> Databricks, the company founded by the creators of Apache Spark, the next generation Big Data engine, today announced the <a href=\"https://databricks.com/spark/certification/certified-spark-distribution\" target=\"_blank\">\u201cCertified Spark Distribution\u201d </a>program for vendors with a commercial Spark distribution. Certification indicates that the vendor\u2019s Spark distribution is compatible with the open source Apache Spark distribution, enabling \u201cCertified on Spark\u201d applications - certified to work with Apache Spark - to run on the vendor\u2019s Spark distribution out-of-the-box.\n\n\u201cOne of Databricks\u2019 goals is to ensure users have a fantastic experience. Our belief is that having the community work together to maintain compatibility and therefore facilitate a vibrant application ecosystem is crucial to this vision,\u201d said Ion Stoica, Databricks CEO. \u201cWe first launched the \u2018Certified on Spark\u2019 program to help build a robust ecosystem of innovative applications on top of Apache Spark. The \u2018Certified Spark Distribution\u2019 program is the other half of the equation, recognizing vendors that are committed to providing a home for these applications to allow the ecosystem to flourish.\u201d\n\nIn keeping with the open source nature of Spark, the certification process is fully transparent with open-source tests, lightweight, and 100% free - a mirror image of the \u201cCertified on Spark\u201d process for Spark applications. Vendors fill out a short questionnaire and then simply execute a set of open-source tests - developed and maintained by the community and used to test each release of Apache Spark - against their build of Spark to demonstrate compatibility.\n\n\u201cCertification shouldn\u2019t be used as a tool for lock-in: Certified Spark Distributions are not required to ship all the bits of Apache Spark, or be open source, or prevented from innovating significantly within and around Spark,\u201d said Arsalan Tavakoli-Shiraji, Business Development Lead at Databricks. \u201cThey simply need to maintain compatibility with Apache Spark to provide support for the application ecosystem.\u201d\n\nAs part of the certification program launch, five vendors have completed the certification process: <a title=\"DataStax\" href=\"http://www.datastax.com\" target=\"_blank\">DataStax</a>, <a title=\"Hortonworks\" href=\"http://www.hortonworks.com\" target=\"_blank\">Hortonworks</a>, <a title=\"IBM\" href=\"http://www.ibm.com\" target=\"_blank\">IBM</a>, <a title=\"Oracle\" href=\"http://www.oracle.com\" target=\"_blank\">Oracle</a>, and <a title=\"Pivotal\" href=\"http://www.gopivotal.com\" target=\"_blank\">Pivotal</a> - industry leaders that have recognized and embraced the power of Spark when integrated with their respective platforms. Each of these vendors put their distributions through the certification process, which included a host of integration tests to ensure full compatibility with the latest Apache Spark release.\n\n\u201cOne of the big risks faced by open source projects is fragmentation among distributors. Fragmentation is bad for both users and application developers, and ultimately for the growth of the project,\u201d said Matei Zaharia, Databricks CTO and VP of the Spark project at Apache. \u201cWe are delighted that these partners - along with others in the certification pipeline - share our vision of an undivided Spark platform based directly around Apache, and will ensure that all applications built on Apache Spark run on their distributions.\u201d\n\nVendors interested in certifying their Spark distribution should visit <a title=\"databricks.com\" href=\"http://www.databricks.com\" target=\"_blank\">www.databricks.com</a> and select \u2018Apply for Certification\u2019. Enterprise users can also visit the Databricks site regularly to see the latest set of certified distributions and applications, and read \u201cspotlight\u201d blog articles that provide deep-dives on the Spark ecosystem by newly certified vendors.\n\nAll the inaugural members will be on hand at the upcoming <a title=\"Spark Summit\" href=\"http://www.spark-summit.org\" target=\"_blank\">Spark Summit</a> from June 30th to July 2nd in San Francisco to provide greater information on the role of Spark in helping better serve their customers. Additionally, there will be an <a title=\"Application Spotlight\" href=\"http://www.spark-summit.org/2014/agenda\" target=\"_blank\">\u201cApplication Spotlight\u201d</a> segment that will highlight innovative \u201cCertified on Spark\u201d applications.\n<h2>Supporting Quotes:</h2>\n\"DataStax is strongly committed to making Cassandra and Spark the best combination for today's online applications,\" said Robin Schumacher, VP of products at DataStax, \"We have demonstrated that commitment with the integration work we have contributed back to both open source communities as well as the certified versions of Spark and Cassandra we provide in DataStax Enterprise for production environments.\"\n\n\u201cWe support the fact that Apache Spark project provides enterprises with an additional processing engine in Hadoop to execute in-memory algorithms for advanced analytics,\u201d said John Kreisa, vice president of strategic marketing at Hortonworks. \u201cWe applaud Databricks\u2019 vision to ensure Spark is fully integrated on YARN, which enterprises have adopted as the data OS for Hadoop.\u201d\n\n\"IBM has a long history of supporting open source projects and the communities that develop around them,\" said Anjul Bhambhri, vice president, IBM Big Data. \"Our work on the Spark Distribution Certification program extends IBM's commitment to delivering innovations on Hadoop to help our clients manage massive amounts of data from multiple sources and perform interactive analytics through applications that consumers and businesses are building.\"\n\n\"Pivotal's open source credentials are quite extensive - Apache-compatible Hadoop, MADLib, RabbitMQ, CloudFoundry - and now we've added Spark to that set,\" said Sarabjeet Chugh, Head of Hadoop Product Management at Pivotal. \"Additionally, we recognize the importance of a unified community to enable the ecosystem to grow and so are thrilled to back this effort\"\n<h2>About Databricks</h2>\nDatabricks was founded by the creators of Apache Spark, who have been working for the past six years on cutting-edge systems to analyze and process Big Data. They believe that Big Data is a tremendous opportunity that is still largely untapped, and are actively working to revolutionize what enterprises can do with it. Databricks is venture-backed by Andreessen Horowitz. For more information, visit <a title=\"http://www.databricks.com\" href=\"http://www.databricks.com\" target=\"_blank\">http://www.databricks.com</a>"}
{"status": "publish", "description": null, "creator": "arsalan", "link": "https://databricks.com/blog/2014/06/27/application-spotlight-elasticsearch.html", "authors": ["Costin Leau (Engineer at Elasticsearch)"], "id": 713, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2014-06-28", "tz": "UTC", "createdOn": "2014-06-28"}, "title": "Application Spotlight: Elasticsearch", "slug": "application-spotlight-elasticsearch", "content": "<div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://www.elasticsearch.com\" target=\"_blank\">Elasticsearch</a> announcing Elasticsearch is now \"Certified on Apache Spark\", the first step in a collaboration to provide tighter integration between Elasticsearch and Spark.</div>\n\n<hr />\n\n<h2>Elasticsearch Now \u201cCertified on Spark\u201d</h2>\nHelping businesses get insights out of their data, fast, is core to the mission of Elasticsearch. Being able to live wherever a business stores their data is obviously critical to that mission, and Hadoop is one of the leaders in providing a way for businesses to store massive amounts of data at scale. Over the course of the past year, we have been working hard to bring the power of our real-time search and analytics engine to the Hadoop ecosystem. Our Hadoop connector, Elasticsearch for Apache Hadoop, is compatible with the top three Hadoop distributions \u2013 Cloudera, Hortonworks and MapR \u2013 and today has achieved another exciting milestone: Spark certification.\n<h2>Elasticsearch + Spark = Rich Search, Immediate Insights</h2>\nSpark is rapidly emerging as a popular processing and analysis tool for Hadoop-like and other data stores. We continue to see it in many of our customers' Hadoop distributions and beyond, and have been working together with Databricks as well as our respective open source communities to bring better connectivity between the two technologies. The combination of Elasticsearch with Spark adds the capabilities of a full-blown search engine that enhances data discovery and exploration - whether it be in a live, customer-facing environment, or behind the scenes for internal analysis - to Spark's unified processing engine. Through Elasticsearch for Apache Hadoop Map/Reduce support, Spark applications can interact with Elasticsearch just as they would with an HDFS resource, allowing them to index and analyze data transparently, in real-time. Our data visualization tool, Kibana, can also be used to explore massive amounts of data in Elasticsearch through easy-to-generate pie charts, bar graphs, scatter plots, histograms and more.\n\nBusinesses continue to adopt Elasticsearch to help them get to the last mile of their Hadoop deployments by providing the ability to ask, iterate and extract actionable insights from their data. A lot of them are in industries like healthcare, finance and telecommunications and have extremely large and sensitive amounts of data they need to mine. Elasticsearch for Apache Hadoop lets them access data, like log files, in minutes instead of hours, so they can detect fraud, identify service issues and analyze customer behavior, letting them come to resolutions faster and giving their rockstar developers the tools they need to directly impact the bottom line of their business.\n\nWe couldn\u2019t be more thrilled to be officially <a href=\"https://databricks.com/certification\" target=\"_blank\">\u201cCertified on Spark\u201d</a>; our Hadoop connector is the first step in our roadmap to make the two more natively integrated, bringing businesses even more advanced search and analytics capabilities to their data.\n<h2>Check Us Out</h2>\nIf you\u2019re going to <a href=\"http://www.spark-summit.org\" target=\"_blank\">Spark Summit</a>, Holden Karau from Databricks will be showing how to streamline search indexing with Elasticsearch and Spark in <a href=\"https://spark-summit.org/2014/talk/streamlining-search-indexing-using-elastic-search-and-spark\" target=\"_blank\">this session</a> on Monday, June 30 at 3:00pm.\n\nWe are also holding a webinar about how Elasticsearch can used for real-time insights on your Hadoop and Spark deployments on Wednesday, August 20th - you can register for that <a href=\"http://www.elasticsearch.org/webinars/elasticsearch-and-apache-hadoop/\" target=\"_blank\">here</a>.\n\nAnd last but not least, if you\u2019d like to get started, download Elasticsearch for Apache Hadoop <a href=\"http://www.elasticsearch.org/overview/hadoop/download/\" target=\"_blank\">here</a> and <a href=\"http://www.elasticsearch.org/community/\" target=\"_blank\">let us know what you think!</a>"}
{"status": "publish", "description": null, "creator": "arsalan", "link": "https://databricks.com/blog/2014/06/30/application-spotlight-pentaho.html", "authors": ["Jake Cornelius (SVP of Product Management at Pentaho)"], "id": 720, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2014-06-30", "tz": "UTC", "createdOn": "2014-06-30"}, "title": "Application Spotlight: Pentaho", "slug": "application-spotlight-pentaho", "content": "[sidenote]This post is guest authored by our friends at <a href=\"http://www.pentaho.com\" target=\"_blank\">Pentaho</a> after having their data integration and analytics platform <a href=\"http://www.databricks.com/certification\" target=\"_blank\">\u201cCertified on Apache Spark.\u201d</a>[/sidenote]\n\n<hr />\n\nOne of Pentaho\u2019s great passions is to empower organizations to take advantage of amazing innovations in <a href=\"http://www.pentaho.com/what-is-big-data\" target=\"_blank\">Big Data</a> to solve new challenges using the existing skill sets they have in their organizations today.  Our Pentaho Labs prototyping and innovation efforts around natively integrating data engineering and analytics with Big Data platforms like <a href=\"http://www.pentaho.com/what-is-hadoop\" target=\"_blank\">Hadoop</a> and <a href=\"http://www.pentaho.com/storm\" target=\"_blank\">Storm</a> have already led dozens of customers to deploy next-generation Big Data solutions. Examples of these solutions include <a href=\"http://www.pentaho.com/Optimize-the-Data-Warehouse\" target=\"_blank\">optimizing data warehousing</a> architectures, leveraging <a href=\"http://www.pentaho.com/solutions/hadoop\" target=\"_blank\">Hadoop</a> as a cost effective <a href=\"http://www.pentaho.com/Streamlined-Data-Refinery\" target=\"_blank\">data refinery</a>, and performing advanced analytics on diverse data sources to achieve a broader <a href=\"http://www.pentaho.com/Customer-360-Degree-View\" target=\"_blank\">360-degree view of customers</a>.\n\nNot since the early days of Hadoop have we seen so much excitement around a new Big Data technology as we see right now with <a href=\"http://spark.apache.org/\" target=\"_blank\">Apache Spark</a>.  <a href=\"http://www.pentaho.com/what-is-apache-spark\" target=\"_blank\">Spark</a> is a Hadoop-compatible computing system that makes big data analysis drastically faster, through in-memory computation, and simpler to write, through easy APIs in Java, Scala and Python.  With the second annual <a href=\"http://spark-summit.org/2014\" target=\"_blank\">Spark Summit</a> taking place this week in San Francisco, I wanted to share some of the early proof-of-concept work Pentaho Labs and our partners over at <a href=\"https://databricks.com/\">Databricks</a> are collaborating on to integrate Pentaho and Spark for delivering high performance, Big Data Analytics solutions.\n\n<h2>Big Data Integration on Spark</h2>\n\n<img class=\"aligncenter size-full wp-image-62\" src=\"https://databricks.com/wp-content/uploads/2014/06/Flow-Picture.png\" width=\"472\" />\n\nAt the core of Pentaho Data Integration (PDI) is a portable \u2018data machine\u2019 for ETL which today can be deployed as a stand-alone Pentaho cluster or inside your Hadoop cluster though MapReduce and YARN.  The Pentaho Labs team is now taking this same concept and working on the ability to deploy inside Spark for even faster Big Data ETL processing.  The potential benefit for ETL designers is the ability to design, test and tune ETL jobs in PDI\u2019s easy-to-use graphical design environment, and then run them at scale on Spark.  This dramatically lowers the skill sets required, increases productivity, and reduces maintenance costs when to taking advantage of Spark for Big Data Integration.\n\n<h2>Advanced Analytics on Spark</h2>\n\nLast year Pentaho Labs introduced a distributed version of Weka, Pentaho\u2019s machine learning and data mining platform. The goal was to develop a platform-independent approach to using Weka with very large data sets by taking advantage of distributed environments like Hadoop and Spark. Our first pilot implementation proved out this architecture by enabling <a href=\"http://markahall.blogspot.com/2013/10/weka-and-hadoop-part-1.html\" target=\"_blank\">parallel, in-cluster model training with Hadoop</a>.\n\n<img class=\"aligncenter size-full wp-image-62\" src=\"https://databricks.com/wp-content/uploads/2014/06/Advanced-Analytics.png\" width=\"472\" />\n\nWe are now working on a similar level of integration with Spark that includes data profiling and evaluating classification and regression algorithms in Spark.  The early feedback from Pentaho Labs confirms that developing solutions on Spark is faster and easier than with MapReduce. In just a couple weeks of development, we have demonstrated a proof-of-concept to perform in-cluster Canopy clustering and are very close to having k-means++ working in Spark as well!\n\n<h2>Next up: Exploring Data Science Pack Integration with MLlib</h2>\n\n<a href=\"https://spark.apache.org/mllib/\" target=\"_blank\">MLlib</a> is already one of the most popular technologies for performing advanced analytics on Big Data.  By integrating Pentaho Data Integration with Spark and MLlib, Data Scientists will benefit by having an easy-to-use environment (PDI) to prepare data for use in MLlib-based solutions.  Furthermore, this integration will make it easier for IT to operationalize the work of the Data Science team by orchestrating the entire end-to-end flow from data acquisition, to data preparation, to execution of MLlib-based jobs to sharing the results, all in one simple PDI Job flow.  To get a sense for how this integration might work, I encourage you to look at a similar integration with R we recently launched as part of the <a href=\"http://www.pentaho.com/press-release/pentaho-data-science-pack-operationalizes-use-r-and-weka\" target=\"_blank\">Data Science Pack</a> for Pentaho Business Analytics 5.1.\n\n<h3>Experiment Today with Pentaho and Spark!</h3>\n\nYou can experiment with Pentaho and Spark today for both ETL and Reporting.  In conjunction with our partners at <a href=\"https://databricks.com/\">Databricks</a>, we recently developed prototypes for the following use cases combining Pentaho and Spark*:\n\n<ul>\n \t<li>Reading data from Spark as part of an ETL workflow by using Pentaho Data Integration\u2019s Table Input step with <a href=\"http://shark.cs.berkeley.edu/\" target=\"_blank\">Apache Shark</a> (Hive SQL layer runs on Spark)</li>\n \t<li>Reporting on Spark data using Pentaho Reporting against Apache Shark</li>\n</ul>\n\nWe are excited about this first step in what we both hope to be a collaborative journey towards deeper integration.\n\nJake Cornelius Sr. Vice President, Product Management Pentaho\n\n<em>* Note that these Databricks integrations constitute a proof-of-concept and are not currently supported for Pentaho customers.</em>"}
{"status": "publish", "description": null, "creator": "arsalan", "link": "https://databricks.com/blog/2014/06/30/sparkling-water-h20-spark.html", "authors": ["SriSatish Ambati (CEO of 0xData)"], "id": 732, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2014-06-30", "tz": "UTC", "createdOn": "2014-06-30"}, "title": "Sparkling Water = H20 + Apache Spark", "slug": "sparkling-water-h20-spark", "content": "<div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://www.0xdata.com\" target=\"_blank\">0xData</a> discussing the release of Sparkling Water - the integration of their H20 offering with the Apache Spark platform.</div>\n\n<hr />\n\n<h3>H20 \u2013 The Killer-App on Apache Spark</h3>\n<img class=\"aligncenter size-full wp-image-62\" src=\"https://databricks.com/wp-content/uploads/2014/06/Spark-+-H20.png\" width=\"472\" />\n\nIn-memory big data has come of age. The Apache Spark platform, with its elegant API, provides a unified platform for building data pipelines. H2O has focused on scalable machine learning as the API for big data applications. Spark + H2O combines the capabilities of H2O with the Spark platform \u2013 converging the aspirations of data science and developer communities. H2O is the Killer-Application for Spark.\n\n<img class=\"aligncenter size-full wp-image-62\" src=\"https://databricks.com/wp-content/uploads/2014/06/H20-the-Killer-App.png\" width=\"472\" />\n<h3>Backdrop</h3>\nOver the past few years, we watched Matei and the team behind Spark build a thriving open-source movement and a great development platform optimized for in-memory big data, Spark. At the same time, H2O built a great open source product with a growing customer base focused on scalable machine learning and interactive data science. These past couple of months the Spark and H2O teams started brainstorming on how to best combine H2O's Machine Learning capabilities with the power of the Spark platform. The result: <strong>Sparkling Water</strong>.\n<h3>Sparkling Water</h3>\nUsers can in a single invocation and process, get the best of Spark - its elegant APIs, RDDs, multi-tenant Context and H2O's speed, columnar-compression and fully-featured Machine Learning and Deep-Learning algorithms.\n\nOne of the primary draws for Spark is its unified nature, enabling end-to-end building of API\u2019s within a single system. This collaboration is designed to seamlessly enable H20\u2019s advanced capabilities to be part of that data pipeline. The first step in this journey is enabling in-memory sharing through Tachyon and RDDs. The roadmap includes deeper integration where H2O\u2019s columnar-compressed capabilities can be natively leveraged through \u2018H2ORDD\u2019.\n\nToday, data gets parsed and exchanged between Spark and H2O via Tachyon. Users can interactively query big data both via SQL and ML from within the same context.\n\n<img class=\"aligncenter size-full wp-image-62\" src=\"https://databricks.com/wp-content/uploads/2014/06/Tachyon-integration.png\" width=\"472\" />\n\nSparkling Water enables use of H2O's Deep Learning and Advanced Algorithms for Spark's user community. H2O as the killer-application provides a robust machine learning engine and API for the Spark Platform. This will further empower application developers on Spark to build intelligent and smarter applications.\n\n<img class=\"aligncenter size-full wp-image-62\" src=\"https://databricks.com/wp-content/uploads/2014/06/H20RDD.png\" width=\"200\" />\n<h3>MLlib and H2O: The Triumph of Open Source!</h3>\nMLlib is a library of efficient implementations of popular algorithms directly built using Spark. We believe that enterprise customers should have the choice to select the best tool for meeting their needs in the context of Spark. Over time, H2O will accelerate the community\u2019s efforts towards production ready scalable machine learning. Fast fully featured algorithms in H2O will add to growing open source efforts in R, MLlib, Mahout and others, disrupting closed and proprietary vendors in machine-learning and predictive analytics.\n\nNatural integration of H2O with the rest of Spark's capabilities is a definitive win for enterprise customers.\n<h3>More info</h3>\n<ul>\n \t<li><a href=\"http://www.slideshare.net/0xdata/sparkling-water-5-2814\" target=\"_blank\">Slides of the first Sparkling Water meetup</a></li>\n \t<li>Sparkling Water code is <a href=\"https://github.com/0xdata/h2o-sparkling\" target=\"_blank\">here</a></li>\n \t<li><a href=\"https://github.com/0xdata/h2o-sparkling/blob/master/README.md\" target=\"_blank\">Install and Test Instructions</a></li>\n</ul>\n<h3>Demo Code</h3>\n<pre>package water.sparkling.demo\n\nimport water.fvec.Frame\nimport water.util.Log\nimport hex.gbm.GBMCall.gbm\n\nobject AirlinesDemo extends Demo {\n  override def run(conf: DemoConf): Unit = {\n    // Prepare data\n    // Dataset\n    val dataset = \"data/allyears2k_headers.csv\"\n    // Row parser\n    val rowParser = AirlinesParser\n    // Table name for SQL\n    val tableName = \"airlines_table\"\n    // Select all flights with destination == SFO\n    val query = \"\"\"SELECT * FROM airlines_table WHERE dest=\"SFO\" \"\"\"\n\n    // Connect to shark cluster and make a query over prostate, transfer data into H2O\n    val frame:Frame = executeSpark[Airlines](dataset, rowParser, conf.extractor, tableName, query, local=conf.local)\n    Log.info(\"Extracted frame from Spark: \")\n    Log.info(if (frame!null) frame.toString + \"\\nRows: \" + frame.numRows() else \"<nothing>\")\n\n    // Now make a blocking call of GBM directly via Java API\n    val model = gbm(frame, frame.vec(\"isDepDelayed\"), 100, true)\n    Log.info(\"Model built!\")\n  }\n\n  override def name: String = \"airlines\"\n}</pre>"}
{"status": "publish", "description": null, "creator": "arsalan", "link": "https://databricks.com/blog/2014/06/30/databricks-unveils-spark-based-cloud-platform.html", "authors": ["Databricks Press Office"], "id": 768, "categories": ["Announcements", "Company Blog"], "dates": {"publishedOn": "2014-06-30", "tz": "UTC", "createdOn": "2014-06-30"}, "title": "Databricks Unveils Apache Spark-Based Cloud Platform; Announces Series B Funding", "slug": "databricks-unveils-spark-based-cloud-platform", "content": "<ul>\n \t<li>Databricks Cloud Allows Users to Get Value from Apache Spark without the Challenges Normally Associated with Big Data Infrastructure</li>\n \t<li>Ease-of-Use of Turnkey Solution Brings the Power of Spark to a Wider Audience and Fuels the Growth of the Spark Ecosystem</li>\n \t<li>Funding Led by NEA with Follow-on Investment from Andreessen Horowitz</li>\n</ul>\n<strong>Berkeley, Calif. (June 30, 2014)</strong>\u2014Databricks, the company founded by the creators of Apache Spark\u2014the powerful open-source processing engine that provides blazingly fast and sophisticated analytics\u2014announced today the launch of <a title=\"Databricks Cloud\" href=\"https://databricks.com/cloud\">Databricks Cloud</a>, a cloud platform built around Apache Spark. In addition to this launch, the company is announcing the close of $33 million in series B funding led by New Enterprise Associates (NEA) with follow-on investment from Andreessen Horowitz.\n\n\u201cGetting the full value out of their Big Data investments is still very difficult for organizations. Clusters are difficult to set up and manage, and extracting value from your data requires you to integrate a hodgepodge of disparate tools, which are themselves hard to use,\u201d said Ion Stoica, CEO of Databricks. \u201cOur vision at Databricks is to dramatically simplify big data processing and free users to focus on turning data into value. Databricks Cloud delivers on this vision by combining the power of Spark with a zero-management hosted platform and an initial set of applications built around common workflows.\u201d\n\nDatabricks Cloud is powered by Spark, a unified processing engine that eliminates the need to stitch together a disjoint set of tools. Spark provides support for interactive queries (Spark SQL), streaming data (Spark Streaming), machine learning (MLlib) and graph computation (GraphX) natively with a single API across the entire pipeline. Additionally, Databricks Cloud reaps the benefit of the rapid pace of innovation in Spark, driven by the 200+ contributors that have made it the most active project in the Hadoop ecosystem.\n\nThe hosted platform also dramatically simplifies the pain of provisioning a Spark cluster. Users simply specify the desired capacity of a new cluster, and the platform handles all the details: provisioning servers on the fly, streamlining import and caching of data, handling all elements of security, and continually patching and updating Spark\u2014freeing users of all the typical headaches and allowing them to explore and harness the power of Spark. The platform is currently available on Amazon Web Services, though expanding to additional cloud providers is on the roadmap.\n\nDatabricks Cloud comes with a set of built-in applications for those eager to immediately begin using Spark to access and analyze data to better compete in the marketplace:\n<ul>\n \t<li><strong>Notebooks</strong>. Provides a rich interface that allows users to perform data discovery and exploration and to plot the results interactively, execute entire workflows as scripts, and enable advanced collaboration features.</li>\n \t<li><strong>Dashboards</strong>. Create and host dashboards quickly and easily. Users can pick any outputs from previously created notebooks, assemble these outputs in a one-page dashboard with a WISIWYG editor, and publish the dashboard to a broader audience. The data and queries underpinning these dashboards can be regularly updated and refreshed.</li>\n \t<li><strong>Job Launcher</strong>. Enables anyone to run arbitrary Apache Spark jobs and trigger their execution, simplifying the process of building data products.</li>\n</ul>\n\u201cOne of the common complaints we heard from enterprise users was that big data is not a single analysis; a true pipeline needs to combine data storage, ETL, data exploration, dashboards and reporting, advanced analytics, and creation of data products. Doing that with today\u2019s technology is incredibly difficult,\u201d continues Stoica. \u201cWe built Databricks Cloud to enable the creation of end-to-end pipelines out of the box while supporting the full spectrum of Spark applications for enhanced and additional functionality. It was designed to appeal to a whole new class of users who will adopt big data now that many of the complexities of using it have been alleviated.\u201d\n\nBeyond the built-in applications, Databricks Cloud enables users to seamlessly deploy and leverage the rapidly growing ecosystem of third-party Spark applications. Databricks Cloud is powered by the 100 percent open source Apache Spark, meaning that it will support all current and future \u201cCertified on Spark\u201d applications out of the box, and that all applications developed on Databricks Cloud will work across any of the \u201cCertified Spark Distributions.\u201d\n\n\u201cDatabricks remains committed to developing and expanding Apache Spark fully in the open and continuing to add to the capabilities that made it a vital big data platform,\u201d said Matei Zaharia, CTO of Databricks. \u201cWe will continue to commit significant resources to drive open-source innovation in Spark alongside the community. Furthermore, we look forward to enabling a whole new set of users and developers to experience and leverage the power of Spark to drive enterprise value.\u201d\n\nDatabricks Cloud is currently in limited availability with several beta users. Databricks is gradually opening up more capacity so visit <a href=\"http://www.databricks.com/cloud\" target=\"_blank\">www.databricks.com/cloud</a> to learn more about the platform and to get on the waiting list for getting access to the platform that is redefining how enterprises utilize big data."}
{"status": "publish", "description": null, "creator": "arsalan", "link": "https://databricks.com/blog/2014/04/28/databricks-application-spotlight-at-spark-summit-2014.html", "authors": ["Arsalan Tavakoli-Shiraji"], "id": 2462, "categories": ["Company Blog", "Events"], "dates": {"publishedOn": "2014-04-29", "tz": "UTC", "createdOn": "2014-04-29"}, "title": "Databricks Application Spotlight at Spark Summit 2014", "slug": "databricks-application-spotlight-at-spark-summit-2014", "content": "At Databricks, we\u2019ve been thrilled to see the rapid pace of adoption of Apache Spark, as it has been embraced by an increasing number of enterprise vendors and has grown to be the most active open source project in the Hadoop ecosystem. We also know that a critical piece of enabling enterprises to unlock its potential is a strong ecosystem of applications built on top of or integrated with Spark.\n\nWe launched the <a href=\"http://www.databricks.com/certification/\">\u201cCertified on Apache Spark\u201d</a> program to support these application developer efforts, and have been blown away at the diverse set of applications being built on top of Spark, and want this great work to be exposed to the broader community. In that light, this year\u2019s Spark Summit will have an \u201cApplication Spotlight\u201d segment that will highlight some of the best we\u2019ve seen.\n\nRead on for details on how to apply and what selection entails. All applications eligible (even if not yet certified) for the Databricks \u201cCertified on Spark\u201d program are encouraged to apply!\n<p style=\"text-align: center;\">[btn href=\"https://sparksummit.submittable.com/submit/29109842-c784-475a-b609-131ccadf15a6\" target=\"_blank\"]Apply Here[/btn]</p>\n\n<h2>Benefits of Application Spotlight</h2>\nDevelopers that are chosen to be part of the Application Spotlight segment will enjoy the following benefits:\n<ul>\n \t<li><strong>High visibility at Spark Summit:</strong> Each selected Application Spotlight awardee will be on stage for a QA session (in front of ~1000 attendees + live stream) to discuss their application and how they\u2019re leveraging Spark.</li>\n \t<li><strong>Marketing events around Spark Summit:</strong> As part of the Spark Summit newsletter, the Application Spotlight awardees will be highlighted with a short blurb, and will appear on the Spark Summit website. For those who have a booth, we will have an \u2018Application Spotlight\u2019 tag attached to their booth to draw in attendees for a demo.</li>\n \t<li><strong>Attendance:</strong> The designated speaker for each awardee will receive a free 2-day pass to the Spark Summit.</li>\n \t<li><strong>General benefits of \u201cCertified on Spark\u201d:</strong> Application developer can tell customers their application works across a wide range of distributions, placed prominently on the Databricks website, issue press release, and get the chance to pen a guest article.</li>\n</ul>\nAdditionally, for those that have already completed the certification process, the additional work is minimal, and for those who have not the certification process is relatively lightweight once the technical integration has been completed.\n<h2>Process</h2>\nThe process that application developers would go through is the following:\n<ol>\n \t<li><strong>Submission:</strong> An application must be submitted through the online submission form by <strong>May 23, 2014</strong>, that contains the following:\n<ul>\n \t<li>General questionnaire filled out for product (e.g., product description, target audience, what makes the application unique)</li>\n \t<li>Spark-centric questionnaire (e.g., how does the product leverage / integrate with Spark, why the decision to leverage spark)</li>\n \t<li>Up to 5 screen-shots of the product</li>\n \t<li>[Optional] Link to a video up to 3 minutes in length</li>\n</ul>\n</li>\n \t<li><strong>Review:</strong> Databricks reviews submissions, and can optionally:\n<ul>\n \t<li>Send an email requesting a limited amount of additional detail for clarification</li>\n \t<li>Request a video chat session with a brief demo of the product for clarification</li>\n</ul>\n</li>\n \t<li><strong>Selection:</strong> Databricks will then select a handful of applications as \u201cApplication Spotlight\u201d awardees and notify them by June 6th.</li>\n \t<li><strong>Certification:</strong> All awardees must submit a certification application by June 20th otherwise they risk being disqualified from the \u201cApplication Spotlight\u201d program</li>\n</ol>\n<p style=\"text-align: center;\">[btn href=\"https://sparksummit.submittable.com/submit/29109842-c784-475a-b609-131ccadf15a6\" target=\"_blank\"]Submit Your App[/btn]</p>\n\n<h2>Requirements</h2>\nAny application that is eligible for the \u201cCertified on Spark\u201d program is eligible for the Application Spotlight program. In practical terms, this means:\n<ul>\n \t<li>Application / technology can be deployed on top of a customer\u2019s stock Apache Spark distribution (e.g., bundled solutions that are internally powered by Spark are not eligible)</li>\n</ul>\nAdditionally, vendor must be will willing to submit certification application by deadline."}
{"status": "publish", "description": null, "creator": "arsalan", "link": "https://databricks.com/blog/2014/05/08/databricks-and-datastax.html", "authors": ["Arsalan Tavakoli-Shiraji"], "id": 2463, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2014-05-08", "tz": "UTC", "createdOn": "2014-05-08"}, "title": "Databricks and Datastax", "slug": "databricks-and-datastax", "content": "<p>Today, Datastax and Databricks announced a partnership in which Apache Spark becomes an integral part of the Datastax offering, tightly integrated with Cassandra. We\u2019re very excited to be embarking on this journey with Datastax for a multitude of reasons:</p>\n\n<h2 id=\"integrating-operational-systems-with-analytics\">Integrating operational systems with analytics</h2>\n<p>One of the use cases that we\u2019ve increasingly been asked about by Spark users is the ability to create a closed loop system: perform advanced analytics directly on operational data that is then fed back into the operational system to drive necessary adaptation. The tight integration of Cassandra and Spark will enable users to achieve this goal by leveraging Cassandra as the high-performance transactional database that powers online applications and Spark as a next generation processing engine that can deliver deeper insights, faster while seamlessly moving between the two.</p>\n\n<h2 id=\"spark-beyond-hadoop\">Spark beyond Hadoop</h2>\n<p>The most talked about usage model for Spark to date has been within Hadoop deployments - Spark can operate directly over data in HDFS (without needing to move the data first) and natively supports YARN and Mesos, popular resource managers for Hadoop. However, Spark\u2019s applicability is much broader: it is designed to be a general Big Data processing engine, and the Spark / Cassandra integration is a prime example of this - native processing without requiring a batch movement of data to Hadoop first (or even a Hadoop cluster). Furthermore, the recently announced SparkSQL will help optimize this integration further - not only will Spark be able to directly access data stored in Cassandra, but it will also be able to execute selected parts of the query in Cassandra itself. It can then pull the resulting data set into Spark for performing machine learning and other advanced analytics.</p>\n\n<h2 id=\"innovation-in-the-open\">Innovation in the Open</h2>\n<p>This partnership also brings together two groups with very strong open source commitments and heritage. Databricks is focused on keeping Apache Spark 100% open source and Datastax has invested numerous resources in growing the Apache Cassandra community, so it should be no surprise that a key tenet of this partnership is delivering joint innovation back to the open source community to help drive greater integration between the Spark and Cassandra communities over time. Look for significant contributions as we move forward on this journey.</p>\n\n<p>Please join us at the upcoming <a href=\"http://spark-summit.org/2014\" target=\"_blank\">Spark Summit</a> to hear more about the value of using Spark and Cassandra together and additional innovations on the horizon in a keynote talk by Martin Van Ryswyk, Datastax\u2019s VP of Engineering.</p>"}
{"status": "publish", "description": null, "creator": "roy", "link": "https://databricks.com/blog/2014/04/30/databricks-partners-with-simba-to-deliver-shark-odbc-driver.html", "authors": ["Databricks Press Office"], "id": 2464, "categories": ["Announcements", "Company Blog"], "dates": {"publishedOn": "2014-04-30", "tz": "UTC", "createdOn": "2014-04-30"}, "title": "Databricks Partners with Simba to Deliver Shark ODBC Driver", "slug": "databricks-partners-with-simba-to-deliver-shark-odbc-driver", "content": "        <p><strong>VANCOUVER, BC. \u2013 April 30, 2014 \u2013</strong> Simba Technologies Inc., the industry\u2019s expert for Big Data connectivity, announced today that Databricks has licensed Simba\u2019s ODBC Driver as its standards-based connectivity solution for Shark, the SQL front-end for Apache Spark, the next generation Big Data processing engine. Founded by the creators of Apache Spark and Shark, Databricks is developing cutting-edge systems to enable enterprises to discover deeper insights, faster.</p>\n\n<p>\u201cWe believe that Big Data is a tremendous opportunity that is still largely untapped, and we are working to revolutionize what organizations can do with it,\u201d says Ion Stoica, Chief Executive Officer at Databricks, and Professor of Computer Science at UC Berkeley. \u201cAs part of this mission, we understand that BI tools will continue to be a key medium for consuming data and analytics and are excited to announce the availability of an enterprise-grade connectivity option for users of BI tools. Simba is the trusted name for enterprise Big Data connectivity, and was the clear partner choice for Databricks as we work to reach new heights in Big Data analytics and query speeds.\u201d</p>\n\n<p>\u201cWhen it comes to distributed data, Shark is cutting edge,\u201d notes Simba Technologies CTO George Chow. \u201cIts innovative distributed memory abstraction enables SQL queries on Big Data at speeds up to 100 times faster than current industry norms. Pair that velocity with Simba\u2019s Shark ODBC Driver to connect industry-leading BI tools (like Tableau and SAP Lumira) with Apache Hadoop distributions, and you\u2019ve got an enterprise solution that revolutionizes Big Data and enables incredibly powerful business insight.\u201d</p>\n\n<p>Shark is an open-source distributed SQL query engine for Hadoop data that was originally developed at UC Berkeley\u2019s AMPLab, delivering state-of-the-art performance and advanced analytics by using the powerful Apache Spark engine to speed up computations. Users can run Hive queries up to 100 times faster in memory or 10 times faster on disk. Shark can run unmodified Hive queries on existing warehouses, is fully compatible with existing Hive data, queries, and UDFs, and can call complex analytics functions like machine learning right from SQL. Shark supports mid-query fault tolerance, letting it scale to very large jobs and serve as the single tool for addressing the spectrum of SQL-query workloads. Furthermore, Shark is an integral part of building end-to-end data workflows with Spark that, in addition to SQL, including streaming data, graph computation, and machine-learning functionality.</p>\n\n<p>Simba Technologies\u2019 standards-based ODBC drivers power business intelligence (BI), analytics, and reporting on Hive-based data for global F2000 leaders like Alteryx, Cloudera, DataStax, Hortonworks, MapR, and Microsoft. Simba\u2019s drivers and providers are available for individual, enterprise, and OEM licensing. For more information about Simba\u2019s Big Data ODBC &amp; JDBC Drivers and for a free 30-day trial, visit: <a href=\"http://www.simba.com/connectors\" target=\"_blank\">www.simba.com/connectors</a>.</p>"}
{"status": "publish", "description": null, "creator": "arsalan", "link": "https://databricks.com/blog/2014/07/01/databricks-announces-partnership-with-sap.html", "authors": ["Databricks Press Office"], "id": 782, "categories": ["Announcements", "Company Blog", "Partners"], "dates": {"publishedOn": "2014-07-01", "tz": "UTC", "createdOn": "2014-07-01"}, "title": "Databricks Announces Partnership with SAP", "slug": "databricks-announces-partnership-with-sap", "content": "<strong>SAN FRANCISCO \u2014 July 1, 2014</strong> \u2014 Databricks, the company founded by the creators of Apache Spark \u2013 the popular open-source processing engine - today announced a new partnership with <a href=\"http://www.sap.com\" target=\"_blank\">SAP (NYSE: SAP)</a> and to deliver a Databricks-certified Apache Spark distribution offering for the SAP HANA\u00ae platform. The full production-ready distribution offering, based on Apache Spark 1.0, is deployable in the cloud or on premise and available for immediate download from SAP at no cost at <a href=\"http://spr.ly/SAP_and_Spark\" target=\"_blank\">spr.ly/SAP_and_Spark</a>. The announcement was made at the Spark Summit 2014, being held June 30 \u2013 July 2 in San Francisco.\n\nThe Databricks-certified distribution offering for SAP HANA contains the Spark processing engine that works with any Hadoop distribution out of the box, providing a more complete data store and processing layer for Hadoop.  Certified by Databricks to be compatible with the Apache Spark Distribution, this enables the rapidly growing set of \u201cCertified on Spark\u201d applications to run out of the box and on SAP HANA. This production-ready distribution offering is the first result of Databrick\u2019s new partnership with SAP.\n\n\u201cWe\u2019re thrilled to be embarking on this journey with SAP to bring together two powerful technologies to better enable enterprises to derive value from their data,\u201d said Ion Stoica, CEO of Databricks. \u201cSAP HANA is both an incredibly powerful and fast analytics engine, as well as a repository for some of the most valuable enterprise data by virtue of the enterprise applications that it helps run. This integration will help enable the large and growing community of Hadoop and Spark developers and applications to harness these capabilities immediately via Spark as well as extend the reach of SAP HANA.\u201d\n\nSAP HANA integrated with Spark will help enable real-time applications and interactive analysis across corporate application data with content stored in Hadoop Distributed File System (HDFS). Developers and data scientists developing on Spark can also benefit from end-to-end data processing acceleration in SAP HANA by leveraging its comprehensive suite of in-memory engines and libraries for transactional applications, analytics, predictive, machine learning, text, graph and geospatial analysis. This helps simplify the integration of mission-critical applications with contextual data stored in Hadoop-like data stores. As a result, in-memory computation is enabled to happen where data resides and can help minimize costly and time-consuming data movement. \n\n\u201cSAP has continually been at the forefront of innovation to simplify and better serve customers, and bringing together Spark and SAP HANA is simply the latest example of this,\u201d said Steve Lucas, president, Platform Solutions, SAP. \u201cThis can allow enterprises to build on SAP HANA\u2019s value proposition by providing some of the best-of-breed capabilities across the full spectrum of data and processing needs without the need to painstakingly stitch together independent solutions.\u201d\n\nDevelopers and data scientists will be enabled to more easily create a new class of applications with SAP HANA and Spark. For example, they can span data domains, such as applications that integrate inventory analysis with social media trends for retailers; combine sensor data with billing systems to deliver personalized resource and cost-saving recommendations for utilities; or converge patient data with epidemiological information to construct better staffing decisions for healthcare providers."}
{"status": "publish", "description": null, "creator": "arsalan", "link": "https://databricks.com/blog/2014/07/01/integrating-spark-and-hana.html", "authors": ["Arsalan Tavakoli-Shiraji"], "id": 785, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2014-07-01", "tz": "UTC", "createdOn": "2014-07-01"}, "title": "Integrating Apache Spark and HANA", "slug": "integrating-spark-and-hana", "content": "This morning SAP released its own \u201cCertified Spark Distribution\u201d as part of a brand new partnership announced between Databricks and SAP. We\u2019re thrilled to be embarking on this journey with them, not just because of what it means for Databricks as a company, but just as importantly because of what it means for Apache Spark and the Spark community.\n<h2>Access to the full corpus of data</h2>\nFundamentally, every enterprise's big data vision is to convert data into value; a core ingredient in this quest is the availability of the data that needs to be mined for insights. Although the growth in volume of data sitting in HDFS has been incredible and continues to grow exponentially, much of this has been contextual data - e.g., social data, click-stream data, sensor data, logs, 3rd party data sources - and historical data. Real-time operational data - e.g., data from foundational enterprise applications such as ERP (Enterprise Resource Planning), CRM (Customer Relationship Management), and Supply Chain and Inventory Management (SCM) systems - has historically been maintained separately and moving data across in either direction to allow for analytics across the data set is cumbersome at best. The union of Spark and HANA is designed to change that.\n\nWith over 200,000 customers and among the largest portfolios of enterprise applications, SAP\u2019s software serves as the gateway to one of the most valuable treasure troves of enterprise data globally, and SAP HANA is the cornerstone of SAP\u2019s platform strategy underpinning these enterprise applications moving forward. Now, the community of Spark developers and users will have full access as well, enabling a richness of analytical possibilities that has been hard to achieve otherwise.\n\nBy the same token, enterprise applications built on top of HANA will now move closer to achieving the holy grail of a fully closed-loop operational system. Many of these applications are critical decision support systems for enterprises that directly drive day-to-day business. Having access to the full corpus of data would enable more accurate and effective decisions across verticals - sensor and weather data for utilities, social trends for retailers, traffic patterns and political events for commodity players. In practice, however, much of this data is held outside of HANA; with the Spark + HANA integration, enterprises can make mission-critical decisions across \u2018100% of the data\u2019.\n<h2>More than just data stores - two powerful engines</h2>\nMuch of the Big Data paradigm has been built on the notion of bringing \u2018compute to the data\u2019, as opposed to simply trying to ETL all the data to a single central repository. Spark and HANA embody this principle by providing powerful engines that operate on data in place. Beyond being a repository for valuable corporate data, HANA also provides a wide variety of advanced analytics packages - including predictive, text/NLP, and geospatial - at blazing fast speeds. Spark also provides advanced analytics capabilities - SQL, streaming data, machine learning, and graph computation - that work natively on HDFS and other data stores such as Cassandra.\n\nBeyond their individual capabilities, the true power of this integration is the ability of Spark and HANA to work closely together. Rather than performing a simple \u2018select *\u2019 query to grab a full data set, Spark can push down more advanced queries (e.g., complex joins, aggregates, and classification algorithms) - leveraging HANA\u2019s horsepower and reducing expensive shuffles of data. A similar mechanism works for HANA users, where TGFs (Table Generating Functions) and Custom UDFs (User Defined functions) provide access to the full breadth of Spark\u2019s capabilities through the Smart Data Access functionality.\n<h2>Certified Spark Distribution - the value of ecosystem</h2>\nCandidly, SAP is not known for its long history of open software use and contributions. That said, they certainly respect the value of it and what it can deliver - their bet on Spark is certainly a testament to that. More importantly, they understand the value of a vibrant ecosystem, and that a unified community is a key ingredient in enabling this. That\u2019s why they\u2019ve been adamant that any SAP Spark distribution is a Certified Spark Distribution - and hence capable of supporting the rapidly growing set of \u201cCertified on Spark\u201d applications and the development ecosystem. This action by a global powerhouse - along with the other Certified Distributions - is a strong testament that maintaining compatibility with the community-driven Apache Spark distribution can be achieved without sacrificing innovation and growth.\n<h2>Significant potential for the road ahead</h2>\nSAP\u2019s distribution of Spark and its current integration with HANA is undoubtedly a terrific start and opens up a wealth of new opportunities for the Spark community. As we look forward, we see no shortage of potential opportunities for deeper integration - greater cross-functional performance, incorporating elements of SAP and HANA's security model in Spark, and facilitating the deployment of Spark and HANA together against a multitude of environments - and are excited to see where the journey leads."}
{"status": "publish", "description": null, "creator": "rxin", "link": "https://databricks.com/blog/2014/07/01/shark-spark-sql-hive-on-spark-and-the-future-of-sql-on-spark.html", "authors": ["Reynold Xin"], "id": 796, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2014-07-02", "tz": "UTC", "createdOn": "2014-07-02"}, "title": "Shark, Spark SQL, Hive on Spark, and the future of SQL on Apache Spark", "slug": "shark-spark-sql-hive-on-spark-and-the-future-of-sql-on-spark", "content": "With the introduction of Spark SQL and the new Hive on Apache Spark effort (<a href=\"https://issues.apache.org/jira/browse/HIVE-7292\">HIVE-7292</a>), we get asked a lot about our position in these two projects and how they relate to Shark. At the <a href=\"http://spark-summit.org/2014\">Spark Summit</a> today, we announced that we are ending development of Shark and will focus our resources towards Spark SQL, which will provide a superset of Shark\u2019s features for existing Shark users to move forward. In particular, Spark SQL will provide both a seamless upgrade path from Shark 0.9 server and new features such as integration with general Spark programs.\n\n<img class=\"alignnone wp-image-818 size-large\" src=\"https://databricks.com/wp-content/uploads/2014/07/sql-directions-1024x691.png\" alt=\"Future of SQL on Spark\" width=\"400\" />\n<h2>Shark</h2>\nWhen the Shark project started 3 years ago, Hive (on MapReduce) was the only choice for SQL on Hadoop. Hive compiled SQL into scalable MapReduce jobs and could work with a variety of formats (through its SerDes). However, it delivered less than ideal performance. In order to run queries interactively, organizations deployed expensive, proprietary enterprise data warehouses (EDWs) that required rigid and lengthy ETL pipelines.\n\nThe stark contrast in performance between Hive and EDWs led to a huge debate in the industry questioning the inherent deficiency of query processing on general data processing engines. Many believed SQL interactivity necessitates an expensive, specialized runtime built for query processing (i.e. EDWs). Shark became <a href=\"https://amplab.cs.berkeley.edu/publication/shark-sql-and-rich-analytics-at-scale/\">one of the first interactive SQL on Hadoop systems</a>, and was the only one built on top of a general runtime (Spark). It demonstrated that none of the deficiencies that made Hive slow were fundamental, and a general engine such as Spark could marry the best of both worlds: it can be as fast as an EDW, and scales as well as Hive/MapReduce.\n\nWhy should you care about this seemingly academic debate? As organizations are looking for ways to give them an edge in businesses, they are employing techniques beyond the simple roll-up and drill-down capabilities that SQL provides. Building a SQL query engine on top of a general runtime unifies many disparate, powerful models, such as batch, streaming, machine learning. It enables data scientists and engineers to employ more sophisticated methods faster. Ideas from Shark were embraced quickly and even inspired some of the major efforts in speeding up Hive.\n<h2>From Shark to Spark SQL</h2>\nShark built on the Hive codebase and achieved performance improvements by swapping out the physical execution engine part of Hive. While this approach enabled Shark users to speed up their Hive queries, Shark inherited a large, complicated code base from Hive that made it hard to optimize and maintain. As we moved to push the boundary of performance optimizations and integrating sophisticated analytics with SQL, we were constrained by the legacy that was designed for MapReduce.\n\nIt is for this reason that we are ending development in Shark as a separate project and moving all our development resources to Spark SQL, a new component in Spark. We are applying what we learned in Shark to Spark SQL, designed from ground-up to leverage the power of Spark. This new approach enables us to innovate faster, and ultimately deliver much better experience and power to users.\n\nFor <b>SQL users</b>, Spark SQL provides state-of-the-art SQL performance and maintains compatibility with Shark/Hive. In particular, like Shark, Spark SQL supports all existing Hive data formats, user-defined functions (UDF), and the Hive metastore. With features that will be introduced in Apache Spark 1.1.0, Spark SQL beats Shark in <a href=\"https://databricks.com/blog/2014/06/02/exciting-performance-improvements-on-the-horizon-for-spark-sql.html\">TPC-DS performance</a> by almost an order of magnitude.\n\nFor <b>Spark users</b>, Spark SQL becomes the narrow-waist for manipulating (semi-) structured data as well as ingesting data from sources that provide schema, such as JSON, Parquet, Hive, or EDWs. It truly unifies SQL and sophisticated analysis, allowing users to mix and match SQL and more imperative programming APIs for advanced analytics.\n\nFor <b>open source hackers</b>, Spark SQL proposes a novel, elegant way of building query planners. It is incredibly easy to add new optimizations under this framework. We have been completely overwhelmed by the support and enthusiasm that the open source community has shown Spark SQL, largely thanks to this new design. Already after merely three months, over 40 contributors have contributed code to it. Thank you.\n<h2>Hive on Spark Project (HIVE-7292)</h2>\nWhile Spark SQL is becoming the standard for SQL on Spark, we do realize many organizations have existing investments in Hive. Many of these organizations, however, are also eager to migrate to Spark. The Hive community proposed a <a href=\"https://issues.apache.org/jira/browse/HIVE-7292\">new initiative</a> to the project that would add Spark as an alternative execution engine to Hive. For these organizations, this effort will provide a clear path for them to migrate the execution to Spark. We are delighted to work with and support the Hive community to provide a smooth experience for end-users.\n\nIn short, we firmly believe Spark SQL will be the future of not only SQL, but also structured data processing on Spark. We are hard at work and will bring you a lot more in the next several releases. And for organizations with legacy Hive deployments, Hive on Spark will provide them a clear path to Spark."}
{"status": "publish", "description": null, "creator": "ion", "link": "https://databricks.com/blog/2014/07/14/databricks-cloud-making-big-data-easy.html", "authors": ["Ion Stoica"], "id": 865, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2014-07-14", "tz": "UTC", "createdOn": "2014-07-14"}, "title": "Databricks: Making Big Data Easy", "slug": "databricks-cloud-making-big-data-easy", "content": "Our vision at Databricks is to <strong>make big data easy</strong> so that we enable <strong>every</strong> organization to turn its data into value. At Spark Summit 2014, we were very excited to unveil <a href=\"https://databricks.com/cloud\" target=\"_blank\">Databricks</a>, our first product towards fulfilling this vision.\n\nIn this post, I\u2019ll briefly go over the challenges that data scientists and data engineers face today when working with big data, and then show how Databricks addresses these challenges.\n<h2>Today\u2019s Big Data Challenges</h2>\nWhile the promise of big data to <a href=\"http://spark-summit.org/2014/talk/using-spark-to-generate-analytics-for-international-cable-tv-video-distribution\" target=\"_blank\">improve businesses</a>, <a href=\"http://spark-summit.org/2014/talk/david-patterson\" target=\"_blank\">save lives</a>, and <a href=\"http://spark-summit.org/2014/talk/A-platform-for-large-scale-neuroscience\" target=\"_blank\">advance science</a> is becoming more and more real, analyzing and processing data remains as hard as ever. Software engineers, data engineers, data scientists at both small and large organizations continue to struggle with setting up and maintaining clusters, dealing with a zoo of systems, which are not only hard to integrate but also hard to use.\n<h3>Set-up and maintain clusters</h3>\nWhen an organization starts a big data initiative, typically, the first step is setting up a Hadoop cluster. Unfortunately, this is hard. Today it may take 6-9 months just to set up a cluster on premise. Even if the organization already has an on premise cluster, it may take 2-3 months to get a few more servers for a new big data project.\n<h3>Integrate a zoo of systems</h3>\nOnce a cluster is in place, the next step is building a data pipeline. As shown in Figure 1, a typical data pipeline based on Hadoop includes ETL, (interactive) data exploration, building dashboards and reports, advanced analytics, and data products, such as a recommendation system.\n\n<img class=\"aligncenter size-full wp-image-62\" style=\"margin-top: 30px; margin-bottom: 5px; max-width: 100%;\" src=\"https://databricks.com/wp-content/uploads/2014/07/Figure1.png\" alt=\"\" align=\"middle\" />\n<p style=\"margin-bottom: 30px;\" align=\"center\"><em>Figure 1: Typical big data pipeline.</em></p>\nUnfortunately, implementing such a data pipeline is very hard, as it requires stitching together a hodgepodge of disparate, complex systems, including batch processing systems (e.g., Hadoop MR), query engines (e.g., Hive, Impala, Apache Drill), business intelligence tools (e.g., Qlik, Tableau), and systems providing support for machine learning and graph-based algorithms (e.g., Giraph, GraphLab, Mahout, R).\n<h3>Hard to use systems</h3>\nUnfortunately, even after setting up the pipeline, the systems themselves remain hard to use. Different systems expose different APIs and programming languages (e.g., Java, Clojure, Scala, Python, R), and, at best, they provide a shell interface. Furthermore, performing advanced analytics is hard, and building data applications is even harder.\n<h2>Databricks</h2>\nOur goal at Databricks is to address all these challenges. To achieve this goal we have introduced Databricks (Figure 2) an end-to-end platform for data analysis and processing that we believe will make big data easier to use than ever before.\n\n<img class=\"aligncenter size-full wp-image-62\" style=\"display: block; max-width: 60%; margin: 30px auto 5px auto;\" src=\"https://databricks.com/wp-content/uploads/2014/07/Figure2.png\" alt=\"\" align=\"middle\" />\n<p style=\"margin-bottom: 30px;\" align=\"center\"><em>Figure 2: Databricks Cloud</em></p>\nDatabricks is built around Apache Spark and consists of two additional components: a hosted platform (Databricks Platform) and a workspace (Databricks Workspace). Next, let\u2019s see how these components address each of the three challenges.\n<h3>Databricks Platform: No need to set-up and maintain clusters</h3>\nDatabricks Platform is a hosted platform that makes it trivially easy to create and manage a cluster. Databricks Platform includes a sophisticated cluster manager which allows users to have a cluster up and running in seconds, while providing everything they need out-of-the-box. In particular, Databricks Platform provides security, resource isolation, it instantiates a fully configured and up-to-date Spark cluster, it allows dynamic scaling, and it provides seamless data import. This way, Databricks Platform obviates the need to set up and maintain an on premise cluster.\n<h3>Apache Spark: Unifying existing big data systems</h3>\nDatabricks is built around Apache Spark, which unifies many of the functionalities provided by today\u2019s big data systems. In particular, Apache Spark provides support for batch processing, interactive query processing, streaming, machine learning, and graph based computations, all with a single API. This enables developers, data scientists, and data engineers to implement their entire pipeline in one system.\n<h3>Databricks Workspace: Making the platform easy to use</h3>\nDatabricks Workspace makes it dramatically easier to use big data frameworks, in general, and Spark, in particular, by providing three powerful web-based applications: notebooks, dashboards, and a job launcher.\n\n<strong>Notebooks</strong> allow users to interactively query and visualize data. Notebooks also provide support for online collaboration, thus allowing multiple users to cooperate on data exploration in real-time. Currently, notebooks allows users to query and analyze data using Python, SQL, and Scala.\n\nOnce users create one or more notebooks, they can take the most interesting results from these notebooks and create sophisticated <strong>dashboards</strong>. They can do so through a powerful yet intuitive dashboard builder, and then publish a dashboard just with the click of a button to other employees in the organization, or to their customers. Dashboards are interactive, as every plot can depend on one or more variables. When these variables are updated, the query behind each plot is automatically re-executed, and the plot is regenerated.\n\nFinally, Databricks Workspace includes a <strong>job launcher</strong> that enables users to programmatically run arbitrary Spark jobs. For instance, users can schedule jobs to run periodically, or run when their inputs change.\n<h3>Building big data pipelines with Databricks</h3>\nFigure 3 shows how Databricks can dramatically simplify the big data pipeline shown in Figure 1.\n\n<img class=\"aligncenter size-full wp-image-62\" style=\"margin-top: 30px; margin-bottom: 5px; max-width: 100%;\" src=\"https://databricks.com/wp-content/uploads/2014/07/Figure3.png\" alt=\"\" align=\"middle\" />\n<p style=\"margin-bottom: 30px;\" align=\"center\"><em>Figure 3: Databricks Cloud, big data pipeline</em></p>\nLet\u2019s assume for simplicity that most customer\u2019s data is in S3. Then, Databricks can operate on S3 data in place (no need to copy it) and implement the entire data pipeline:\n<ul>\n \t<li>One can do ETL using Spark jobs or notebooks, which can run periodically or when their inputs change</li>\n \t<li>One can do interactive exploration and advanced analytics using notebooks</li>\n \t<li>One can create dashboards in reports using the Dashboard builder</li>\n \t<li>Finally, one can create, build and run data products as Spark jobs or notebooks.</li>\n</ul>\nIn addition, Databricks can read inputs from other storage systems and databases available in AWS, and lets users use their favorite business intelligence tools through an ODBC connector.\n\nThis way Databricks allows users to focus on finding answers and building great data products rather than wrestling with setting up clusters, and stitching together disparate systems that are difficult to use.\n<h2>Third Party Applications</h2>\nWhile Databricks Workspace makes Databricks instantly useful out-of-the-box and it allows users to build non-trivial data pipelines, we are planning to add support for third party applications, beyond our own applications.\n\n<img class=\"aligncenter size-full wp-image-62\" style=\"display: block; max-width: 60%; margin: 30px auto 5px auto;\" src=\"https://databricks.com/wp-content/uploads/2014/07/Figure4.png\" alt=\"\" align=\"middle\" />\n<p style=\"margin-bottom: 30px;\" align=\"center\"><em>Figure 4: Supporting 3rd party apps</em></p>\nWe are already working with some of the certified Spark application developers to run their applications on top of Databricks. We are looking forward to extending Databricks with a vibrant application ecosystem.\n<h2>Databricks and Spark Community</h2>\nDatabricks will have a beneficial impact on the Apache Spark project, and it reaffirms our commitment to making Spark the best big data framework. Databricks will dramatically accelerate Spark\u2019s adoption, as it will make it much easier to learn and use Apache Spark. In addition, Databricks runs 100% Apache Spark. This means that there is no lock in. Users can take their jobs and applications they develop on Databricks and run them on any certified Spark distribution be it on premise or in the cloud.\n<h2>Summary</h2>\nIt is our belief that Databricks will dramatically simplify big data analysis, and it will become the best place to develop, test, and run data products. This will further fuel the growth of the Spark community by making big data easier to use than ever before. For more information about availability and deployment scenarios, please check the <a href=\"http://www.databricks.com/cloud#cloud-faq\" target=\"_blank\">following FAQ</a>."}
{"status": "publish", "description": null, "creator": "Xiangrui", "link": "https://databricks.com/blog/2014/07/16/new-features-in-mllib-in-spark-1-0.html", "authors": ["Xiangrui Meng"], "id": 909, "categories": ["Apache Spark", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2014-07-16", "tz": "UTC", "createdOn": "2014-07-16"}, "title": "New Features in MLlib in Apache Spark 1.0", "slug": "new-features-in-mllib-in-spark-1-0", "content": "MLlib is an Apache Spark component focusing on machine learning. It became a standard component of Spark in version 0.8 (Sep 2013). The initial contribution was from Berkeley AMPLab. Since then, 50+\u00a0developers from the open source community have contributed to its codebase. With the release of Apache Spark 1.0, I\u2019m glad to share some of the new features in MLlib. Among the most important ones are:\n<ul>\n \t<li>sparse data support</li>\n \t<li>regression and classification trees</li>\n \t<li>distributed matrices</li>\n \t<li>PCA and SVD</li>\n \t<li>L-BFGS optimization algorithm</li>\n \t<li>new user guide and code examples</li>\n</ul>\nThis is the first in a series of blog posts about features and optimizations in MLlib. We will focus on one feature new in 1.0 \u2014 sparse data support.\n<h2>Large-scale \u2248 Sparse</h2>\nWhen I was in graduate school, I wrote \u201clarge-scale sparse least squares\u201d in a paper draft. My advisor crossed out the word \u201csparse\u201d and left a comment: \u201cLarge-scale already implies sparsity, so you don\u2019t need to mention it twice.\u201d It is a good argument. Sparse datasets are indeed very common in the big data world, where the sparsity may come from many sources, e.g.,\n<ul>\n \t<li>feature transformation: one-hot encoding, interaction, and binarization,</li>\n \t<li>large feature space: n-grams,</li>\n \t<li>missing data: rating matrix.</li>\n</ul>\nTake the Netflix Prize qualifying dataset as an example. It contains around 100 million ratings generated by 480,189 users on 17,770 movies. Therefore, the rating matrix contains only 1% nonzeros. If an algorithm can utilize this sparsity, it can see significant improvements.\n<h2>Exploiting Sparsity</h2>\nIn Apache Spark 1.0, MLlib adds full support for sparse data in Scala, Java, and Python (previous versions only supported it in specific algorithms like alternating least squares). It takes advantage of sparsity in both storage and computation in methods including SVM, logistic regression, Lasso, naive Bayes, k-means, and summary statistics.\n\nTo give a concrete example, we ran k-means clustering on a dataset that contains more than 12 million examples with 500 feature dimensions. There are about 600 million nonzeros and hence the density is about 10%. The result is listed in the following table:\n<table style=\"width: 350px; height: 120px;\" border=\"0\" cellspacing=\"5\" cellpadding=\"5\" align=\"center\">\n<thead>\n<tr style=\"border-bottom-style: solid; border-bottom-width: 1px; border-color: black;\">\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\">sparse</th>\n<th style=\"text-align: center;\">dense</th>\n</tr>\n<tr>\n<td style=\"text-align: center;\">storage</td>\n<td style=\"text-align: center;\">7GB</td>\n<td style=\"text-align: center;\">47GB</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">time</td>\n<td style=\"text-align: center;\">58s</td>\n<td style=\"text-align: center;\">240s</td>\n</tr>\n</thead>\n</table>\n&nbsp;\n\nSo, not only did we save 40GB of storage by switching to the sparse format, but we also received a 4x speedup. If your dataset is sparse, we strongly recommend you to try this feature.\n<h2>Getting Started</h2>\nBoth sparse and dense feature vectors are supported via the Vector interface. A sparse vector is represented by two parallel arrays: indices and values. Zero entries are not stored. A dense vector is backed by a double array representing its entries. For example, a vector [1., 0., 0., 0., 0., 0., 0., 3.] can be represented in the sparse format as (7, [0, 6], [1., 3.]), where 7 is the size of the vector, as illustrated below:\n<p align=\"center\"><img class=\"alignnone wp-image-939\" src=\"https://databricks.com/wp-content/uploads/2014/07/xAJI2gUHspdUO4rO3JxsGPlHCsUvIZc9xI08QJA_14sebOFMDFMTSvbYi1c4AaPS-rh7Ly-FBJukdGxOo7mjKj9q4wb1ehZXfFFZHOLdn2MQGhLjtooF7Cm053PbwvXHUg.png\" alt=\"mllib-sparse-vector\" width=\"249\" height=\"112\" /></p>\nTake the Python API as an example. MLlib recognizes the following types as dense vectors:\n<ul>\n \t<li>NumPy\u2019s <code><a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html\">array</a>,</code></li>\n \t<li>Python\u2019s list, e.g., <code>[1, 2, 3].</code></li>\n</ul>\nand the following as sparse vectors:\n<ul>\n \t<li>MLlib\u2019s <code><a href=\"http://spark.apache.org/docs/1.0.1/api/python/pyspark.mllib.linalg.SparseVector-class.html\">SparseVector</a>,</code></li>\n \t<li>SciPy\u2019s\u00a0<code><a href=\"http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html#scipy.sparse.csc_matrix\">csc_matrix</a>\u00a0</code>with a single column.</li>\n</ul>\nWe recommend using NumPy arrays over lists for efficiency, and using the factory methods implemented\u00a0in <a href=\"http://spark.apache.org/docs/1.0.0/api/python/pyspark.mllib.linalg.Vectors-class.html\"><code>Vectors</code></a> to create sparse vectors.\n\n[python]\nimport numpy as np\nimport scipy.sparse as sps\nfrom pyspark.mllib.linalg import Vectors\n\n# Use a NumPy array as a dense vector.\ndv1 = np.array([1.0, 0.0, 3.0])\n# Use a Python list as a dense vector.\ndv2 = [1.0, 0.0, 3.0]\n# Create a SparseVector.\nsv1 = Vectors.sparse(3, [0, 2], [1.0, 3.0])\n# Use a single-column SciPy csc_matrix as a sparse vector.\nsv2 = sps.csc_matrix((np.array([1.0, 3.0]), np.array([0, 2]), \n    np.array([0, 2])), shape = (3, 1))\n[/python]\n\nK-means takes an RDD of vectors as input. For supervised learning, the training dataset is represented by an RDD of labeled points. A LabeledPoint contains a label and a vector, either sparse or dense. Creating a labeled point is straightforward.\n\n[python]\nfrom pyspark.mllib.linalg import SparseVector\nfrom pyspark.mllib.regression import LabeledPoint\n\n# Create a labeled point with a positive label and a dense vector.\npos = LabeledPoint(1.0, [1.0, 0.0, 3.0])\n\n# Create a labeled point with a negative label and a sparse vector.\nneg = LabeledPoint(0.0, SparseVector(3, [0, 2], [1.0, 3.0]))\n[/python]\n\nMLlib also supports reading and saving labeled data in LIBSVM format. For more information on the usage, please visit the <a href=\"https://spark.apache.org/docs/1.0.1/mllib-guide.html\">MLlib guide</a> and <a href=\"https://github.com/apache/spark/tree/v1.0.1/examples/src/main/scala/org/apache/spark/examples/mllib\">code examples</a>.\n<h2>When to Exploit\u00a0Sparsity</h2>\nFor many large-scale datasets, it is not feasible to store the data in a dense format. Nevertheless, for medium-sized data, it is natural to ask when we should switch from a dense format to sparse. In MLlib, a sparse vector requires 12nnz+4 bytes of storage, where nnz is the number of nonzeros, while a dense vector needs 8n bytes, where n is the vector size. So storage-wise, the sparse format is better than the dense format when more than 1/3 of the elements are zero. However, assuming that the data can be fit into memory in both formats, we usually need sparser data to observe a speedup, because the sparse format is not as efficient as the dense format in computation. Our experience suggests a sparsity of around 10%, while the exact switching point for the running time is indeed problem-dependent.\n\nSparse data support is part of Apache Spark 1.0, which is available for download right now at <a href=\"http://spark.apache.org/\">http://spark.apache.org/</a>. We will cover more new features in MLlib in a series of posts. So stay tuned."}
{"status": "publish", "description": null, "creator": "matei", "link": "https://databricks.com/blog/2014/07/18/the-state-of-apache-spark-in-2014.html", "authors": ["Matei Zaharia"], "id": 965, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2014-07-19", "tz": "UTC", "createdOn": "2014-07-19"}, "title": "The State of Apache Spark in 2014", "slug": "the-state-of-apache-spark-in-2014", "content": "<div class=\"post-meta\">This post originally appeared in <a href=\"http://inside-bigdata.com/2014/07/15/theres-spark-theres-fire-state-apache-spark-2014/\" target=\"_blank\">insideBIGDATA</a> and is reposted here with permission.</div>\n\n<hr />\n\nWith the second <a href=\"http://spark-summit.org/2014\">Spark Summit</a> behind us, we wanted to take a look back at our journey since 2009 when Apache Spark, the fast and general engine for large-scale data processing, was initially developed. It has been exciting and extremely gratifying to watch Spark mature over the years, thanks in large part to the vibrant, open source community that latched onto it and busily began contributing to make Spark what it is today.\n\nThe idea for Spark first emerged in the AMPLab (AMP stands for Algorithms, Machines, and People) at the University of California, Berkeley. With its significant industry funding and exposure, the AMPlab had a unique perspective on what is important and what issues exist among early adopters of big data. We had worked with most of the early users of Hadoop and consistently saw the same issues arise. Spark itself started as the solution to one such problem\u2014speeding machine learning applications on clusters, which machine learning researchers in the lab were having trouble doing using Hadoop. However, we soon realized that we could easily cover a much broader set of applications.\n<h2>The Vision</h2>\nWhen we worked with early Hadoop users, we saw that they were all excited about the scalability of MapReduce. However, as soon as these users began using MapReduce, they needed more than the system could offer. First, users wanted faster data analysis\u2014instead of waiting tens of minutes to run a query, as was required with MapReduce\u2019s batch model, they wanted to query data interactively, or even continuously in real-time. Second, users wanted more sophisticated processing, such as iterative machine learning algorithms, which were not supported by the rigid, one-pass model of MapReduce.\n\nAt this point, several systems had started to emerge as point solutions to these problems, e.g., systems that ran only interactive queries, or only machine learning applications. However, these systems were difficult to use with Hadoop, as they would require users to learn and stitch together a zoo of different frameworks to build pipelines. Instead, we decided to try to generalize the MapReduce model to support more types of computation in a single framework.\n\nWe achieved this using only two simple extensions to the model. First, we added support for storing and operating on data in memory\u2014a key optimization for the more complex, iterative algorithms required in applications like machine learning, and one that proved shrewd with the continued drop in memory prices. Second, we modeled execution as general directed acyclic graphs (DAGs) instead of the rigid model of map-and-reduce, which led to significant speedups even on disk. With these additions we were able to cover a wide range of emerging workloads, matching and sometimes exceeding the performance of specialized systems while keeping a single, simple unified programming model.\n\nThis decision allowed, over time, new functionality such as Shark (SQL over Spark), Spark Streaming (stream processing), MLlib (efficient implementations of machine learning algorithms), and GraphX (graph computation over Spark) to be built. These modules in Spark are not separate systems, but libraries that users can combine together into a program in powerful ways. Combined with the more than 80 basic data manipulation operators in Spark, they make it dramatically simpler to build big data applications compared to previous, multi-system pipelines. And we have sought to make them available in a variety of programming languages, including Java, Scala, and Python (available today) and soon R.\n\n<img class=\"aligncenter size-full wp-image-62\" style=\"max-width: 85%; display: block; margin: 30px auto 5px auto;\" src=\"https://databricks.com/wp-content/uploads/2014/07/Databricks_stack.png\" alt=\"\" align=\"middle\" />\n<p style=\"margin-bottom: 30px;\" align=\"center\"><em>Figure 1: The Spark Stack</em></p>\nAs interest in Spark increased, we received a lot of questions about how Spark related to Hadoop and whether Spark was its replacement. The reality is that Hadoop consists of three parts: a file system (HDFS), resource management (YARN/Mesos), and a processing layer (MapReduce). Spark is only a processing engine and thus is an alternative for that last layer. Having it operate over HDFS data was a natural starting point because the volume of data in HDFS was growing rapidly. However, Spark\u2019s architecture has also allowed it to support a host of storage systems beyond Hadoop. Spark is now being used as a processing layer for other data stores (e.g., Cassandra, MongoDB) or even to seamlessly join data from multiple data stores (e.g., HDFS and an operational data store).\n<h2>Success Comes from the Community</h2>\nPerhaps the one decision that has made Spark so robust is our continuing commitment to keep it 100 percent open source and work with a large set of contributors from around the world. That commitment continues to pay dividends in Spark\u2019s future and effectiveness.\n\nIn a relatively short time, enthusiasm rose among the open source community and is still growing. Indeed, in just the last 12 months, Spark has had more than 200 people from more than 50 organizations contribute code to the project, making it the most active open source project in the Hadoop ecosystem. Even after reaching this point, Spark is still enjoying steady growth, moving us toward the inflection point of the hockey stick adoption curve.\n\nNot only has the community invested countless hours in development, but it has also gotten people excited about Spark and brought users together to share ideas. One of the more exciting events was the first Spark Summit held in December 2013 in San Francisco, which drew nearly 500 attendees. At this year\u2019s Summit, held June 30, we had double the amount of participants. The 2014 Summit included more than 50 community talks on applications, data science, and research using Spark. In addition, Spark community meetups have sprouted all over the United States and internationally, and we anticipate that number to grow.\n\n<img class=\"aligncenter size-full wp-image-62\" style=\"max-width: 100%; display: block; margin: 30px auto 5px auto;\" src=\"https://databricks.com/wp-content/uploads/2014/07/Databricks_contrib.png\" alt=\"\" align=\"middle\" />\n<p style=\"margin-bottom: 30px;\" align=\"center\"><em>Figure 2: Spark Contributors By Release</em></p>\n\n<h2>Spark in the Enterprise</h2>\nWe have seen a real rise in excitement among enterprises as well. After going through the initial proof-of-concept process, Spark has found its place in the enterprise ecosystem and every major Hadoop distributor has made Spark part of their distribution. For many of these distributions, support came from the bottom up: we heard from vendors that customers were downloading and using Spark on their own and then contacting vendors to ask them to support it.\n\nSpark is being used across many verticals including large Internet companies, government agencies, financial service companies, and some major players such as Yahoo, eBay, Alibaba, and NASA. These enterprises are deploying Spark for a variety of use cases including ETL, machine learning, data product creation, and complex event processing with streaming data. Vertical-specific cases include churn analysis, fraud detection, risk analytics, and 360-degree customer views. And many companies are conducting advanced analytics using Spark\u2019s scalable machine learning library (MLlib), which contains high-quality algorithms that leverage iteration to yield better results.\n\nFinally, in addition to being used directly by customers, Spark is increasingly the backend for a growing number of higher-level business applications. Major business intelligence vendors such as Microstrategy, Pentaho, and Qlik have all certified their applications on Spark, while a number of innovative startups such as Adatao, Tresata, and Alpine are basing products on it. These applications bring the capabilities of Spark to a much broader set of users throughout the enterprise.\n<h2>Spark\u2019s Future</h2>\nWe recently released version 1.0 of Apache Spark \u2013 a major milestone for the project. This version includes a number of added capabilities such as:\n<ul>\n \t<li>A stable application programming interface to provide compatibility across all 1.x releases.</li>\n \t<li>Spark SQL to provide schema-aware data modeling and SQL language support.</li>\n \t<li>Support for Java 8 lambda syntax to simplify writing applications in Java.</li>\n \t<li>Enhanced MLlib with several new algorithms; MLlib continues to be extremely active on its own with more than 40 contributors since it was introduced in September 2013.</li>\n \t<li>Major updates to Spark\u2019s streaming and graph libraries.</li>\n</ul>\nThese wouldn\u2019t have happened without the support of the community. Many of these features were requested directly by users, while others were contributed by the dozens of developers who worked on this release. One of our top priorities is to continue to make Spark more robust and focus on key enterprise features, such as security, monitoring, and seamless ecosystem integration.\n\nAdditionally, the continued success of Spark is dependent on a vibrant ecosystem. For us, it is exciting to see the community innovate and enhance above, below, and around Spark. Maintaining compatibility across the various Spark distributions will be critical, as we\u2019ve seen how destructive forking and fragmentation can be to open source efforts. We would like to define and center compatibility around the Apache version of Spark, where we continue to make all our contributions. We are excited to see the community rally around this vision.\n\nWhile Spark has come far in the past five years, we realize that there is still a lot to do. We are working hard on new features and improvements in both the core engine and the libraries built on top. We look forward to future Spark releases, an expanded ecosystem, and future Summits and meetups where people are generating ideas that go far beyond what we imagined years ago at UC Berkeley."}
{"status": "publish", "description": null, "creator": "Xiangrui", "link": "https://databricks.com/blog/2014/07/23/scalable-collaborative-filtering-with-spark-mllib.html", "authors": ["Burak Yavuz", "Xiangrui Meng", "Reynold Xin"], "id": 980, "categories": ["Apache Spark", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2014-07-23", "tz": "UTC", "createdOn": "2014-07-23"}, "title": "Scalable Collaborative Filtering with Apache Spark MLlib", "slug": "scalable-collaborative-filtering-with-spark-mllib", "content": "Recommendation systems are among the most popular applications of machine learning. The idea is to predict whether a customer would like a certain item: a product, a movie, or a song.\u00a0Scale is a key concern for recommendation systems, since computational complexity increases with the size of a company's customer base. In this blog post, we discuss how Apache Spark MLlib enables building recommendation models from billions of records in just a few lines of Python (<a href=\"http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html\">Scala/Java APIs also available</a>).<!--more-->\n\n[python]\nfrom pyspark.mllib.recommendation import ALS\n\n# load training and test data into (user, product, rating) tuples\ndef parseRating(line):\n  fields = line.split()\n  return (int(fields[0]), int(fields[1]), float(fields[2]))   \ntraining = sc.textFile(&quot;...&quot;).map(parseRating).cache()\ntest = sc.textFile(&quot;...&quot;).map(parseRating)\n\n# train a recommendation model\nmodel = ALS.train(training, rank = 10, iterations = 5)\n\n# make predictions on (user, product) pairs from the test data\npredictions = model.predictAll(test.map(lambda x: (x[0], x[1])))\n[/python]\n\n<h2>What\u2019s Happening under the Hood?</h2>\nRecommendation algorithms are usually divided into:\n\n(1) <strong>Content-based filtering</strong>: recommending items similar to what users already like. An example would be to play a Megadeth song after a Metallica song.\n\n(2) <strong>Collaborative filtering</strong>: recommending items based on what similar users like, e.g., recommending video games after someone purchased a game console because other people who bought game consoles also bought video games.\n<p align=\"center\"><img class=\"aligncenter wp-image-984\" src=\"https://databricks.com/wp-content/uploads/2014/07/als-illustration.png\" alt=\"\" width=\"350px\" /></p>\nSpark MLlib implements a collaborative filtering algorithm called <strong>Alternating Least Squares (ALS)</strong>, which has been implemented in many machine learning libraries and widely studied and used in both academia and industry. ALS models the rating matrix (R) as the multiplication of low-rank user (U) and product (V) factors, and learns these factors by minimizing the reconstruction error of the observed ratings. The unknown ratings can subsequently be computed by multiplying these factors. In this way, companies can recommend products based on the predicted ratings and increase sales and customer satisfaction.\n\nALS is an iterative algorithm. In each iteration, the algorithm alternatively fixes one factor matrix and solves for the other, and this process continues until it converges. MLlib features a blocked implementation of the ALS algorithm that leverages Spark\u2019s efficient support for distributed, iterative computation. It uses native LAPACK to achieve high performance and scales to billions of ratings on commodity clusters.\n<h2>Scalability, Performance, and Stability</h2>\nRecently we did an experiment to benchmark ALS implementations in Spark MLlib at scale. The benchmark was conducted on EC2 using m3.2xlarge instances set up by the Spark EC2 script. We ran Spark using out-of-the-box configurations. To help understand state-of-the-art, we also built Mahout from GitHub and tested it. This benchmark is reproducible on EC2 using the scripts at <a href=\"https://github.com/databricks/als-benchmark-scripts\">https://github.com/databricks/als-benchmark-scripts</a>.\n\nWe ran 5 iterations of ALS on scaled copies of the <a href=\"https://snap.stanford.edu/data/web-Amazon.html\" target=\"_blank\">Amazon Reviews dataset</a>, which contains 35 million ratings collected from 6.6 million users on 2.4 million products. For each user, we create pseudo-users that have the same ratings. That is, for every rating as (userId, productId, rating), we generate (userId+i, productId, rating) where 0 &lt;= i &lt; s and s is the scaling factor.\n<p align=\"center\"><img class=\"alignnone size-full wp-image-1015\" src=\"https://databricks.com/wp-content/uploads/2014/07/als-perf.png\" alt=\"ALS performance\" width=\"600\" /></p>\nThe current version of Mahout runs on Hadoop MapReduce, whose scheduling overhead and lack of support for iterative computation substantially slows down ALS. Mahout recently announced switching to Spark as the execution engine, which will hopefully address the performance concerns.\n\nSpark MLlib demonstrated excellent performance and scalability, as demonstrated in the chart above. MLlib can also\u00a0scale to much larger datasets and to\u00a0larger number of nodes, thanks to its fault-tolerance design.\u00a0With 50 nodes, we\u00a0ran 10 iterations of MLlib's ALS on 100 copies of the Amazon Reviews dataset in only 40 minutes. And with EC2 spot instances the total cost was less than $2.\u00a0Users can use Spark MLlib to reduce the model training time and the cost for ALS, which is historically very expensive to run because the algorithm is very communication intensive and computation\u00a0intensive.\n<table style=\"width: 500px; height: 70px;\" cellspacing=\"5\" cellpadding=\"5\" align=\"center\">\n<thead>\n<tr style=\"border-bottom-style: solid; border-bottom-width: 1px; border-color: black;\">\n<td style=\"text-align: center;\">#\u00a0ratings</td>\n<td style=\"text-align: center;\">#\u00a0users</td>\n<td style=\"text-align: center;\">#\u00a0products</td>\n<td style=\"text-align: center;\">\u00a0time</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">3.5 billion</td>\n<td style=\"text-align: center;\">660 million</td>\n<td style=\"text-align: center;\">2.4 million</td>\n<td style=\"text-align: center;\">40 mins</td>\n</tr>\n</thead>\n</table>\n&nbsp;\n\nIt is our belief at Databricks and the broader Spark community that machine learning frameworks need to be performant, scalable, and be able to cover a wide range of workloads including data exploration and feature extraction. MLlib integrates seamlessly with other Spark components, delivers best-in-class performance, and substantially simplifies operational complexity by running on top of a fault-tolerant engine.\u00a0That said, our work is not done and we are working on making machine learning easier. Stay tuned for more exciting features.\n\n&nbsp;\n\nNote: The blog post was updated on July 24, 2014 to reflect a new performance optimization that will be included in Spark MLlib 1.1. The runtime for 3.5B ratings went down from 90 mins\u00a0in MLlib 1.0 to 40 mins in MLlib 1.1."}
{"status": "publish", "description": null, "creator": "matei", "link": "https://databricks.com/blog/2014/07/21/distributing-the-singular-value-decomposition-with-spark.html", "authors": ["Li Pu", "Reza Zadeh"], "id": 1049, "categories": ["Apache Spark", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2014-07-22", "tz": "UTC", "createdOn": "2014-07-22"}, "title": "Distributing the Singular Value Decomposition with Apache Spark", "slug": "distributing-the-singular-value-decomposition-with-spark", "content": "<div class=\"post-meta\">Guest post by Li Pu from Twitter and Reza Zadeh from Databricks on their recent contribution to Apache Spark's machine learning library.</div>\n\n<hr />\n\nThe <a href=\"http://en.wikipedia.org/wiki/Singular_value_decomposition\">Singular Value Decomposition (SVD)</a> is one of the cornerstones of linear algebra and has widespread application in many real-world modeling situations. Problems such as recommender systems, linear systems, least squares, and many others can be solved using the SVD. It is frequently used in statistics where it is related to principal component analysis (PCA) and to correspondence analysis, and in signal processing and pattern recognition. Another usage is latent semantic indexing in natural language processing.\n\nDecades ago, before the rise of distributed computing, computer scientists developed the single-core <a href=\"http://www.caam.rice.edu/software/ARPACK/\">ARPACK package</a> for computing the eigenvalue decomposition of a matrix. Since then, the package has matured and is in widespread use by the numerical linear algebra community, in tools such as <a href=\"http://docs.scipy.org/doc/scipy/reference/tutorial/arpack.html\">SciPy</a>, <a href=\"http://www.gnu.org/software/octave/doc/interpreter/External-Packages.html\">GNU Octave</a>, and <a href=\"http://www.mathworks.com/products/matlab/index.html\">MATLAB</a>.\n\nAn important feature of ARPACK is its ability to allow for arbitrary matrix storage formats. This is possible because it doesn't operate on the matrices directly, but instead acts on the matrix via prespecified operations, such as matrix-vector multiplies. When a matrix operation is required, ARPACK gives control to the calling program with a request for a matrix-vector multiply. The calling program must then perform the multiply and return the result to ARPACK.\n\nBy using the distributed-computing power of Spark, we can distribute the matrix-vector multiplies, and thus exploit the years of numerical computing expertise that have gone into building ARPACK, and the years of distributing computing expertise that have gone into Spark.\n\nSince ARPACK is written in Fortran77, it cannot immediately be used on the Java Virtual Machine. However, through the <a href=\"https://github.com/fommil/netlib-java\">netlib-java</a> and <a href=\"https://github.com/scalanlp/breeze\">breeze</a> interfaces, we can use ARPACK on the JVM. This also means that low-level hardware optimizations can be exploited for any local linear algebraic operations. As with all linear algebraic operations within MLlib, we use hardware acceleration whenever possible.\n\nWe are building the linear algebra capabilities of <a href=\"http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html\">MLlib</a>. Currently in Apache Spark 1.0 there is support for Tall and Skinny <a href=\"https://github.com/apache/spark/pull/88\">SVD and PCA,</a> and as of Apache Spark 1.1, we will have support for SVD and PCA <a href=\"https://github.com/apache/spark/pull/964\">via ARPACK</a>.\n<h2>Example Runtime</h2>\nA very popular matrix in the recommender systems community is the Netflix Prize Matrix. The matrix has 17,770 rows, 480,189 columns, and 100,480,507 non-zeros. Below we report results on several larger matrices (up to 16x larger) we experimented with at Twitter.\n\nWith the Spark implementation of SVD using ARPACK, calculating wall-clock time with 68 executors and 8GB memory in each, looking for the top 5 singular vectors, we can factorize larger matrices distributed in RAM across a cluster, in a few seconds.\n<table class=\"table\">\n<tbody>\n<tr>\n<td><b>Matrix size</b></td>\n<td><b>Number of nonzeros</b></td>\n<td><b>Time per iteration (s)</b></td>\n<td><b>Total time (s)</b></td>\n</tr>\n<tr>\n<td>23,000,000 x 38,000</td>\n<td>51,000,000</td>\n<td>0.2</td>\n<td>10</td>\n</tr>\n<tr>\n<td>63,000,000 x 49,000</td>\n<td>440,000,000</td>\n<td>1</td>\n<td>50</td>\n</tr>\n<tr>\n<td>94,000,000 x 4,000</td>\n<td>1,600,000,000</td>\n<td>0.5</td>\n<td>50</td>\n</tr>\n</tbody>\n</table>\nApart from being fast, SVD is also easy to run. Here is a <a href=\"https://gist.github.com/vrilleup/9e0613175fab101ac7cd\">code snippet</a> showing how to run it on sparse data loaded from a text file."}
{"status": "publish", "description": null, "creator": "scott", "link": "https://databricks.com/blog/2014/07/22/spark-summit-2014-highlights.html", "authors": ["Scott Walent"], "id": 1081, "categories": ["Company Blog", "Events"], "dates": {"publishedOn": "2014-07-23", "tz": "UTC", "createdOn": "2014-07-23"}, "title": "Spark Summit 2014 Highlights", "slug": "spark-summit-2014-highlights", "content": "From June 30 to July 2, 2014 we held the <a href=\"http://spark-summit.org/2014\">second Spark Summit</a>, a conference focused on promoting the adoption and growth of <a href=\"http://spark.apache.org\">Apache Spark</a>. This was an exciting year for the Spark community and we are proud to share\u00a0some highlights.\n<ul>\n\t<li>1,164 participants from over 453 companies attended</li>\n\t<li>Spark Training sold out at 300 participants</li>\n\t<li>31 organizations sponsored the event</li>\n\t<li>12 keynotes and 52 community presentations were given</li>\n</ul>\n&nbsp;\n\nVideos and slides from all presentations are now available on the <a href=\"http://spark-summit.org/2014/agenda\">Summit 2014 agenda</a>\u00a0page.\u00a0Some highlights include:\n<ul>\n\t<li>Spark Summit\u00a0<a href=\"https://www.youtube.com/watch?v=lO7LhVZrNwA&amp;index=2&amp;list=PL-x35fyliRwiST9gF7Z8Nu3LgJDFRuwfr\">keynote from Databricks CEO Ion Stoica</a>\u00a0introducing <a href=\"http://www.databricks.com/cloud\">Databricks Cloud</a></li>\n\t<li>Open source community updates by\u00a0<a href=\"https://www.youtube.com/watch?v=e-Ys-2uVxM0&amp;index=2&amp;list=PL-x35fyliRwiST9gF7Z8Nu3LgJDFRuwfr\">Matei Zaharia</a>\u00a0and\u00a0<a href=\"https://www.youtube.com/watch?v=2iXQnTVgHuw&amp;list=PL-x35fyliRwjCR-gDhk1ekG4jh2ltgKSV&amp;index=3\">Patrick Wendell</a></li>\n\t<li>Keynotes from\u00a0<a href=\"https://www.youtube.com/watch?v=KuFaBJiFzmI&amp;index=2&amp;list=PL-x35fyliRwjCR-gDhk1ekG4jh2ltgKSV\">SAP</a>,\u00a0<a href=\"https://www.youtube.com/watch?v=8kcdwnbHnJo&amp;list=PL-x35fyliRwjCR-gDhk1ekG4jh2ltgKSV&amp;index=2\">Cloudera</a>,\u00a0<a href=\"https://www.youtube.com/watch?v=tznIN_mUcR8&amp;index=12&amp;list=PL-x35fyliRwjCR-gDhk1ekG4jh2ltgKSV\">MapR</a>\u00a0and\u00a0<a href=\"https://www.youtube.com/watch?v=3qrFAcNQGHY&amp;index=4&amp;list=PL-x35fyliRwiST9gF7Z8Nu3LgJDFRuwfr\">Datastax</a></li>\n\t<li>Application talks including how Spark is used in\u00a0<a href=\"https://www.youtube.com/watch?v=Gg_5fWllfgA&amp;index=5&amp;list=PL-x35fyliRwjCR-gDhk1ekG4jh2ltgKSV\">neuroscience</a>,\u00a0<a href=\"https://www.youtube.com/watch?v=I6qmEcGNgDo&amp;list=PL-x35fyliRwgNYIV1P9prgtKyiMrTKp5k&amp;index=2\">image processing</a>\u00a0and\u00a0<a href=\"https://www.youtube.com/watch?v=RwyEEMw-NR8&amp;list=PL-x35fyliRwiST9gF7Z8Nu3LgJDFRuwfr&amp;index=6\">genomics</a></li>\n\t<li>Talks from the core developers on components including\u00a0<a href=\"https://www.youtube.com/watch?v=7WwwLkRs3-Y&amp;list=PL-x35fyliRwiDhtOvRSNgMdw05xFMzvhU&amp;index=2\">Spark SQL</a>,\u00a0<a href=\"https://www.youtube.com/watch?v=SF4Xv2bvZW0&amp;list=PL-x35fyliRwiuc6qy9z2erka2VX8LY53x&amp;index=7\">MLlib</a>,\u00a0<a href=\"https://www.youtube.com/watch?v=MFSUAkDBSdQ&amp;index=7&amp;list=PL-x35fyliRwiDhtOvRSNgMdw05xFMzvhU\">JSON support</a>\u00a0and\u00a0<a href=\"https://www.youtube.com/watch?v=CUX1SG9zTkU&amp;index=2&amp;list=PL-x35fyliRwiuc6qy9z2erka2VX8LY53x\">SparkR</a></li>\n</ul>\nAdditionally, we have posted the training videos, slides and all hands-on exercises for free on the <a href=\"http://spark-summit.org/2014/training\">Databricks Spark Training page</a>.\n<h3>Spark Summit comes to NYC in 2015</h3>\n<a href=\"http://spark-summit.org/east/2015\">Spark Summit East</a>\u00a0will debut in New York City in early 2015. If you would like to be notified when tickets are on sale, please\u00a0<a href=\"https://docs.google.com/forms/d/1xrVXJfzalGzol0eGONd2zr9zG7jSCJwUBPquviB-SkI/viewform?usp=send_form\">pre-register here</a>. While we are excited to expand into another city, we are actively planning for our third Spark Summit in San Francisco. Make sure to <a href=\"https://docs.google.com/forms/d/1OgZx4uCwElR0Y9UkPlowgxtS4MRLzzehgQ7pWDJAhZo/viewform?usp=send_form\">pre-register here</a>\u00a0to get updates about the event.\n<h3 id=\"other-summit-resources\">Other Summit Resources</h3>\n<ul>\n\t<li>Follow the <a href=\"http://twitter.com/spark_summit\">@spark_summit</a> twitter handle</li>\n\t<li>Like the <a href=\"http://facebook.com/ApacheSparkSummit\">Summit Facebook page</a></li>\n</ul>"}
{"status": "publish", "description": null, "creator": "arsalan", "link": "https://databricks.com/blog/2014/08/08/when-stratio-met-spark-a-true-love-story.html", "authors": ["Oscar Mendez (CEO of Stratio)"], "id": 1144, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2014-08-08", "tz": "UTC", "createdOn": "2014-08-08"}, "title": "When Stratio Met Apache Spark: A True Love Story", "slug": "when-stratio-met-spark-a-true-love-story", "content": "<div class=\"post-meta\">This is a guest post from our friends at <a href=\"http://www.stratio.com\" target=\"_blank\">Stratio</a> announcing that their platform is now a \"Certified Apache Spark Distribution\".</div>\n\n<hr />\n\n<h2>Certified distribution</h2>\nStratio is delighted to announce that it is officially a Certified Apache Spark Distribution. The certification is very important for us because we deeply believe that the certification program provides many benefits to the Spark community: It facilitates collaboration and integration, offers broad evolution and support for the rich Spark ecosystem, simplifies adoption of critical security updates and allows development of applications valid for any certified distribution - a key ingredient for a successful ecosystem.\n<!--more-->\nThis post is a brief history of how we started with big data technologies until we made the shift to Spark.\n<h2>When Stratio met Spark: A true love story</h2>\nWe started using Big Data technologies more than 7 years ago, when Hadoop was still in Beta. We did not start using Big Data technologies because we were so smart as to predict it was going to be the future of data, it was just chance and necessity.\n\nWe had a product for ORM (online reputation management). This product was collecting Internet comments about more than 400 hundred companies. After collecting all these comments we were processing them with our semantic engine, to make reports and to send alerts to our clients. With Web 2.0 the information in the Internet started to grow exponentially; all the blogs were generating millions of posts and comments. Thus, our semantic engine was taking longer to process each day, until we were not able to send the reports before 9am. We discussed ways of solving the problem, e.g., to increase the number of servers as well as other solutions. As you have already imagined, one of the solutions was to use a very incipient technology called Hadoop to optimize our semantic engine processing. We were young and brave (we still are) and we followed that path. It was really hard; Hadoop Beta had a lot of bugs. After several months, we were able to implement a MapReduce style program for our semantic engine and make our first test. The results were impressive; we shortened the time needed to process all the comments from 12 hours to less than 30 minutes. It was awesome; of course we knew that the semantic processing was pretty \u201cmap reducible\u201d, but it blew our minds anyway. That was the moment we fell in love with Big Data technology.\n\nBy 2013 we had been developing Big Data projects for more than 6 years. Those were the times when we were implementing Nathan Marz Lambda architectures combining Hadoop and Storm. We achieved fantastic goals, and our clients were impressed with the results. But we also found some limitations: there were no interactive queries nor real time data streaming analysis. And the projects were becoming more complex to develop, deploy and to support. So we were looking for a better way and a technology to serve our clients, and hence we found Spark.\n<h2>Spark</h2>\nWe started using Spark during 2013. The in-memory processing and the elegance of the architecture were able to provide incredible power and possibilities with maximum simplicity - it was just awesome. We decided to incorporate Spark in our platform before it became an Apache Incubator project, because we saw that the concepts behind it, and the improvements and possibilities were so huge that we did not have any doubt that it was the new \u201cHadoop Map Reduce\u201d engine. During 2013, Spark streaming was added to Spark, and in 2014 Spark became an Apache Top Level project, so time has proven us right. In fact, we did not incorporate Spark keeping Hadoop Map Reduce; we completely replaced Hadoop Map Reduce, creating a Pure Spark Platform.\n<h2>Stratio: A pure Spark enterprise platform</h2>\nTo include spark in our platform so early was a bit risky, but to become such early adopters has had a big return for us. We have been able to create a pure spark enterprise platform in record time and our first version was launched at the end of March.\n\nThanks to Spark and our Pure Spark approach, our Enterprise Platform was leaner and simpler than former platforms:\n\n<img class=\"aligncenter size-full wp-image-62\" style=\"max-width: 85%; display: block; margin: 30px auto 20px auto;\" src=\"https://databricks.com/wp-content/uploads/2014/08/STRATIO_presentacion2_admin.png\" alt=\"\" align=\"middle\" />\n\nWe have integrated some of the Hadoop tools such as Flume, and also created some modules that were needed in order to have a real Enterprise ready Spark platform.\n<h2>Admin</h2>\nThe first one was our admin module. \u201cAdmin\u201d is responsible for:\n<ul>\n \t<li>Installation, Full deployment on-premise and cloud</li>\n \t<li>Platform management and monitoring</li>\n \t<li>Security</li>\n \t<li>System Dashboards and Reporting</li>\n \t<li>System alerts</li>\n</ul>\n<h2>CrossData</h2>\nAfter 7 years doing Big Data projects, we have seen once again how difficult it was for the clients to use Big Data technology, so we tried to simplify the use of Big data technology with three main objectives in mind:\n<ul>\n \t<li>Allow the clients to use the system just using SQL. Nothing else needed.</li>\n \t<li>Combine the best processing engine with the best NoSQL Databases, to leverage the benefits of both worlds.</li>\n \t<li>Allow combination: Different storage systems ( HDFS, MongoDB, Elastic Search\u2026) or data stored with data streaming entering in real time into the system (past data with current data)</li>\n</ul>\nIn order to achieve the goals described above we created \u201cCrossData\u201d:\n\n<img class=\"aligncenter size-full wp-image-62\" style=\"max-width: 85%; display: block; margin: 30px auto 20px auto;\" src=\"https://databricks.com/wp-content/uploads/2014/08/Spark-hub-06-06.png\" alt=\"\" align=\"middle\" />\n\n\u201cCrossData\u201d is not only an easy SQL interface; it combines data, and also uses Spark to complement the NoSQL Databases with features not implemented in their APIs, e.g., make SQL joint in MongoDB or to allow any other before \u201cimpossible\u201d sentences.\n<h2>Spark Streaming and Stratio Streaming</h2>\nWe have been using Spark streaming from the very beginning. In fact we were replacing Storm in some of the projects we developed before Spark Streaming was launched.\n\nWe at Stratio think that Spark Streaming is the perfect tool to build a powerful solution for interactive complex event processing. Therefore we have combined Spark Streaming with complex engine processing (CEP) and Kafka creating Stratio Streaming:\n\n<img class=\"aligncenter size-full wp-image-62\" style=\"max-width: 85%; display: block; margin: 30px auto 20px auto;\" src=\"https://databricks.com/wp-content/uploads/2014/08/Imagen5.png\" alt=\"\" align=\"middle\" />\n<h2>Use cases</h2>\nWe only use Spark for processing, and not just in POCs but for real projects for big companies. Comparing the projects we were doing with former technologies with the ones we are doing now with Spark, we can point out several benefits:\n<ul>\n \t<li><strong>Developers</strong>: Easier and faster development</li>\n \t<li><strong>System Engineers</strong>: Easier deployment and cost reduction support</li>\n \t<li><strong>Clients and Business</strong>: More value with less complexity</li>\n</ul>\nWe have measured the improvement of using Spark against the previous Big Data platforms for some of our Banking Clients, and Spark has been up to 20X faster, reducing hours of processing down to minutes. Here are some examples of real use cases of Spark and the Stratio platform:\n<ul>\n \t<li><strong>NH Hotels</strong>: They wanted to aggregate customer satisfaction data and guests\u2019 reviews from social networks and combine them with financial data. Using the Stratio platform they were able to manage about 200K reviews per year. The Quality Focus Online tool is used by more than 400 hotel directors world-wide and the 15% of the variable income of employees depends on its measurements. Additionally, they have considerably reduced their negative reviews by using this tool.</li>\n \t<li><strong>Telefonica</strong>: The cyber security group has the necessity to analyze all their logs in order to detect or even prevent possible hacking attacks. After checking many of the existing technologies they decide to use the Stratio platform for such a task. Currently they can take profit of all the available information coming from different sources (access logs, DNS, email, reviews, etc) by detecting and resolving possible security weaknesses or exploits and generate exhaustive informs.</li>\n</ul>\n<img class=\"aligncenter size-full wp-image-62\" style=\"max-width: 85%; display: block; margin: 30px auto 20px auto;\" src=\"https://databricks.com/wp-content/uploads/2014/08/Logos.png\" alt=\"\" align=\"middle\" />\n<h2>The future</h2>\nWe see Spark as the evolution of Hadoop Map Reduce, extended and with none of the previous limitations, so we are just at the beginning of a revolution for Big Data processing. And it is only improving with new modules and possibilities every year that will make products and projects which were once impossible with former technologies a reality. So stay connected because this adventure has just started."}
{"status": "publish", "description": null, "creator": "rxin", "link": "https://databricks.com/blog/2014/08/14/mining-graph-data-with-spark-at-alibaba-taobao.html", "authors": ["Andy Huang (Alibaba Taobao Data Mining Team)", "Wei Wu (Alibaba Taobao Data Mining Team)"], "id": 1170, "categories": ["Apache Spark", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2014-08-15", "tz": "UTC", "createdOn": "2014-08-15"}, "title": "Mining Ecommerce Graph Data with Apache Spark at Alibaba Taobao", "slug": "mining-graph-data-with-spark-at-alibaba-taobao", "content": "<div class=\"post-meta\">This is a guest blog post from our friends at Alibaba Taobao.</div>\n\n<hr />\n\nAlibaba Taobao operates one of the world\u2019s largest e-commerce platforms. We collect hundreds of petabytes of data on this platform and use Apache Spark to analyze these enormous amounts of data. Alibaba Taobao probably runs some of the largest Spark jobs in the world. For example, some Spark jobs run for weeks to perform feature extraction on petabytes of image data. In this blog post, we share our experience with Spark and GraphX from prototype to production at the Alibaba Taobao Data Mining Team.\n<!--more-->\n\nEvery day, hundreds of millions of users and merchants interact on Alibaba Taobao\u2019s marketplace. These interactions can be expressed as complicated, large scale graphs. Mining data requires a distributed data processing engine that can support fast interactive queries as well as sophisticated algorithms.\n\nSpark and GraphX embed a standard set of graph mining algorithms, including PageRank, triangle counting, connected components, shortest path. The implementation of these algorithms focuses on reusability. Users can implement variants of these algorithms in order to exploit performance optimization opportunities for specific workloads. In our experience, the best way to learn GraphX is to read and understand the source code of these algorithms.\n\nAlibaba Taobao started prototyping with GraphX in Apache Spark 0.9 and went into production in May 2014 around the time that Apache Spark 1.0 was released.\n\nOne thing to note is that GraphX is still evolving quickly. Although the user-facing APIs are relatively stable, the internals have seen fairly large refactoring and improvements from 0.8 to 1.0. Based on our experience, each minor version upgrade provided 10 - 20% performance improvements even without modifying our application code.\n<h2>Graph Inspection Platform</h2>\nGraph-based structures model the many relationships between our users and items in our store. Our business and product teams constantly need to make decisions based on the value and health of each relationship. Before Spark, they used their intuition to estimate such properties, resulting in decisions which were not a good fit with reality. To solve this problem, we developed a platform to scientifically quantify all these metrics in order to provide evidence and insights for product decisions.\n\nThis platform requires constantly re-iterating the set of metrics it provides to users, depending on product demand. The interactive nature of both Spark and GraphX proves very valuable in building this platform. Some of the metrics this platform measures\u00a0are:\n\n<b>Degree Distribution</b>: Degree distribution measures the distribution of vertex degrees (e.g. how many users have 50 friends). It also provides valuable information on the number of high degree vertices (so-called super vertices). Often our downstream product infrastructure needs to accommodate super vertices in a special manner (because they have a high impact on propagation algorithms), and thus it is crucial to understand their distribution among our data. GraphX\u2019s VertexRDD provides built-in support for both in-degrees and out-degrees.\n\n<b>Second Degree Neighbors</b>: Modeling social relationships often requires measuring the second-degree neighbor distribution. For example, in an instant messaging platform we developed, the number of \u201cretweets\u201d correlates with the number of second degree neighbors (e.g. number of friends of friends). While GraphX does not yet provide built-in support for counting second degree neighbors, we implemented it using two rounds of propagations: the first round propagates each vertex\u2019s ID to its neighbors, and the second round propagates all IDs from neighbors to second degree neighbors. After the two rounds of propagations, each vertex calculates the number of second degree neighbors using a hash set.\n\nOne thing to note in this calculation is that we use the aforementioned degree distribution to remove super vertices from the second degree neighbor calculation. Otherwise, these super vertices would create too many messages, leading to high computation skew and high memory usage.\n\n<b>Connected Components</b>: Connected components refer to some set of subgraphs that are \u201cconnected\u201d, i.e. there exists a path connecting any pair of vertices in the subgraph. Connected component is very useful in dividing a large graph into multiple, smaller graphs, and then operations that are computationally too expensive to run on the large graph. This algorithm can also be adapted to discover tightly connected networks.\n\nWe are developing more metrics using both built-in functions provided by Spark and GraphX, as well as new ones implemented internally. This platform nurtures a new culture such that our product decisions are no longer based on instinct and intuition, but rather on metrics mined from data.\n<h2>Multi-graph Merging</h2>\nThe Graph Inspection Platform provides us with different properties for modeling relationships. Each relationship structure has its own strengths and weaknesses. For example, some relationship structure provides more valuable information in connected components, while another other structure might work better for interactions. We often make decisions based on multiple different properties and structural representations of the same underlying graph. Based on GraphX, we developed a multi-graph merging framework that creates \u201cintersections\u201d of multiple graphs.\n\n<img class=\"alignnone size-full wp-image-1173\" src=\"https://databricks.com/wp-content/uploads/2014/08/image00.png\" alt=\"image00\" width=\"578\" height=\"196\" />\n\nThe attached figure illustrates the algorithm to merge graph A and graph B to create graph C: edges are created in graph C if any of its vertices exist in graph A or graph B.\n\nThis merging framework was implemented using the outerJoinVertices operator provided by GraphX. In addition to naively merging two graphs, the framework can also assign different weights to the input graphs. In practice, our analysis pipelines often merge multiple graphs in a variety of ways and run them on the Graph Inspection Platform.\n<h2>Belief Propagation</h2>\nWeighted belief propagation is a classic way of modeling graph data, often used to predict a user\u2019s influence or credibility. The intuition is simple: highly credited users often interact with other highly credited users, while lower credited users often interact with other lower credited users. Although the algorithm is simple, historically we did not attempt to run these on our entire graph due to the computation cost to scale this to hundreds of millions of users and billions of interactions. Using GraphX, we are able to scale this analysis to the entire graphs we have.\n\nEach run of the algorithm requires 3 iterations, and each iteration requires 8 iterations in GraphX\u2019s Pregel API. After a total of 30 iterations in Pregel, the AUC (area under the curve) increased from 0.6 to 0.9, which is a very satisfactory prediction rate.\n\nWhile we are still in the early stages of our journey with GraphX, already today we have been able to generate impressive insights with graph modeling and analysis that would have been very hard to accomplish at our scale without GraphX. We plan to enrich and further develop our various platforms and frameworks to include an even wider array of metrics and apply them to tag/topic inference, demographics inference, transaction prediction, which will in turn improve our various recommendation systems\u2019 effectiveness.\n<h2>Authors</h2>\nAndy Huang leads the data mining team at Taobao. He is a very early adopter of Spark, using it production since Spark 0.5.\n\nWei Wu is an engineer at Taobao\u2019s data mining team. His interests span distributed systems, large-scale machine learning and data mining.\n\nThis guest blog post is a translation of part of an article published by <a href=\"http://www.csdn.net/article/2014-08-07/2821097\">CSDN Programmer Magazine</a>."}
{"status": "publish", "description": null, "creator": "Xiangrui", "link": "https://databricks.com/blog/2014/08/27/statistics-functionality-in-spark.html", "authors": ["Doris Xin", "Burak Yavuz", "Xiangrui Meng", "Hossein Falaki"], "id": 1301, "categories": ["Apache Spark", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2014-08-27", "tz": "UTC", "createdOn": "2014-08-27"}, "title": "Statistics Functionality in Apache Spark 1.1", "slug": "statistics-functionality-in-spark", "content": "One of our philosophies in Apache Spark is to provide rich and friendly built-in libraries so that users can easily assemble data pipelines. With Spark, and MLlib in particular, quickly gaining traction among data scientists and machine learning practitioners, we\u2019re observing a growing demand for data analysis support outside of model fitting. To address this need, we have started to add scalable implementations of common statistical functions to facilitate various components of a\u00a0data pipeline. <!--more-->We\u2019re pleased to announce Apache Spark 1.1. ships with built-in support for several statistical algorithms common in exploratory data pipelines:\n<ul>\n \t<li><strong>correlations</strong>: data dependence analysis</li>\n \t<li><strong>hypothesis testing</strong>: goodness of fit; independence test</li>\n \t<li><strong>stratified sampling</strong>: scaling training set with controlled label distribution</li>\n \t<li><strong>random data generation</strong>: randomized algorithms; performance tests</li>\n</ul>\nAs ease of use is one of the main missions of Spark, we\u2019ve devoted a great deal of effort to API designing for the statistical functions. Spark's statistics APIs\u00a0borrow inspirations from widely adopted statistical packages, such as R and SciPy.stats, which are shown in a recent O\u2019Reilly survey to be the most popular tools among data scientists.\n<h2>Correlations</h2>\n<a href=\"http://en.wikipedia.org/wiki/Correlation\" target=\"_blank\">Correlations</a> provide quantitative measurements of the statistical dependence between two random variables. Implementations for correlations are provided under <span style=\"font-family: monospace; font-size: 80%;\">mllib.stat.Statistics</span>.\n<table cellspacing=\"5\" cellpadding=\"5\" align=\"center\">\n<thead>\n<tr style=\"border-bottom-style: solid; border-bottom-width: 1px; border-color: silver;\">\n<td style=\"text-align: center;\">MLlib</td>\n<td style=\"text-align: left;\">\n<ul>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%; line-height: 1;\">corr(x, y = None, method = \"pearson\" | \"spearman\")</span></li>\n</ul>\n</td>\n</tr>\n<tr style=\"border-bottom-style: solid; border-bottom-width: 1px; border-color: silver;\">\n<td style=\"text-align: center;\">R</td>\n<td style=\"text-align: left;\">\n<ul>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%; line-height: 1;\">cor(x, y = NULL, method = c(\"pearson\", \"kendall\", \"spearman\"))</span></li>\n</ul>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">SciPy</td>\n<td style=\"text-align: left;\">\n<ul>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%; line-height: 1;\">pearsonr(x, y)</span></li>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%; line-height: 1;\">spearmanr(a, b = None)</span></li>\n</ul>\n</td>\n</tr>\n</thead>\n</table>\nAs shown in the table, R and SciPy.stats present us with two very different directions for correlations API in MLlib. We ultimately converged on the R style of having a single function that takes in the correlation method name as a string argument out of considerations for extensibility as well as conciseness of the API list. The initial set of methods contains \"pearson\" and \"spearman\", the two most commonly used correlations.\n<h2>Hypothesis testing</h2>\n<a href=\"http://en.wikipedia.org/wiki/Hypothesis_testing\" target=\"_blank\">Hypothesis testing</a> is essential for data-driven applications. A test result shows the statistical significance of an event unlikely to have occurred by chance. For example, we can test whether there is a significant association between two samples via independence tests. In Apache Spark 1.1, we implemented chi-squared tests for goodness-of-fit and independence:\n<table cellspacing=\"5\" cellpadding=\"5\" align=\"center\">\n<thead>\n<tr style=\"border-bottom-style: solid; border-bottom-width: 1px; border-color: silver;\">\n<td style=\"text-align: center;\">MLlib</td>\n<td style=\"text-align: left;\">\n<ul>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%;\">chiSqTest(observed: Vector, expected: Vector)</span></li>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%;\">chiSqTest(observed: Matrix)</span></li>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%;\">chiSqTest(data: RDD[LabeledPoint])</span></li>\n</ul>\n</td>\n</tr>\n<tr style=\"border-bottom-style: solid; border-bottom-width: 1px; border-color: silver;\">\n<td style=\"text-align: center;\">R</td>\n<td style=\"text-align: left;\">\n<ul>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%;\">chisq.test(x, y = NULL, correct = TRUE, p = rep(1/length(x), length(x)), rescale.p = FALSE, simulate.p.value = FALSE)</span></li>\n</ul>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">SciPy</td>\n<td style=\"text-align: left;\">\n<ul>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%;\">chisquare(f_obs, f_exp = None, ddof = 0, axis = 0)</span></li>\n</ul>\n</td>\n</tr>\n</thead>\n</table>\nWhen designing the chi-squared test APIs, we took the greatest common denominator of the parameters in R\u2019s and SciPy\u2019s APIs and edited out some of the less frequently used parameters for API simplicity. Note that as with R and SciPy, the input data types determine whether the goodness of fit or the independence test is conducted. We added a special case support for input type <span style=\"font-family: monospace; font-size: 80%;\">RDD[LabeledPoint]</span> to enable feature selection via chi-squared independence tests.\n<h2>Stratified sampling</h2>\nIt is common for a large population to consist of various-sized subpopulations (strata), for example, a training set with many more positive instances than negatives. To sample such populations, it is advantageous to sample each stratum independently to reduce the total variance or to represent small but important strata. This sampling design is called <a href=\"http://en.wikipedia.org/wiki/Stratified_sampling\" target=\"_blank\">stratified sampling</a>. Unlike the other statistics functions, which reside in MLlib, we placed the stratified sampling methods in Spark Core, as sampling is widely used in data analysis. We provide two versions of stratified sampling, <span style=\"font-family: monospace; font-size: 80%;\">sampleByKey</span> and <span style=\"font-family: monospace; font-size: 80%;\">sampleByKeyExact</span>. Both apply to an RDD of key-value pairs with key indicating the stratum, and both take a map from users that specifies the sampling probability for each stratum. Neither R nor SciPy provides built-in support for stratified sampling.\n<table cellspacing=\"5\" cellpadding=\"5\" align=\"center\">\n<thead>\n<tr>\n<td style=\"text-align: center;\">MLlib</td>\n<td style=\"text-align: left;\">\n<ul>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%;\">sampleByKey(withReplacement, fractions, seed)</span></li>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%;\">sampleByKeyExact(withReplacement, fractions, seed)</span></li>\n</ul>\n</td>\n</tr>\n</thead>\n</table>\nSimilar to <span style=\"font-family: monospace; font-size: 80%;\">RDD.sample</span>, <span style=\"font-family: monospace; font-size: 80%;\">sampleByKey</span> applies Bernoulli sampling or Poisson sampling to each item independently, which is cheap but doesn\u2019t guarantee the exact sample size for each stratum (the size of the stratum times the corresponding sampling probability). <span style=\"font-family: monospace; font-size: 80%;\">sampleByKeyExact</span> utilizes scalable sampling algorithms that guarantee the exact sample size for each stratum with high probability, but it requires multiple passes over\u00a0the data. We have a separate method name to underline the fact that it is significantly more expensive.\n<h2>Random data generation</h2>\nRandom data generation is useful for testing of existing algorithms and implementing randomized algorithms, such as random projection. We provide methods under <span style=\"font-family: monospace; font-size: 80%;\">mllib.random.RandomRDDs</span> for generating RDDs that contains i.i.d. values drawn from a distribution, e.g., uniform, standard normal, or Poisson.\n<table cellspacing=\"5\" cellpadding=\"5\" align=\"center\">\n<thead>\n<tr style=\"border-bottom-style: solid; border-bottom-width: 1px; border-color: silver;\">\n<td style=\"text-align: center;\">MLlib</td>\n<td style=\"text-align: left;\">\n<ul>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%; line-height: 1;\">normalRDD(sc, size, [numPartitions, seed])</span></li>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%; line-height: 1;\">normalVectorRDD(sc, numRows, numCols, [numPartitions, seed])</span></li>\n</ul>\n</td>\n</tr>\n<tr style=\"border-bottom-style: solid; border-bottom-width: 1px; border-color: silver;\">\n<td style=\"text-align: center;\">R</td>\n<td style=\"text-align: left;\">\n<ul>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%; line-height: 1;\">rnorm(n, mean=0, sd=1)</span></li>\n</ul>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">SciPy</td>\n<td style=\"text-align: left;\">\n<ul>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%; line-height: 1;\">randn(d0, d1, ..., dn)</span></li>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%; line-height: 1;\">normal([loc, scale, size])</span></li>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%; line-height: 1;\">standard_normal([size])</span></li>\n</ul>\n</td>\n</tr>\n</thead>\n</table>\nThe random data generation APIs exemplify a case where we added Spark-specific customizations to a commonly supported API. We show a side-by-side comparison of MLlib\u2019s normal distribution data generation APIs vs. those of R and SciPy in the table above. We provide 1D (<span style=\"font-family: monospace; font-size: 80%;\">RDD[Double]</span>) and 2D (<span style=\"font-family: monospace; font-size: 80%;\">RDD[Vector]</span>) support, compared to only 1D in R and arbitrary dimension in NumPy, since both are prevalent in MLlib functions. In addition to the Spark specific parameters, such as SparkContext and number of partitions, we also allow users to set the seed for reproducibility. Apart from the built-in distributions, users can plug in their own via <span style=\"font-family: monospace; font-size: 80%;\">RandomDataGenerator</span>.\n<h2>What about SparkR?</h2>\nAt this point you may be asking why we\u2019re providing native support for statistics functions inside Spark given the existence of the SparkR project. As an R package, SparkR is a great lightweight solution for empowering familiar R APIs with distributed computation support. What we\u2019re aiming to accomplish with these built-in Spark statistics APIs is cross language support as well as seamless integration with other components of Spark, such as Spark SQL and Streaming, for a unified data product development platform. We expect these features to be callable from SparkR in the future.\n<h2>Concluding remarks</h2>\nBesides a set of familiar APIs, statistics functionality in Spark also brings R and SciPy users huge benefits including scalability, fault tolerance, and seamless integration with existing big data pipelines. Both R and SciPy run on a single machine, while Spark can easily scale up to hundreds of machines and distribute the computation. We compared the running times of MLlib\u2019s Pearson\u2019s correlation on a 32-node cluster with R\u2019s, not counting the time needed for moving data to the node with R installed. The results shown below demonstrates a clear advantage of Spark over R on performance and scalability.\n<p align=\"center\"><a href=\"https://databricks.com/wp-content/uploads/2014/08/Spark-vs-R-pearson.png\"><img class=\"aligncenter size-full wp-image-1239\" src=\"https://databricks.com/wp-content/uploads/2014/08/Spark-vs-R-pearson.png\" alt=\"Spark-vs-R-pearson\" width=\"400\" /></a></p>\nAs the Statistics APIs are experimental, we\u2019d love feedback from the community on the usability of these designs. We also welcome contributions from the community to enhance statistics functionality in Spark."}
{"status": "publish", "description": null, "creator": "patrick", "link": "https://databricks.com/blog/2014/09/11/announcing-spark-1-1.html", "authors": ["Patrick Wendell"], "id": 1360, "categories": ["Apache Spark", "Engineering Blog", "Streaming"], "dates": {"publishedOn": "2014-09-12", "tz": "UTC", "createdOn": "2014-09-12"}, "title": "Announcing Apache Spark 1.1", "slug": "announcing-spark-1-1", "content": "Today we\u2019re thrilled to announce the release of Apache Spark 1.1! Apache Spark 1.1 introduces many new features along with scale and stability improvements. This post will introduce some key features of Apache Spark 1.1 and provide context on the priorities of Spark for this and the next release.<!--more--> In the next two weeks, we\u2019ll be publishing blog posts with more details on feature additions in\u00a0each of the major components. Apache Spark 1.1 is already available to Databricks customers and has also been posted today on the <a href=\"http://spark.apache.org/releases/spark-release-1-1-0.html\">Apache Spark website</a>.\n\n<!--more-->\n<h2>Maturity of SparkSQL</h2>\nThe 1.1 released upgrades Spark SQL significantly from the preview delivered in Apache Spark 1.0. At Databricks, we\u2019ve migrated all of our customer workloads from Shark to Spark SQL, with between 2X and 5X <a href=\"https://databricks.com/blog/2014/06/02/exciting-performance-improvements-on-the-horizon-for-spark-sql.html\">performance improvements</a> across the board. Apache Spark 1.1 adds a JDBC server for Spark SQL, a key feature allowing direct upgrade of Shark installations which relied on JDBC. We\u2019ve also opened up the Spark SQL type system with a public types API, allowing for rich integration with third party data sources. This will provide an extension point for many future integrations, such as the Datastax Cassandra driver. Using\u00a0this types API, we\u2019ve added turn-key\u00a0support for loading JSON data into Spark's native ShemaRDD format:\n\n[python]\n# Create a JSON RDD in Python\n&gt;&gt;&gt; people = sqlContext.jsonFile(\u201cs3n://path/to/files...\u201d)\n# Visualize the inferred schema\n&gt;&gt;&gt; people.printSchema()\n# root\n#  |-- age: integer (nullable = true)\n#  |-- name: string (nullable = true)\n[/python]\n\n<h2>Expansion of MLlib</h2>\nSpark\u2019s machine learning library adds several new algorithms, including a library for <a href=\"https://databricks.com/blog/2014/08/27/statistics-functionality-in-spark.html\">standard exploratory statistics</a> such as sampling, correlations, chi-squared tests, and randomized inputs. This allows data scientists to avoid exporting data to single-node systems (R, SciPy, etc) and instead directly operate on large scale datasets in Spark. Optimizations to internal primitives provide a 2-5X performance improvement in most MLlib algorithms out of the box. Decision trees, a popular algorithm, has been ported to Java and Python. Several other algorithms have also been added, including TF-IDF, SVD\u00a0via Lanczos, and nonnegative matrix factorization. The next release of MLlib will introduce an enhanced API for end-to-end machine learning pipelines.\n<h2>Sources and Libraries for Spark Streaming</h2>\nSpark streaming extends its library of ingestion sources in this release adding two new sources. The first is support for Amazon Kinesis, a hosted stream processing engine. Spark Streaming also adds H/A source for Apache Flume using a new data source which provides transactional hand-off of events from Flume to gracefully tolerate worker failures. Apache Spark 1.1 adds the first of a set of online machine learning algorithms with the introduction of a streaming linear regression. Looking forward, the Spark Streaming roadmap\u00a0will feature a general recoverability mechanism for all input sources, along with an ever-growing list of connectors. The example below shows training a linear model using incoming data, then using an updated model to make a prediction:\n\n[scala]\n &gt; val stream = KafkaUtils.createStream(...)\n\n // Train a linear model on a data stream\n &gt; val model = new StreamingLinearRegressionWithSGD()\n   .setStepSize(0.5)\n   .setNumIterations(10)\n   .setInitialWeights(Vectors.dense(...))\n   .trainOn(DStream.map(record =&gt; createLabeledPoint(record))\n\n // Predict using the latest updated model\n &gt; model.latestModel().predict(myDataset)\n[/scala]\n\n<h2>Performance in Spark Core</h2>\nThis release adds significant internal changes to Spark focused on improving performance for large scale workloads. Apache Spark 1.1 features a new implementation of the Spark shuffle, a key internal primitive used by almost all data-intensive programs. The new shuffle improves performance by more than 5X for workloads with extremely high degree of parallelism, a key pain point in earlier versions of Spark. Apache Spark 1.1 also adds a variety of other improvements to decrease memory usage and improve performance.\n<h2>Optimizations and Features in PySpark</h2>\nSeveral of the disk-spilling modifications introduced in Apache Spark 1.0 have been ported to Spark\u2019s Python runtime extension. This release also adds support in Python for reading and writing data from SequenceFiles, Avro, and other Hadoop-based input formats. PySpark now supports the entire Spark SQL API, including support for nested types inside of SchemaRDD\u2019s.\n\nThe efforts on improving scale and robustness of Spark and PySpark are based on feedback from the community along with direct interactions with our customer workloads at Databricks. The next release of Spark will continue along this theme, with a focus on improving instrumentation and debugging for users to pinpoint performance bottlenecks.\n\nThis post only scratches the surface of interesting features in Apache Spark 1.1. Head on over to the <a href=\"http://spark.apache.org/releases/spark-release-1-1-0.html\">official release notes</a> to learn more about this release and stay tuned to hear more about Apache Spark 1.1 from Databricks over the coming days!"}
{"status": "publish", "description": null, "creator": "arsalan", "link": "https://databricks.com/blog/2014/09/16/spark-1-1-the-state-of-spark-streaming.html", "authors": ["Arsalan Tavakoli-Shiraji", "Tathagata Das", "Patrick Wendell"], "id": 1386, "categories": ["Apache Spark", "Engineering Blog", "Streaming"], "dates": {"publishedOn": "2014-09-16", "tz": "UTC", "createdOn": "2014-09-16"}, "title": "Apache Spark 1.1: The State of Spark Streaming", "slug": "spark-1-1-the-state-of-spark-streaming", "content": "With Apache Spark 1.1 recently released, we\u2019d like to take this occasion to feature one of the most popular Spark components - Spark Streaming - and highlight who is using Spark Streaming and why.\n\nApache Spark 1.1. adds several new features to Spark Streaming. \u00a0In particular, Spark Streaming extends its library of ingestion sources to include Amazon Kinesis, a hosted stream processing engine, as well as to provide high availability for Apache Flume sources. \u00a0Moreover, Apache Spark 1.1 adds the first of a set of online machine learning algorithms with the introduction of a streaming linear regression.\n\nMany organizations have evolved from exploratory, discovery use cases of big data to use cases that require reasoning on data as it arrives in order to make decisions in real time. \u00a0Spark Streaming enables this category of high-value use cases, providing a system for processing fast and large streams of data in real time.\n\n<b>What is it?</b>\n\nSpark Streaming is an extension of the core Spark API that enables high-throughput, reliable processing of live data streams. Spark Streaming ingests data from any source including Amazon Kinesis, Kafka, Flume, Twitter and file systems such as S3 and HDFS. \u00a0Users can express sophisticated algorithms easily using high-level functions to process the data streams. \u00a0The core innovation behind Spark Streaming is to treat streaming computations as a series of deterministic micro-batch computations on small time intervals, executed using Spark's distributed data processing framework. \u00a0Micro-batching unifies the programming model of streaming with that of batch use cases and enables strong fault recovery guarantees while retaining high performance. \u00a0The processed data can then be stored in any file system (including HDFS), database (including Hbase), or live dashboards.\n\n<b>Where is it being used?</b>\n\nSpark Streaming has seen a significant uptake in adoption in the past year as enterprises increasingly use it as part of Spark deployments. \u00a0The Databricks team is aware of more than 40 organizations that have deployed Spark Streaming in production.\n\nJust as impressive is the breadth of industries across which Spark Streaming is being used. \u00a0For instance:\n<ul>\n \t<li>A leading software vendor leverages Spark Streaming to power its <b>real-time supply chain analytics platform</b>, which provides more than 1,000 supplier performance metrics to users via dashboards and detailed operational reports.</li>\n \t<li>A large <b>hardware vendor</b> is using Spark Streaming for security intelligence operations, notably a first check of known threats.</li>\n \t<li>A leading <b>advertising</b> technology firm is processing click stream data for its real-time ad auction platform, leading to more accurate targeting of display advertising, better consumer engagement and higher conversion.</li>\n \t<li>A global <b>telecommunications</b> provider is collecting metrics from millions of mobile phones and analyzing them to determine where to place new cell phone towers and upgrade aging infrastructure, resulting in improved service quality.</li>\n \t<li>A pre-eminent video analytics provider is using Spark Streaming to help <b>broadcasters and media companies</b> monetize video with personalized, interactive experiences for every screen.</li>\n</ul>\nWe\u2019ve seen Spark Streaming benefit many parts of an organization, as the following examples illustrate:\n<ul>\n \t<li><b>Marketing &amp; Sales</b>: \u00a0Analysis of customer engagement and conversion, powering real-time recommendations while customers are still on the site or in the store.</li>\n \t<li><b>Customer Service &amp; Billing</b>: \u00a0Analysis of contact center interactions, enabling accurate remote troubleshooting before expensive field technicians are dispatched.</li>\n \t<li><b>Manufacturing</b>: Real-time, adaptive analysis of machine data (e.g., sensors, control parameters, alarms, notifications, maintenance logs, and imaging results) from industrial systems (e.g., equipment, plant, fleet) for visibility into asset health, proactive maintenance planning, and optimized operations.</li>\n \t<li><b>Information Technology</b>: \u00a0Log processing to detect unusual events occurring in streams of data, empowering IT to take remedial action before service quality degrades.</li>\n \t<li><b>Risk Management</b>: \u00a0Anomaly detection and root cause forensics, which sometimes makes it possible to stop fraud while it happens.</li>\n</ul>\n<b>Why is it being used?</b>\n\nThe reasons enterprises give for adopting (and in many cases transitioning to) Spark Streaming often start with the advantages that Spark itself brings. \u00a0All the strengths of Spark\u2019s unified programming model apply to Spark Streaming, which is particularly relevant for real-time analytics that combine historical data with fresh data:\n<ul>\n \t<li><b>Making data science accessible to non-scientists</b>: \u00a0Spark\u2019s declarative APIs enable users who have domain expertise but lack data science expertise to express a business problem and its associated processing algorithm and data pipeline using simple high-level operators.</li>\n \t<li><b>Higher productivity for data workers</b>: \u00a0Spark\u2019s write-once-run-anywhere approach unifies batch and stream processing. \u00a0In fact, Spark ties together the different parts of an analytics pipeline in the same tool, such as discovery, ETL, data engineering, machine learning model training and execution, across all types of structured and unstructured data.</li>\n \t<li><b>Exactly-once semantics: </b>\u00a0Many business critical use cases have a need for exactly-once stateful processing, not at-most-once (which includes zero) or at-least-once (which includes duplicates). \u00a0Exactly-once provides users with certainty on questions such as the exact number of frauds, emergencies or outages occurring today.</li>\n \t<li><b>No compromises on scalability and throughput</b>. \u00a0Spark Streaming is designed for hyper-scale environments and combines statefulness and persistence with high throughput.</li>\n \t<li><b>Ease of operations</b>: \u00a0Spark provides a unified run time across different processing engines. \u00a0One physical cluster and one set of operational processes covers the full spectrum of use cases.</li>\n</ul>\nWe have also learned from the community that the high throughput that Spark Streaming provides is just as important as latency. \u00a0In fact, latency of a few hundred milliseconds is sufficient for the vast majority of streaming use cases. \u00a0Rare exceptions include algorithmic trading.\n\nOne capability that allows Spark Streaming to be deployed in such a wide variety of situations is that users have a choice of three resource managers: \u00a0Full integration with YARN and Mesos as well as the ability to rely on Spark\u2019s easy-to-use stand-alone resource manager. \u00a0Moreover, Spark and Spark Streaming are supported already by leading vendors such as Cloudera, MapR and Datastax. \u00a0We expect other vendors will include and support Spark in their Hadoop distributions in the near future.\n\nPlease stay tuned for future posts on Spark Streaming technical design patterns and practical use cases."}
{"status": "publish", "description": null, "creator": "Xiangrui", "link": "https://databricks.com/blog/2014/09/22/spark-1-1-mllib-performance-improvements.html", "authors": ["Burak Yavuz", "Xiangrui Meng"], "id": 1393, "categories": ["Apache Spark", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2014-09-22", "tz": "UTC", "createdOn": "2014-09-22"}, "title": "Apache Spark 1.1: MLlib Performance Improvements", "slug": "spark-1-1-mllib-performance-improvements", "content": "With an ever-growing community, Apache Spark has had it\u2019s <a href=\"https://databricks.com/blog/2014/09/11/announcing-spark-1-1.html\" target=\"_blank\">1.1 release</a>. MLlib has had its fair share of contributions and now supports many new features. We are excited to share some of the performance improvements observed in MLlib since the 1.0 release, and discuss two key contributing factors: torrent broadcast and tree aggregation.\n<h2>Torrent broadcast</h2>\nThe beauty of Spark as a unified framework is that any improvements made on the core engine come for free in its standard components like MLlib, Spark SQL, Streaming, and GraphX. In Apache Spark 1.1, we changed the default broadcast implementation of Spark from the traditional <code>HttpBroadcast</code> to <code>TorrentBroadcast</code>, a BitTorrent like protocol that evens out the load among the driver and the executors. When an object is broadcasted, the driver divides the serialized object into multiple chunks, and broadcasts the chunks to different executors. Subsequently, executors can fetch chunks individually from other executors that have fetched the chunks previously.\n<p align=\"center\"><a href=\"https://databricks.com/wp-content/uploads/2014/09/broadcast.png\"><img class=\"aligncenter size-full wp-image-1399\" src=\"https://databricks.com/wp-content/uploads/2014/09/broadcast.png\" alt=\"broadcast\" width=\"85%\" /></a></p>\nHow does this change in Spark Core affect MLlib\u2019s performance?\n\nA common communication pattern in machine learning algorithms is the one-to-all broadcast of intermediate models at the beginning of each iteration of training. In large-scale machine learning, models are usually huge and broadcasting them via http can make the driver a severe bottleneck because all executors (workers) are fetching the models from the driver. With the new torrent broadcast, this load is shared among executors as well. It leads to significant speedup, and MLlib takes it for free.\n<h2>Tree aggregation</h2>\nSimilar to broadcasting models at the beginning of each iteration, the driver builds new models at the end of each iteration by aggregating partial updates collected from executors. This is the basis of the MapReduce paradigm. One performance issue with the <code>reduce</code> or <code>aggregate</code> functions in Spark (and the original MapReduce) is that the aggregation time scales linearly with respect to the number of partitions of data (due to the CPU cost in merging partial results and the network bandwidth limit).\n<p align=\"center\"><a href=\"https://databricks.com/wp-content/uploads/2014/09/aggregation.png\"><img class=\"aligncenter size-full wp-image-1398\" src=\"https://databricks.com/wp-content/uploads/2014/09/aggregation.png\" alt=\"aggregation\" width=\"85%\" /></a></p>\nIn MLlib 1.1, we introduced a new aggregation communication pattern based on multi-level aggregation trees. In this setup, model updates are combined partially on a small set of executors before they are sent to the driver, which dramatically reduces the load the driver has to deal with. Tests showed that these functions reduce the aggregation time by an order of magnitude, especially on datasets with a large number of partitions.\n<h2>Performance improvements</h2>\nChanging the way models are broadcasted and aggregated has a huge impact on performance. Below, we present empirical results comparing the performance on some of the common machine learning algorithms in MLlib. The x-axis can be thought of the speedup the 1.1 release has over the 1.0 release. Speedups between 1.5-5x can be observed across all algorithms. The tests were performed on an EC2 cluster with 16 slaves, using m3.2xlarge instances. The scripts to run the tests are a part of the \u201cspark-perf\u201d test suite which can be found on <a href=\"https://github.com/databricks/spark-perf\" target=\"_blank\">https://github.com/databricks/spark-perf</a>.\n<p style=\"text-align: center; font-size: 75%;\" align=\"center\"><img class=\"wp-image-1412 size-full\" src=\"https://databricks.com/wp-content/uploads/2014/09/mllib-perf-test.png\" alt=\"mllib-perf-test\" width=\"85%\" />\nFor ridge regression and logistic regression, the Tall identifier corresponds to a tall-skinny matrix (1,000,000 x 10,000) and Fat corresponds to a short-fat matrix (10,000 x 1,000,000).</p>\nPerformance improvements in distributed machine learning typically come from a combination of communication pattern improvements and algorithmic improvements. We focus on the former in this post, and algorithmic improvements will be discussed later. So <a href=\"http://spark.apache.org/downloads.html\" target=\"_blank\">download</a> Spark 1.1 now, enjoy the performance improvements, and stay tuned for future posts."}
{"status": "publish", "description": null, "creator": "arsalan", "link": "https://databricks.com/blog/2014/09/15/application-spotlight-talend.html", "authors": ["Gavin Targonski (Product Management at Talend)"], "id": 1411, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2014-09-15", "tz": "UTC", "createdOn": "2014-09-15"}, "title": "Application Spotlight: Talend", "slug": "application-spotlight-talend", "content": "<div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://www.talend.com\" target=\"_blank\">Talend</a> after having Talend Studio <a href=\"http://www.databricks.com/certification\" target=\"_blank\">\u201cCertified on Apache Spark.\u201d</a></div>\n\n<hr />\n\nAs the move to the next generation of integration platforms grows momentum, the need to implement a proven and scalable technology is critical. Databricks and Apache Spark, delivered on the major Hadoop distributions, is one such area where the delivery of massively scalable technology with low risk implementation is really key.\n\nAt Talend we see a wide array of batch processes, moving to an operational and real time perspective, driven by the consumers of the data. In this vein, the uptake in adoption and the growing community of Apache Spark, the powerful open-source processing engine, has been hard to miss. In a relatively short time, it is now a part of every major Hadoop vendor\u2019s offering, is the most active open source project in the Big Data space, and has been deployed in production across a number of verticals.\n\nTraditional ETL and enterprise integration provides limitations, in both timeliness of extract, speed of load and importantly the supportability of the integration infrastructure itself. Spark delivers a new execution topology, allowing your Big Data platform to deliver much more than just \u2018traditional\u2019 Hadoop MapReduce tasks.\n\nThe power of integrating Spark and Talend Studio is that Talend users will get to immediately harness the power of Spark, all 80+ operations and sophisticated analytics, directly from the familiar and easy to use Talend Studio interface. With Talend and its unique approach to code generation, the code required to load data and execute a query in Spark is managed for you. The designers simply need to identify the data, Talend can then provide the tools to deliver the data at the right time, in the right format and into the desired Hadoop environment.\n\nAdditionally, we\u2019re thrilled to announce \u2013 in conjunction with Databricks - that Talend Studio is now officially \u201cCertified on Spark\u201d. With the certification of Talend 5.5, the interoperability of your integration job created with Talend, and its execution on any Certified Spark Distribution is guaranteed. It also means that as a technology user you can benefit from the power of the platforms without having to maintain your own detailed roadmap of component upgrade, update and continued refactoring of jobs \u2013 Talend manages this for you. More broadly speaking, Talend is also supportive of the open and transparent nature of the certification process, which is designed to maintain compatibility within the Spark ecosystem while simultaneously encouraging innovation at the platform and application layers.\n\nBeyond the technical integration, Talend Labs worked closely with the R&amp;D team, based in Paris, to create an end-to-end scenario to showcase the key features and functions of the integrated Spark solution. This means that users have a fully-functional starting point, available from Talend and proven with Spark, to get you started on your journey.\n\n<img class=\"aligncenter size-full wp-image-62\" style=\"max-width: 100%; display: block; margin: 30px auto 5px auto;\" src=\"https://databricks.com/wp-content/uploads/2014/09/talend_screenshot.png\" alt=\"\" align=\"middle\" />\n<p style=\"margin-bottom: 30px;\" align=\"center\"><em>Figure 1: Talend Studio and Spark</em></p>\nTalend is always evolving its certification in line with its key partners and the Big Data Ecosystem, and Spark is no exception. With such a fast moving project, significant features and improvements are being rolled out rapidly. Talend is committed to supporting Spark and will be moving fast to certify and ensure compatibility with future Spark releases."}
{"status": "publish", "description": null, "creator": "matei", "link": "https://databricks.com/blog/2014/09/17/spark-1-1-bringing-hadoop-inputoutput-formats-to-pyspark.html", "authors": ["Nick Pentreath (Graphflow)", "Kan Zhang (IBM)"], "id": 1431, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2014-09-18", "tz": "UTC", "createdOn": "2014-09-18"}, "title": "Apache Spark 1.1: Bringing Hadoop Input/Output Formats to PySpark", "slug": "spark-1-1-bringing-hadoop-inputoutput-formats-to-pyspark", "content": "<div class=\"post-meta\">This is a guest post by Nick Pentreath of <a href=\"http://graphflow.com\">Graphflow</a> and Kan Zhang of <a href=\"http://ibm.com\">IBM</a>, who contributed Python input/output format support to Apache Spark 1.1.</div>\n\n<hr />\n\nTwo powerful features of Apache Spark include its native APIs provided in Scala, Java and Python, and its compatibility with any Hadoop-based input or output source. This language support means that users can quickly become proficient in the use of Spark even without experience in Scala, and furthermore can leverage the extensive set of third-party libraries available (for example, the many data analysis libraries for Python).\n\nBuilt-in Hadoop support means that Spark can work \"out of the box\" with any data storage system or format that implements Hadoop's <code>InputFormat</code> and <code>OutputFormat</code> interfaces, including HDFS, HBase, Cassandra, Elasticsearch, DynamoDB and many others, as well as various data serialization formats such as SequenceFiles, Parquet, Avro, Thrift and Protocol Buffers.\n\nPreviously, Hadoop InputFormat/OutputFormat support was provided only in Scala or Java. To access such data sources in Python, other than simple text files, users would need to first read the data in Scala or Java, and write it out as a text file for reading again in Python. With the release of <a href=\"http://spark.apache.org/releases/spark-release-1-1-0.html\">Apache Spark 1.1</a>, Python users can now read and write their data directly from and to any Hadoop-compatible data source.\n<h2>An Example: Reading SequenceFiles</h2>\n<a href=\"http://wiki.apache.org/hadoop/SequenceFile\">SequenceFile</a> is the standard binary serialization format for Hadoop. It stores records of <code>Writable</code> key-value pairs, and supports splitting and compression. SequenceFiles are a commonly used format in particular for intermediate data storage in Map/Reduce pipelines, since they are more efficient than text files.\n\nSpark has long supported reading SequenceFiles natively using the <code>sequenceFile</code> method available on a <code>SparkContext</code> instance, which also utilizes Scala features to allow specifying the key and value type in the method call parameters. For example, to read a SequenceFile with <code>Text</code> keys and <code>DoubleWritable</code> values in Scala, we would do the following:\n\n<!--more-->\n\n[scala]val rdd = sc.sequenceFile[String, Double](path)[/scala]\n\nSpark takes care of converting <code>Text</code> to <code>String</code> and <code>DoubleWritable</code> to <code>Double</code> for us automatically.\n\nThe new PySpark API functionality exposes a <code>sequenceFile</code> method on a Python <code>SparkContext</code>instance that works in much the same way, with the key and value types being inferred by default. The <code>saveAsSequenceFile</code> method available on a PySpark <code>RDD</code> allows users to save an <code>RDD</code> of key-value pairs as a SequenceFile. For example, we can create an <code>RDD</code> from a Python collection, save it as a SequenceFile, and read it back using the following code snippet:\n\n<!--more-->\n\n[python]rdd = sc.parallelize([('key1', 1.0), ('key2', 2.0), ('key3', 3.0)])\nrdd.saveAsSequenceFile('/tmp/pysequencefile/')\n...\n\nsc.sequenceFile('/tmp/pysequencefile/').collect()\n[(u'key1', 1.0), (u'key2', 2.0), (u'key3', 3.0)]\n[/python]\n\n<h2>Under the Hood</h2>\nThis feature is built on top of the existing Scala/Java API methods. For it to work in Python, there needs to be a bridge that converts Java objects produced by Hadoop<code>InputFormats</code> to something that can be serialized into pickled Python objects usable by PySpark (and vice versa).\n\nFor this purpose, a <code>Converter</code> trait is introduced, along with a pair of default implementations that handle the standard Hadoop <a href=\"http://hadoop.apache.org/docs/current/api/org/apache/hadoop/io/Writable.html\">Writables</a>.\n<h2>Custom Hadoop Converters</h2>\nWhile the default converters handle the most common Writable types, users need to supply custom converters for custom Writables, or for serialization frameworks that do not produce Writables. To see an illustration of this, some additional converters for <a href=\"https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/pythonconverters/HBaseConverters.scala\">HBase</a> and <a href=\"https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/pythonconverters/CassandraConverters.scala\">Cassandra</a>, together with related <a href=\"https://github.com/apache/spark/tree/master/examples/src/main/python\">PySpark scripts</a>, are included in the Apache Spark 1.1 example sub-project.\n<h2>A More Detailed Example: Custom Converters for Avro</h2>\nFor those who want to dive deeper, we will show how to write more complex custom PySpark converters, using the <a href=\"http://avro.apache.org/docs/current/\">Apache Avro</a> serialization format as an example.\n\nOne thing to consider is what input data the converter will be getting. In our case, we intend to use the converter with <code>AvroKeyInputFormat</code> and the input data will be Avro records wrapped in an AvroKey. Since we want to work with all 3 Avro data mappings (Generic, Specific and Reflect), for each Avro schema type, we need to handle all possible data types produced by those mappings. For example, for Avro <code>BYTES</code> type, both Generic and Specific mappings output <code>java.nio.ByteBuffer</code>, while Reflect mapping outputs <code>Array[Byte]</code>. So our <code>unpackBytes</code> method needs to handle both cases.\n\n<!--more-->\n\n[scala]def unpackBytes(obj: Any): Array[Byte] = {\n  val bytes: Array[Byte] = obj match {\n    case buf: java.nio.ByteBuffer =&amp;gt; buf.array()\n    case arr: Array[Byte] =&amp;gt; arr\n    case other =&amp;gt; throw new SparkException(\n      s&quot;Unknown BYTES type ${other.getClass.getName}&quot;)\n  }\n  val bytearray = new Array[Byte](bytes.length)\n  System.arraycopy(bytes, 0, bytearray, 0, bytes.length)\n  bytearray\n}\n[/scala]\n\nAnother thing to consider is what data types the converter will output, or equivalently, what data types PySpark will see. For example, for the Avro <code>ARRAY</code> type, the Reflect mapping may produce primitive arrays, Object arrays or <code>java.util.Collection</code> depending on its input. We convert all of these to<code>java.util.Collection</code>, which are in turn serialized into instances of a Python <code>List</code>.\n\n<!--more-->\n\n[scala]def unpackArray(obj: Any, schema: Schema): java.util.Collection[Any] = obj match {\n  case c: JCollection[_] =&amp;gt;\n    c.map(fromAvro(_, schema.getElementType))\n  case arr: Array[_] if arr.getClass.getComponentType.isPrimitive =&amp;gt;\n    arr.toSeq\n  case arr: Array[_] =&amp;gt;\n    arr.map(fromAvro(_, schema.getElementType)).toSeq\n  case other =&amp;gt; throw new SparkException(\n    s&quot;Unknown ARRAY type ${other.getClass.getName}&quot;)\n}\n[/scala]\n\nFinally, we need to handle nested data structures. This is done by recursively calling between individual <code>unpack*</code> methods and the central switch <code>fromAvro</code>, which handles the dispatching for all Avro schema types.\n\n<!--more-->\n\n[scala]def fromAvro(obj: Any, schema: Schema): Any = {\n  if (obj == null) {\n    return null\n  }\n  schema.getType match {\n    case UNION   =&gt; unpackUnion(obj, schema)\n    case ARRAY   =&gt; unpackArray(obj, schema)\n    case FIXED   =&gt; unpackFixed(obj, schema)\n    case MAP     =&gt; unpackMap(obj, schema)\n    case BYTES   =&gt; unpackBytes(obj)\n    case RECORD  =&gt; unpackRecord(obj)\n    case STRING  =&gt; obj.toString\n    case ENUM    =&gt; obj.toString\n    case NULL    =&gt; obj\n    case BOOLEAN =&gt; obj\n    case DOUBLE  =&gt; obj\n    case FLOAT   =&gt; obj\n    case INT     =&gt; obj\n    case LONG    =&gt; obj\n    case other   =&gt; throw new SparkException(\n      s&quot;Unknown Avro schema type ${other.getName}&quot;)\n  }\n}\n[/scala]\n\nThe complete source code for <code>AvroWrapperToJavaConverter</code> can be found in the Spark examples, in <a href=\"https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/pythonconverters/AvroConverters.scala\">AvroConverters.scala</a>, while the related PySpark script for using the converter can be found <a href=\"https://github.com/apache/spark/blob/master/examples/src/main/python/avro_inputformat.py\">here</a>.\n<h2>Conclusion and Future Work</h2>\nWe're excited to bring this new feature to PySpark in the <a href=\"http://spark.apache.org/releases/spark-release-1-1-0.html\">1.1 release</a>, and look forward to seeing how users make use of both the built-in functionality, and custom converters.\n\nOne limitation of the current <code>Converter</code> interface is that there is no way to set custom configuration options. A future improvement could be to allow converters to take a Hadoop <a href=\"https://hadoop.apache.org/docs/current/api/org/apache/hadoop/conf/Configuration.html\">Configuration</a> that would allow configuration at runtime."}
{"status": "publish", "description": null, "creator": "vida", "link": "https://databricks.com/blog/2014/09/23/databricks-reference-applications.html", "authors": ["Vida Ha"], "id": 1460, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2014-09-24", "tz": "UTC", "createdOn": "2014-09-24"}, "title": "Databricks Reference Applications", "slug": "databricks-reference-applications", "content": "At Databricks, we are often asked how to go beyond the basic Apache Spark tutorials and start building real applications with Spark. \u00a0As a result, we are developing reference applications <a href=\"http://github.com/databricks/reference-apps\" target=\"_blank\">on github</a> to demonstrate that. \u00a0We believe this is a great way to learn Spark, and we plan on incorporating more features of Spark into the applications over time. \u00a0We also hope to highlight any technologies that are compatible with Spark and include best practices.\n<h3>Log Analyzer Application</h3>\nOur first reference application is log analysis with Spark. \u00a0Logs are a large and common data set that contain a rich set of information. Log data can be used for monitoring web servers, improving business and customer intelligence, building recommendation systems, preventing fraud, and much more. \u00a0Spark is a wonderful tool to use on logs - Spark can process logs faster than Hadoop MapReduce, it is easy to code so we can compute many statistics with ease, and many of Spark\u2019s libraries can be used on log data. \u00a0Also, Spark SQL can be used for querying logs using familiar SQL syntax and Spark Streaming can be used to provide real-time logs analysis.\n\nThe log analysis reference application is broken down into sections with detailed explanations and small code examples to demonstrate various features of Spark. \u00a0We start off with a gentle example of processing historical log data using standalone Spark, then cover how to do the same analysis with Spark SQL, before covering Spark Streaming. \u00a0Along the way, we highlight any caveats or recommended best practices for using Spark - such as how to refactor your code for reuse between the batch and streaming libraries. \u00a0The examples are self-contained and emphasize one aspect of Spark at a time, so you can experiment and modify the examples to deepen your understanding of that topic.\n\n<img src=\"https://databricks.com/wp-content/uploads/2014/09/app_diagram.png\" alt=\"\" width=\"100%\" />\n\nCode from the examples is combined to form a sample end-to-end application as shown in the diagram above. \u00a0For now, the final application is an MVP log analysis streaming application. \u00a0The application monitors a directory for new log input files, and when it receives one - the file is processed. \u00a0\u00a0Spark Streaming is used to compute statistics for the last N time as well as all of time, and refreshed. \u00a0Each time interval, an html file is written which reflects the latest log data in the folder.\n<h3>Twitter Streaming Language Classifier</h3>\nThe second reference application is a popular demo that the Databricks team has given, and was video taped here:\n\n<a href=\"https://www.youtube.com/watch?v=FjhRkfAuU7I#t=2035\"><img src=\"https://databricks.com/wp-content/uploads/2014/09/aaron-yaaay.png\" alt=\"\" width=\"100%\" /></a>\n\nThis application demonstrates the following:\n<ol>\n \t<li>Collecting Twitter data using Spark Streaming.</li>\n \t<li>Examine the tweets using Spark SQL and training a Kmeans clustering model for classifying the language of a tweet using Spark MLLib.</li>\n \t<li>Applying the model in realtime using Spark MLLib and Spark Streaming.</li>\n</ol>\nNow, we are releasing the code so you can run the demo yourself and walk through how it works.\n<h3>Get Started with the Databricks Reference Applications</h3>\nThese applications are just a start though - we will add more and improve our reference applications over time. Please follow these steps to get started:\n<ol>\n \t<li>Read <a href=\"http://databricks.gitbooks.io/databricks-spark-reference-applications/\" target=\"_blank\">the documentation online</a> in book format.</li>\n \t<li>Go to our <a href=\"http://github.com/databricks/reference-apps\" target=\"_blank\">github repo</a> to view the code for the reference applications.</li>\n \t<li>Please open an <a href=\"https://github.com/databricks/reference-apps/issues\" target=\"_blank\">issue on github</a> with any useful feedback about our reference applications.</li>\n</ol>"}
{"status": "publish", "description": null, "creator": "john", "link": "https://databricks.com/blog/2014/09/18/databricks-and-oreilly-media-launch-certification-program-for-apache-spark-developers.html", "authors": ["John Tripier", "Paco Nathan"], "id": 1504, "categories": ["Announcements", "Company Blog"], "dates": {"publishedOn": "2014-09-19", "tz": "UTC", "createdOn": "2014-09-19"}, "title": "Databricks and O'Reilly Media launch Certification Program for Apache Spark Developers", "slug": "databricks-and-oreilly-media-launch-certification-program-for-apache-spark-developers", "content": "When Databricks was initially founded a little more than a year ago, there was tremendous excitement around Apache Spark, but it was still early days.  The project had ~60 contributors over the previous 12 months, and was not yet available commercially.  One of our main focus areas since then has been continuing to grow Spark and the community and making it easily accessible for enterprises and users alike.\n\nTaking a step back, it\u2019s terrific to see the progress that Spark has made since then.  Spark is today the most active open source project in the Big Data ecosystem with over 300 contributors in the last 12 months alone, and is available through several platform vendors, including all of the major Hadoop distributors.  The <a href=\"http://www.spark-summit.org\" target=\"_blank\">Spark Summit</a>, dedicated to bringing together the Spark community, more than doubled in size a short 6 months after the inaugural version, and Spark meetups continue to grow in size, frequency, and cities spanned. \n \nThis momentum has also led to a significant surge in enterprise usage of Spark as organizations move along the adoption curve.  Not only does the number of enterprises leveraging Spark in a PoC or development environment continue to grow rapidly, but a significant number have moved on to large-scale production workloads.  Production uses span a number of verticals, including telecom, financial services, media and entertainment, retail, and the public sector.  Many great examples of these are publicly available, whether in the archives of the most recent Spark Summit, or in blog posts such as the recent one by <a href=\"https://databricks.com/blog/2014/08/14/mining-graph-data-with-spark-at-alibaba-taobao.html\">Alibaba Taobao</a>.  At the same time, a growing number of applications - BI, ETL, and domain-specific - are being ported over to Spark or just being built on top of it from the start.\n\nThe next phase in this evolution, as enterprises increasingly focus on converting data into value, is to build enterprise applications that capture domain-specific business logic on top of Spark.  The challenge is that the pace of Spark adoption has outstripped the pool of Spark experts, creating high demand for developers with Apache Spark expertise.  Essentially, enterprises need people who have demonstrated expertise in how to implement best practices and expand depth of insights through Spark\u2019s combination of sophisticated analytics and blazing speed. \n\nAt Databricks, as the company founded by the creators of Spark and the continued driving force behind the project, we get daily requests soliciting our help from companies who are looking to build increasingly complex and sophisticated solutions on top of their Spark deployments. Unfortunately, we're not a professional services company and we're simply not built for those type of engagements. The typical follow-on question is then around how to identify Spark experts that could help the enterprise with these engagements. \n\nAs a result, we're establishing the <a href=\"http://www.oreilly.com/data/sparkcert.html?cmp=ex-strata-na-lp-na_apache_spark_certification\" target=\"_blank\">Certified Spark Developer program</a>, which provides the industry standard for demonstrating Spark expertise.\n\nThe O\u2019Reilly team was seeing the same demand and so Databricks\u2019 Spark experts and the O\u2019Reilly Media editorial team have partnered to address those needs. This new program  \u2013 consisting of a formal exam and subsequent certification \u2013 establishes the industry standard for measuring and validating technical expertise in Spark. \n\nFor developers, this program allows them to validate their knowledge of Spark and it publicly recognizes their expertise.  For Enterprises, this program provides clear guidelines and the confidence that their teams have the requisite expertise. For professional services companies, this program certifies their capabilities to deliver solutions based on Spark and provides an additional form of validation when communicating with customers.\n\nThe developer certification program by Databricks+O\u2019Reilly augments our efforts to grow the Spark community and enable the ecosystem as a critical component of all successful platforms.  Recent Databricks initiatives have included the <a href=\"https://databricks.com/spark/certification/certified-on-spark\">\u2018Certified on Spark\u2019</a> and <a href=\"https://databricks.com/spark/certification/certified-spark-distribution\">\u2018Certified Spark Distribution\u2019</a> programs designed to ensure compatibility between Spark applications and distributions. There is also a long-running and rapidly expanding training program, and events such as Spark Summit which bring together thousands of members of the Spark community from across the globe.\n\nSpark Developer Certification will be launched formally the upcoming O\u2019Reilly Media conference Strata NY + Hadoop World in New York City, October 15-17. Initial exams will be conducted during the conference. For more information:  <a href=\"http://www.oreilly.com/data/sparkcert.html?cmp=ex-strata-na-lp-na_apache_spark_certification\" target=\"_blank\">http://oreilly.com/go/sparkcert</a>"}
{"status": "publish", "description": null, "creator": "arsalan", "link": "https://databricks.com/blog/2014/09/24/apache-spark-improves-the-economics-of-video-distribution-at-nbc-universal.html", "authors": ["Christopher Burdorf (Senior Software Engineer at NBC Universal)"], "id": 1619, "categories": ["Company Blog", "Customers"], "dates": {"publishedOn": "2014-09-24", "tz": "UTC", "createdOn": "2014-09-24"}, "title": "Apache Spark Improves the Economics of Video Distribution at NBC Universal", "slug": "apache-spark-improves-the-economics-of-video-distribution-at-nbc-universal", "content": "<div class=\"post-meta\">This is a guest blog post from our friends at NBC Universal outlining their Apache Spark use case.</div>\n\n<hr />\n\n<h2>Business Challenge</h2>\nNBC Universal is one of the world\u2019s largest media and entertainment companies with revenues of US$ 26 billion. It operates television networks, cable channels, motion picture and television production companies as well as branded theme parks worldwide. Popular brands include NBC, Universal Pictures, Universal Parks &amp; Resorts, Telemundo, E!, Bravo and MSNBC.\n\nDigital video media clips for NBC Universal\u2019s cable TV programs and commercials are produced and broadcast from its Los Angeles office to cable TV channels in Asia Pacific, Europe, Latin America and the United States. Moreover, viewers increasingly consume NBC Universal\u2019s vast content library online and on-demand.\n\nTherefore, NBC Universal\u2019s IT Infrastructure team needs to make decisions on how best to serve that content, which involves a trade-off between storage and bandwidth cost versus consumer convenience. NBC Universal can keep all content available online and cached at the edge of the network to minimize latency. This way, all of the content could be delivered instantly to consumers across all countries where NBC Universal has a presence. But this would also be the most costly option.\n\nTherefore, the business challenge is to determine the optimal mix between storing the most popular content locally close to its viewers, and serving less popular content only on demand, which incurs higher network costs, or taking it offline altogether.\n<h2>Solution</h2>\nNBC Universal turned to Spark to analyze all the content meta-data for its international content distribution. Metadata associated with the media clips is stored in an Oracle database and in broadcast automation playlists. Apache Spark is used to query the Oracle database and distribute the metadata from the broadcast automation playlists into multiple large in-memory resilient distributed datasets (RDDs). One RDD stores Scala objects containing media IDs, time codes, schedule dates and times, channels for airing etc. It then creates multiple RDDs containing broadcast frequency counts by week, month, and year and uses Spark\u2019s map/reduceByKey to generate the counts. The resulting data is bulk loaded into HBase where it is queried from a Java/Spring web application. The application converts the queried results into graphs illustrating media broadcast frequency counts by week, month, and year on an aggregate and a per channel basis.\n\nA secondary procedure then queries filepath information from Oracle and builds another RDD that contains that information along with the date the file was written. It then computes a Spark join of that RDD and the previous frequency count data RDD to produce a new RDD that contains the usage frequency based on the file age sorted in ascending order. This resulting data is used to generate histograms to help determine if the offlining mechanism is working optimally.\n\nNBC Universal runs Apache Spark in production in conjunction with Mesos, HBase and HDFS and uses Scala as the programming language. The rollout in production happened in Q1 2014 and was smooth.\n\nSpark provides both extremely fast processing times, leveraging its distributed in-memory approach, as well as much better IT staff productivity. In my Spark Summit 2014 talk, I highlighted two aspects of our Spark deployment:\n<ul>\n \t<li><strong>Developer productivity:</strong> The combination of Spark and Scala provides an \u201cideal programming environment\u201d</li>\n \t<li><strong>Operational stability:</strong> Mesos for cluster management.</li>\n</ul>\nAlternative approaches not based on Spark would have required a much more complicated data processing pipeline and workflow. For example, since the main memory usage requires more than what is available on a single server, a much slower procedure would have to be used that would have involved processing the data in chunks, and writing intermediate results to HDFS and then loading the chunks of data back into main memory and processing it in a manner similar to traditional Hadoop map/reduce jobs which has been shown to be much slower than Spark, through various examples on the Apache Spark website.\n<h2>Value Realized</h2>\nSpark provides a fast and easy way to assemble a data pipeline and conduct analyses that drive decisions on which content to keep online versus take off-line. Moreover, infrastructure administrators gain valuable insights into network utilization. They can detect patterns that help them understand wastage of bandwidth in the multi-system operator (MSO) network. The initial results are promising, which prompted NBC to expand its use of Spark to machine learning.\n<h2>To Learn More:</h2>\n<ol>\n \t<li><a href=\"https://www.youtube.com/watch?v=bhunR-Wb7KY&amp;list=PL-x35fyliRwimkfjeg9CEFUSDIE1F7qLS&amp;index=6\" target=\"_blank\">https://www.youtube.com/watch?v=bhunR-Wb7KY&amp;list=PL-x35fyliRwimkfjeg9CEFUSDIE1F7qLS&amp;index=6</a></li>\n \t<li><a href=\"http://spark-summit.org/wp-content/uploads/2014/06/Using-Spark-to-Generate-Analytics-for-International-Cable-TV-Video-Distribution-Christopher-Burdorf.pdf\" target=\"_blank\">http://spark-summit.org/wp-content/uploads/2014/06/Using-Spark-to-Generate-Analytics-for-International-Cable-TV-Video-Distribution-Christopher-Burdorf.pdf</a></li>\n</ol>"}
{"status": "publish", "description": null, "creator": "joseph", "link": "https://databricks.com/blog/2014/09/29/scalable-decision-trees-in-mllib.html", "authors": ["Manish Amde (Origami Logic)", "Joseph Bradley (Databricks)"], "id": 1507, "categories": ["Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2014-09-30", "tz": "UTC", "createdOn": "2014-09-30"}, "title": "Scalable Decision Trees in MLlib", "slug": "scalable-decision-trees-in-mllib", "content": "<div class=\"post-meta\">This is a post written together with one of our friends at <a href=\"http://www.origamilogic.com/\">Origami Logic</a>. Origami Logic provides a Marketing Intelligence Platform that uses Apache Spark for heavy lifting analytics work on the backend.</div>\n\n<hr />\n\nDecision trees and their ensembles are industry workhorses for the machine learning tasks of classification and regression. Decision trees are easy to interpret, handle categorical and continuous features, extend to multi-class classification, do not require feature scaling and are able to capture non-linearities and feature interactions.\n\nDue to their popularity, almost every machine learning library provides an implementation of the decision tree algorithm. However, most are designed for single-machine computation and seldom scale elegantly to a distributed setting. Apache Spark is an ideal platform for a scalable distributed decision tree implementation since Spark's in-memory computing allows us to efficiently perform multiple passes over the training dataset.\n\nAbout a year ago, open-source developers joined forces to come up with a fast distributed decision tree implementation that has been a part of the Spark MLlib library since release 1.0. The Spark community has actively improved the decision tree code since then. This blog post describes the implementation, highlighting some of the important optimizations and presenting test results demonstrating scalability.\n\n<b>New in Spark 1.1</b>: MLlib decision trees now support multiclass classification and include several performance optimizations. There are now APIs for Python, in addition to Scala and Java.\n<h2>Algorithm Background</h2>\nAt a high level, a decision tree model can be thought of as hierarchical if-else statements that test feature values in order to predict a label. An example model for a binary classification task is shown below. It is based upon car mileage data from the 1970s! It predicts the mileage of the vehicle (high/low) based upon the weight (heavy/light) and the horsepower.\n<p align=\"center\"><a href=\"https://databricks.com/wp-content/uploads/2014/09/decision-tree-example.png\"><img class=\"alignnone size-full wp-image-1597\" src=\"https://databricks.com/wp-content/uploads/2014/09/decision-tree-example.png\" alt=\"decision-tree-example\" width=\"503\" height=\"297.5\" /></a></p>\nA model is learned from a training dataset by building a tree top-down. The if-else statements, also known as splitting criteria, are chosen to maximize a notion of information gain --- it reduces the variability of the labels in the underlying (two) child nodes compared the parent node. The learned decision tree model can later be used to predict the labels for new instances.\n\nThese models are interpretable, and they often work well in practice. Trees may also be combined to build even more powerful models, using ensemble tree algorithms. Ensembles of trees such as random forests and boosted trees are often top performers in industry for both classification and regression tasks.\n<h2>Simple API</h2>\nThe example below shows how a decision tree in MLlib can be easily trained using a few lines of code using the new Python API in Spark 1.1. It reads a dataset, trains a decision tree model and then measures the training error of the model. Java and Scala examples can be found in <a href=\"https://spark.apache.org/docs/latest/mllib-decision-tree.html\">the Spark documentation on DecisionTree</a>.\n\n[python]\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.mllib.tree import DecisionTree\nfrom pyspark.mllib.util import MLUtils\n\n# Load and parse the data file into an RDD of LabeledPoint.\n# Cache data since we will use it again to compute training error.\ndata = MLUtils.loadLibSVMFile(sc, 'mydata.txt').cache()\n\n# Train a DecisionTree model.\nmodel = DecisionTree.trainClassifier(\n    data,\n    numClasses=2,\n    categoricalFeaturesInfo={}, # all features are continuous\n    impurity='gini',\n    maxDepth=5,\n    maxBins=32)\n\n# Evaluate model on training instances and compute training error\npredictions = model.predict(data.map(lambda x: x.features))\nlabelsPredictions = data.map(lambda lp: lp.label).zip(predictions)\ntrainErr = labelsPredictions.filter(lambda (v, p): v != p).count() \\\n    / float(data.count())\nprint('Training Error = ' + str(trainErr))\nprint('Learned classification tree model:')\nprint(model)\n[/python]\n\n<h2>Optimized Implementation</h2>\nSpark is an ideal compute platform for a scalable distributed decision tree implementation due to its sophisticated DAG execution engine and in-memory caching for iterative computation. We mention a few key optimizations.\n\n<b>Level-wise training</b>: We select the splits for all nodes at the same level of the tree simultaneously. This level-wise optimization reduces the number of passes over the dataset exponentially: we make one pass for each level, rather than one pass for each node in the tree. It leads to significant savings in I/O, computation and communication.\n\n<b>Approximate quantiles</b>: Single machine implementations typically use sorted unique feature values for continuous features as split candidates for the best split calculation. However, finding sorted unique values is an expensive operation over a distributed dataset. The MLlib decision tree uses quantiles for each feature as split candidates. It's a standard tradeoff for improving decision tree performance without significant loss of accuracy.\n\n<b>Avoiding the map operation</b>: The early prototype implementations of the decision tree used both map and reduce operations when selecting best splits for tree nodes. The current code uses significantly less computation and communication by exploiting the known structure of the pre-computed split candidates to avoid the map step.\n\n<b>Bin-wise computation</b>: The best split computation discretizes features into bins, and those bins are used for computing sufficient statistics for splitting. We precompute the binned representations of each instance, saving computation on each iteration.\n<h2>Scalability</h2>\nWe demonstrate the scalability of MLlib decision trees with empirical results on various datasets and cluster sizes.\n<h4>Scaling with dataset size</h4>\nThe two figures below show the training times of decision trees as we scale the number of instances and features in the dataset. The training times increased linearly, highlighting the scalability of the implementation.\n<p align=\"center\"><a href=\"https://databricks.com/wp-content/uploads/2014/09/DT-scaling-instances.png\"><img class=\"alignnone size-full wp-image-1519\" src=\"https://databricks.com/wp-content/uploads/2014/09/DT-scaling-instances.png\" alt=\"DT-scaling-instances\" width=\"394.5\" height=\"251\" /></a></p>\n<p align=\"center\"><a href=\"https://databricks.com/wp-content/uploads/2014/09/DT-scaling-features.png\"><img class=\"alignnone size-full wp-image-1518\" src=\"https://databricks.com/wp-content/uploads/2014/09/DT-scaling-features.png\" alt=\"DT-scaling-features\" width=\"386\" height=\"248.5\" /></a></p>\nThese tests were run on an EC2 cluster with a master node and 15 worker nodes, using r3.2xlarge instances (8 virtual CPUs, 61 GB memory). The trees were built out to 6 levels, and the datasets were generated by the <a href=\"https://github.com/databricks/spark-perf\">spark-perf library</a>.\n<h4>Spark 1.1 speedups</h4>\nThe next two figures show improvements in Apache Spark 1.1, relative to the original Apache Spark 1.0 implementation. On the same datasets and cluster, the new implementation is 4-5X faster on many datasets!\n<p align=\"center\"><a href=\"https://databricks.com/wp-content/uploads/2014/09/DT-speedups-instances.png\"><img class=\"alignnone size-full wp-image-1606\" src=\"https://databricks.com/wp-content/uploads/2014/09/DT-speedups-instances.png\" alt=\"DT-speedups-instances\" width=\"358\" height=\"248.5\" /></a></p>\n<p align=\"center\"><a href=\"https://databricks.com/wp-content/uploads/2014/09/DT-speedups-features.png\"><img class=\"alignnone size-full wp-image-1607\" src=\"https://databricks.com/wp-content/uploads/2014/09/DT-speedups-features.png\" alt=\"DT-speedups-features\" width=\"356\" height=\"248.5\" /></a></p>\n\n<h2>What\u2019s Next?</h2>\nThe tree-based algorithm development beyond release 1.1 will focus primarily on ensemble algorithms such as random forests and boosting. We will also keep optimizing the decision tree code for performance and plan to add support for more options in the upcoming releases.\n\nTo get started using decision trees yourself, <a href=\"http://spark.apache.org/\">download Spark 1.1 today</a>!\n<h2>Further Reading</h2>\n<ul>\n \t<li>See examples and the API in <a href=\"https://spark.apache.org/docs/latest/mllib-decision-tree.html\">the MLlib decision tree documentation</a>.</li>\n \t<li>Watch <a href=\"http://spark-summit.org/2014/talk/scalable-distributed-decision-trees-in-spark-mllib\">the decision tree presentation</a> from the 2014 Spark Summit.</li>\n \t<li>Check out <a href=\"http://functional.tv/post/98342564544/sfscala-sfbaml-joseph-bradley-decision-trees-on-spark\">video</a> and <a href=\"https://speakerdeck.com/jkbradley/mllib-decision-trees-at-sf-scala-baml-meetup\">slides</a> from another talk on decision trees at a Sept. 2014 SF Scala/Bay Area Machine Learning meetup.</li>\n</ul>\n&nbsp;\n<h3>Acknowledgements</h3>\nThe Spark MLlib decision tree work was initially performed jointly with Hirakendu Das (Yahoo Labs), Evan Sparks (UC Berkeley AMPLab), and Ameet Talwalkar and Xiangrui Meng (Databricks). More contributors have joined since then, and we welcome your input too!"}
{"status": "publish", "description": null, "creator": "arsalan", "link": "https://databricks.com/blog/2014/09/25/guavus-embeds-apache-spark-into-its-operational-intelligence-platform-deployed-at-the-worlds-largest-telcos.html", "authors": ["Eric Carr (VP Core Systems Group at Guavus)"], "id": 1626, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2014-09-25", "tz": "UTC", "createdOn": "2014-09-25"}, "title": "Guavus Embeds Apache Spark into its Operational Intelligence Platform Deployed at the World\u2019s Largest Telcos", "slug": "guavus-embeds-apache-spark-into-its-operational-intelligence-platform-deployed-at-the-worlds-largest-telcos", "content": "<div class=\"post-meta\">This is a guest blog post from our friends at <a href=\"http://www.guavus.com\" target=\"_blank\">Guavus</a> - now a Certified Apache Spark Distribution - outlining how they leverage Spark to deliver value to telecom companies.</div>\n\n<hr />\n\n<h2>Business Challenge</h2>\nGuavus is a leading provider of big data analytics solutions for the Communications Service Provider (CSP) industry. The company counts 4 of the top 5 mobile network operators, 3 of the top 5 Internet backbone providers, as well as 80% of cable MSOs in North America as customers. The Guavus Reflex platform provides operational intelligence to these service providers. Reflex currently analyzes more than 50% of all US mobile data traffic and processes more than 2.5 petabytes of data per day.\n\nYet that data grows at an exponential rate. Ever increasing data volume and velocity makes it harder to generate timely insights. For instance, one operational issue can quickly cascade into multiple issues down-stream in the network, which makes it critical to go from insight to action in a very short time frame.\n\nMany service providers have deployed data lakes to explore more of their information. These hold data across all time periods and are well suited to analytic jobs that take several minutes to complete at best. Yet a much more urgent and important challenge is to enable decision making on incoming streams of data in real time. Therefore, Guavus needed an ability to produce meaningful, real-time insights on just the last seconds of data, correlating data sources across network equipment, end customer devices, subscriber and billing systems. A particular challenge for the first generation of the Reflex Platform was to filter specific events from large amounts of incoming data, which is necessary in order to achieve the goal of generating real-time operational intelligence.\n<h2>Solution</h2>\nSince the company\u2019s launch in February 2006, Guavus has been developing big data analytics solutions based on Hadoop and Map Reduce technologies. As customers\u2019 requirements have become more time sensitive and the technologies have matured, Guavus looked to evolve its solutions from batch-processing (\n\nGuavus\u2019 second product generation Reflex 2.0\u2122 was unveiled at Spark Summit and is built on Apache Spark and Hadoop YARN. Thanks to the capabilities of Apache Spark, the latest version of Reflex expands beyond batch processing to enable truly real-time continuous analysis at scale from multiple sources including data from billing systems, CRM, OSS, networks, applications, devices and clouds. Reflex 2.0 continuously correlates, fuses and analyzes these data streams with data at rest to provide communications service providers with a 360-degree view of what\u2019s going on in their network.\n\nSpark\u2019s capabilities for iterative processing, filtering and enrichment of event stream data made it ideal for use with existing event filtering algorithms. Developers and end users embraced the new platform because of its ease of use and high-level abstractions. These included Spark machine learning libraries but also a distributed SQL query engine for Hadoop data.\n\nAs part of the company\u2019s commitment to the open source community and strong belief in Apache Spark\u2019s capabilities for streaming analytics, the Guauvs Reflex 2.0 platform is now also a Certified Spark Distribution. Certification is significant, as it will allow Guavus to innovate even faster and enhance the Reflex platform with greater flexibility, while ensuring compatibility with the latest standards and support for the growing ecosystem around Spark.\n\nMoreover, Guavus contributes back to the open source community in the area of real-time event processing. When conducting operational analyses on network data, it is important to be able to distinguish between the actual time the event happened, and the time stamp when the event was received by the system. Guavus\u2019 team has evolved the D-streams (discretized streams) feature to a concept it calls bin-streams, which better addresses this common service provider use case. This represents Guavus\u2019 first contribution back into the Spark community.\n<h2>Value Realized</h2>\nSpark enables an \u201canalyze first\u201d approach that allows service providers to reason on the data as it arrives, versus the traditional \u201cstore first ask questions later\u201d approach of Hadoop. This real-time analytic capability is disproportionately valuable, since it allows the service provider to take action while the shopper is in the store, while the customer is on the line with the call center agent, or while fraudulent transactions are in progress.\n\nThe metrics Guavus shared at Spark Summit are certainly impressive. Reflex 2.0 processes over 2.5 petabytes of data per day, which equals 250 billion records per day, and 2.5 million transactions per second. Guavus has observed a 3 to 5x performance improvement for its Reflex 2.0 product versus its 1.x product generation, while drastically reducing the hardware footprint required.\n\nBy analyzing data as it arrives within milliseconds of when it hits the network, Guavus customers can trigger immediate actions and improve decision-making.\n\nGuavus has built a data layer that sits on top of the Reflex platform that transforms, aggregates and correlates data from multiple sources, including streaming and stored data, and applies machine learning algorithms to then feed the data into an analytic application. The data layer works in conjunction with the application to deliver rapid time to value, in some cases reducing development time by as much as 12 months. The data layer can also feed into third party applications and into data lakes for maximum extensibility. And by creating an abstraction layer that manages the data complexity and optimizes the processing for Extract Transform Load (ETL) and Enterprise Data Warehouse (EDW) at the edge vs. in a central repository, Guavus fundamentally changes the economics of analyzing the data.\n\nFor example, a Tier 1 US Multiple System Operator (MSO) leverages the Guavus Reflex platform to correlate call center events and network streaming events to detect anomalies and identify the root cause for timely resolution. Based on these insights, the MSO was able to make adjustments in the moment to improve the customer experience. The CareReflex application allowed the MSO to discriminate between device and network related issues using one-click root-cause analysis. From there, the Interactive Voice Response (IVR) could be deflected and the customer service agent script modified accordingly. In addition, field operations were dispatched to repair network faulty equipment vs. customer premise equipment saving the MSO millions in unnecessary truck rolls and improving mean time to resolution (MTTR) for customer call agents. The MSO estimates that this single application will result in $50 million in savings annually.\n\nOther examples of how CSPs can use Guavus Reflex 2.0 operational intelligence platform include:\n<ul>\n \t<li>Develop solutions that can be embedded into workflows and business processes to improve CAPEX/OPEX efficiencies</li>\n \t<li>Identify and prevent fraudulent activity in the network as it happens</li>\n \t<li>Create highly targeted, personalized marketing mobile ad campaigns based on subscriber activities</li>\n</ul>\n<h2>To Learn More:</h2>\n<ol>\n \t<li><a href=\"http://spark-summit.org/wp-content/uploads/2014/07/Building-Big-Data-Operational-Intelligence-Platform-with-Apache-Spark-Eric-Carr.pdf\" target=\"_blank\">http://spark-summit.org/wp-content/uploads/2014/07/Building-Big-Data-Operational-Intelligence-Platform-with-Apache-Spark-Eric-Carr.pdf</a></li>\n \t<li><a href=\"https://www.youtube.com/watch?v=qMD4XIOtgh0&amp;index=3&amp;list=PL-x35fyliRwhD4eFBjkHpYyZjTWW9qZDu\" target=\"_blank\">https://www.youtube.com/watch?v=qMD4XIOtgh0&amp;index=3&amp;list=PL-x35fyliRwhD4eFBjkHpYyZjTWW9qZDu</a></li>\n</ol>"}
{"status": "publish", "description": null, "creator": "arsalan", "link": "https://databricks.com/blog/2014/10/01/spark-as-a-platform-for-large-scale-neuroscience.html", "authors": ["Jeremy Freeman (Freeman Lab)"], "id": 1648, "categories": ["Apache Spark", "Engineering Blog", "Streaming"], "dates": {"publishedOn": "2014-10-01", "tz": "UTC", "createdOn": "2014-10-01"}, "title": "Apache Spark as a platform for large-scale neuroscience", "slug": "spark-as-a-platform-for-large-scale-neuroscience", "content": "The brain is the most complicated organ of the body, and probably one of the most complicated structures in the universe. It\u2019s millions of neurons somehow work together to endow organisms with the extraordinary ability to interact with the world around them. Things our brains control effortlessly -- kicking a ball, or reading and understanding this sentence -- have proven extremely hard to implement in a machine.\n\nFor a long time, our efforts were limited by experimental technology. Despite the brain having many neurons, most technologies could only monitor the activity of one, or a handful, at once. That these approaches taught us so much -- for example, that there are neurons that respond only when you look at a particular object -- is a testament to experimental ingenuity.\n\nIn the next era, however, we will be limited not by our recordings, but our ability to make sense of the data. New technologies make it possible to monitor the activity of many thousands of neurons at once -- from a small region of the mouse brain, or from the entire brain of the larval zebrafish. These advances in recording come with dramatic increases in data size. Several years ago, a large neural data set might have been a few GB, amassed across months or years. Today, monitoring the entire zebrafish brain can yield several TBs in an hour (see <a href=\"http://thefreemanlab.com/pdf/vladimirov-2014-nature-methods.pdf\" target=\"_blank\">this paper</a> for a description of recent experimental technology).\n\n<iframe src=\"//www.youtube.com/embed/YLVdRPVj-XM\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n<p style=\"margin-bottom: 30px;\" align=\"center\"><em>Whole-brain activity measured in a larval zebrafish while simultaneously presenting a moving visual stimulus (upper left) and monitoring intended swimming behavior (size of circle)</em></p>\nThere are many challenges to analyzing neural data. The measurements are indirect, and useful signals must be extracted and transformed, in a manner tailored to each experimental technique -- our version of ETL. At a higher-level, our analyses must find patterns of biological interest from the sea of data. An analysis is only as good as the experiment it motivates; the faster we can explore data, the sooner we can generate a hypothesis and move research forward.\n\nIn the past, neuroscience analysis has largely relied on single-workstation solutions. In the future, it will increasingly rely on some form of distributed computing, and we think Spark is the ideal platform. This post explains why.\n<h2>Why Apache Spark?</h2>\nThe first challenge in bringing distributed computing to a community not currently using it is deployment. Apache Spark can run out-of-the-box on Amazon\u2019s EC2, which immediately opens it up to a wide community. Although the cloud has many advantages, several universities and research institutes have existing high-performance computing clusters. We were pleased and surprised by how straightforward it was to integrate Spark into our own cluster, which runs the Univa Grid Engine. We used the Spark standalone scripts, alongside the existing UGE scheduler, to enable our users to launch their own private Spark cluster with a pre-specified number of nodes. We did not need to setup Hadoop, because Spark can natively load data from our networked file system. Our infrastructure is common to many academic and research institutions, and we hope it will be easy to replicate our approach elsewhere.\n\nOne of the reasons neural data analysis is so challenging -- and so fascinating -- is that little of what we do is standardized. Unlike, say, trying to maximize accuracy of user recommendations, or performance of a classifier, we are trying to maximize our understanding. To be sure, there are families of workflows, analyses, and algorithms that we use regularly, but it\u2019s just a toolbox, and a constantly evolving one. To understand data, we must try many analyses, look at the results, modify at many levels -- whether adjusting preprocessing parameters, or developing an entirely new algorithm -- and inspect the results again.\n\n<img class=\"aligncenter size-full wp-image-62\" style=\"max-width: 100%; display: block; margin: 30px auto 5px auto;\" src=\"https://databricks.com/wp-content/uploads/2014/10/databricks-blog-post-graphic-01.png\" alt=\"\" align=\"middle\" />\n<p style=\"margin-bottom: 30px;\" align=\"center\"><em>Examples of analyses performed using Spark, including basic processing of activity patterns (left), matrix factorization to characterize functionally similar regions (as depicted by different colors) (middle), and embedding dynamics of whole-brain activity into lower-dimensional trajectories (right)</em></p>\nThe ability to cache a large data set in RAM and repeatedly query it with multiple analyses is critical for this exploratory process, and is a key advantage of Spark compared to conventional MapReduce systems. Any data scientist knows there is a \u201cworking memory for analysis\u201d: if you need to wait more than a few minutes for a result, you forget what you were doing. If you need to wait overnight, you\u2019re lost. With Spark, especially once data is cached, we can get answers to new queries in seconds or minutes, instead of hours or days. For exploratory analysis, this is a game changer.\n<h2>Why PySpark?</h2>\nSpark offers elegant and powerful APIs in Scala, Java, and Python. We are developing a library for neural data analysis, called <a href=\"http://thunder-project.org/\" target=\"_blank\">Thunder</a>, largely in the Python API (PySpark), which for us offers several unique advantages. (A paper describing this library and its applications, in collaboration with the lab of Misha Ahrens, and featuring Spark developer Josh Rosen as a co-author, was recently published in Nature Methods, and is available <a href=\"http://thefreemanlab.com/pdf/freeman-2014-nature-methods.pdf\" target=\"_blank\">here</a>.)\n\n1) <strong>Ease of use</strong>. Although Scala has many advantages (and I personally prefer it), for most users Python is an easier language to learn and use for developing analyses. Due to its scientific computing libraries (see below), Python adoption is increasing in neuroscience and other scientific fields. And in some cases, users with existing analyses can bring them into Spark straightaway. To consider a simple example, a common workflow is to fit individual models to thousands or millions of neurons or neural signals independently. We can easily express this embarrassingly-parallel problem in PySpark (see an <a href=\"http://research.janelia.org/zebrafish/tuning.html\" target=\"_blank\">example</a>). If a new user wants to do the same analysis with their own, existing model-fitting routine, already written in Python and vetted on smaller scale data, it would be plug-and-play. More exciting still, working in Spark means they can use the exact same platform to try more complex distributed operations: for example, take the parameters from those independently fitted models and perform clustering or dimensionality reduction -- all in Python.\n\n2) <strong>Powerful libraries</strong>. With libraries like NumPy, SciPy, and scikit-learn, Python has become a powerful platform for scientific computing. When using PySpark, we can easily leverage these libraries for components of our analyses, including signal processing (e.g. fourier transforms), linear algebra, optimization, statistical computations, and more. Spark itself offers, through its MLlib library, many high-performance distributed implementations of machine learning algorithms. These implementations nicely complement the analyses we are developing. In so far as there is overlap, and our analyses are sufficiently general-purpose, we are either using -- or are in the process of contributing to -- analyses in MLLib. But much of what we do, and how we implement it, is specific to our problems and data types (e.g. images and time series). With Thunder, we hope to provide an example of how an external library can thrive on top of Spark (see also the <a href=\"http://bdgenomics.org/\" target=\"_blank\">ADAM</a> library for genomic analysis from the AmpLab).\n\n3) <strong>Visualization</strong>. Especially for exploratory data analysis, the ability to to visualize intermediate results is critical, and often these visualizations must be tailored and tweaked depending on the data. Again, the combination of Spark and Python offers many advantages. Python has a core plotting library, matplotlib, and new libraries are improving its aesthetics and capabilities (e.g. <a href=\"http://mpld3.github.io/\" target=\"_blank\">mpld3</a>, <a href=\"https://github.com/mwaskom/seaborn\" target=\"_blank\">seaborn</a>). In an iPython notebook, we can perform analyses with Spark and visually inspect results (see an <a href=\"http://nbviewer.ipython.org/url/research.janelia.org/zebrafish/notebooks/optomotor-response-PCA.ipynb\" target=\"_blank\">example</a>). We are developing workflows in which custom visualizations are tightly integrated into each of our analyses, and consider this crucial as our analyses become increasingly complex.\n<h2>The future: Spark Streaming</h2>\nSpark has already massively sped up our post-hoc data processing and analysis. But what if we want an answer during an experiment? We are beginning to use Spark Streaming for real-time analysis and visualization of neural data. Because Spark Streaming is built in to the Spark ecosystem, rather than an independent platform, we can leverage a common code-base, and also the same deployment and installation. Streaming analyses will let us adapt our experiments on the fly. And as technology marches ever forward, we may soon collect data so large and so fast that we couldn\u2019t store the complete data sets even if we wanted to. By analyzing more and more online, and storing less, Spark Streaming may give us a solution.\n<h2>Conclusion</h2>\nWe are at the beginning of an exciting moment in large-scale neuroscience. We think Spark will be core to our analytics, but significant challenges lie ahead. Given the scale and complexity of our problems, different research groups must work together to unify analysis efforts, vet alternative approaches, and share data and code. We believe that any such effort must be open-source through-and-through, and we are fully committed to building open-source solutions. We also need to work alongside the broader data science and machine learning community to develop new analytical approaches, which could in turn benefit communities far beyond neuroscience. Understanding the brain will require all of our biological and analytical creativity -- and we just might help revolutionize data science in the process."}
{"status": "publish", "description": null, "creator": "arsalan", "link": "https://databricks.com/blog/2014/10/07/sharethrough-uses-spark-streaming-to-optimize-advertisers-return-on-marketing-investment.html", "authors": ["Russell Cardullo (Sharethrough)"], "id": 1668, "categories": ["Company Blog", "Customers"], "dates": {"publishedOn": "2014-10-07", "tz": "UTC", "createdOn": "2014-10-07"}, "title": "Sharethrough Uses Apache Spark Streaming to Optimize Advertisers' Return on Marketing Investment", "slug": "sharethrough-uses-spark-streaming-to-optimize-advertisers-return-on-marketing-investment", "content": "<div class=\"post-meta\">This is a guest blog post from our friends at <a href=\"http://www.sharethrough.com\" target=\"_blank\">Sharethrough</a> providing an update on how their use of Apache Spark has continued to expand.</div>\n\n<hr />\n\n<h2>Business Challenge</h2>\nSharethrough is an advertising technology company that provides native, in-feed advertising software to publishers and advertisers. Native, in-feed ads are designed to match the form and function of the sites they live on, which is particularly important on mobile devices where interruptive advertising is less effective. For publishers, in-feed monetization has become a major revenue stream for their mobile sites and applications. For advertisers, in-feed ads have been proven to drive more brand lift than interruptive banner advertisements.\n\nSharethrough\u2019s publisher and advertiser technology suite is capable of optimizing the format of an advertisement for seamless placement on content publishers websites and apps. This involves four major steps: Targeting the right users, optimizing the creative content for the target, scoring the content quality, and allowing the advertiser to bid on the actual ad placement in a real-time auction.\n\nAll four steps are necessary to optimize an advertiser\u2019s return on marketing investment. But this requires real-time capabilities in the following three areas:\n<ul>\n \t<li><strong>Creative optimization:</strong> Choosing the best performing content variant from a seemingly infinite number of variations in thumbnails, headlines, descriptions etc.</li>\n \t<li><strong>Spend tracking:</strong> Advertisers expect automatic adjustment (\u201cprogrammatic\u201d in advertising parlance) to real-time bidding algorithms to achieve their campaign goals given their parameters and budget. A required feature of Sharethrough\u2019s platform is to provide real-time adjustments into how content engagement consumes an advertising budget.</li>\n \t<li><strong>Operational monitoring:</strong> When expected behaviour falls outside of positive or negative norms (e.g. traffic spikes during the Oscars or lowered spend), these need to be understood and addressed in a timely manner to answer the question - \u201cIs this event expected and/or acceptable?\u201d</li>\n</ul>\nBetter creative content and optimal placement translate into better consumer engagement and higher conversion, but Sharethrough needs to measure the business impact of these optimizations in real time.\n<h2>Technology Challenge</h2>\nThe technology that Sharethrough was using prior to Spark was not able to accommodate the short feedback cycles required to meet these three objectives.\n\nAfter migrating from Amazon Elastic MapReduce in 2010, we deployed the Cloudera Distribution of Hadoop on Amazon Web Services, primarily for batch use cases such as Extract, Transform and Load (ETL). These batch runs are used for intermittent performance reporting and billing throughout the day, with delays on the order of hours, not minutes. After the launch of our new platform in 2013, it became apparent that Hadoop was not well suited to serve Sharethrough\u2019s increasingly real-time needs.\n\nSharethrough\u2019s data processing pipeline relies on Apache Flume to write web server log data into HDFS in 64MB increments based on the default HDFS block size. Sharethrough runs a set of MapReduce jobs at periodic intervals with the resulting output written to a data warehouse using Sqoop.\n\nThis setup generated insights with a delay of more than one hour. Sharethrough was unable to update the models sooner than these existing batch workflows allowed. This meant that advertisers could not be sure that they had optimized the return on their content investment because any decisions were taken on data that was a few hours old.\n<h2>Solution</h2>\nIn the middle of 2013, we turned to Apache Spark and in particular Spark Streaming because we needed a system to process click stream data in real time.\n\nIn my Spark Summit 2014 <a href=\"https://www.youtube.com/watch?v=0QXzKqMPgWQ&amp;index=2&amp;list=PL-x35fyliRwimkfjeg9CEFUSDIE1F7qLS\" target=\"_blank\">talk</a>, I highlighted the reasons for choosing Spark:\n\n\u201cWe found Spark Streaming to be a perfect fit for us because of its easy integration into the Hadoop ecosystem, powerful functional programming API, low-friction interoperability with our existing batch workflows and broad ecosystem endorsement.\u201d\n\nSpark is compatible with our existing investments in Hadoop. This means that existing HDFS files can be used as an input for Spark computations, and Spark can use HDFS for persistent storage.\n\nAt the same time, Spark makes it easy for developers lacking an understanding of the various elements of Hadoop to become productive. While Spark integrates with Hadoop, it does not require knowledge of HDFS, MapReduce or the various Hadoop processing engines. At Sharethrough, much of the backend code is written using Scala, and therefore blends into Spark very naturally since Spark supports the Scala APIs.\n\nThis allows our developers to work at the level of the actual business logic and data pipeline that specify what has to happen. Spark then figures out how this has to happen, coordinating lower-level tasks such as data movement and recovery. The resulting code is quite concise due to the Spark API.\n\nWe\u2019re using Spark for streaming now but the opportunity to also use Spark for batch processing is really appealing to us. Spark provides a unified development environment and runtime across both batch and real-time workloads, allowing reusability between batch and streaming jobs. It also makes it much easier to combine arriving real-time data with historical data in one analysis.\n\nFinally, the community support available with Spark is quite helpful, from mailing lists and an ecosystem of code contributors all the way to companies like Cloudera, MapR and Datastax that offer professional support.\n<h2>Deployment in Detail</h2>\nSharethrough runs Spark on 10 AWS m1.xlarge nodes, ingests 200 GB per day and is using Mesos for cluster management.\n\nFollowing the principles of the Lambda architecture, Sharethrough uses Hadoop for the batch layer of its architecture, what we call the \u201ccold path\u201d, and Spark for the \u201chot path\u201d real-time layer.\n\nIn the hot path, Flume writes out all the clickstream data to RabbitMQ. Next, Spark reads from RabbitMQ at a (configurable) batch size of five seconds. The resulting output updates the predictive models that run our business. The end-to-end process completes within a few seconds, including the Spark processing time and the time taken by Flume to transmit the data to RabbitMQ.\n\nBecause of API consistency, our engineers design and test locally in a simple batch mode and then run the same job in production using streaming mode. This enables the system to achieve the desired optimization required for real-time bidding.\n\nGoing forward, we aim to simplify the upstream components of our data pipeline using Amazon Kinesis. Kinesis would supplant existing queueing systems like RabbitMQ by connecting to all sources such as web servers, machine logs or mobile devices. Kinesis would then form the central hub from which all applications including Spark can pull data. Spark support for Kinesis was added as part of the recent Spark 1.1 release in September 2014.\n<h2>Value Realized</h2>\nSpark delivers on our business objectives of improving creative optimization, spend tracking and operational monitoring. Spark makes it easier to deliver ads on budget, which is particularly critical for campaigns that may only run for a few days. Spend can be tracked and operational issues adjusted in real-time. For instance, if Sharethrough releases code that does not render well on some third-party sites, this can now be detected and fixed immediately.\n\nBut Spark also creates value for our technical team. Engineers can conduct and learn from real-time experiments much more quickly than before. Code re-use and testing is another significant benefit. Because of the higher abstraction level and unified programming model of Spark, Sharethrough can much more easily reuse the code from one job to create another job in a modular fashion just by replacing a few lines of code. This results in much cleaner looking code, which is easier to debug, test, reuse and maintain. Furthermore we can use a single analytics cluster to provide both real-time stream processing as well as batch analytic workflows without the operational and resource overhead of supporting two different clusters with different latency requirements.\n<h2>To Learn More:</h2>\n<ol>\n \t<li><a href=\"https://www.youtube.com/watch?v=0QXzKqMPgWQ&amp;index=2&amp;list=PL-x35fyliRwimkfjeg9CEFUSDIE1F7qLS\" target=\"_blank\">https://www.youtube.com/watch?v=0QXzKqMPgWQ&amp;index=2&amp;list=PL-x35fyliRwimkfjeg9CEFUSDIE1F7qLS</a></li>\n \t<li><a href=\"http://spark-summit.org/wp-content/uploads/2014/07/Spark-Streaming-for-Realtime-Auctions-Russell-Cardullo.pdf\" target=\"_blank\">http://spark-summit.org/wp-content/uploads/2014/07/Spark-Streaming-for-Realtime-Auctions-Russell-Cardullo.pdf</a></li>\n</ol>"}
{"status": "publish", "description": null, "creator": "arsalan", "link": "https://databricks.com/blog/2014/10/09/application-spotlight-trifacta.html", "authors": ["Sean Kandel (CTO at Trifacta)"], "id": 1678, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2014-10-09", "tz": "UTC", "createdOn": "2014-10-09"}, "title": "Application Spotlight: Trifacta", "slug": "application-spotlight-trifacta", "content": "<div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://www.trifacta.com\" target=\"_blank\">Trifacta</a> after having their data transformation platform <a href=\"http://www.databricks.com/certification\" target=\"_blank\">\u201cCertified on Spark.\u201d</a></div>\n\n<hr>\n\nToday we announced v2 of the Trifacta Data Transformation Platform, a release that emphasizes the important role that Hadoop plays in the new big data enterprise architecture. With Trifacta v2 we now support transforming data of all shapes and sizes in Hadoop. This means supporting Hadoop-specific data formats as both inputs and outputs in Trifacta v2 - data formats such as Avro, ORC and Parquet. It also means intelligently executing data transformation scripts through not only MapReduce, which was available in Trifacta v1, but also Spark. Trifacta v2 has been officially Certified on Spark by Databricks.\n\nOur partnership with Databricks brings the performance and flexibility of the Spark data processing engine to the world of data wrangling. It has been a pleasure to work with the original creators of Spark and to introduce a new category of applications to the Spark community. Our inspiration to integrate with Spark was in part the sheer power of the technology. But it was also prompted by the tremendous momentum of the open source Apache Spark community and project.  We\u2019ve seen a growing number of technology and Fortune 500 companies select Spark as a critical component of their investments in Hadoop implementations.\n\nWith support now from all of the major Hadoop distributions, including Cloudera, Hortonworks, MapR and Pivotal, Spark is certainly here to stay as a foundational component of the Hadoop ecosystem. And having tested Spark against many different data transformation use cases, we now know why. With Spark under the hood of Trifacta, we can now execute large-scale data transformations at interactive response rates. This capability complements the execution frameworks that we introduced in Trifacta v1, where we supported instant and batch execution. In Trifacta v1, we could either execute over small data instantaneously in the browser or we could operate over large volumes in batch mode by compiling transformation scripts to execute in MapReduce. Now in Trifacta v2, we can intelligently select between in-browser, Spark and MapReduce execution for the user, so that our customers can focus on analysis instead of technical details.\n\nWe leverage Spark\u2019s flexible execution model to drive low-latency processing for a variety of data transformation workloads. For instance, Spark is suitable for interactive data structuring and cleaning transformations, iterative machine learning routines that power Trifacta\u2019s Predictive InteractionTM technology and efficient analytic queries for Visual Data Profiling.\n\nWhat made it relatively easy to plug native Spark execution into our Data Transformation Platform was Trifacta\u2019s declarative transformation language, Wrangle. Wrangle is a data transformation language that is designed to translate visual interactions that users have in Trifacta into natively executable code that can run on variety of different processing frameworks including MapReduce and Spark.\n\nWith the immense technical talent of both organizations, I am looking forward to seeing what we\u2019re able to build together and the impact it will have on big data. If you\u2019re interested in learning more about Wrangle, Trifacta\u2019s Domain-Specific Language or how our architecture makes it easy to plug into multiple data processing frameworks, I\u2019ll be speaking next week with Joe Hellerstein at Strata New York on the <a href=\"http://strataconf.com/stratany2014/public/schedule/detail/36612\" target=\"_blank\">topic</a> or stay tuned to the <a href=\"http://www.trifacta.com/product/platform/\" target=\"_blank\">product page</a> on our website."}
{"status": "publish", "description": null, "creator": "rxin", "link": "https://databricks.com/blog/2014/10/10/spark-petabyte-sort.html", "authors": ["Reynold Xin"], "id": 1685, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2014-10-10", "tz": "UTC", "createdOn": "2014-10-10"}, "title": "Apache Spark the fastest open source engine for sorting a petabyte", "slug": "spark-petabyte-sort", "content": "<strong>Update November 5, 2014</strong>: Our benchmark entry has been reviewed by the benchmark committee and Apache Spark has won the <a href=\"http://sortbenchmark.org/\">Daytona GraySort contest</a> for 2014! Please see this <a href=\"https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html\">new blog post for update</a>.\n\nApache Spark has seen phenomenal adoption, being widely slated as the successor to Hadoop MapReduce, and being deployed in clusters from a handful to thousands of nodes. While it was clear to everybody that Spark is more efficient than MapReduce for data that fits in memory, we heard that some organizations were having trouble pushing it to large scale datasets that could not fit in memory. Therefore, since the inception of Databricks, we have devoted much effort, together with the Spark community, to improve the stability, scalability, and performance of Spark. Spark works well for gigabytes or terabytes of data, and it should also work well for petabytes.\n\nTo evaluate these improvements, we decided to participate in the <a href=\"http://sortbenchmark.org/\">Sort Benchmark</a>. With help from Amazon Web Services, we participated in the Daytona Gray category, an industry benchmark on how fast a system can sort 100 TB of data (1 trillion records). Although our entry is still under review, we are eager to share with you our submission. The previous world record was 72 minutes, set by Yahoo using a Hadoop MapReduce cluster of 2100 nodes. Using Spark on 206 EC2 nodes, we completed the benchmark in 23 minutes. This means that Spark sorted the same data <b>3X faster</b> using <b>10X fewer machines</b>. All the sorting took place on disk (HDFS), without using Spark's in-memory cache.\n\nAdditionally, while no official petabyte (PB) sort competition exists, we pushed Spark further to also sort 1 PB of data (10 trillion records) on 190 machines in under 4 hours. This PB time beats previously reported results based on Hadoop MapReduce (16 hours on 3800 machines). To the best of our knowledge, this is the first petabyte-scale sort ever done in a public cloud.\n<table class=\"table\">\n<thead>\n<tr>\n<th width=\"25%\"></th>\n<th width=\"25%\"><a href=\"http://sortbenchmark.org/Yahoo2013Sort.pdf\"><b>Hadoop\nWorld Record</b></a></th>\n<th width=\"25%\"><b>Spark\n100 TB *</b></th>\n<th width=\"25%\"><b>Spark\n1 PB</b></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Data Size</td>\n<td>102.5 TB</td>\n<td>100 TB</td>\n<td>1000 TB</td>\n</tr>\n<tr>\n<td>Elapsed Time</td>\n<td>72 mins</td>\n<td>23 mins</td>\n<td>234 mins</td>\n</tr>\n<tr>\n<td># Nodes</td>\n<td>2100</td>\n<td>206</td>\n<td>190</td>\n</tr>\n<tr>\n<td># Cores</td>\n<td>50400</td>\n<td>6592</td>\n<td>6080</td>\n</tr>\n<tr>\n<td># Reducers</td>\n<td>10,000</td>\n<td>29,000</td>\n<td>250,000</td>\n</tr>\n<tr>\n<td>Rate</td>\n<td>1.42 TB/min</td>\n<td>4.27 TB/min</td>\n<td>4.27 TB/min</td>\n</tr>\n<tr>\n<td>Rate/node</td>\n<td>0.67 GB/min</td>\n<td>20.7 GB/min</td>\n<td>22.5 GB/min</td>\n</tr>\n<tr>\n<td>Sort Benchmark Daytona Rules</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Environment</td>\n<td>dedicated data center</td>\n<td>EC2 (i2.8xlarge)</td>\n<td>EC2 (i2.8xlarge)</td>\n</tr>\n</tbody>\n</table>\n* not an official sort benchmark record\n\n<strong><strong>\u00a0</strong></strong>\n<h2>Why sorting?</h2>\nAt the core of sorting is the <em>shuffle</em> operation, which moves data across all machines. Shuffle underpins almost all distributed data processing workloads. For example, a SQL query joining two disparate data sources uses shuffle to move tuples that should be joined together onto the same machine, and collaborative filtering algorithms such as <a href=\"https://databricks.com/blog/2014/07/23/scalable-collaborative-filtering-with-spark-mllib.html\">ALS</a> rely on shuffle to send user/product ratings and weights across the network.\n\nMost data pipelines start with a large amount of raw data, but as the pipeline progresses, the amount of data is reduced due to filtering out irrelevant data or more compact representation of intermediate data. A SQL query on 100 TB of raw input data most likely only shuffles a tiny fraction of the 100 TB across the network. This pattern is also reflected in the naming of MapReduce itself.\n\nSorting, however, is one of the most challenging because there is no reduction of data along the pipeline. Sorting 100 TB of input data requires shuffling 100 TB of data across the network. As a matter of fact, the Daytona competition requires us to replicate both input and output data for fault-tolerance, and thus sorting 100 TB of data effectively generates 500 TB of disk I/O and 200 TB of network I/O.\n\nFor the above reasons, when we were looking for metrics to measure and improve Spark, sorting, one of the most demanding workloads, became a natural choice to focus on.\n\n<strong><strong>\u00a0</strong></strong>\n<h2>Tell me the technical work that went behind making this possible</h2>\nA lot of development has gone into improving Spark for very large scale workloads. In particular, there are three major pieces of work that are highly relevant to this benchmark.\n\nFirst and foremost, in Apache Spark 1.1 we introduced a new shuffle implementation called <b>sort-based shuffle</b> (<a href=\"https://issues.apache.org/jira/browse/SPARK-2045\">SPARK-2045</a>). The previous Spark shuffle implementation was hash-based that required maintaining P (the number of reduce partitions) concurrent buffers in memory. In sort-based shuffle, at any given point only a single buffer is required. This has led to substantial memory overhead reduction during shuffle and can support workloads with hundreds of thousands of tasks in a single stage (our PB sort used 250,000 tasks).\n\nSecond, we revamped the <b>network module</b> in Spark based on Netty\u2019s Epoll native socket transport via JNI (<a href=\"https://issues.apache.org/jira/browse/SPARK-2468\">SPARK-2468</a>). The new module also maintains its own pool of memory, thus bypassing JVM\u2019s memory allocator, reducing the impact of garbage collection.\n\nLast but not least, we created a new <b>external shuffle service</b> (<a href=\"https://issues.apache.org/jira/browse/SPARK-3796\">SPARK-3796</a>) that is decoupled from the Spark executor itself. This new service builds on the aforementioned network module and ensures that Spark can still serve shuffle files even when the executors are in GC pauses.\n<img class=\"alignnone size-full wp-image-1732\" src=\"https://databricks.com/wp-content/uploads/2014/10/ganglia-network-daytona.png\" alt=\"Network activity during sort\" width=\"550\" />\nWith these three changes, our Spark cluster was able to sustain 3GB/s/node I/O activity during the map phase, and 1.1 GB/s/node network activity during the reduce phase, saturating the 10Gbps link available on these machines.<strong><strong>\n</strong></strong>\n\n<strong><strong>\u00a0</strong></strong>\n<h2>What other nitty-gritty details have you not told me yet?</h2>\n<b>TimSort</b>: In Apache Spark 1.1, we switched our default sorting algorithm from quicksort to <a href=\"https://en.wikipedia.org/wiki/Timsort\">TimSort</a>, a derivation of merge sort and insertion sort. It performs better than quicksort in most real-world datasets, especially for datasets that are partially ordered. We use TimSort in both the map and reduce phases.\n\n<b>Exploiting Cache Locality</b>: In the sort benchmark, each record is 100 bytes, where the sort key is the first 10 bytes. As we were profiling our sort program, we noticed the cache miss rate was high, because each comparison required an object pointer lookup that was random. We redesigned our record in-memory layout to represent each record as one 16-byte record (two longs in the JVM), where the first 10 bytes represent the sort key, and the last 4 bytes represent the position of the record (in reality it is slightly more complicated than this due to endianness and signedness). This way, each comparison only required a cache lookup that was mostly sequential, rather than a random memory lookup. Originally proposed by Chris Nyberg et al. in AlphaSort, this is a common technique used in high-performance systems.\n\nSpark's nice programming abstraction and architecture allow us to implement these improvements in the user space (without modifying Spark) in a few lines of code. Combining TimSort with our new layout to exploit cache locality, the CPU time for sorting was reduced by a factor of 5.\n\n<b>Fault-tolerance at Scale</b>: At scale a lot of things can break. In the course of this experiment, we have seen nodes going away due to network connectivity issues, the Linux kernel spinning in a loop, or nodes pausing due to memory defrag. Fortunately, Spark is fault-tolerant and recovered from these failures.\n\n<b>Power of the Cloud (AWS)</b>: As mentioned previously, we leveraged 206 i2.8xlarge instances to run this I/O intensive experiment. These instances deliver high I/O throughput via SSDs. We put these instances in a placement group in a VPC to enable enhanced networking via single root I/O virtualization (SR-IOV). Enabling enhanced networking results in higher performance (10Gbps), lower latency, and lower jitter. We would like to thank everyone involved at AWS for their help making this happen including: the AWS EC2 services team, AWS EC2 Business Development team, AWS product marketing and AWS solutions architecture team. Without them this experiment would not have been possible.\n<h2>Isn\u2019t Spark in-memory only?</h2>\nThis has always been one of the most common misconceptions about Spark, especially for people new to the community. Spark is well known for its in-memory performance, but from its inception Spark was designed to be a general execution engine that works both in-memory and on-disk. Almost all Spark operators perform external operations when data does not fit in memory. More generally, Spark\u2019s operators are a strict superset of MapReduce.\n\nAs demonstrated by this experiment, Spark is capable of processing datasets many times larger than the aggregate memory in a cluster.\n<h2>Summary</h2>\nDatabricks, with the help of the Spark community, has contributed many improvements to Apache Spark to improve its performance, stability, and scalability. This enabled Databricks to use Apache Spark to sort 100 TB of data on 206 machines in 23 minutes, which is 3X faster than the previous Hadoop 100TB result on 2100 machines. Similarly, Databricks sorted 1 PB of data on 190 machines in less than 4 hours, which is over 4X faster than the previous Hadoop 1PB result on 3800 machines.\n\nOutperforming large Hadoop MapReduce clusters on sorting not only validates the work we have done, but also demonstrates that Spark is fulfilling its promise to serve as a faster and more scalable engine for data processing of all sizes. We hope that Spark enables equally dramatic improvements in time and cost for all our users."}
{"status": "publish", "description": null, "creator": "arsalan", "link": "https://databricks.com/blog/2014/10/20/efficient-similarity-algorithm-now-in-spark-twitter.html", "authors": ["Reza Zadeh"], "id": 1743, "categories": ["Apache Spark", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2014-10-20", "tz": "UTC", "createdOn": "2014-10-20"}, "title": "Efficient similarity algorithm now in Apache Spark, thanks to Twitter", "slug": "efficient-similarity-algorithm-now-in-spark-twitter", "content": "<div class=\"post-meta\">Our friends at Twitter have contributed to MLlib, and this post uses material from Twitter\u2019s description of its <a href=\"https://blog.twitter.com/2014/all-pairs-similarity-via-dimsum\" target=\"_blank\">open-source contribution</a>, with permission. The associated <a href=\"https://github.com/apache/spark/pull/1778\" target=\"_blank\">pull request</a> is slated for release in Apache Spark 1.2.</div>\n\n<hr />\n\n<h2>Introduction</h2>\nWe are often interested in finding users, hashtags and ads that are very similar to one another, so they may be recommended and shown to users and advertisers. To do this, we must consider many pairs of items, and evaluate how \u201csimilar\u201d they are to one another.\n\nWe call this the \u201call-pairs similarity\u201d problem, sometimes known as a \u201csimilarity join.\u201d We have developed a new efficient algorithm to solve the similarity join called \u201cDimension Independent Matrix Square using MapReduce,\u201d or <a href=\"http://arxiv.org/abs/1304.1467\" target=\"_blank\">DIMSUM</a> for short, which made one of Twitter\u2019s most expensive batch computations 40% more efficient.\n\nTo describe the problem we\u2019re trying to solve more formally, when given a dataset of sparse vector data, the all-pairs similarity problem is to find all similar vector pairs according to a similarity function such as <a href=\"https://en.wikipedia.org/wiki/Cosine_similarity\" target=\"_blank\">cosine similarity</a>, and a given similarity score threshold.\n\nNot all pairs of items are similar to one another, and yet a naive algorithm will spend computational effort to consider even those pairs of items that are not very similar. The brute force approach of considering all pairs of items quickly breaks, since its computational effort scales quadratically.\n\nFor example, for a million vectors, it is not feasible to check all roughly trillion pairs to see if they\u2019re above the similarity threshold. Having said that, there exist clever sampling techniques to focus the computational effort on only those pairs that are above the similarity threshold, thereby making the problem feasible. We\u2019ve developed the DIMSUM sampling scheme to focus the computational effort on only those pairs that are highly similar, thus making the problem feasible.\n<h2>Intuition</h2>\nThe main insight that allows gains in efficiency is sampling columns that have many non-zeros with lower probability. On the flip side, columns that have fewer non-zeros are sampled with higher probability. This sampling scheme can be shown to provably accurately estimate cosine similarities, because those columns that have many non-zeros have more trials to be included in the sample, and thus can be sampled with lower probability.\n\nThere is an in-depth description of the algorithm on the <a href=\"https://blog.twitter.com/2014/all-pairs-similarity-via-dimsum\" target=\"_blank\">Twitter Engineering blog post</a>.\n<h2>Experiments</h2>\nWe run DIMSUM on a production-scale ads dataset. Upon replacing the traditional cosine similarity computation in late June, we observed 40% improvement in several performance measures, plotted below.\n\n<img class=\"aligncenter size-full wp-image-62\" style=\"max-width: 100%; display: block; margin: 30px auto 5px auto;\" src=\"https://databricks.com/wp-content/uploads/2014/10/Dimsum-first.png\" alt=\"\" align=\"middle\" />\n\n<img class=\"aligncenter size-full wp-image-62\" style=\"max-width: 35%; display: block; margin: 30px auto 5px auto;\" src=\"https://databricks.com/wp-content/uploads/2014/10/Dimsum-second.png\" alt=\"\" align=\"middle\" />\n<h2>Usage from Spark</h2>\nThe algorithm is available in Apache Spark MLlib as a method in <a href=\"http://spark.apache.org/docs/1.1.0/mllib-data-types.html#rowmatrix\" target=\"_blank\">RowMatrix</a>. This makes it easy to use and access:\n\n[scala]\n// Arguments for input and threshold\nval filename = args(0)\nval threshold = args(1).toDouble\n\n// Load and parse the data file.\nval rows = sc.textFile(filename).map { line =&gt;\n  val values = line.split(' ').map(_.toDouble)\n  Vectors.dense(values)\n}\nval mat = new RowMatrix(rows)\n\n// Compute similar columns perfectly, with brute force.\nval simsPerfect = mat.columnSimilarities()\n\n// Compute similar columns with estimation using DIMSUM\nval simsEstimate = mat.columnSimilarities(threshold)\n[/scala]\n\nHere is an <a href=\"https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/mllib/CosineSimilarity.scala\" target=\"_blank\">example invocation of DIMSUM</a>. This functionality will be available as of Spark 1.2.\n\nAdditional information can be found in the <a href=\"https://gigaom.com/2014/09/24/twitter-open-sourced-a-recommendation-algorithm-for-massive-datasets/\" target=\"_blank\">GigaOM article</a> covering the DIMSUM algorithm."}
{"status": "publish", "description": null, "creator": "arsalan", "link": "https://databricks.com/blog/2014/10/15/application-spotlight-tableau-software.html", "authors": ["Jeff Feng (Product Manager at Tableau Software)"], "id": 1773, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2014-10-15", "tz": "UTC", "createdOn": "2014-10-15"}, "title": "Application Spotlight: Tableau Software", "slug": "application-spotlight-tableau-software", "content": "<div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://www.tableausoftware.com\" target=\"_blank\">Tableau Software</a>, whose visual analytics software is now <a href=\"http://www.databricks.com/certification\" target=\"_blank\">\u201cCertified on Apache Spark.\u201d</a></div>\n\n<hr />\n\n<img class=\"aligncenter size-full wp-image-62\" style=\"max-width: 100%; display: block; margin: 30px auto 5px auto;\" src=\"https://databricks.com/wp-content/uploads/2014/10/Tableau-SparkSQL.png\" alt=\"\" align=\"middle\" />\n<h2>Apache Spark - The Next Big Innovation</h2>\nOnce every few years or so, the big data open source community experiences a major innovation that advances the capabilities of data processing frameworks. For many years, MapReduce and the Hadoop open-source platform served as an effective foundation for the distributed processing of large data sets. Then last year, the introduction of YARN provided the resource manager needed to enable interactive workloads, bringing data processing performance to another level. However, as organizations entrust big data platforms to handle more of their critical business information, the volume and variety of data will continue to grow rapidly as will the need for speed to insight and action on that data. As most of the community would agree, we believe that Apache Spark is the next big innovation and platform to help take on the data challenges of tomorrow.\n\nThe decision to support Spark was easy \u2013 it was largely driven by our customers. Spark\u2019s usefulness as a powerful all-around big data platform for interactive queries and data processing has made it <a href=\"http://community.tableausoftware.com/ideas/3445\" target=\"_blank\">one of the most frequently requested data sources</a> in the last couple months. As a company, we strive to make the data sources that are important to our customers universally accessible. It was also prompted by the strong momentum of the Apache Spark project and the broad uptake in community support. Within the last 8 months, 10 of the Hadoop distributors including Cloudera, Hortonworks and MapR have committed to ship Spark as a part of their distribution as well as accelerate the development of the project. Lastly, Tableau was inspired to integrate with Spark because it is a technology that was architected intelligently from the very beginning as demonstrated by some of the <a href=\"https://databricks.com/blog/2014/10/10/spark-breaks-previous-large-scale-sort-record.html\" target=\"_blank\">early performance results</a>. In addition, our co-founders like to say that we are just getting started at Tableau, and we believe the same is true of Spark.\n<h2>Tableau Software is \u201cCertified on Spark\u201d</h2>\nToday, we are delighted to announce that Tableau Software is now \u201cCertified on Spark.\u201d Tableau sought qualification in the program so that our customers feel confident that the integration of the technologies works seamlessly and delightfully. We also want to help maintain the compatibility of Spark SQL across different distributions as it helps to facilitate a vibrant open source community - one of collaboration and integration. Tableau is committed to supporting Spark and ensuring compatibility with future releases.\n\nIn conjunction with our certification and our mission to \u201chelp people see and understand their data,\u201d Tableau is launching a new native Spark SQL connector for both Windows and Mac (currently in beta). We are excited to work with Databricks to bring the performance and versatility of the Spark data processing engine to the masses through visual analysis.\n<h2>Tableau + Spark = Better Together</h2>\nTableau\u2019s integration with Spark brings tremendous value to the Spark community - users can visually analyze their data without writing a single line of Spark SQL code. That\u2019s a big deal because creating a visual interface to your data expands the Spark technology beyond data scientists and data engineers to all business users. The Spark connector takes advantage of Tableau\u2019s flexible connection architecture that gives customers the option to connect live and issue interactive queries, or use Tableau\u2019s fast in-memory database engine. Tableau also provides users the capability to blend Spark data with data from any of our other 40+ direct connectors, empowering users to leverage their existing data assets wherever they are.\n\nNow to see Tableau and Spark SQL in action, we have created a short video demonstrating how users can connect to a Spark cluster and interact with data in Tableau.\n\n<iframe src=\"//www.youtube.com/embed/OKcIf6UdK7c\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n<h2>To Learn More:</h2>\n<strong>Read:</strong> To read more about Tableau\u2019s integration with Spark SQL, please check out our post on the <a href=\"http://www.tableausoftware.com/about/blog/2014/10/tableau-spark-sql-big-data-just-got-even-more-supercharged-33799\" target=\"_blank\">Tableau blog</a>.\n\n<strong>Join the beta:</strong> In order to use Tableau directly against Spark, you\u2019ll need to be a part of the beta program. If you\u2019re interested in joining, <a href=\"mailto:jfeng@tableausoftware.com\" target=\"_blank\">please send us an email</a>."}
{"status": "publish", "description": null, "creator": "scott", "link": "https://databricks.com/blog/2014/10/23/spark-summit-east-cfp-now-open.html", "authors": ["Scott Walent"], "id": 1809, "categories": ["Announcements", "Company Blog", "Events"], "dates": {"publishedOn": "2014-10-23", "tz": "UTC", "createdOn": "2014-10-23"}, "title": "Spark Summit East - CFP now open", "slug": "spark-summit-east-cfp-now-open", "content": "The call for presentations for the inaugural <a href=\"http://spark-summit.org/east\">Spark Summit East</a> is now open. Please join us in New York City on March 18-19, 2015 to share your experience with Apache Spark and celebrate its growing community.\n\nSpark Summit East is looking for presenters who would like to showcase how Spark and its related technologies are used in applications, development, data science and research. Please visit our <a href=\"http://www.spark-summit.org/east/2015/CFP\">submission page</a> for additional details. The Deadline for submissions is December 5, 2014 at 11:59pm PST.\n\nSpark Summit East is the leading event for <a href=\"http://spark.apache.org\">Apache Spark </a>users, developers and vendors.\u00a0It is an exciting opportunity to meet analysts, researchers, developers and executives\u00a0interested in utilizing Spark technology to answer big data questions.\n\nIf you missed <a href=\"http://spark-summit.org/2014\">Spark Summit 2014</a>, all the content is available online for free."}
{"status": "publish", "description": null, "creator": "arsalan", "link": "https://databricks.com/blog/2014/10/27/application-spotlight-faimdata.html", "authors": ["Ari Himmel (CEO at Faimdata)", "Nan Zhu (Chief Architect at Faimdata)"], "id": 1820, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2014-10-27", "tz": "UTC", "createdOn": "2014-10-27"}, "title": "Application Spotlight: Faimdata", "slug": "application-spotlight-faimdata", "content": "<div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://www.faimdata.com\" target=\"_blank\">Faimdata</a>, whose Consumer Data Intelligence Service is now <a href=\"http://www.databricks.com/certification\" target=\"_blank\">\u201cCertified on Apache Spark.\u201d</a></div>\n\n<hr />\n\n<h2>Forecasting, Analytics, Intelligence, Machine Learning</h2>\nFaimdata\u2019s Consumer Data Intelligence Service is a turnkey Big Data solution that provides comprehensive infrastructure and applications to retailers. We help our clients form close connections with their customers and make timely business decisions, using their existing data sources. The unified data processing pipeline deployed by Faimdata has three core focuses. They are (i) our Personalization Service that identifies the personal preferences and buying behaviors of each individual consumer using recommendation/machine learning algorithms; (ii) our Data Analytic Workbench where clients execute high performance multi-dimensional analytics across all distributed data sources using pre-defined or ad-hoc SQL-like languages; and (iii) our Social Intelligence Engine where clients can monitor social media related to their brands, products and competitors and link customer profiles to existing CRM data for more effective sales.\n\nBy using Faimdata, our retail clients have integrated their major data sources into a high performance, unified data pipeline for the very first time. Faimdata\u2019s infrastructure reduces overhead related to managing and maintaining data, and our applications empower our clients to make data-driven business decisions that increase customer satisfaction, strengthen marketing and product innovation, and most importantly increase revenue.\n<h2>Transforming Technology</h2>\nData driven decision-making is now mandatory for retailers to compete in the marketplace. Businesspeople want to use their data sources to realize value and understand their customers. They want to design, market and sell the right products to the right people at the right time. By listening to data with advanced analytical tools, businesses can offer customers highly relevant products and brand experiences.\n\nYet, scalability, reliability and extracting value beyond retroactive reporting is costly and difficult for businesses, even though most enterprise technologies already collect a tremendous amount of information. Many retailers simply do not have the technology infrastructure to maximize the usefulness of their data. We strive to resolve the tension between business needs, existing technologies and IT department limitations by empowering technology managers to satisfy client demands, simply and affordably.\n\nThe IT department's prevailing responsibilities of managing and maintaining technologies are now moving to 3rd Platform technology services. Faimdata is in the middle of this shift, providing a turnkey solution that matches the customer\u2019s rich source data to our customizable, real-time and predictive Big Data application. We thereby enable a broad range of retail activities, including: product innovation, data intelligence, CRM, supply chain, merchandising, marketing and ecommerce.\n<h2>The Apache Spark Advantage: Simplicity + Power</h2>\nAs such, it is an honor and pleasure to announce that the Faimdata Consumer Data Intelligence Service is now \u201cCertified on Apache Spark\u201d. Faimdata has been using Spark to enable our Services since the very beginning of our development and it\u2019s a decision that has served our clients well. Often, the biggest challenge in providing a Big Data solution is the complexity of the underlying infrastructure. Thanks to Spark\u2019s brilliant architecture of integrated solutions for different types of workloads, Faimdata is able to develop and seamlessly deploy turnkey applications that satisfy our customer\u2019s requirements within a unified framework. Faimdata\u2019s core systems all leverage Spark, including our ETL System (Spark Core), our Personalization Service (Mllib, GraphX), our Analytics Workbench (Spark Streaming, Spark Core, and Spark SQL), and our Social Intelligence (Spark Core and Spark SQL).\n\nSpark also has unbeatable speed that is critical for retailers. For example, in a scenario of iterative algorithms, Spark is able to perform 100x faster than traditional Hadoop. Spark SQL is also one of the fastest SQL query engines available. This is exactly what our client\u2019s demand, and what Faimdata needs, in order to provide real-time actionable insights and predictive analytics.\n\nThe recent performance of Databricks at Graysort is further testimony to Spark\u2019s speed and scalability and we congratulate Databricks on this notable achievement.\n<h2>Continued Collaboration</h2>\nIt is very exciting for us to join leading innovators in Big Data and be a part of the exemplary Spark open source community. We look forward to the future with Databricks and Spark.\n\nTo learn more about Faimdata please visit our website <a href=\"http://www.faimdata.com\" target=\"_blank\">faimdata.com</a> and send us an email at <a href=\"mailto:ari@faimdata.com\" target=\"_blank\">ari@faimdata.com</a> or on twitter @arihimmel"}
{"status": "publish", "description": null, "creator": "arsalan", "link": "https://databricks.com/blog/2014/10/31/hortonworks-a-shared-vision-for-apache-spark-on-hadoop.html", "authors": ["John Kreisa (VP of Strategic Marketing at Hortonworks)"], "id": 1823, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2014-10-31", "tz": "UTC", "createdOn": "2014-10-31"}, "title": "Hortonworks: A shared vision for Apache Spark on Hadoop", "slug": "hortonworks-a-shared-vision-for-apache-spark-on-hadoop", "content": "<div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://www.hortonworks.com\" target=\"_blank\">Hortonworks</a> announcing a broader partnership with Databricks around Apache Spark.</div>\n\n<hr>\n\nAt Hortonworks we are very excited by the emerging use cases and potential of Apache Spark and Apache Hadoop.   Spark is representative of just one of the shifts underway in the data landscape towards memory optimized processing, that when combined with Hadoop, can enable a new generation of applications.\n\nWe are excited to announce that Hortonworks and Databricks have extended our partnership focus from providing a <a href=\"https://databricks.com/spark/certification/certified-spark-distribution\" target=\"_blank\">Certified Spark Distribution</a> to include a shared vision to further Apache Spark as an enterprise ready component of the Hortonworks Data Platform. We are closely aligned on a strategy and vision of bringing 100% open source software to market for the enterprise and supporting the customer use cases.\n\nHaving two leaders in our respective communities come together makes sense for the community and for customers. Together with Databricks\u2019 expertise in Apache Spark combined with Hortonworks expertise in building a complete enterprise Hadoop data platform, we are better able to engineer solutions that meet the enterprise requirements for big data processing.\n\nFrom the Hortonworks perspective, our view has been very consistent: enabling a wide range of batch, interactive, real-time data processing applications to run simultaneously within a single <a href=\"http://www.hortonworks.com/hdp\" target=\"_blank\">enterprise Hadoop data platform</a> against shared datasets.  We believe applications leveraging Spark can benefit greatly from enabling it as a natively integrated engine within the Hortonworks Data Platform: integrated with YARN and supported by a common set of services for Security, Operations and Governance.\n\nIn June of 2014 we endorsed the standard set of open APIs for application development for Spark on the Hortonworks Data Platform making it a Certified Spark Distribution. This allows developers to build applications on this new engine while enabling operators to leverage a common data platform (Hadoop).\n\nWe are extending our partnership to include a commitment to invest in the following areas with Databricks:\n\n\n<ul>\n<li><strong>Engineering:</strong>  Spark optimized on YARN enables Spark-based applications to share the resources and operate along side other workloads, whether batch or streaming.  Additionally integrating Spark with the Security, Operations, and Governance components of the Hortonworks Data Platform/Apache Hadoop provides fully tested and enterprise-ready modern data platform.</li>\n<li><strong>Customers:</strong> Hortonworks and Databricks will jointly collaborate to support the usage of Spark and the Hortonworks Data Platform for our customers.</li>\n<li><strong>Open Source Foundation:</strong> We share a common vision for working with the open source community and delivering innovation, which will land into the upstream projects and is then delivered as enterprise ready software.</li>\n</ul>\n\nWe look forward to working with the Databricks team to further enable Spark on Hadoop."}
{"status": "publish", "description": null, "creator": "john", "link": "https://databricks.com/blog/2014/11/24/application-spotlight-skytree-infinity.html", "authors": ["Sachin Chawla (VP of Engineering)"], "id": 1974, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2014-11-25", "tz": "UTC", "createdOn": "2014-11-25"}, "title": "Application Spotlight: Skytree Infinity", "slug": "application-spotlight-skytree-infinity", "content": "<div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://www.skytree.net\" target=\"_blank\">Skytree</a>, whose Skytree Infinity platform is now <a href=\"http://www.databricks.com/certification\" target=\"_blank\">\u201cCertified on Apache Spark.\u201d</a></div>\n\n<hr />\n\n<h2>To Infinity and Beyond - Big Data at the speed of light</h2>\nAstronomers were into Big Data before it was big. In order to learn about the history of the universe, they needed to observe and record billions and billions of astronomical objects and perform heavy-duty analysis on the resulting massive datasets. Available predictive methods were not scalable to the size of data sets they were dealing with so they turned to Skytree to obtain unprecedented performance and accuracy on the largest datasets ever collected. Fast-forward a decade or so and the need to store, access, process and analyze datasets of astronomical sizes is now mainstream in the guise of Big Data analytics.\n\n<a href=\"http://www.skytree.net/products/skytree-infinity/\" target=\"_blank\"><img class=\"first certified\" src=\"/wp-content/uploads/2014/11/Skytree.png\" alt=\"\" width=\"\" height=\"300\" /></a>\n\nWe have built our Advanced Analytics platform, Skytree Infinity, from the ground up to provide an ultra-fast platform that makes it possible to use the most advanced and accurate machine learning methods on extremely large datasets to extract actionable insights and predictions. Furthermore, the infrastructure available for big data has matured to provide scalable and reliable data management and processing capabilities. The combination of our state-of-the-art machine learning and scalable infrastructure allows for an unprecedented, easy-to-consume solution for big data analysis.\n<h2>Skytree Infinity integration with Apache Spark</h2>\nWhile Skytree Infinity\u2019s analysis capabilities are remarkable, we are acutely aware of the need for easy, seamless integration to other closely related pieces of the Big Data analytics workflow. The Apache Spark project has garnered broad interest and adoption by the industry. Spark offers a rich platform for iterative processing and data manipulation that is well suited to machine learning. The combination of Skytree and Spark provides a widely usable solution for big data analytics that provides a fast, reliable, scalable, and manageable data cleansing/munging/analysis platform for the most challenging business problems.\n\nOur most recent offering in this direction is the certification of Skytree Infinity, the enterprise Machine Learning platform, on Apache Spark by Databricks.\n\n<a href=\"http://www.skytree.net/products/skytree-infinity/\" target=\"_blank\"><img class=\"first certified\" src=\"/wp-content/uploads/2014/11/skytree-2.png\" alt=\"\" width=\"\" height=\"300\" /></a>\n\nUsers can access data from multiple sources including RDBMSs, HDFS and Hive, and use Spark calls through Skytree Infinity to perform the most common ML-centric data pre-processing, munging and featurization tasks, to which they can apply advanced machine learning methods of their choice to obtain accurate, actionable predictions and insights from their data.\n<h2>Continued Collaboration</h2>\nWe are happy to be a member of the Spark community, and we are excited to explore the new vistas that this new integration opens.\n\nTo learn more about Skytree Infinity, please visit our website at <a href=\"http://www.skytree.net\" target=\"_blank\">www.skytree.net</a>"}
{"status": "publish", "description": null, "creator": "john", "link": "https://databricks.com/blog/2014/12/02/application-spotlight-nube-reifier.html", "authors": ["Sonal Goyal (CEO)"], "id": 2006, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2014-12-02", "tz": "UTC", "createdOn": "2014-12-02"}, "title": "Application Spotlight: Nube Reifier", "slug": "application-spotlight-nube-reifier", "content": "<div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://nubetech.co/\" target=\"_blank\">Nube Technologies</a>, whose Reifier platform is now <a href=\"http://www.databricks.com/certification\" target=\"_blank\">\u201cCertified on Apache Spark.\u201d</a></div>\n\n<hr />\n\n<h2>About Nube Technologies</h2>\nNube Technologies builds business applications to better decision making through better data. Nube\u2019s fuzzy matching product Reifier helps companies get a holistic view of enterprise data. By linking and resolving entities across various sources, Reifier helps optimize the sales and marketing funnel, promotes enhanced security and risk management and better consolidation and reporting of business data. We help our customers build better and effective models by ensuring that their underlying master data is accurate.\n<h2>Why Apache Spark</h2>\nData matching within a single source or across sources is a very core problem faced by almost every enterprise and we wanted to create a really smart way to solve this. Solving data matching problems is made even more difficult given that most data suffers from poor data quality with foremost reasons being errors and omissions during data collection, multi-field records and large data sizes.\n\nThe problem is an inherently quadratic problem, and although there are techniques to reduce the number of comparisons and boost up speed, applying them intelligently to unknown data is a challenging problem. While building Reifier, our aim is to be able to deal with various kinds of data in different domains be it customer information, product catalogs, organizations or any other variety of data.\n\nWe also wanted to build a system that was lightening fast as well as massively scalable with respect to the huge volumes of data seen by the modern day enterprise. On the development side, our wishlist included a friendly API, robust and scalable architecture, easy to use and well documented framework and inbuilt job dependency management.\n<h2>How we use Spark</h2>\nWhen we evaluated Spark, we were blown away by its speed, power and functionality. Spark\u2019s support for machine learning helped us create a supervised learning product which can completely learn combined similarity rules across different fields of a record from labeled positive and negative samples. We can hence use the same product across different data types easily.\n\nOur algorithms sit atop the base Spark framework and using the custom partitioning by Spark, many times we compare only less than 0.5% of all possible pairs, which is a big performance boost. Our commitment to Spark was bolstered when Reifier got certified on Spark.\n<h2>Nube and Spark Going Forward</h2>\nUsing Spark has clearly been the best architecture decision we took, and we are very happy to be part of the thriving Spark community. We are now looking forward to exploiting other Spark functionality to provide real time distributed fuzzy matching.\n\nDo visit <a href=\"http://www.nubetech.co\" target=\"_blank\">www.nubetech.co</a> to learn more about Reifier and feel free to contact me directly at <a href=\"mailto:sonal@nubetech.co\" target=\"_blank\">sonal@nubetech.co</a> for questions, trials and demonstrations."}
{"status": "publish", "description": null, "creator": "john", "link": "https://databricks.com/blog/2014/12/08/pearson-uses-spark-streaming-for-next-generation-adaptive-learning-platform.html", "authors": [" Dibyendu Bhattacharya (Big Data Architect)"], "id": 2027, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2014-12-09", "tz": "UTC", "createdOn": "2014-12-09"}, "title": "Pearson uses Apache Spark Streaming for next generation adaptive learning platform", "slug": "pearson-uses-spark-streaming-for-next-generation-adaptive-learning-platform", "content": "<div class=\"post-meta\">This is a guest blog post from our friends at Pearson outlining their Apache Spark use case.</div>\n\n<hr />\n\n<h2>Introduction of Pearson</h2>\nPearson is a British multinational publishing and education company headquartered in London. It is the largest education company and the largest book publisher in the world. Recently, Pearson announced a new organization structure in order to accelerate their push into digital learning, education services and emerging markets. I am part of Pearson Higher Education group, which provides textbooks and digital technologies to teachers and students across Higher Education. Pearson's higher education brands include eCollege, Mastering/MyLabs and Financial Times Publishing.\n<h2>What we wanted to do</h2>\nWe are building a next generation adaptive learning platform which delivers immersive learning experiences designed for the way today\u2019s students read, think, and learn. This learning platform is a scalable, reliable, cloud-based platform providing services to power the next generation of products for Higher Education. With a common data platform, we analyze student performance across product and institution boundaries and deliver efficacy insights to learners and institutions, which we were not able to deliver before. This platform will help Pearson to build new products faster and offer the world's greatest collection of educational content, while delivering most advanced data, analytics, adaptive and personalized capabilities for education.\n<h2>Why we chose Apache Spark and Spark Streaming</h2>\nNow to get the deep understanding of millions of learners, we needed a big data approach, and we found that a huge opportunity exists for ground-breaking industry-leading innovation in learner analytics. We have various use cases ranging from Near Real Time services, building Learner Graph, developing a common learner model for performing adaptive learning and recommendation, and different search based analytics etc. We found Apache Spark is one product which can bring all such capabilities into one platform. Spark supports both batch and real time mode of data processing along with graph analytics and machine learning libraries.\n\nPearson Near Real Time architecture is designed using Spark Streaming. Spark MLLib will be useful for Pearson Machine Learning use cases and Spark Graph Library will be useful for building learner graph in single common platform. Having common APIs and data processing semantics, Spark will help Pearson to build its skills and capabilities in a single platform rather than learning and managing various disparate tools.\n\nHaving a single platform and common API paradigm is one of the key reason we have moved our real time stack to Spark Streaming from our earlier solution which was designed using Apache Storm.\n<h2>What we did</h2>\nPearson's stream processing architecture is built using Apache Kafka and Spark Streaming.\n\nApache Kafka is a distributed messaging infrastructure and in Pearson's implementation, all students' activity and contextual data comes to Kafka cluster from different learning applications. Spark Streaming collects this data from Kafka in near-real-time and perform necessary transformations and aggregation on the fly to build the common learner data model and persists the data in NoSQL store (presently we are using Cassandra). For search related use cases, Spark Streaming consumes messages from Kafka and index them into Apache Blur, which is a distributed search engine on top of HDFS. For both these use cases, we needed a reliable, fault-tolerant Kafka consumer which can consume messages from Kafka topics without any data loss scenarios. Below diagram shows a high level overview of different components in this data pipeline.\n\n<a href=\"http://home.pearsonhighered.com/\" target=\"_blank\"><img class=\"first certified\" src=\"/wp-content/uploads/2014/12/pearson-1.png\" alt=\"\" width=\"\" height=\"400\" /></a>\n\nNext figure highlights the functionality of the Spark Streaming application. All the student/instructor activities and domain events from different learning applications are combined together to continuously update the learning model of each student.\n\n<a href=\"http://home.pearsonhighered.com/\" target=\"_blank\"><img class=\"first certified\" src=\"/wp-content/uploads/2014/12/pearson-2.png\" alt=\"\" width=\"\" height=\"200\" /></a>\n\nIn real time streaming use cases, Kafka is becoming a most adopted platform for distributed messaging system and this prompts various streaming layer like Spark Streaming to have a built-in Kafka Consumer, so that streaming layer can seamlessly fit into this architecture. When we started working on Spark Streaming and Kafka, we wanted to achieve better performance and stronger guarantees than those provided by the built-in high-level Kafka receiver of Spark Streaming. Hence, we chose to write our custom Kafka receiver. This custom Kafka Consumer for Spark Streaming uses the Low Level Kafka Consumer APIs, and is the most robust, high performant Kafka consumer available for Spark. This consumer handles Kafka node failures, leader changes, manages committed offset in ZK and can have tunable data rate throughput. It also solves the data loss scenarios on Receiver failures.\n\nPearson runs Spark Streaming in Amazon Cloud with YARN managed cluster. The building of common learner data model architecture using Spark Streaming will be in production by end of 2014. The Search based solution will be in production by Q1 2015. Other solutions like Learner Graph or advanced Machine Learning solution will be developed in 2015.\n<h2>To Learn More:</h2>\nFor more information, please refer to my recent <a href=\"https://www.youtube.com/watch?v=n7lfYhJgtJo&amp;list=PLU6n9Voqu_1FM8nmVwiWWDRtsEjlPqhgP&amp;index=25\" target=\"_blank\">talk</a> <a href=\"http://www.slideshare.net/lucidworks/near-real-time-indexing-kafka-messages-into-apache-blur-presented-by-dibyendu-bhattacharya-pearson-north-america\" target=\"_blank\">(slides)</a> at the Lucene/SolrRevolution conference."}
{"status": "publish", "description": null, "creator": "rxin", "link": "https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html", "authors": ["Reynold Xin"], "id": 2465, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2014-11-05", "tz": "UTC", "createdOn": "2014-11-05"}, "title": "Apache Spark officially sets a new record in large-scale sorting", "slug": "spark-officially-sets-a-new-record-in-large-scale-sorting", "content": "A month ago, we shared with you our entry to the 2014 Gray Sort competition, a 3rd-party benchmark measuring how fast a system can sort 100 TB of data (1 trillion records). Today, we are happy to announce that our entry has been reviewed by the benchmark committee and we have officially won the <a href=\"http://sortbenchmark.org/\">Daytona GraySort contest</a>!\n\nIn case you missed our <a href=\"https://databricks.com/blog/2014/10/10/spark-petabyte-sort.html\">earlier blog post</a>, using Spark on 206 EC2 machines, we sorted 100 TB of data on disk in 23 minutes. In comparison, the previous world record set by Hadoop MapReduce used 2100 machines and took 72 minutes. This means that Apache Spark sorted the same data <strong>3X faster</strong> using <strong>10X fewer machines</strong>. All the sorting took place on disk (HDFS), without using Spark\u2019s in-memory cache. This entry tied with a UCSD research team building high performance systems and we jointly set a new world record.\n<table class=\"table\">\n<thead>\n<tr>\n<th width=\"25%\"></th>\n<th width=\"25%\"><b>Hadoop MR\nRecord</b></th>\n<th width=\"25%\"><b>Spark\nRecord</b></th>\n<th width=\"25%\"><b>Spark\n1 PB</b></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Data Size</td>\n<td>102.5 TB</td>\n<td>100 TB</td>\n<td>1000 TB</td>\n</tr>\n<tr>\n<td>Elapsed Time</td>\n<td>72 mins</td>\n<td>23 mins</td>\n<td>234 mins</td>\n</tr>\n<tr>\n<td># Nodes</td>\n<td>2100</td>\n<td>206</td>\n<td>190</td>\n</tr>\n<tr>\n<td># Cores</td>\n<td>50400 physical</td>\n<td>6592 virtualized</td>\n<td>6080 virtualized</td>\n</tr>\n<tr>\n<td>Cluster disk throughput</td>\n<td>3150 GB/s\n(est.)</td>\n<td>618 GB/s</td>\n<td>570 GB/s</td>\n</tr>\n<tr>\n<td>Sort Benchmark Daytona Rules</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Network</td>\n<td>dedicated data center, 10Gbps</td>\n<td>virtualized (EC2) 10Gbps network</td>\n<td>virtualized (EC2) 10Gbps network</td>\n</tr>\n<tr>\n<td><strong>Sort rate</strong></td>\n<td><strong>1.42 TB/min</strong></td>\n<td><strong>4.27 TB/min</strong></td>\n<td><strong>4.27 TB/min</strong></td>\n</tr>\n<tr>\n<td><strong>Sort rate/node</strong></td>\n<td><strong>0.67 GB/min</strong></td>\n<td><strong>20.7 GB/min</strong></td>\n<td><strong>22.5 GB/min</strong></td>\n</tr>\n</tbody>\n</table>\nNamed after Jim Gray, the benchmark workload is resource intensive by any measure: sorting 100 TB of data following the strict rules generates 500 TB of disk I/O and 200 TB of network I/O. Organizations from around the world often build dedicated sort machines (specialized software and sometimes specialized hardware) to compete in this benchmark.\n\nWinning this benchmark as a general, fault-tolerant system marks an important milestone for the Spark project. It demonstrates that Spark is fulfilling its promise to serve as a faster and more scalable engine for data processing of all sizes, from GBs to TBs to PBs. In addition, it validates the work that we and others have been contributing to Spark over the past few years.\n\nSince the inception of Databricks, we have devoted much effort to improve the scalability, stability and performance of Spark. This benchmark builds upon some of our major recent work in Spark, including sort-based shuffle (<a href=\"https://issues.apache.org/jira/browse/SPARK-2045\">SPARK-2045</a>), the new Netty-based transport module (<a href=\"https://issues.apache.org/jira/browse/SPARK-2468\">SPARK-2468</a>), and external shuffle service (<a href=\"https://issues.apache.org/jira/browse/SPARK-3796\">SPARK-3796</a>). The former has been released in Apache Spark 1.1, and the latter two will be part of the upcoming Apache Spark 1.2 release.\n\nYou can read <a href=\"https://databricks.com/blog/2014/10/10/spark-petabyte-sort.html\">our earlier blog post</a> to learn more about our winning entry to the competition. Also expect future blog posts on these major new Spark features.\n\nFinally, we thank Aaron Davidson, Norman Maurer, Andrew Wang, Min Zhou, the EC2 and EBS teams from Amazon Web Services, and the Spark community for their help along the way. We also thank the benchmark committee members Chris Nyberg, Mehul Shah, and Naga Govindaraju for their support."}
{"status": "publish", "description": null, "creator": "john", "link": "https://databricks.com/blog/2014/11/14/application-spotlight-bedrock.html", "authors": ["Matt MacKinnon (Director of Product Management at Zaloni)"], "id": 2466, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2014-11-14", "tz": "UTC", "createdOn": "2014-11-14"}, "title": "Application Spotlight: Bedrock", "slug": "application-spotlight-bedrock", "content": "<div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://www.zaloni.com\" target=\"_blank\">Zaloni</a>, whose Bedrock platform is now <a href=\"http://www.databricks.com/certification\" target=\"_blank\">\u201cCertified on Apache Spark.\u201d</a></div>\n\n<hr />\n\n<h2>Bedrock\u2019s Managed Data Pipeline now includes Apache Spark</h2>\nIt was evident from the all the buzz at the Strata + Hadoop World conference that Apache Spark has now shifted from the early adopter phase to establishing itself as an integral and permanent part of the Hadoop ecosystem. The rapid pace of adoption is impressive!\n\nGiven the entrance of Spark into the mainstream Hadoop world, we are glad to announce that Bedrock is now officially Certified on Spark.\n<h2>How does Spark enhance Bedrock?</h2>\nBedrock\u2122 defines a Managed Data Pipeline as consisting of Ingest, Organize, and Prepare stages. Bedrock\u2019s strength lies in the integrated nature of the way data is handled through these stages.\n\u25cf Ingest: Bring data from various sources into Hadoop\n\u25cf Organize: Apply business, technical, and operational metadata to the incoming data\n\u25cf Prepare: Orchestrate workflows that perform data quality checks, mask sensitive fields, run change data capture actions and transformations.\n\n<a href=\"http://www.zaloni.com/\" target=\"_blank\"><img class=\"first certified\" src=\"/wp-content/uploads/2014/11/bedrock-archi.png\" alt=\"\" width=\"\" height=\"200\" /></a>\n\nThe Prepare stage of the Managed Data Pipeline is where Bedrock and Spark truly complement each other. Data preparation is all aspects of getting raw data ready for analytics and reporting. Spark is ideally suited to perform the type of processing required for data preparation. Combining high-speed, in-memory execution with a robust set of native actions and transformations makes Spark a natural fit. In Bedrock 3.1, Spark is now part of Bedrock\u2019s workflow design palette that supports 20+ built in workflow actions that you can drag and drop onto the canvas to create your preparation workflow. If you prefer using SQL, you can use the Bedrock SparkSQL action.\n\nLet us look at a typical Bedrock Managed Data Pipeline to see where Spark fits. One very common use-case for Bedrock is moving data from a traditional relational database into HDFS and making the data available in Hive. To create this Managed Data Pipeline in Bedrock, we start with the Bedrock landing zone to reliably ingest the data into HDFS. Business and technical metadata is managed and operational metadata is captured as the data arrives and loads into Hadoop. In the Prepare phase, the built-in Bedrock capabilities for checking data quality, masking sensitive data, or merging incremental changes may be required. We finally round out the Prepare stage of the Managed Data Pipeline through the use of Spark or SparkSQL to implement custom or proprietary transformations, aggregations, and analysis.\n<h2>Looking to the Future</h2>\nThis is only the beginning of the value that Bedrock and Spark together can bring to the Managed Data Pipeline. In the future, Bedrock will be extended to support the full Spark ecosystem and existing Bedrock preparation actions, such as masking, data quality checks, and change data capture, will be available with Spark implementations.\n\nTo learn more about Bedrock and a Spark enabled Managed Data Pipeline, visit us online <a href=\"http://www.zaloni.com\" target=\"_blank\">zaloni.com</a>, or feel free to contact me directly at <a href=\"mailto:mmackinnon@zaloni.com\" target=\"_blank\">mmackinnon@zaloni.com</a>"}
{"status": "publish", "description": null, "creator": "john", "link": "https://databricks.com/blog/2014/11/14/the-spark-certified-developer-program.html", "authors": ["John Tripier", "Paco Nathan"], "id": 2467, "categories": ["Announcements", "Company Blog"], "dates": {"publishedOn": "2014-11-15", "tz": "UTC", "createdOn": "2014-11-15"}, "title": "The Apache Spark Certified Developer Program", "slug": "the-spark-certified-developer-program", "content": "More and more companies are using Apache Spark, and many Spark based pilots are currently deploying in production. In social media, at every big data conference or meetup, people describe new POC, prototypes, and production deployments using Spark.\n\nBehind this momentum, a growing need for Spark developers is developing; people who have demonstrated expertise in how to implement best practices for Spark. People who can help the enterprise building increasingly complex and sophisticated solutions on top of their Spark deployments.\n\nAt Databricks, we get contacted by many enterprises looking for Spark resources to help with their next data-driven initiative. And so beyond our effort to train people on Spark directly or through partners all around the world, we have teamed up with O\u2019Reilly for offering the first industry standard for measuring and validating a developer\u2019s expertise on Spark.\n<h2>Benefits of being a Spark Certified Developer</h2>\nThe Spark Developer Certification is the way for a developer to:\n<ul>\n \t<li>Demonstrate recognized validation for your expertise</li>\n \t<li>Meet the global standards to ensure compatibility between Spark applications and distributions</li>\n \t<li>Stay up to date with the latest advances and training in Spark</li>\n \t<li>Be a part of the Spark developers community</li>\n</ul>\nThe first set of exams have taken place at <a href=\"http://www.oreilly.com/data/sparkcert.html\" target=\"_blank\">Strata Barcelona</a> on November 20th 2014.\n\nShortly, developers will be able to take the exam online <a href=\"http://www.oreilly.com/data/sparkcert.html?cmp=ex-strata-na-lp-na_apache_spark_certification\" target=\"_blank\">here</a>. We also expect to run certification sessions at other conferences.\n<h2>How to prepare for the exam</h2>\nYou will take the test on your own computer, under the monitoring of a proctoring team. The test is about 90 minutes with a series of randomly generated questions covering all aspects of Spark.\n\nThe test will include questions in Scala, Python, Java, and SQL. However, deep proficiency in any of those languages is not required, since the questions focus on Spark and its model of computation.\n\nTo prepare for the Spark certification exam, we recommend that you:\n<ul>\n \t<li>Are comfortable coding the advanced exercises in Spark Camp or related training (<a href=\"http://spark-summit.org/2014/training\" target=\"_blank\">example exercises can be found here</a>).</li>\n \t<li>Have mastered the material released so far in the O'Reilly book, Learning Spark</li>\n \t<li>Have some hands-on experience developing Spark apps in production already</li>\n</ul>"}
{"status": "publish", "description": null, "creator": "john", "link": "https://databricks.com/blog/2014/11/21/samsung-sds-uses-spark-for-prescriptive-analytics-at-large-scale.html", "authors": ["Luis Quintela (Sr. Manager of Big Data Analytics)", "Yan Breek (Data Scientist)", "Girish Kathalagiri (Data Analytics Engineer)"], "id": 2468, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2014-11-22", "tz": "UTC", "createdOn": "2014-11-22"}, "title": "Samsung SDS uses Apache Spark for prescriptive analytics at large scale", "slug": "samsung-sds-uses-spark-for-prescriptive-analytics-at-large-scale", "content": "<div class=\"post-meta\">This is a guest blog post from our friends at Samsung SDS outlining their Apache Spark use case.</div>\n\n<hr />\n\n<h2>Business Challenge</h2>\nSamsung SDS is the business and IT solutions arm of Samsung Group. A global ICT service provider with over 17,000 employees worldwide and 6.7 billion USD in revenues, Samsung SDS tackles the challenges of some of the largest global enterprises in such industries as manufacturing, financial services, health care and retail.\n\nIn the different areas Samsung is focused on, the ability to make timely decisions that maximize the value to a business becomes critical. Prescriptive analytics methods have been used effectively to support decision making by leveraging probable future outcomes determined by predictive models and suggesting actions that provide maximal business value.\n\nOne of the main challenges in applying prescriptive analytics in these areas is the need to analyze a combination of structured and unstructured data at large scale, which requires a flexible and comprehensive computation framework.\n\nTo demonstrate the effectiveness of prescriptive analytics algorithms implemented by scalable technologies in realizing decision making use-cases, Samsung SDS Research America (SDSRA) has prototyped a framework that Samsung SDS business units can leverage and incorporate as part of the go-to-market products.\n<h2>Why Apache Spark</h2>\nDeveloping such a solution required three main areas of effort:\n<ul>\n \t<li>high volume data processing for feature extraction as a means of modeling business environment state;</li>\n \t<li>prescriptive model training on historical events;</li>\n \t<li>real-time processing of decision requests and corresponding prescribed actions;</li>\n</ul>\nThere are different technologies that can be used to support these effort threads but integrating these technologies can turn into a significant undertaking, one which is not directly bringing value to the project. SDSRA turned to Apache Spark due to its ability to provide efficient solutions for all three areas of effort through multiple components that are unified in one single distributed computation paradigm, at the same time as providing the level of fault tolerance expected.\n\nOur first direct contact with Spark was at the Strata Conference earlier in 2014 in Santa Clara, attending the Berkeley Data Analytics Stack tutorial, when the power of the framework and simplicity of the API became apparent. Coming back to the lab, the team experimented with the framework by attempting to implement data mining algorithms such as Apriori, for finding frequent item sets. After this initial experience, the decision was made to apply the framework in our prescriptive analytics proof-of-concept project, triggering two parallel efforts: one to implement a prescriptive analytics algorithm at scale with Spark and a second effort thread to develop a real-time framework based on Spark Streaming to get prescriptions as a response to a continuous stream of requests.\n\nWith Spark, the original raw data can be loaded into a resilient distributed dataset (RDD) and transformed into the set of features that define state. The states constitute the input for the prescriptive model training, also performed on the Spark framework through a series of RDD transformations. The resulting transformed data set is then used as input for an MLlib regression model for approximation of a value function, which is the main element of the prescriptive model.\n\n<img class=\"first certified\" src=\"/wp-content/uploads/2014/11/Samsung-SDS.png\" alt=\"\" width=\"\" height=\"500\" />\n\nAfter deriving a policy from the trained model, Spark Streaming is applied for processing the stream of requests, using the model for prescribing actions and maintaining the states as part of the stream.\n\nThere are three main characteristics of the Spark ecosystem that makes it a perfect match for this solution: the ability to cache Spark data sets and Spark Streaming data streams in memory, the distributed architecture (allowing horizontal linear scalability on commodity server clusters), and a single development paradigm across all components.\n<h2>What is next</h2>\nAt SDSRA, we see Spark as a key technology providing high throughput and low latency in processing large volume of data ingested at high speed. We look forward to experimenting with additional components of the ecosystem such as SparkSQL and GraphX, as we evolve our decision making engine into a full solution.\n\nTo learn more about SDSRA and this platform, feel free to contact me directly at <a href=\"mailto:l.quintela@samsung.com\" target=\"_blank\">l.quintela@samsung.com</a>"}
{"status": "publish", "description": null, "creator": "arsalan", "link": "https://databricks.com/blog/2014/12/02/announcing-two-spark-based-moocs.html", "authors": ["Ameet Talwalkar", "Anthony Joseph"], "id": 2469, "categories": ["Announcements", "Company Blog"], "dates": {"publishedOn": "2014-12-02", "tz": "UTC", "createdOn": "2014-12-02"}, "title": "Databricks to run two massive online courses on Apache Spark", "slug": "announcing-two-spark-based-moocs", "content": "In the age of \u2018Big Data,\u2019 with datasets rapidly growing in size and complexity and cloud computing becoming more pervasive, data science techniques are fast becoming core components of large-scale data processing pipelines.\n\nApache Spark offers analysts and engineers a powerful tool for building these pipelines, and learning to build such pipelines will soon be a lot easier. Databricks is excited to be working with professors from University of California Berkeley and University of California Los Angeles to produce two new upcoming Massive Open Online Courses (MOOCs). Both courses will be freely available on the edX MOOC platform in <del>spring</del> summer 2015. edX Verified Certificates are also available for a fee.\n\n<img class=\"aligncenter size-full wp-image-62\" style=\"max-width: 100%; display: block; margin: 30px auto 5px auto;\" src=\"https://databricks.com/wp-content/uploads/2014/12/MOOC1.png\" alt=\"\" align=\"middle\" />\n\nThe first course, called <a href=\"https://www.edx.org/course/uc-berkeleyx/uc-berkeleyx-cs100-1x-introduction-big-6181\" target=\"_blank\">Introduction to Big Data with Apache Spark</a>, will teach students about Apache Spark and performing data analysis. Students will learn how to apply data science techniques using parallel programming in Spark to explore big (and small) data. The course will include hands-on programming exercises including Log Mining, Textual Entity Recognition, Collaborative Filtering that teach students how to manipulate data sets using parallel processing with PySpark (part of Apache Spark). The course is also designed to help prepare students for taking the <a href=\"http://go.databricks.com/spark-certified-developer\" target=\"_blank\">Spark Certified Developer</a> exam. The course is being taught by Anthony Joseph, a professor at UC Berkeley and technical advisor at Databricks, and will start on <del>February 23rd</del> June\u00a01st, 2015.\n\n<img class=\"aligncenter size-full wp-image-62\" style=\"max-width: 100%; display: block; margin: 30px auto 5px auto;\" src=\"https://databricks.com/wp-content/uploads/2014/12/MOOC2.png\" alt=\"\" align=\"middle\" />\n\nThe second course, called <a href=\"https://www.edx.org/course/uc-berkeleyx/uc-berkeleyx-cs190-1x-scalable-machine-6066\" target=\"_blank\">Scalable Machine Learning</a>, introduces the underlying statistical and algorithmic principles required to develop scalable machine learning pipelines, and provides hands-on experience using PySpark. It presents an integrated view of data processing by highlighting the various components of these pipelines, including exploratory data analysis, feature extraction, supervised learning, and model evaluation. Students will use Spark to implement scalable algorithms for fundamental statistical models while tackling real-world problems from various domains. The course is being taught by Ameet Talwalkar, an assistant professor at UCLA and technical advisor at Databricks, and will start on <del>April 14th</del> June\u00a029th, 2015.\n\nBoth courses are available for free on the edX website. You can sign up for them today:\n<ul>\n\t<li><a href=\"https://www.edx.org/course/uc-berkeleyx/uc-berkeleyx-cs100-1x-introduction-big-6181\" target=\"_blank\">Introduction to Big Data with Apache Spark</a></li>\n\t<li><a href=\"https://www.edx.org/course/uc-berkeleyx/uc-berkeleyx-cs190-1x-scalable-machine-6066\" target=\"_blank\">Scalable Machine Learning</a></li>\n</ul>"}
{"status": "publish", "description": null, "creator": "john", "link": "https://databricks.com/blog/2014/12/03/application-spotlight-technicolor-virdata-internet-of-things-platform.html", "authors": ["Lieven Gesquiere (Virdata Lead Core R&D)"], "id": 2470, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2014-12-04", "tz": "UTC", "createdOn": "2014-12-04"}, "title": "Application Spotlight: Technicolor Virdata Internet of Things platform", "slug": "application-spotlight-technicolor-virdata-internet-of-things-platform", "content": "<div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://www.technicolor.com/\" target=\"_blank\">Technicolor</a>, whose Virdata platform is now <a href=\"http://www.databricks.com/certification\" target=\"_blank\">\u201cCertified on Apache Spark.\u201d</a></div>\n\n<hr />\n\n<h2>About Virdata</h2>\nVirdata is Technicolor\u2019s cloud-native Internet of Things platform offering real-time monitoring, configuration and management of the unprecedented number of connected devices and applications. Combining its highly-scalable data ingestion and messaging capabilities with real-time and historical analytics, Virdata brings value across multiple data-driven markets.\n\nThe Virdata platform was launched at CES Las Vegas in January, 2014.\nThe Virdata cloud-based platform architecture integrates state-of-the-art open source software components into a homogeneous, high-availability data-processing environment.\n<h2>Virdata and Apache Spark</h2>\nThe Virdata solution architecture comprises 3 areas: Messaging, Data Processing and Applications - all accessed through APIs. Its publish/subscribe based messaging infrastructure contains a high-throughput distributed message broker and distributed complex event processing and bidirectional message routing components.\n\nCompleting Virdata's \u201cfull stack\u201d Internet of Things (IoT) platform, the solution provides extensive Apache Spark-driven in-memory and post-processing capability in order to transform, store and analyze the huge stream of messages generated by the world of IoT.\n\nSpark was integrated in the Virdata architecture in early 2013 as the data processing framework to analyze incoming IoT messages published by the millions of devices monitored by the Virdata cloud platform. Spark is used in Virdata for batch processing and real-time processing of device data in order to compute time-series, pre-calculated complex data visualizations and custom monitoring reports.\n\nVirdata\u2019s data processing implementation incorporates a Lambda architecture (http://manning.com/marz/) well-suited to a combination of Spark and Spark Streaming.\n\nVirdata migrated its data processing approach from the initial stream-oriented framework where every message was processed independently to adopting Spark Streaming in order to optimize processing and message storage in a distributed manner. The Spark Streaming micro-batching approach specifically enabled eliminating any occurrences of \u2018impedance mismatch\u2019 with the storage of data in the Virdata databases.\n\nThe growing adoption of Spark by the open-source community plays an important role. As just one example, the recently released Spark-Cassandra Driver has allowed Virdata to replace custom code with a specialized component, delivering improved performance characteristics.\n<h2>How Virdata benefits from Spark</h2>\nVirdata benefits from Spark in 3 ways:\n<ul>\n \t<li>Spark offers Virdata a single framework for both batch and real-time processing.</li>\n \t<li>Spark offers the programming languages favored by its own developers and the wider data science community.</li>\n \t<li>Spark supports Virdata's native cloud dev-op and configuration environment.</li>\n</ul>\n<h2>Virdata and Spark Going Forward</h2>\nVirdata is especially excited about the growing richness of libraries in the Spark ecosystem and is already considering the integration of additional functionality such as SparkSQL, the recently announced Spark-ElasticSearch integration and the Spark Machine Learning library (MLlib) and looks forward to the announcement of many others .\n\nTo learn more about Technicolor Virdata, please visit our website at <a href=\"http://www.technicolor.com/\" target=\"_blank\">www.technicolor.com</a> and feel free to send us an email at <a href=\"mailto:Jeremy.DeClercq@technicolor.com\" target=\"_blank\">Jeremy.DeClercq@technicolor.com</a> for questions and demonstrations."}
{"status": "publish", "description": null, "creator": "kavitha", "link": "https://databricks.com/blog/2015/01/13/databricks-expands-bay-area-presence-moves-hq-to-san-francisco.html", "authors": ["by Databricks Press Office"], "id": 2294, "categories": ["Announcements", "Company Blog"], "dates": {"publishedOn": "2015-01-13", "tz": "UTC", "createdOn": "2015-01-13"}, "title": "Databricks Expands Bay Area Presence, Moves HQ to San Francisco", "slug": "databricks-expands-bay-area-presence-moves-hq-to-san-francisco", "content": "<strong>Highlights:</strong>\n<ul>\n\t<li>Databricks Expands Bay Area Presence, Moves HQ to San Francisco</li>\n\t<li>Company Names Kavitha Mariappan as Marketing Vice President</li>\n</ul>\nPress Release: <a title=\"http://finance.yahoo.com/news/databricks-expands-bay-area-presence-140000610.html\" href=\"http://finance.yahoo.com/news/databricks-expands-bay-area-presence-140000610.html\">http://finance.yahoo.com/news/databricks-expands-bay-area-presence-140000610.html</a>\n\n<strong>San Francisco, Calif. \u2013 January 13, 2015 \u2013 </strong><a href=\"http://www.databricks.com\">Databricks</a>, the company founded by the creators of the popular open-source Big Data processing engine Apache Spark with its flagship product, Databricks Cloud, today announced the relocation of their headquarters to San Francisco from Berkeley, California. The expansion is a reflection of Databricks\u2019 growth heading into 2015. The company grew more than 200 percent in headcount over the last year and adds talent to its executive bench with the appointment of Kavitha Mariappan, former Marketing Vice President at Maginatics, an EMC Corporation Company, as Vice President of Marketing.\n\nOriginally founded in Berkeley, Databricks\u2019 relocation is spurred by the startup\u2019s rapid growth over the past year, which has led to numerous new hires, increasing the total headcount from 14 to 46 full-time employees. In anticipation of continued growth, the company has secured a prime new space in the SOMA district of San Francisco with over 15,000 square feet \u2013 triple the size of the previous office. The new location will also enable Databricks to host their routinely-attended <a href=\"http://www.meetup.com/spark-users/\">Spark MeetUps</a> and training sessions in-house \u2013 an initiative the company organizes for Spark users around the Bay Area.\n\n\u201cSan Francisco is a melting pot for raw talent and technology innovation, and gives us excellent access to customers and partners who share our vision of simplifying Big Data processing to generate real value,\u201d said Ion Stoica, CEO of Databricks. \u201cDatabricks has deep roots in the Bay Area and we are excited to move to this hub of innovation and opportunity, while still having a view of Berkeley from our windows.\u201d\n\nDatabricks is also announcing today the appointment of Kavitha Mariappan as Vice President of Marketing. With 20 years of industry experience, Mariappan will be instrumental in the development and execution of Databricks' global marketing across sales channels and communication platforms, and will work hand-in-hand with the customer engagement, product and engineering teams to communicate the value, use cases, and unique capabilities of Databricks Cloud to the enterprise and developer communities. Mariappan comes to Databricks from Maginatics, an enterprise storage solutions company recently acquired by EMC, where she built and led the team responsible for all aspects of product, technical, channel and outbound marketing and communications.\n\n\u201cDatabricks is completely revolutionizing the concept of big data analytics by significantly simplifying the process and enabling customers to extract tremendous value from their data. They are no strangers to technology innovation, with their unique pedigree and industry leadership as the creators of Apache Spark,\u201d adds Mariappan. \u201cI am truly excited to be joining a company that is fundamentally changing the way businesses are turning their data into economic value.\u201d\n\nDatabricks is currently hiring for various positions within engineering, sales &amp; marketing, field engineering and data science. Visit their careers page for more details: <a title=\"https://databricks.com/careers\" href=\"https://databricks.com/careers\">databricks.com/careers</a>"}
{"status": "publish", "description": null, "creator": "kavitha", "link": "https://databricks.com/blog/2015/01/16/spark-certified-developer-exams-available-online.html", "authors": ["Kavitha Mariappan"], "id": 2345, "categories": ["Announcements", "Company Blog"], "dates": {"publishedOn": "2015-01-16", "tz": "UTC", "createdOn": "2015-01-16"}, "title": "Apache Spark Certified Developer exams available online!", "slug": "spark-certified-developer-exams-available-online", "content": "Complementing our on-going direct and partner-led Apache Spark training efforts, Databricks has teamed up with O\u2019Reilly to offer the industry\u2019s first standard for measuring and validating a developer\u2019s expertise with Spark.\n\nDatabricks and O\u2019Reilly are proud to announce the online availability of the Spark Certified Developer exams. You can now sign up and take the exam online<a href=\" http://go.databricks.com/spark-certified-developer\"> here</a>.\n\n<b>What is the Spark Certified Developer program?</b>\n\nApache Spark is the most active project in the Big Data ecosystem and is fast becoming the open source alternative of choice for many enterprises. Spark provides enterprises with the scale and sophistication they require to gain insights from their Big Data by providing a unified framework for building data pipelines. Databricks was founded by the team that created and continues to lead both development and training around Spark, and<a href=\"https://databricks.com/product\"> Databricks Cloud</a>, the cloud platform built around Spark to significantly simplify big data processing.\n\nBehind this momentum, there is a growing need for Spark developers: developers who have both demonstrated expertise in how to implement best practices for Spark, and can help the enterprise build increasingly complex and sophisticated solutions on top of their Spark deployments. To address this growing demand for Spark developers and to complement our on-going direct and partner-led Spark training efforts, Databricks has teamed up with O\u2019Reilly to offer the industry\u2019s first standard for measuring and validating a developer\u2019s expertise with Spark.\n\nThe first set of certification exams took place at the<a href=\"http://www.oreilly.com/data/sparkcert.html\"> Strata Conference in Barcelona</a> on November 20th 2014. In addition to our online certification, we also expect to run certification sessions at other upcoming conferences.\n\n<b>Why become a Spark Certified Developer?</b>\n\nAs a Spark Certified Developer you will be able to:\n<ul>\n \t<li>Demonstrate industry recognized validation for your expertise.</li>\n \t<li>Meet global standards required to ensure compatibility between Spark applications and distributions.</li>\n \t<li>Stay up to date with the latest advances and training in Spark.</li>\n \t<li>Become an integral part of the growing Spark developer community.</li>\n</ul>\n<b>I\u2019m interested! How to prepare for the exam?</b>\n\nYou will take the test on your own computer under the supervision of a proctoring team.\n\nThe test is about 90-minutes in duration and includes a series of randomly generated questions covering all aspects of Spark. It will include some questions in Scala, Python, Java, and SQL, however, deep proficiency in any of those languages is not mandatory. Instead the questions will primarily focus on Spark and its computational model.\n\nTo prepare for the Spark certification exam, we recommend that you:\n<ul>\n \t<li>Are comfortable coding the advanced exercises in Spark Camp or related training (<a href=\"http://spark-summit.org/2014/training\">example exercises can be found here</a>).</li>\n \t<li>Have mastered the material released so far in the O\u2019Reilly book, <a href=\"http://shop.oreilly.com/product/0636920028512.do\">Learning Spark</a>.</li>\n \t<li>Already have some hands-on experience developing Spark applications in production environments.</li>\n</ul>\nDon\u2019t forget to sign up and take the exam online<a href=\" http://go.databricks.com/spark-certified-developer\"> here</a>."}
{"status": "publish", "description": null, "creator": "kavitha", "link": "https://databricks.com/blog/2015/01/20/spark-summit-east-2015-agenda-is-now-available.html", "authors": ["Kavitha Mariappan"], "id": 2359, "categories": ["Company Blog", "Events"], "dates": {"publishedOn": "2015-01-20", "tz": "UTC", "createdOn": "2015-01-20"}, "title": "Spark Summit East 2015 Agenda is Now Available", "slug": "spark-summit-east-2015-agenda-is-now-available", "content": "We are thrilled to announce the availability of the <a href=\"http://go.spark-summit.org/e1t/c/*W6stDzJ6_3DYhW6Y-qp35L8r5j0/*W4PZ7v36VwsQzW58WPXZ57MJJH0/5/f18dQhb0Sq5z8YHrDTW8HLj0x5VQHw7W6bFhBV6P7FhxW4R4BZM57mvC2W1BQYgg4P0TLvW85Q81T83G7d1W9dtj1h7NQNCqW4zWTRG33K-8nW7NMj-x9bTNXYW954KlM4P0Yt6W2d4hSK3bWrh8W2YH1kR47xfHKW2HRyfR6trFPNW47YlYy4bfcHbW47Xx4z3C811XW4-SZvb2KQ2YYW3_VZwP5ThdHgW3s1XjF51G0BJW4Zh8Y-57-WqMW3H_Pty2DzCtRW1zBkSq1sQ3b4W8V-D1g5rcXhJW7JS0c27BQjYmVJB4Mm896Q7XW94B_1g7v78c8W8NqNPC5qWyC0W7JTtyJ2Xm03sW3FBZ5D9lNHw9W6_b40v3vyNkPW6J4Ypk8lBfs0W3bnqM_1C-9rFVL--5_1Pct9JW2mPjk95hqX5PW9lKhck4H6s3gN4m21WR6Q977Vb98_P6s16_2W8Ph58-59BvQ0W7y34GD1FmQY-W7r71Hq2PhWHMW7tprCG95RqNQW2j-Sgt2L5GhqW3G6xft6TMH99W6-cC_w3wXTtZW6Sytzy9fTwQmN3FYx-Q_HpmRf6dY7D511\" target=\"_blank\">agenda</a>\u00a0for Spark Summit East 2015! This inaugural New York City event on <span class=\"aBn\" tabindex=\"0\" data-term=\"goog_929332804\"><span class=\"aQJ\">March 18-19, 2015</span></span> has over thirty jam-packed sessions \u2013\u00a0offering a combination of longer deep-dive presentations and shorter intensive talks. You will have the opportunity to engage the speakers and your peers in discussion and a cross-pollination of ideas.\n\nWant to guarantee your seat now? Don\u2019t forget to register at <a href=\"http://go.spark-summit.org/e1t/c/*W6stDzJ6_3DYhW6Y-qp35L8r5j0/*W4ggqXl1bLCgyW2ZPnv83G16gb0/5/f18dQhb0SfHx9dsQGlW8HLj0x5VQHw7W6bFhBV6P7FhxW4R4L8v57mvC2W1BQYdv5Zh1T-W5LrNKl5rGt24W8YVWJx1bx6QJW24Zwjc6Lj7n4W6TnS3121WTXPW72rCNX1ZYXTRW6_94QZ6zCFTdW6twNJp2Wf0HMW5VbtmT1ksGJhW1R4_lk6991h2W7Wm1v220RS8TW59tT3Q2_n9qCW70fl6p7PJ6bdW2gM1cw1VHbdHW1fwyVV6yBPJMW30q8CN2kGg8MN5J92T6z1rlFW1VJryk5lX8fqW13bD4h6dkxBXW6fvMJz52HdgTW30gwjq2MfbGXW5MGRYh2Q9cfPW8gVS916cFGHNW4zybTB7pzzRxW3mH4TM9fy50qW56jN884Np4Y7W3qt5n883FkFRW4NQWD_31fBBTW9g8zmk6dXHHbW56k4J76VRYz6W7K2qZ28gbdv6W30nKq65p0cz2W4y0QxT1Fn75qVy9d1K4Ts_3BW4Pw1Vk2sbPxnW2fxmCv3C2Mz5W2BmRqF3B_qNxW2nMLdv2sG3RZW5Tb4y25hfQL4W8pwqMd8yzslQN6JrKTyM7W7nW91cJF_4HyTJfW7v9ypm70VxgnW87DwKM4HzlXl0\" target=\"_blank\">Spark Summit East</a>.\n\nWe also have a limited number of rooms at The Sheraton New York Times Square Hotel for our special room rate of $299 before tax. <a href=\"http://go.spark-summit.org/e1t/c/*W6stDzJ6_3DYhW6Y-qp35L8r5j0/*W2lv3pg1m0SXqW4fsBfK1dKPp40/5/f18dQhb0S3j46_HXFDW12hNB01vYxXLVpfHHm73jjjdW6MZKRp8kYQKyW6bwr5V2Jd4M1W3h5NFK4xy3gfW933vZR4B_cGsN1KnR8gwLv_-W1DxTjd45S_ygW1pRXdM4dTysDW5ptRNz8bX804W1FqhSS7yr9hMW2Qmqdx3ZSHV4W6dlvKs8-5BS2W6QFrX65V9GJqW14Pw-v594Qm7W1xYXmQ3xbLR-W53SHFb1BfnT3W4kKG2n7V5_1rW4Wv2Qf8B5-KLW27bJHq5k2HT6W1gNYDn3TWG9kW7WBmJP7S7Z5yW8qnJcF4-vRv-VL1njb83HYBBW3nH3Z-2Hnv0gW1Tc36l2Fn5vxV15QbM3wGgM5W95qxHK89gczRW3dZ_zP5yD0HkVM1N875mkvTtW51fN7d5frBc_VzXCkp9gPdlNW8yXZNd6p0vyrW6BcvqD8WJFKZN28lM2btTJTDW2VlSjj18-TvYVBbZ9W20L_wSW4lGlJL1lB3xrW72-43R2lK-02W1YJD248-TkxZW81nB_L2BHhTjW7QkGZm70nCd3V1vGks8X7pkdW6Hn_tg6YLnVwW7QSNVK45829wW2mQr6p1dRrZsW96m-QD7_cYKLW8C0HL_9h7w6PW7j4FS95v2pJxW1KdYFP95c1ZcW7fMghf2hPWpWVgcGJ_9ltkzW103\" target=\"_blank\">Book now</a> to take advantage of this great low rate!\n\nLooking forward to seeing you in New York City in March!\n\n<strong>Quick Links</strong>\n\nEvent registration - <a href=\"http://go.spark-summit.org/e1t/c/*W6stDzJ6_3DYhW6Y-qp35L8r5j0/*N5H7Ql72mslMW2L-RmB1DfTcP0/5/f18dQhb0Sq5x8YHttqW8HLj0x5VQHw7W6bFhBV6P7FhxW4R4FNs57mvC2W1BQYgg4P0TLvW85Q81T83G7d1W9dtj1h7NQNCqW4zWTRG33KX8CW8Z0ByM7Mzn-vW5mNLNv7bqTzMW7vp5SX8bKv1lVprvP-3GYwqtN3Mdkt7ZThmmW4DqHqz6dgmw3W4yv2r03LmJLKW20Y98h1YfY94W72DxTp5RpkcdW3bqp8H9grnZZW2hBQwB1MqhJKW7ldyjx608MF6W772f_V7ldNWMW7zLHZZ65MGdKW83L8wq2kXv5mW1R-HQ87h6pcvW1vv0l17q6mr1W1sG9Y4602Fx-W1YSSpz88rdPbW88nwfg7WC8gkW1kmcBl6YyRMsW62VQ_x1H6fX9W19thHC1hs0BcW3fhDGL7YzgyfW69VM4M7VRPscW88nBch1D7RCnW8lTbVb3m23cDN2cKpZKVR6H2W4q1Pz560-7fXW1SdD8T8-hl8QN1RQf8zJp0X-W2pSCpp7z2LRRW5hfQL48pwqMdW8yzslQ6JrKTyVM7W7n91cJF_W4HyTJf7v9ypmW70Vxgn2_GCTjf4313mb04\" target=\"_blank\">http://</a><a href=\"http://go.spark-summit.org/e1t/c/*W6stDzJ6_3DYhW6Y-qp35L8r5j0/*W23pWJx8ss1hdW8RwgMl99npjz0/5/f18dQhb0Sq5x8YHttqW8HLj0x5VQHw7W6bFhBV6P7FhxW4R4FNs57mvC2W1BQYgg4P0TLvW85Q81T83G7d1W9dtj1h7NQNCqW4zWTRG33KX8CW8Z0ByM7Mzn-vW5mNLNv7bqTzMW7vp5SX8bKv1lVprvP-3GYwqtN3Mdkt7ZThmmW4DqHqz6dgmw3W4yv2r03LmJLKW20Y98h1YfY94W72DxTp5RpkcdW3bqp8H9grnZZW2hBQwB1MqhJKW7ldyjx608MDxW2d4JJy6XdSmDW61RmwL7MJZ0QW7p239b2fT730W29RM3l21W3nyW7x3b_675fKF9W692b_h7TG4tqW7CGkhm1xykylW1PqznQ1P_kY3W1Y-3L67GFk2vW7l6Y_y88gLfmW6bPh4S22Yg87W82mwwh1ylJvdW61TWj01jJ0j1W83Dxlz1FtSddW8lTbVb3m23cDN2cKpZKVR6H2W4q1Pz560-7fXW1SdD8T8-hl8QN1RQf8zJp0X-W2pSCpp7z2LRRW5hfQL48pwqMdW8yzslQ6JrKTyVM7W7n91cJF_W4HyTJf7v9ypmW70Vxgn723xGpf4313mb04\" target=\"_blank\">spark-summit.org/east/<wbr />2015/register</a>\n\nHotel Booking - <a href=\"http://go.spark-summit.org/e1t/c/*W6stDzJ6_3DYhW6Y-qp35L8r5j0/*W6Y2Wq17n_LkBN4SPl45L56C80/5/f18dQhb0SmhT9dZyDgW80lLdp2qwv27W328Kjm4c9pPpMf5bb0XD6prW39DrXh50CtBwW2ykK417cFHF9W7fD2Fy7b_l6_W51frwm6bprKDW6PZb3m51TTwjW7m1FTD90GnbpW1pyc7w2zqhn2W2J56SM8p_3qgW3TsvCz69_P_QN3WkGFKdpNTKW1wZrc07fZrRSW6HnC4g1qf-l2W4X9S6P61SSZmW7mG7sD51vX4yW6HC4dq50QxN4N31qRN0NL6l8W8rBSk135V7DfVQJJn63Lqnm_W8q5FTl4K45-LW328h7y3_khN2W6cR12t4r1Pm7W1nq9rt2kJDdkW3NnV4F1nJpwvW26pdfj35ybjLW8pT83J95C60gV_QY2T3JZpwvW28cVfq2xqVJWW3M1q1-5H6VqbVdNdB_2zqh3dW5G07VT8nhPm4W3LcghS5qZdnHW8qT0W_4YjHwsVf6gKB97zps7W64JxFY4RDBl7W3HsWnh3b4_jCMVwyQbZvDm-W3NGMyj3PHjmtW6TfzdD6QP4KgW12mv4D1Bh91kW4YxKrk4PxC8jW448lxL4fPG21W3LRdbK3skj8NW3M0DBZ3CBqdlN7ly8-WMc6kxW8j1W00976FWKW26nf0w3XTTJlW7Dk8ps5lH1hkW7PSXQP4dLPdTW2QYgDg2-1mjy102\" target=\"_blank\">https://www.starwoodmeeting.<wbr />com/Book/Spark</a>\n\nConference Agenda -\u00a0<a href=\"http://go.spark-summit.org/e1t/c/*W6stDzJ6_3DYhW6Y-qp35L8r5j0/*W5fq7JB9gVPcPN8Xsb_zvDv3w0/5/f18dQhb0Sq5z8YHrDTW8HLj0x5VQHw7W6bFhBV6P7FhxW4R4BZM57mvC2W1BQYgg4P0TLvW85Q81T83G7d1W9dtj1h7NQNCqW4zWTRG33K-8nW7NMj-x9bTNXYW954KlM4P0Yt6W2d4hSK3bWrh8W2YH1kR47xfHKW2HRyfR6trFPNW47YlYy4bfcHbW47Xx4z3C811XW4-SZvb2KQ2YYW3_VZwP5ThdHgW3s1XjF51G0BJW4Zh8Y-57-WqMW3H_Pty2DzCtRW1zBkVT4_thNbVth64v2MKKkXW5RQwcK3GBthPN78zTvQJ9typW6zst6C2LLg9DW5SqQzd9dSsrYW3y994M2XrNDyW18TYqG4FJR9JW54R4Yw5dxn7bW6CH7QT1TnnNTW3nF65-3PcqzMW5m_k0n7pHv4mW3P4Qnj5y7KV8W880J-m5gnb8LM72-Yn6Q977Vb98_P6s16_2W8Ph58-59BvQ0W7y34GD1FmQY-W7r71Hq2PhWHMW7tprCG95RqNQW2j-Sgt2L5GhqW3G6xft6TMH99W6-cC_w3wXTtZW6Sytzy9fTwQmN3FYx-Q_Hpp4f3SbMCL11\" target=\"_blank\">http://spark-summit.org/<wbr />east/2015/agenda</a>\n\n<strong>Important Dates: </strong>\n\n<span class=\"aBn\" tabindex=\"0\" data-term=\"goog_929332805\"><span class=\"aQJ\">February 16, 2015</span></span>: Last day for hotel discount\n\n<span class=\"aBn\" tabindex=\"0\" data-term=\"goog_929332806\"><span class=\"aQJ\">March 18, 2015</span></span>: Keynotes and Sessions\n\n<span class=\"aBn\" tabindex=\"0\" data-term=\"goog_929332807\"><span class=\"aQJ\">March 19, 2015</span></span>: Workshops\n\nWe look forward to seeing you all there! Let us know you are coming and stay connected! <strong>#SparkSummitEast</strong>"}
{"status": "publish", "description": null, "creator": "michael", "link": "https://databricks.com/blog/2015/02/02/an-introduction-to-json-support-in-spark-sql.html", "authors": ["Yin Huai (Databricks)"], "id": 2376, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2015-02-02", "tz": "UTC", "createdOn": "2015-02-02"}, "title": "An introduction to JSON support in Spark SQL", "slug": "an-introduction-to-json-support-in-spark-sql", "content": "[sidenote]Note: Starting Spark 1.3, SchemaRDD will be renamed to DataFrame.[/sidenote]\n\n<hr />\n\nIn this blog post, we introduce Spark SQL\u2019s JSON support, a feature we have been working on at Databricks to make it dramatically easier to query and create JSON data in Spark. With the prevalence of web and mobile applications, JSON has become the de-facto interchange format for web service API\u2019s as well as long-term storage. With existing tools, users often engineer complex pipelines to read and write JSON data sets within analytical systems. Spark SQL\u2019s JSON support, released in Apache Spark\u00a01.1 and enhanced in Apache Spark 1.2, vastly simplifies the end-to-end-experience of working with JSON data.<!--more-->\n<h2>Existing practices</h2>\nIn practice, users often face difficulty in manipulating JSON data with modern analytical systems. To write a dataset to JSON format, users first need to write logic to convert their data to JSON. To read and query JSON datasets, a common practice is to use an ETL pipeline to transform JSON records to a pre-defined structure. In this case, users have to wait for this process to finish before they can consume their data. For both writing and reading, defining and maintaining schema definitions often make the ETL task more onerous, and eliminate many of the benefits of the semi-structured JSON format. If users want to consume fresh data, they either have to laboriously define the schema when they create external tables and then use a custom JSON serialization/deserialization library, or use a combination of JSON UDFs to query the data.\n\nAs an example, consider a dataset with following JSON schema:\n<pre>{\"name\":\"Yin\", \"address\":{\"city\":\"Columbus\",\"state\":\"Ohio\"}}\n{\"name\":\"Michael\", \"address\":{\"city\":null, \"state\":\"California\"}}</pre>\nIn a system like Hive, the JSON objects are typically stored as values of a single column. To access this data, fields in JSON objects are extracted and flattened using a UDF. In the SQL query shown below, the outer fields (name and address) are extracted and then the nested address field is further extracted.\n\n<em>In the following example it is assumed that the JSON dataset shown above is stored in a table called people and JSON objects are stored in the column called jsonObject.</em>\n<pre>SELECT\n  v1.name, v2.city, v2.state \nFROM people\n  LATERAL VIEW json_tuple(people.jsonObject, 'name', 'address') v1 \n     as name, address\n  LATERAL VIEW json_tuple(v1.address, 'city', 'state') v2\n     as city, state;</pre>\n<h2>JSON support in Spark SQL</h2>\nSpark SQL provides a natural syntax for querying JSON data along with automatic inference of JSON schemas for both reading and writing data. Spark SQL understands the nested fields in JSON data and allows users to directly access these fields without any explicit transformations. The above query in Spark SQL is written as follows:\n<pre>SELECT name, age, address.city, address.state FROM people</pre>\n<h3>Loading and saving JSON datasets in Spark SQL</h3>\nTo query a JSON dataset in Spark SQL, one only needs to point Spark SQL to the location of the data. The schema of the dataset is inferred and natively available without any user specification. In the programmatic APIs, it can be done through jsonFile and jsonRDD methods provided by SQLContext. With these two methods, you can create a SchemaRDD for a given JSON dataset and then you can register the SchemaRDD as a table. Here is an example:\n<pre>// Create a SQLContext (sc is an existing SparkContext)\nval sqlContext = new org.apache.spark.sql.SQLContext(sc)\n// Suppose that you have a text file called people with the following content:\n// {\"name\":\"Yin\", \"address\":{\"city\":\"Columbus\",\"state\":\"Ohio\"}}\n// {\"name\":\"Michael\", \"address\":{\"city\":null, \"state\":\"California\"}}\n// Create a SchemaRDD for the JSON dataset.\nval people = sqlContext.jsonFile(\"[the path to file people]\")\n// Register the created SchemaRDD as a temporary table.\npeople.registerTempTable(\"people\")</pre>\nIt is also possible to create a JSON dataset using a purely SQL API. For instance, for those connecting to Spark SQL via a JDBC server, they can use:\n<pre>CREATE TEMPORARY TABLE people\n    USING org.apache.spark.sql.json\n    OPTIONS (path '[the path to the JSON dataset]')</pre>\nIn the above examples, because a schema is not provided, Spark SQL will automatically infer the schema by scanning the JSON dataset. When a field is JSON object or array, Spark SQL will use STRUCT type and ARRAY type to represent the type of this field. Since JSON is semi-structured and different elements might have different schemas, Spark SQL will also resolve conflicts on data types of a field. To understand what is the schema of the JSON dataset, users can visualize the schema by using the method of printSchema() provided by the returned SchemaRDD in the programmatic APIs or by using DESCRIBE [table name] in SQL. For example, the schema of people visualized through people.printSchema() will be:\n<pre>root\n |-- address: struct (nullable = true)\n |    |-- city: string (nullable = true)\n |    |-- state: string (nullable = true)\n |-- name: string (nullable = true)</pre>\nOptionally, a user can apply a schema to a JSON dataset when creating the table using jsonFile and jsonRDD. In this case, Spark SQL will bind the provided schema to the JSON dataset and will not infer the schema. Users are not required to know all fields appearing in the JSON dataset. The specified schema can either be a subset of the fields appearing in the dataset or can have field that does not exist.\n\nAfter creating the table representing a JSON dataset, users can easily write SQL queries on the JSON dataset just as they would on regular tables. As with all queries in Spark SQL, the result of a query is represented by another SchemaRDD. For example:\n<pre>val nameAndAddress = sqlContext.sql(\"SELECT name, address.city, address.state FROM people\")\nnameAndAddress.collect.foreach(println)</pre>\nThe result of a SQL query can be used directly and immediately by other data analytic tasks, for example a machine learning pipeline. Also, JSON datasets can be easily cached in Spark SQL\u2019s built in in-memory columnar store and be save in other formats such as Parquet or Avro.\n<h3>Saving SchemaRDDs\u00a0as JSON files</h3>\nIn Spark SQL, SchemaRDDs can be output in JSON format through the toJSON method. Because a SchemaRDD always contains a schema (including support for nested and complex types), Spark SQL can automatically convert the dataset to JSON without any need for user-defined formatting. SchemaRDDs can themselves be created from many types of data sources, including Apache Hive tables, Parquet files, JDBC, Avro file, or as the result of queries on existing SchemaRDDs. This combination means users can migrate data into JSON format with minimal effort, regardless of the origin of the data source.\n<h2>What's next?</h2>\nThere are also several features in the pipeline that with further improve Spark SQL's support for semi-structured JSON data.\n<h3>Improved SQL API support to read/write JSON datasets</h3>\nIn Apache Spark 1.3, we will introduce improved JSON support based on the new data source API for reading and writing various format using SQL. Users can create a table from a JSON dataset with an optional defined schema like what they can do with jsonFile and jsonRDD. Also, users can create a table and ask Spark SQL to store its rows in JSON objects. Data can inserted into this table through SQL. Finally, a CREATE TABLE AS SELECT statement can be used to create such a table and populate its data.\n<h3>Handling JSON datasets with a large number of fields</h3>\nJSON data is often semi-structured, not always following a fixed schema. In the future, we will expand Spark SQL\u2019s JSON support to handle the case where each object in the dataset might have considerably different schema. For example, consider a dataset where JSON fields are used to hold key/value pairs representing HTTP headers. Each record might introduce new types of headers and using a distinct column for each one would produce a very wide schema. We plan to support auto-detecting this case and instead use a Map type. Thus, each row may contain a Map, enabling querying its key/value pairs. This way, Spark SQL will handle JSON datasets that have much less structure, pushing the boundary for the kind of queries SQL-based systems can handle.\n\n<em>To try out these new Spark features,\u00a0<a href=\"https://databricks.com/try-databricks\">get a free trial of Databricks or use the Community Edition</a>.</em>"}
{"status": "publish", "description": null, "creator": "Xiangrui", "link": "https://databricks.com/blog/2015/01/28/introducing-streaming-k-means-in-spark-1-2.html", "authors": ["Jeremy Freeman (Howard Hughes Medical Institute)"], "id": 2382, "categories": ["Apache Spark", "Engineering Blog", "Streaming"], "dates": {"publishedOn": "2015-01-28", "tz": "UTC", "createdOn": "2015-01-28"}, "title": "Introducing streaming k-means in Apache Spark 1.2", "slug": "introducing-streaming-k-means-in-spark-1-2", "content": "Many real world data are acquired sequentially over time, whether messages from social media users, time series from wearable sensors, or \u2014 in a case we are particularly excited about \u2014 the firing of large populations of neurons. In these settings, rather than wait for all the data to be acquired before performing our analyses, we can use streaming algorithms to identify patterns over time, and make more targeted predictions and decisions.\n\nOne simple strategy is to build machine learning models on static data, and then use the learned model to make predictions on an incoming data stream. But what if the patterns in the data are themselves dynamic? That's where streaming algorithms come in.\n\nA key advantage of Apache Spark is that its machine learning library (MLlib) and its library for stream processing (Spark Streaming) are built on the same core architecture for distributed analytics. This facilitates adding extensions that leverage and combine components in novel ways without reinventing the wheel. We have been developing a family of streaming machine learning algorithms in Spark within MLlib. In this post we describe streaming k-means clustering, included in the recently released Apache Spark 1.2.\n<h2>Algorithm</h2>\nThe goal of k-means is to partition a set of data points into k clusters. The now classic k-means algorithm \u2014 developed by Stephen Lloyd in the 1950s for efficient digital quantization of analog signals \u2014 iterates between two steps. First, given an initial set of k cluster centers, we find which cluster each data point is closest to. Then, we compute the average of each of the new clusters and use the result to update our cluster centers. At each of these steps \u2014 re-assigning and updating \u2014 we are making the points within each cluster more and more similar to one another (more formally, we are in both steps shrinking the within-cluster-sum-of-squares). By iterating between these two steps repeatedly, we can usually converge to a good solution.\n[breadcrumb slug=\"image-1\"]\n<p align=\"center\"><img src=\"https://databricks.com/wp-content/uploads/2015/01/image04.gif\" alt=\"\" width=\"85%\" /></p>\nIn the streaming setting, our data arrive in batches, with potentially many data points per batch. The simplest extension of the standard k-means algorithm would be to begin with cluster centers \u2014 usually random locations, because we haven't yet seen any data \u2014 and for each new batch of data points, perform the same two-step operation described above. Then, we use the new centers to repeat the procedure on the next batch. Above is a movie showing the behavior of this algorithm for two-dimensional data streaming from three clusters that are slowly drifting over time. The centers track the true clusters and adapt to the changes over time.\n<h2>Forgetfulness</h2>\nIf the source of the data is constant \u2014 the same three clusters forever \u2014 the above streaming algorithm will converge to a similar solution as if k-means was run offline on the entire accumulated data set. In fact, in this case the streaming algorithm is identical to a well-known offline k-means algorithm, \u201cmini-batch\u201d k-means, which repeatedly trains on random subsets of the data to avoid loading the entire data set into memory.\n\nHowever, what if the sources of data are changing over time? How can we make our model reflect those changes?\n\nFor this setting, we have extended the algorithm to support <b>forgetfulness</b>, allowing the model to adapt to changes over time. The key trick is to add a new parameter that balances the relative importance of new data versus past history. One setting of this parameter will be equivalent to the scenario described above, where all data from the beginning of time are treated equally. At the other extreme, only the most recent data will be used. Settings in between will combine the present with a partial reflection of the past. Here is an animation showing two settings of this forgetfulness parameter, in streams where the centers change half-way through. Watch how the cluster centers quickly adjust to the new locations in the second case, but take a while to shift in the first.\n[breadcrumb slug=\"image-2\"]\n<p align=\"center\"><img src=\"https://databricks.com/wp-content/uploads/2015/01/image03.gif\" alt=\"\" width=\"85%\" />\n<img src=\"https://databricks.com/wp-content/uploads/2015/01/image00.gif\" alt=\"\" width=\"85%\" /></p>\nWith the appropriate setting of the parameter, we can have cluster centers that smoothly adapt to dynamic changes in the data. In this animation, watch five clusters drift over time, and the centers track them.<strong><strong>\u00a0</strong></strong>\n<p align=\"center\"><img src=\"https://databricks.com/wp-content/uploads/2015/01/image02.gif\" alt=\"\" width=\"85%\" /></p>\nMathematically, forgetfulness amounts to adding an extra parameter to the <b>update rule</b>: the equation describing how to update centers given a new batch of data. However, as a scalar value between 0 to 1, it is not a particularly intuitive parameter. So instead, we expose a <b>half-life</b>, which describes the time it takes before past data contributes to only one half of the current model. To demonstrate, we\u2019ll use a one-dimensional version of the examples above. We start with data drawn from two clusters, and then switch to data from two different clusters. The half life determines how many batches it will take for the contribution from the initial set of points to reduce to half. You can see the effect of changing the half-life in the time it takes for the clusters adjust. With a half-life of 0.5 batches the change finishes in about 1 batch, but with a half-life of 5 it takes about 10 batches.\n[breadcrumb slug=\"image-3\"]\n<p align=\"center\"><img src=\"https://databricks.com/wp-content/uploads/2015/01/image05.gif\" alt=\"\" width=\"85%\" />\n<img src=\"https://databricks.com/wp-content/uploads/2015/01/image01.gif\" alt=\"\" width=\"85%\" /></p>\nUsers may want to think about their half-life in terms of either the number of batches (which have a fixed duration in time), or the number of points. If you have 1000 data points in one batch and 10 in the other, perhaps you want those 1000 to have a proportionately larger impact. On the other hand, you might want to remain stable across fluctuations in data points, and instead treat all periods of time equally. To solve this, we've introduced the concept of a <b>time unit</b> that can be specified as either batches or points. Given a user-specified half life and time unit, the algorithm automatically calculates the appropriate forgetfulness behavior.\n\nA final feature included is a check to eliminate <strong>dying</strong> clusters. If there is a dramatic change in the data generating process, one of the estimated clusters may suddenly be far from any data, and stay stuck in its place. To prevent this scenario, clusters are checked for such behavior on each batch. A cluster detected as dying is eliminated, and the largest cluster is split in two. In this one-dimensional demo, two clusters are initially far apart, but then one changes to be much closer to the other. At first the incorrect cluster persists (top line), but soon it disappears, and the other cluster splits to correctly lock on to the new cluster centers.\n[breadcrumb slug=\"image-4\"]\n<p align=\"center\"><img src=\"https://databricks.com/wp-content/uploads/2015/01/7-cluster-dying.gif\" alt=\"\" width=\"85%\" /></p>\n\n<h2>Getting started</h2>\nTo get started using streaming k-means yourself, <a href=\"http://spark.apache.org/downloads.html\" target=\"_blank\">download Apache Spark 1.2</a> today, read more about <a href=\"http://spark.apache.org/docs/latest/mllib-clustering.html\" target=\"_blank\">streaming k-means in the Apache Spark 1.2 documentation</a>, and try the <a href=\"https://github.com/apache/spark/blob/branch-1.2/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala\" target=\"_blank\">example code</a>. To generate your own visualizations of streaming clustering like the ones shown here, and explore the range of settings and behaviors, check out the code in the <a href=\"http://spark-packages.org/package/28\" target=\"_blank\">spark-ml-streaming</a> package.\n<h2>Looking forward</h2>\nMany algorithms and analyses can benefit from streaming implementations. Along with streaming linear regression (as of 1.1) and streaming clustering (as of 1.2), we plan to add streaming versions of of factorization and classification in future releases, incorporate them into the new Python Streaming API, and use our new forgetfulness parameterization across the algorithms as a unified way to control dynamic model updating.\n\nSpecial thanks to Xiangrui Meng, Tathagata Das, and Nicholas Sofroniew (for work on algorithm development) and Matthew Conlen (for visualizations)."}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2015/02/05/apache-spark-selected-for-infoworld-2015-technology-of-the-year-award.html", "authors": ["Dave Wang (Databricks)"], "id": 2454, "categories": ["Announcements", "Company Blog"], "dates": {"publishedOn": "2015-02-05", "tz": "UTC", "createdOn": "2015-02-05"}, "title": "Apache Spark selected for Infoworld 2015 Technology of the Year Award", "slug": "apache-spark-selected-for-infoworld-2015-technology-of-the-year-award", "content": "Recently <a href=\"http://www.infoworld.com/article/2871935/application-development/infoworlds-2015-technology-of-the-year-award-winners.html\" target=\"_blank\">Infoworld unveiled the 2015 Technology of the Year Award winners</a>, which range from open source software to stellar consumer technologies like the iPhone. \u00a0Being the <a title=\"Announcing Spark 1.2\" href=\"https://databricks.com/blog/2014/12/19/announcing-spark-1-2.html\" target=\"_blank\">creators behind Apache Spark</a>, Databricks is thrilled to see Spark in their ranks. \u00a0In fact, we built our flagship product, <a title=\"Databricks Cloud Overview\" href=\"https://databricks.com/product\">Databricks</a>, on top of Spark with the ambition to revolutionize big data processing in ways similar to how iPhone revolutionized the mobile experience.\n\nThe iPhone was revolutionary in a number of ways: first, it integrated a disparate set of consumer electronic capabilities such as mobile phone, camera, GPS, and even laptop; second, it created a seamless experience navigating these capabilities with iOS; lastly, it was easily extensible through 3rd party applications, which gave rise to a whole ecosystem of products built around the iPhone. \u00a0In short, the iPhone is <i>the</i> simple and powerful platform to meet all the mobile needs of users.\n\nDatabricks is precisely the analogous product in the big data world.\n\nFirst, Databricks unifies disparate functionalities: It runs 100% open source Spark, which is a lightning fast big data general processing engine that includes a broad set of standard libraries. \u00a0This means a user will have access to lightning fast performance up to 100x faster than Hadoop MapReduce both in memory and on disk, and also have capabilities such as <a title=\"An introduction to JSON support in Spark SQL\" href=\"https://databricks.com/blog/2015/02/02/an-introduction-to-json-support-in-spark-sql.html\" target=\"_blank\">SQL</a>, <a title=\"ML Pipelines: A New High-Level API for MLlib\" href=\"https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html\" target=\"_blank\">machine learning</a>, <a title=\"Mining Ecommerce Graph Data with Spark at Alibaba Taobao\" href=\"https://databricks.com/blog/2014/08/14/mining-graph-data-with-spark-at-alibaba-taobao.html\" target=\"_blank\">graph processing</a> out of the box. \u00a0Users can get all the big data processing tools through Spark instead of integrate disparate tools.\n\nSecond, Databricks also includes features that makes deploying and working with Spark much simpler. \u00a0The cluster manager allows users to create, modify, and teardown Spark clusters in their Amazon Virtual Private Cloud (VPC) in a few clicks. \u00a0The interactive workspace allows users to easily visualize data and share results with colleagues. \u00a0The job scheduler allows production data processing pipelines to be built in a matter of minutes. \u00a0Through Databricks, users can have a seamless experience working with big data - whether it\u2019s interactive exploration or batch processing.\n\nLastly, Databricks provides common connectors to allow 3rd party applications, such as <a title=\"Application Spotlight: Tableau Software\" href=\"https://databricks.com/blog/2014/10/15/application-spotlight-tableau-software.html\" target=\"_blank\">Tableau</a> and many of the common BI tools used by corporations today, to seamlessly integrate. \u00a0This allows non-technical users to gain access to big data, and further increases the usability of Databricks.\n\nWe built Databricks to make big data simple: easy to use, work at lightning fast speed, and provide instant results for all big data processing needs. \u00a0As Spark gains momentum, many are finding that Databricks is the best way to run Spark due to its rapid deployment, seamless user experience, and performance. \u00a0If you would like to try out Databricks, simply <a href=\"https://databricks.com/registration\" target=\"_blank\">sign up</a> for a trial!"}
{"status": "publish", "description": null, "creator": "patrick", "link": "https://databricks.com/blog/2014/12/19/announcing-spark-1-2.html", "authors": ["Patrick Wendell"], "id": 2471, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2014-12-19", "tz": "UTC", "createdOn": "2014-12-19"}, "title": "Announcing Apache Spark 1.2", "slug": "announcing-spark-1-2", "content": "We at Databricks are thrilled to announce the release of Apache Spark 1.2! Apache Spark 1.2 introduces many new features along with scalability, usability and performance improvements. This post will introduce some key features of Apache Spark 1.2 and provide context on the priorities of Spark for this and the next release. In the next two weeks, we\u2019ll be publishing blog posts with more details on feature additions in each of the major components. Apache Spark 1.2 has been posted today on the <a href=\"http://spark.apache.org/releases/spark-release-1-2-0.html\">Apache Spark website</a>.\n\nLearn more\u00a0about specific new features\u00a0in related in-depth\u00a0posts:\n<ul>\n \t<li><a title=\"Spark SQL Data Sources API: Unified Data Access for the Spark Platform\" href=\"https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html\" target=\"_blank\">Spark SQL data sources API</a></li>\n \t<li><a title=\"An introduction to JSON support in Spark SQL\" href=\"https://databricks.com/blog/2015/02/02/an-introduction-to-json-support-in-spark-sql.html\" target=\"_blank\">JSON support in Spark SQL</a><a title=\"Introducing streaming k-means in Spark 1.2\" href=\"https://databricks.com/blog/2015/01/28/introducing-streaming-k-means-in-spark-1-2.html\" target=\"_blank\">\u00a0</a></li>\n \t<li><a title=\"ML Pipelines: A New High-Level API for MLlib\" href=\"https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html\" target=\"_blank\">ML pipeline API</a></li>\n \t<li><a title=\"Introducing streaming k-means in Spark 1.2\" href=\"https://databricks.com/blog/2015/01/28/introducing-streaming-k-means-in-spark-1-2.html\" target=\"_blank\">Streaming k-means</a></li>\n \t<li><a title=\"Random Forests and Boosting in MLlib\" href=\"https://databricks.com/blog/2015/01/21/random-forests-and-boosting-in-mllib.html\" target=\"_blank\">Random forests and GBTs</a></li>\n \t<li><a title=\"Improved Fault-tolerance and Zero Data Loss in Spark Streaming\" href=\"https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html\" target=\"_blank\">Improved fault tolerance in Spark streaming</a></li>\n</ul>\n<h2>Optimizations in Spark's core engine</h2>\nApache Spark 1.2 includes several cross-cutting optimizations focused on performance for large scale workloads. Two new features Databricks developed for our <a href=\"https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html\">world record\u00a0petabyte sort</a> with Spark are turned on by default in Apache Spark 1.2. The first is a re-architected network transfer subsystem that exploits Netty 4\u2019s zero-copy IO and off heap buffer management. The second is Spark's sort based shuffle implementation, which we've now made the default after significant testing in Apache Spark 1.1. Together, we've seen these features give as much as 5X\u00a0performance improvement\u00a0for\u00a0workloads with very large shuffles.\n<h2>Spark SQL data sources and Hive 13</h2>\nUntil now, Spark SQL has supported accessing any data described in an Apache Hive metastore, along with a small number of native bindings for popular formats such as Parquet and JSON. This release introduces a standard API for native integration with other file formats and storage systems. The API supports low level optimizations such as predicate pushdown and direct access to Spark SQL\u2019s table catalog. Any data sources written for this API are automatically queryable in Java, Scala and Python. At Databricks, we\u2019ve released an <a href=\"https://github.com/databricks/spark-avro\">Apache Avro connector</a> based on this API (itself requiring less than 100 lines of code), and we expect several other connectors to appear in the coming months from the community. Using the input API is as simple as listing the desired format:\n\n<em>Creating a Parquet Table</em><em>Creating a JSON Table</em>\n<table class=\"table\">\n<thead></thead>\n<tbody>\n<tr>\n<td width=\"50%\">\n\n<pre>CREATE TEMPORARY TABLE\n  users_parquet\nUSING\n  org.apache.spark.sql.parquet\nOPTIONS\n  (path 'hdfs://parquet/users');</pre>\n\n</td>\n<td width=\"50%\">\n\n<pre>CREATE TEMPORARY TABLE\n  users_json\nUSING\n  org.apache.spark.sql.json\nOPTIONS\n  (path 'hdfs://json/users');</pre>\n\n</td>\n</tr>\n</tbody>\n</table>\nNote that neither the schema nor the partitioning layout is specified here. Spark SQL is able to learn that automatically in both cases. For users reading data from Hive tables, we've bumped our support to Hive 0.13 and included support for its fixed-precision decimal type.\n<h2>Spark Streaming H/A and Python API</h2>\nIn this release, Spark Streaming adds a full H/A mode that uses a persistent Write Ahead Log (WAL) to provide recoverability for input sources if nodes crash. This feature removes any single-point-of-failure from Spark Streaming, a common request from production Spark Streaming users. The WAL mechanism is supported out-of-the-box for Apache Kafka, and the more general API for third-party connectors has been extended with durability support. In addition, this release adds a Python API for Spark Streaming, letting you create and transform streams entirely in Python.\n<h2>Machine learning pipelines</h2>\nWe've extended Spark's machine learning library with a new, higher-level API for constructing pipelines, in the <code>spark.ml</code> package. In practice, most machine learning workflows involve multiple preprocessing and\u00a0featurization\u00a0steps, as well as training and evaluating multiple models. The ML pipelines API provides first-class support for these types of pipelines, including the ability to search for parameters and automatically score models. It is modeled after high-level machine learning libraries like SciKit-Learn, and brings the same ease of use to learning on big data.\n<table class=\"table\">\n<thead>\n<tr>\n<th><em>Defining a three-stage ML pipeline</em></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\n\n<pre>val tokenizer = new Tokenizer()\n  .setInputCol(\"text\")\n  .setOutputCol(\"words\")\nval hashingTF = new HashingTF()\n  .setInputCol(tokenizer.getOutputCol)\n  .setOutputCol(\"features\")\nval lr = new LogisticRegression().setMaxIter(10)\nval pipeline = new Pipeline()\n  .setStages(Array(tokenizer, hashingTF, lr))</pre>\n\n</td>\n</tr>\n</tbody>\n</table>\nThe new ML API is experimental in Apache Spark 1.2 as we get feedback from users, but will be stabilized in 1.3.\n<h2>Stable GraphX API</h2>\nThe GraphX project graduates from alpha in this release, providing a stable API. This means applications written against GraphX can be safely migrated to future Spark 1.X versions without code changes. Coinciding with API stabilization, a handful of issues have been fixed which affect very large scale and highly iterative graphs seen in production workloads.\n\n<hr />\n\nThis post only scratches the surface of interesting features in Apache Spark 1.2. Overall, this release contains more than 1000 patches from 172 contributors<b>\u00a0</b>making it our largest yet despite a year of tremendous growth. Head over to the <a href=\"http://spark.apache.org/releases/spark-release-1-2-0.html\">official release notes</a> to learn more about this release, and watch the Databricks blog for more detailed posts about the major features in the next few days!"}
{"status": "publish", "description": null, "creator": "Xiangrui", "link": "https://databricks.com/blog/2014/12/22/announcing-spark-packages.html", "authors": ["Xiangrui Meng", "Patrick Wendell"], "id": 2472, "categories": ["Apache Spark", "Ecosystem", "Engineering Blog"], "dates": {"publishedOn": "2014-12-22", "tz": "UTC", "createdOn": "2014-12-22"}, "title": "Announcing Apache Spark Packages", "slug": "announcing-spark-packages", "content": "Today, we are happy to announce\u00a0<em>Apache Spark Packages</em>\u00a0(<a title=\"http://spark-packages.org\" href=\"http://spark-packages.org\">http://spark-packages.org</a>), a community package index to track the growing number of\u00a0open source\u00a0packages and libraries that work with Apache Spark. <em>Spark Packages</em> makes it easy for users to find, discuss, rate, and install packages for any version of Spark, and makes it easy for developers to contribute packages.\n\n<!--more-->\n\n<em>Spark Packages</em> will feature\u00a0integrations with various\u00a0data sources, management tools, higher level domain-specific libraries, machine learning algorithms, code samples, and other Spark content. Thanks to the package authors, the initial listing of packages includes <a href=\"http://spark-packages.org/package/6\">scientific computing libraries</a>, a <a href=\"http://spark-packages.org/package/10\">job execution server</a>,\u00a0a connector for <a href=\"http://spark-packages.org/package/3\">importing Avro data</a>, tools for launching Spark on <a href=\"http://spark-packages.org/package/9\">Google Compute Engine</a>, and many others. We expect this list to grow substantially in 2015, and to help fuel this growth we\u2019re continuing to invest in extension points to Spark such as the Spark SQL data sources API, the Spark streaming Receiver API, and the Spark ML pipeline API. Package authors who submit a listing retain full rights to your code, including your choice of open-source license.\n\nPlease give <em>Spark Packages</em> a try and let us know if you have any questions when working with the site! We expect to extend the site in the coming months while also building mechanisms in Spark to make\u00a0using packages even easier. We hope <em>Spark Packages</em>\u00a0lets you find even more great ways to work with Spark."}
{"status": "publish", "description": null, "creator": "Xiangrui", "link": "https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html", "authors": ["Xiangrui Meng", "Joseph Bradley", "Evan Sparks (UC Berkeley)", "Shivaram Venkataraman (UC Berkeley)"], "id": 2473, "categories": ["Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2015-01-07", "tz": "UTC", "createdOn": "2015-01-07"}, "title": "ML Pipelines: A New High-Level API for MLlib", "slug": "ml-pipelines-a-new-high-level-api-for-mllib", "content": "MLlib\u2019s goal is to make practical machine learning (ML) scalable and easy. Besides new algorithms and performance improvements that we have seen in each release, a great deal of time and effort has been spent on making MLlib <i>easy</i>. Similar to Spark Core, MLlib provides APIs in three languages: Python, Java, and Scala, along with user guide and example code, to ease the learning curve for users coming from different backgrounds. In Apache Spark 1.2, Databricks, jointly with AMPLab, UC Berkeley, continues this effort by introducing a pipeline API to MLlib for easy creation and tuning of practical ML pipelines.\n\nA practical ML pipeline often involves a sequence of data pre-processing, feature extraction, model fitting, and validation stages. For example, classifying text documents might involve text segmentation and cleaning, extracting features, and training a classification model with cross-validation. Though there are many libraries we can use for each stage, connecting the dots is not as easy as it may look, especially with large-scale datasets. Most ML libraries are not designed for distributed computation or they do not provide native support for pipeline creation and tuning. Unfortunately, this problem is often ignored in academia, and it has received largely ad-hoc treatment in industry, where development tends to occur in manual one-off pipeline implementations.\n\nIn this post, we briefly describe the work done to address ML pipelines in MLlib, a joint effort between Databricks and AMPLab, UC Berkeley, and inspired by the <a href=\"http://scikit-learn.org/\" target=\"_blank\">scikit-learn</a> project and some earlier work on <a href=\"https://amplab.cs.berkeley.edu/publication/mli-an-api-for-distributed-machine-learning/\" target=\"_blank\">MLI</a>.\n<h2>Dataset abstraction</h2>\nIn the new pipeline design, a dataset is represented by Spark SQL\u2019s SchemaRDD and an ML pipeline by a sequence of dataset transformations. <strong>(Update</strong>: SchemaRDD was renamed to DataFrame in Spark 1.3). \u00a0Each transformation takes an input dataset and outputs the transformed dataset, which becomes the input to the next stage. We leverage on Spark SQL for several reasons: data import/export, flexible column types and operations, and execution plan optimization.\n\nData import/export is the start/end point of an ML pipeline. \u00a0MLlib currently provides import/export utilities for several application-specific types: LabeledPoint for classification and regression, Rating for collaborative filtering, and so on. \u00a0However, realistic datasets may contain many types, such as user/item IDs, timestamps, or raw records. \u00a0The current utilities cannot easily handle datasets with combinations of these types, and they use inefficient text storage formats adopted from other ML libraries.\n\nFeature transformations usually form the majority of a practical ML pipeline. A feature transformation can be viewed as appending new columns created from existing columns. For example, text tokenization breaks a document up into a bag of words, and tf-idf converts a bag of words into a feature vector,\u00a0while during the transformations the labels need to be preserved for model fitting. More complex feature transformations are quite common in practice. Hence, the dataset needs to support columns of different types, including dense and sparse vectors, and operations that create new columns from existing ones.\n<p align=\"center\"><img src=\"https://databricks.com/wp-content/uploads/2015/01/pipeline-0.png\" alt=\"\" width=\"90%\" /></p>\nIn the example above, id, text, and words are carried over during transformations. They are unnecessary for model fitting, but useful in prediction and model inspection. It doesn\u2019t provide much information if the prediction dataset only contains the predicted labels. If we want to inspect the prediction results, e.g., checking false positives, it is quite useful to look at the predicted labels along with the raw input text and tokenized words. The columns needed at each stage are quite different. It would be ideal that the underlying execution engine can optimize for us and only load the required columns.\n\nFortunately, Spark SQL already provides most of the desired functions and we don\u2019t need to reinvent the wheel. Spark SQL supports import/export SchemaRDDs from/to Parquet, an efficient columnar storage format, and easy conversions between RDDs and SchemaRDDs. It also supports pluggable external data sources like Hive and <a href=\"http://spark-packages.org/package/3\" target=\"_blank\">Avro</a>. Creating (or declaring to be more precise) new columns from existing columns is easy with user-defined functions. The materialization of SchemaRDD is lazy. Spark SQL knows how to optimize the execution plan based on the columns requested, which fits our needs well. SchemaRDD supports standard data types. To make it a better fit for ML, we worked together with the Spark SQL team and added Vector type as a user-defined type that supports both dense and sparse feature vectors.\n\nWe show a simple Scala code example for ML dataset import/export and simple operations. More complete dataset examples in <a href=\"https://github.com/apache/spark/blob/branch-1.2/examples/src/main/scala/org/apache/spark/examples/mllib/DatasetExample.scala\" target=\"_blank\">Scala</a>\u00a0and\u00a0<a href=\"https://github.com/apache/spark/blob/branch-1.2/examples/src/main/python/mllib/dataset_example.py\" target=\"_blank\">Python</a> can be found under the `examples/` folder of the Spark repository. We refer users to <a href=\"http://spark.apache.org/docs/1.2.0/sql-programming-guide.html\" target=\"_blank\">Spark SQL\u2019s user guide</a> to learn more about SchemaRDD and the operations it supports.\n\n[scala]\nval sqlContext = SQLContext(sc)\nimport sqlContext._ // implicit conversions\n\n// Load a LIBSVM file into an RDD[LabeledPoint].\nval labeledPointRDD: RDD[LabeledPoint] =\n  MLUtils.loadLibSVMFile(&quot;/path/to/libsvm&quot;)\n\n// Save it as a Parquet file with implicit conversion\n// from RDD[LabeledPoint] to SchemaRDD.\nlabeledPointRDD.saveAsParquetFile(&quot;/path/to/parquet&quot;)\n\n// Load the parquet file back into a SchemaRDD.\nval dataset = parquetFile(&quot;/path/to/parquet&quot;)\n\n// Collect the feature vectors and print them.\ndataset.select('features).collect().foreach(println)\n[/scala]\n\n<h2>Pipeline</h2>\nThe new pipeline API lives under a new package named \u201cspark.ml\u201d. A pipeline consists of a sequence of stages. There are two basic types of pipeline stages: Transformer and Estimator. A Transformer takes a dataset as input and produces an augmented dataset as output. E.g., a tokenizer is a Transformer that transforms a dataset with text into an dataset with tokenized words. An Estimator must be first fit on the input dataset to produce a model, which is a Transformer that transforms the input dataset. E.g., logistic regression is an Estimator that trains on a dataset with labels and features and produces a logistic regression model.\n\nCreating a pipeline is easy: simply declare its stages, configure their parameters, and chain them in a pipeline object. For example the following code creates a simple text classification pipeline consisting of a tokenizer, a hashing term frequency feature extractor, and logistic regression.\n\n[scala]\nval tokenizer = new Tokenizer()\n  .setInputCol(&quot;text&quot;)\n  .setOutputCol(&quot;words&quot;)\nval hashingTF = new HashingTF()\n  .setNumFeatures(1000)\n  .setInputCol(tokenizer.getOutputCol)\n  .setOutputCol(&quot;features&quot;)\nval lr = new LogisticRegression()\n  .setMaxIter(10)\n  .setRegParam(0.01)\nval pipeline = new Pipeline()\n  .setStages(Array(tokenizer, hashingTF, lr))\n[/scala]\n\nThe pipeline itself is an Estimator, and hence we can call fit on the entire pipeline easily.\n\n[scala]\nval model = pipeline.fit(trainingDataset)\n[/scala]\n\nThe fitted model consists of the tokenizer, the hashing TF feature extractor, and the fitted logistic regression model. The following diagram draws the workflow, where the dash lines only happen during pipeline fitting.\n<p align=\"center\"><img src=\"https://databricks.com/wp-content/uploads/2015/01/pipeline-1.png\" alt=\"\" width=\"90%\" /></p>\nThe fitted pipeline model is a transformer that can be used for prediction, model validation, and model inspection.\n\n[scala]\nmodel.transform(testDataset)\n  .select('text, 'label, 'prediction)\n  .collect()\n  .foreach(println)\n[/scala]\n\nOne unfortunate characteristic of ML algorithms is that they have many hyperparameters that must be tuned. These hyperparameters - e.g. degree of regularization - are distinct from the model parameters being optimized by MLlib. It is hard to guess the best combination of hyperparameters without expert knowledge on both the data and the algorithm. Even with expert knowledge, it may become unreliable as the size of the pipeline and the number of hyperparameters grows. Hyperparameter tuning (choosing parameters based on performance on held-out data) is usually necessary to obtain meaningful results in practice. For example, we have two hyperparameters to tune in the following pipeline and we put three candidate values for each. Therefore, there are nine combinations in total (four shown in the diagram below) and we want to find the one that leads to the model with the best evaluation result.\n<p align=\"center\"><img src=\"https://databricks.com/wp-content/uploads/2015/01/pipeline-2.png\" alt=\"\" width=\"90%\" /></p>\nWe support cross-validation for hyperparameter tuning. We view cross-validation as a meta-algorithm, which tries to fit the underlying estimator with user-specified combinations of parameters, cross-evaluate the fitted models, and output the best one. Note that there is no specific requirement on the underlying estimator, which could be a pipeline, as long as it could be paired with an Evaluator that outputs a scalar metric from predictions, e.g., precision. Tuning a pipeline is easy:\n\n[scala]\n// Build a parameter grid.\nval paramGrid = new ParamGridBuilder()\n  .addGrid(hashingTF.numFeatures, Array(10, 20, 40))\n  .addGrid(lr.regParam, Array(0.01, 0.1, 1.0))\n  .build()\n\n// Set up cross-validation.\nval cv = new CrossValidator()\n  .setNumFolds(3)\n  .setEstimator(pipeline)\n  .setEstimatorParamMaps(paramGrid)\n  .setEvaluator(new BinaryClassificationEvaluator)\n\n// Fit a model with cross-validation.\nval cvModel = cv.fit(trainingDataset)\n[/scala]\n\nIt is important to note that users can embed their own transformers or estimators into an ML pipeline, as long as they implement the pipeline interfaces. The API makes it easy to use and share code maintained outside MLlib. More complete code examples in <a href=\"https://github.com/apache/spark/blob/branch-1.2/examples/src/main/java/org/apache/spark/examples/ml/JavaCrossValidatorExample.java\" target=\"_blank\">Java</a> and <a href=\"https://github.com/apache/spark/blob/branch-1.2/examples/src/main/scala/org/apache/spark/examples/ml/CrossValidatorExample.scala\" target=\"_blank\">Scala</a> can be found under the \u2018examples/\u2019 folder of the Spark repository. We refer users to the <a href=\"http://spark.apache.org/docs/latest/ml-guide.html\" target=\"_blank\">spark.ml user guide</a> for more information about the pipeline API.\n<h2>Concluding remarks</h2>\nThe blog post describes the ML pipeline API introduced in Spark 1.2 and the rationale behind it. The work is covered by several JIRAs: <a href=\"https://issues.apache.org/jira/browse/SPARK-3530\" target=\"_blank\">SPARK-3530</a>, <a href=\"https://issues.apache.org/jira/browse/SPARK-3569\" target=\"_blank\">SPARK-3569</a>, <a href=\"https://issues.apache.org/jira/browse/SPARK-3572\" target=\"_blank\">SPARK-3572</a>, <a href=\"https://issues.apache.org/jira/browse/SPARK-4192\" target=\"_blank\">SPARK-4192</a>, and <a href=\"https://issues.apache.org/jira/browse/SPARK-4209\" target=\"_blank\">SPARK-4209</a>. We refer users to the design docs posted on each JIRA page for more information about the design choices. And we would like to thank everyone who participated in the discussion and provided valuable feedback.\n\nThat being said, the pipeline API is experimental in Spark 1.2 and the work is still far from done. For example, more feature transformers can help users quickly assemble pipelines. We would like to mention some ongoing work relevant to the pipeline API:\n<ul>\n \t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-5097\" target=\"_blank\">SPARK-5097</a>: Adding data frame APIs to SchemaRDD</li>\n \t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-4586\" target=\"_blank\">SPARK-4586</a>: Python API for ML pipeline</li>\n \t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-3702\" target=\"_blank\">SPARK-3702</a>: Class hierarchy for learning algorithms and models</li>\n</ul>\nThe pipeline API is part of Spark 1.2, which is available for download at <a href=\"http://spark.apache.org/\" target=\"_blank\">http://spark.apache.org/</a>. We look forward to hearing back from you about it, and we welcome your contributions and feedback."}
{"status": "publish", "description": null, "creator": "michael", "link": "https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html", "authors": ["Michael Armbrust"], "id": 2474, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2015-01-09", "tz": "UTC", "createdOn": "2015-01-09"}, "title": "Spark SQL Data Sources API: Unified Data Access for the Apache Spark Platform", "slug": "spark-sql-data-sources-api-unified-data-access-for-the-spark-platform", "content": "Since the inception of Spark SQL in Apache Spark 1.0, one of its most popular uses has been as a conduit for pulling data into the Spark platform. \u00a0Early users loved Spark SQL\u2019s support for reading data from existing Apache Hive tables as well as from the popular Parquet columnar format. We\u2019ve since added support for other formats, such as <a href=\"https://spark.apache.org/docs/latest/sql-programming-guide.html#json-datasets\">JSON</a>. \u00a0In Apache Spark 1.2, we've taken the next step to allow Spark to integrate natively with a far larger number of input sources. \u00a0These new integrations are made possible through the inclusion of the new Spark SQL Data Sources API.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/01/DataSourcesApiDiagram.png\"><img class=\"wp-image-2372 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/01/DataSourcesApiDiagram-1024x526.png\" alt=\"DataSourcesApiDiagram\" width=\"516\" height=\"265\" /></a>\n\nThe Data Sources API provides a pluggable mechanism for accessing structured data though Spark SQL. Data sources can be more than just simple pipes that convert data and pull it into Spark. The tight optimizer integration provided by this API means that filtering and column pruning can be pushed all the way down to the data source in many cases. \u00a0Such integrated optimizations can vastly reduce the amount of data that needs to be processed and thus can significantly speed up Spark jobs.\n\nUsing a data sources is as easy as referencing it from SQL (or your favorite Spark language):\n\n<pre>CREATE TEMPORARY TABLE episodes\n    USING com.databricks.spark.avro\n    OPTIONS (path \"episodes.avro\")</pre>\n\nAnother strength of the Data Sources API is that it gives users the ability to manipulate data in all of the languages that Spark supports, regardless of how the data is sourced. Data sources that are implemented in Scala, for example, can be used by pySpark users without any extra effort required of the library developer. Furthermore, Spark SQL makes it easy to join data from different data sources using a single interface. Taken together, these capabilities further unify the big data analytics solution provided by Apache Spark 1.2.\n\nEven though this API is still young, there are already several libraries built on top of it, including <a href=\"http://spark-packages.org/package/3\">Apache Avro</a>, <a href=\"http://spark-packages.org/package/12\">Comma Separated Values (csv)</a>, and even <a href=\"https://github.com/mraad/spark-dbf\">dBASE Table File Format (dbf</a>). \u00a0Now that Apache Spark 1.2 has been officially released, we expect this list to grow quickly. We know of efforts underway to support HBase, JDBC, and more. Check out <a href=\"http://spark-packages.org/\">Spark Packages</a> to find an up-to-date list of libraries that are available.<strong><strong>\n</strong></strong>\n\nFor developers that are interested in writing a library for their favorite format, we suggest that you study <a href=\"https://github.com/databricks/spark-avro\">the reference library for reading Apache Avro</a>, check out the <a href=\"https://github.com/apache/spark/tree/master/sql/core/src/test/scala/org/apache/spark/sql/sources\">example sources</a>, or <a href=\"http://www.youtube.com/watch?v=GQSNJAzxOr8\">watch this meetup video</a>.\n\nAdditionally, stay tuned for extensions to this API. \u00a0In Apache Spark 1.3 we are hoping to add support for partitioning, persistent tables, and optional user specified schema."}
{"status": "publish", "description": null, "creator": "joseph", "link": "https://databricks.com/blog/2015/01/21/random-forests-and-boosting-in-mllib.html", "authors": ["Joseph K. Bradley (Databricks)", "Manish Amde (Origami Logic)"], "id": 2475, "categories": ["Apache Spark", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2015-01-21", "tz": "UTC", "createdOn": "2015-01-21"}, "title": "Random Forests and Boosting in MLlib", "slug": "random-forests-and-boosting-in-mllib", "content": "<div class=\"post-meta\">This is a post written together with Manish Amde from <a href=\"http://www.origamilogic.com/\">Origami Logic</a>.</div>\n\n<hr />\n\nApache Spark 1.2 introduces <a href=\"http://en.wikipedia.org/wiki/Random_forest\">Random Forests</a> and <a href=\"http://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting\">Gradient-Boosted Trees (GBTs)</a> into MLlib. Suitable for both classification and regression, they are among the most successful and widely deployed machine learning methods. Random Forests and GBTs are <i>ensemble learning algorithms</i>, which combine multiple decision trees to produce even more powerful models. In this post, we describe these models and the distributed implementation in MLlib. We also present simple examples and provide pointers on how to get started.\n<h2>Ensemble Methods</h2>\nSimply put, <a href=\"http://en.wikipedia.org/wiki/Ensemble_learning\">ensemble learning algorithms</a> build upon other machine learning methods by combining models. The combination can be more powerful and accurate than any of the individual models.\n\nIn MLlib 1.2, we use <a href=\"http://en.wikipedia.org/wiki/Decision_tree\">Decision Trees</a> as the base models. We provide two ensemble methods: <a href=\"http://en.wikipedia.org/wiki/Random_forest\">Random Forests</a> and <a href=\"http://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting\">Gradient-Boosted Trees (GBTs)</a>. The main difference between these two algorithms is the order in which each component tree is trained.\n\nRandom Forests train each tree independently, using a random sample of the data. This randomness helps to make the model more robust than a single decision tree, and less likely to overfit on the training data.\n\nGBTs train one tree at a time, where each new tree helps to correct errors made by previously trained trees. With each tree added, the model becomes even more expressive.\n\nIn the end, both methods\u00a0produce a weighted collection of Decision Trees. The ensemble model makes predictions by combining results\u00a0from the individual trees. The figure below shows a simple\u00a0example of an ensemble with three trees.\n<p align=\"center\"><a href=\"https://databricks.com/wp-content/uploads/2015/01/Ensemble-example.png\"><img class=\"alignnone size-full wp-image-2273\" src=\"https://databricks.com/wp-content/uploads/2015/01/Ensemble-example.png\" alt=\"Ensemble example\" width=\"372\" height=\"265\" /></a></p>\nIn the example regression ensemble above, each tree predicts a real value. These three predictions are then combined to produce the ensemble's final prediction. Here, we combine predictions using the mean (but the algorithms use different techniques depending on the prediction task).\n<h2>Distributed Learning of Ensembles</h2>\nIn MLlib, both Random Forests and GBTs partition data by instances (rows). The implementation builds upon the original Decision Tree code, which distributes learning of single trees (described in <a href=\"https://databricks.com/blog/2014/09/29/scalable-decision-trees-in-mllib.html\">an earlier blog post</a>). Many of our optimizations are based upon <a href=\"http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/36296.pdf\">Google's PLANET project</a>, one of the major published works on learning ensembles of trees in the distributed setting.\n\n<i>Random Forests</i>: Since each tree in a Random Forest is trained independently, multiple trees can be trained in parallel (in addition to the parallelization for single trees). MLlib does exactly that: A variable number of sub-trees are trained in parallel, where the number is optimized on each iteration based on memory constraints.\n\n<i>GBTs</i>: Since GBTs must train one tree at a time, training is only parallelized at the single tree level.\n\nWe would like to highlight\u00a0two key optimizations used in MLlib:\n<ul>\n \t<li>Memory: Random Forests use a different subsample of the data to train each tree. Instead of replicating data explicitly, we save memory by using a TreePoint structure which stores the number of replicas of each instance in each subsample.</li>\n \t<li>Communication: Whereas Decision Trees are usually trained by selecting from all features at each decision node in the tree, Random Forests often limit the selection to a random subset of features at each node. MLlib\u2019s implementation takes advantage of this subsampling to reduce communication: e.g., if only 1/3 of the features are used at each node, then we can reduce communication by a factor of 1/3.</li>\n</ul>\nFor more details, see the <a href=\"http://spark.apache.org/docs/latest/mllib-ensembles.html\">Ensembles Section in the MLlib Programming Guide</a>.\n<h2>Using MLlib Ensembles</h2>\nWe demonstrate how to learn ensemble models using MLlib. The following Scala examples show how to read in a dataset, split the data into training and test sets, learn a model, and print the model and its test accuracy. Refer to the <a href=\"http://spark.apache.org/docs/latest/mllib-ensembles.html\">MLlib Programming Guide</a> for examples in Java and Python. Note that GBTs do not yet have a Python API, but we expect it to be in the Spark 1.3 release (via <a href=\"https://www.github.com/apache/spark/pull/3951\">Github PR 3951</a>).\n<h4>Random Forest Example</h4>\n\n[scala]\nimport org.apache.spark.mllib.tree.RandomForest\nimport org.apache.spark.mllib.tree.configuration.Strategy\nimport org.apache.spark.mllib.util.MLUtils\n\n// Load and parse the data file.\nval data =\n  MLUtils.loadLibSVMFile(sc, &quot;data/mllib/sample_libsvm_data.txt&quot;)\n// Split data into training/test sets\nval splits = data.randomSplit(Array(0.7, 0.3))\nval (trainingData, testData) = (splits(0), splits(1))\n\n// Train a RandomForest model.\nval treeStrategy = Strategy.defaultStrategy(&quot;Classification&quot;)\nval numTrees = 3 // Use more in practice.\nval featureSubsetStrategy = &quot;auto&quot; // Let the algorithm choose.\nval model = RandomForest.trainClassifier(trainingData,\n  treeStrategy, numTrees, featureSubsetStrategy, seed = 12345)\n\n// Evaluate model on test instances and compute test error\nval testErr = testData.map { point =&gt;\n  val prediction = model.predict(point.features)\n  if (point.label == prediction) 1.0 else 0.0\n}.mean()\nprintln(&quot;Test Error = &quot; + testErr)\nprintln(&quot;Learned Random Forest:n&quot; + model.toDebugString)\n[/scala]\n\n<h4>Gradient-Boosted Trees Example</h4>\n\n[scala]\nimport org.apache.spark.mllib.tree.GradientBoostedTrees\nimport org.apache.spark.mllib.tree.configuration.BoostingStrategy\nimport org.apache.spark.mllib.util.MLUtils\n\n// Load and parse the data file.\nval data =\n  MLUtils.loadLibSVMFile(sc, &quot;data/mllib/sample_libsvm_data.txt&quot;)\n// Split data into training/test sets\nval splits = data.randomSplit(Array(0.7, 0.3))\nval (trainingData, testData) = (splits(0), splits(1))\n\n// Train a GradientBoostedTrees model.\nval boostingStrategy =\n  BoostingStrategy.defaultParams(&quot;Classification&quot;)\nboostingStrategy.numIterations = 3 // Note: Use more in practice\nval model =\n  GradientBoostedTrees.train(trainingData, boostingStrategy)\n\n// Evaluate model on test instances and compute test error\nval testErr = testData.map { point =&gt;\n  val prediction = model.predict(point.features)\n  if (point.label == prediction) 1.0 else 0.0\n}.mean()\nprintln(&quot;Test Error = &quot; + testErr)\nprintln(&quot;Learned GBT model:n&quot; + model.toDebugString)\n[/scala]\n\n<h2>Scalability</h2>\nWe demonstrate the scalability of MLlib ensembles with empirical results on a binary classification problem. Each figure below compares Gradient-Boosted Trees (\"GBT\") with Random Forests (\"RF\"), where the trees are built out to different maximum depths.\n\nThese tests were on a regression task of predicting song release dates from audio features (the <a href=\"https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD\">YearPredictionMSD dataset</a> from the UCI ML repository). We used EC2 r3.2xlarge machines. Algorithm parameters were left as defaults except where noted.\n<h4>Scaling model size: Training time and test error</h4>\nThe two figures below show the effect of increasing the number of trees in the ensemble. For both, increasing\u00a0trees require more time to learn (first figure) but also provide\u00a0better results in terms of test Mean Squared Error (MSE) (second figure).\n\nComparing the two methods, Random Forests are faster to train, but they often require deeper trees than GBTs to achieve the same error. GBTs can further reduce the error with each iteration, but they can begin to overfit (increase test error) after too many iterations. Random Forests do not overfit as easily, but their test error plateaus.\n<p align=\"center\"><a href=\"https://databricks.com/wp-content/uploads/2015/01/Ensembles-trees-x-time.png\"><img class=\"alignnone size-full wp-image-2280\" src=\"https://databricks.com/wp-content/uploads/2015/01/Ensembles-trees-x-time.png\" alt=\"Ensembles - trees x time\" width=\"440\" height=\"249.5\" /></a></p>\nBelow, for a basis for understanding the MSE, note that the left-most points show the error when using a single decision tree (of depths 2, 5, or 10, respectively).\n<p align=\"center\"><a href=\"https://databricks.com/wp-content/uploads/2015/01/Ensembles-trees-x-mse.png\"><img class=\"alignnone size-full wp-image-2279\" src=\"https://databricks.com/wp-content/uploads/2015/01/Ensembles-trees-x-mse.png\" alt=\"Ensembles - trees x mse\" width=\"440\" height=\"248.5\" /></a></p>\n<i>Details: 463,715 training instances. 16 workers.</i>\n<h4>Scaling training dataset size: Training time and test error</h4>\nThe next two figures show the effect of using larger training datasets. With more data, both methods take longer to train but achieve better test results.\n<p align=\"center\"><a href=\"https://databricks.com/wp-content/uploads/2015/01/Ensembles-ntrain-x-time.png\"><img class=\"alignnone size-full wp-image-2278\" src=\"https://databricks.com/wp-content/uploads/2015/01/Ensembles-ntrain-x-time.png\" alt=\"Ensembles - ntrain x time\" width=\"437.5\" height=\"249.5\" /></a></p>\n<p align=\"center\"><a href=\"https://databricks.com/wp-content/uploads/2015/01/Ensembles-ntrain-x-mse.png\"><img class=\"alignnone size-full wp-image-2277\" src=\"https://databricks.com/wp-content/uploads/2015/01/Ensembles-ntrain-x-mse.png\" alt=\"Ensembles - ntrain x mse\" width=\"435.5\" height=\"250.5\" /></a></p>\n<i>Details: 16 workers.</i>\n<h4>Strong scaling: Faster training with more workers</h4>\nThis final figure shows the effect of using a larger compute cluster to solve the same problem. Both methods are significantly faster when using more workers. For example, GBTs with depth-2 trees train about 4.7 times faster on 16 workers than on 2 workers, and larger datasets produce even better speedups.\n<p align=\"center\"><a href=\"https://databricks.com/wp-content/uploads/2015/01/Ensembles-workers-x-time.png\"><img class=\"alignnone size-full wp-image-2281\" src=\"https://databricks.com/wp-content/uploads/2015/01/Ensembles-workers-x-time1.png\" alt=\"Ensembles - workers x time\" width=\"442\" height=\"296.5\" /></a></p>\n<i>Details: 463,715 training instances.</i>\n<h2>What\u2019s Next?</h2>\nGBTs will soon include a Python API. The other top item for future development is pluggability: ensemble methods can be applied to almost any classification or regression algorithm, not only Decision Trees. The Pipelines API introduced by Spark 1.2\u2019s <a href=\"http://spark.apache.org/docs/latest/ml-guide.html\">experimental spark.ml package</a> will allow us to generalize ensemble methods to be truly pluggable.\n\nTo get started using decision trees yourself, <a href=\"http://spark.apache.org/\">download Spark 1.2 today</a>!\n<h2>Further Reading</h2>\n<ul>\n \t<li>See examples and the API in <a href=\"http://spark.apache.org/docs/latest/mllib-ensembles.html\">the MLlib ensembles documentation</a>.</li>\n \t<li>Learn more background info about the decision trees used to build ensembles in <a href=\"https://databricks.com/blog/2014/09/29/scalable-decision-trees-in-mllib.html\">this previous blog post</a>.</li>\n</ul>\n<h2>Acknowledgements</h2>\nMLlib ensemble algorithms have been developed collaboratively by the authors of this blog post, Qiping Li (Alibaba), Sung Chung (Alpine Data Labs), and Davies Liu (Databricks). We also thank Lee Yang, Andrew Feng, and Hirakendu Das (Yahoo) for help with design and testing. We will welcome your contributions too!"}
{"status": "publish", "description": null, "creator": "tdas", "link": "https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html", "authors": ["Tathagata Das"], "id": 2476, "categories": ["Apache Spark", "Engineering Blog", "Streaming"], "dates": {"publishedOn": "2015-01-15", "tz": "UTC", "createdOn": "2015-01-15"}, "title": "Improved Fault-tolerance and Zero Data Loss in Apache Spark Streaming", "slug": "improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming", "content": "Real-time stream processing systems must be operational 24/7, which requires them to recover from all kinds of failures in the system. Since its beginning, Apache Spark Streaming has included support for recovering from failures of both driver and worker machines. However, for some data sources, input data could get lost while recovering from the failures. In Apache Spark 1.2, we have added preliminary support for write ahead logs (also known as journaling) to Spark Streaming to improve this recovery mechanism and give stronger guarantees of zero data loss for more data sources. In this blog, we are going to elaborate on how this feature works and how developers can enable it to get those guarantees in Spark Streaming applications.\n<h2>Background</h2>\nSpark and its RDD abstraction is designed to seamlessly handle failures of any worker nodes in the cluster. Since Spark Streaming is built on Spark, it enjoys the same fault-tolerance for worker nodes. However, the demand of high uptimes of a Spark Streaming application require that the application also has to recover from failures of the<a href=\"https://spark.apache.org/docs/latest/cluster-overview.html\"> driver process</a>, which is the main application process that coordinates all the workers. Making the Spark driver fault-tolerant is tricky because it is an arbitrary user program with arbitrary computation patterns. However, Spark Streaming applications have an inherent structure in the computation -- it runs the same Spark computation periodically on every micro-batch of data. This structure allows us to save (aka, checkpoint) the application state periodically to reliable storage and recover the state on driver restarts.\n\nFor sources like files, this driver recovery mechanism was sufficient to ensure zero data loss as all the data was reliably stored in a fault-tolerant file system like HDFS or S3. However, for other sources like Kafka and Flume, some of the received data that was buffered in memory but not yet processed could get lost. This is because of how Spark applications operate in a distributed manner. When the driver process fails, all the executors running in a standalone/yarn/mesos cluster are killed as well, along with any data in their memory. In case of Spark Streaming, all the data received from sources like Kafka and Flume are buffered in the memory of the executors until their processing has completed. This buffered data cannot be recovered even if the driver is restarted. To avoid this data loss, we have introduced write ahead logs in Spark Streaming in the Apache Spark 1.2 release.\n<h2>Write Ahead Logs</h2>\nWrite Ahead Logs (also known as a journal) are used in database and file systems to ensure the durability of any data operations. The intention of the operation is first written down into a durable log , and then the operation is applied to the data. If the system fails in the middle of applying the operation, it can recover by reading the log and reapplying the operations it had intended to do. Let us see how we use this concept to ensure the durability of the received data.\n\nSources like Kafka and Flume use Receivers to receive data. They run as long-running tasks in the executors, and are responsible for receiving the data from the source, and if supported by the source, acknowledge the received data. They store the received data in the memory of the executors and the driver then runs tasks on the executors to process the tasks.\n\nWhen write ahead logs are enabled, all the received data is also saved to log files in a fault-tolerant file system. This allows the received data to durable across any failure in Spark Streaming. Additionally, if the receiver correctly acknowledges receiving data only after the data has been to write ahead logs, the buffered but unsaved data can be resent by the source after the driver is restarted. These two together can ensure that there is zero data loss - all data is either recovered from the logs or resent by the source.\n<h2>Configuration</h2>\nWrite ahead logs can be enabled if required by do the following.\n<ul>\n \t<li>Setting the checkpoint directory using streamingContext.checkpoint(path-to-directory). This directory can be set to any Hadoop API compatible file system, and is used to save both streaming checkpoints as well as write ahead logs.</li>\n \t<li><a href=\"http://spark.apache.org/docs/latest/configuration.html#spark-properties\">Setting the SparkConf property</a> spark.streaming.receiver.writeAheadLog.enable to true (default is false).</li>\n</ul>\nWhen the logs are enabled, all receivers enjoy the benefit of recovering data that were reliably received. It is recommended that the in-memory replication be disabled (by setting the appropriate<a href=\"http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence\"> persistence level</a> in the input stream) as the fault-tolerant file system used for the write ahead log likely to be replicating the data as well.\n\nAdditionally, if you want to recover even the buffered data, you will have to use a source that support acking (like Kafka, Flume and Kinesis), and<a href=\"http://spark.apache.org/docs/latest/streaming-custom-receivers.html\"> implement a reliable receiver</a> that correctly acks the source when data is reliably stored in the log. The built in Kafka and Flume Polling receivers already are reliable.\n\nFinally, it is worth noting that there may be a slight reduction in the data ingestion throughput on enabling the write ahead logs. Since all the received data will be written to a fault-tolerant file system, the write throughput of the file system, and the network bandwidth used for the replication can become potential bottlenecks. In that case, either create<a href=\"http://spark.apache.org/docs/latest/streaming-programming-guide.html#reducing-the-processing-time-of-each-batch\"> more receivers be used for increasing the parallelism of receiving the data</a> and/or use better hardware to increase the throughput of the fault-tolerant file system.\n<h2>Implementation Details</h2>\nLet us dive a bit deeper to understand how the write ahead logs work. In the context, let us walk through the general Spark Streaming architecture.\n\nWhen a Spark Streaming application starts (i.e., the driver starts), the associated StreamingContext (starting point of all streaming functionality) uses the SparkContext to launch Receivers as long running tasks. These Receivers receive and save the streaming data into Spark\u2019s memory for processing. The lifecycle of this data received through users is as follows (refer to the diagram below).\n<ul>\n \t<li>Receiving data (blue arrows) - A Receiver chunks up the stream of data into blocks, that are stored in the memory of the executor. Additionally, if enabled, the data is also written to a write ahead log in a fault tolerant file systems.</li>\n \t<li>Notifying driver (green arrows) - The metadata of the received blocks are sent to the StreamingContext in the driver. This metadata includes \u00a0- (i) reference ids of the blocks for locating their data in the executor memory, (ii) offset information of the block data in the logs (if enabled).</li>\n \t<li>Processing the data (red arrow) - Every batch interval, the StreamingContext uses the block information to generate RDDs and jobs on them. The SparkContext executes these jobs by running tasks to process the in-memory blocks in the executors.</li>\n \t<li>Checkpointing the computation (orange arrow) - To recover, the streaming computation (i.e. the DStreams set up with the StreamingContext) is periodically checkpointed to another set of files in the same fault-tolerant file system.</li>\n</ul>\n<a href=\"https://databricks.com/wp-content/uploads/2015/01/blog-ha-52.png\"><img src=\"https://databricks.com/wp-content/uploads/2015/01/blog-ha-52-1024x502.png\" alt=\"Checkpointing the computation and saving the received block data and metadata to write ahead logs\" width=\"625\" height=\"306\" /></a>\n\nWhen a failed driver is restart, the following occurs (see the next diagram).\n<ul>\n \t<li><i>Recover computation (orange arrow)</i> - The checkpointed information is used to restart the driver, reconstruct the contexts and restart all the receivers.</li>\n \t<li><i>Recover block metadata (green arrow)</i> - The metadata of all the blocks that will be necessary to continue the processing will be recovered.</li>\n \t<li><i>Re-generate incomplete jobs (red arrow)</i> - For the batches with processing that has not completed due to the failure, the RDDs and corresponding jobs are regenerated using the recovered block metadata.</li>\n \t<li><i>Read the block saved in the logs (blue arrow)</i> - When those jobs are executed, the block data is read directly from the write ahead logs. This recovers all the necessary data that were reliably saved to the logs.</li>\n \t<li><i>Resend unacknowledged data (purple arrow)</i> - The buffered data that was not saved to the log at the time of failure will be sent again by the source. as it had not been acknowledged by the receiver.</li>\n</ul>\n<a href=\"https://databricks.com/wp-content/uploads/2015/01/blog-ha-4.jpg\"><img src=\"https://databricks.com/wp-content/uploads/2015/01/blog-ha-4.jpg\" alt=\"Recovering the computation from and data on driver restart\" width=\"656\" height=\"324\" /></a>\n\nSo with the write ahead logs as well as the reliable receivers, Spark Streaming can guarantee that no input data will be lost due to driver failures (or for that matter, any failures).\n<h2>Future Directions</h2>\nSome of the possible future directions regarding write ahead logs are as follows.\n<ul>\n \t<li>Systems like Kafka can replicate data for reliability. Enabling write ahead logs effectively replicates the same data twice - once by Kafka and another time by Spark Streaming. Future versions of Spark will include native support for fault tolerance with Kafka that avoids a second log.</li>\n \t<li>Performance improvements (especially throughput) in writing to the write ahead logs.</li>\n</ul>\n<h2>Credits</h2>\nMajor credits for implementing this feature goes to the following.\n<ul>\n \t<li>Tathagata Das (Databricks) - Overall design and major parts of the implementation.</li>\n \t<li>Hari Shreedharan (Cloudera) - Writing and reading of write ahead logs.</li>\n \t<li>Saisai Shao (Intel) - Improvements to\u00a0the built in Kafka support.</li>\n</ul>\n<h2>Further References</h2>\n<ul>\n \t<li>Refer to the <a href=\"https://spark.apache.org/docs/latest/streaming-programming-guide.html\">Spark Streaming Programming Guide</a> for more information about checkpoint and write ahead logs.</li>\n \t<li>Spark <a href=\"http://youtu.be/jcJq3ZalXD8?t=27m13s\">Meetup talk</a> on this topic</li>\n \t<li>Associated JIRA - <a href=\"https://issues.apache.org/jira/browse/SPARK-3129\">SPARK-3129</a></li>\n</ul>"}
{"status": "publish", "description": null, "creator": "kavitha", "link": "https://databricks.com/blog/2015/01/27/big-data-projects-are-hungry-for-simpler-and-more-powerful-tools-survey-validates-apache-spark-is-gaining-developer-traction.html", "authors": ["Kavitha Mariappan"], "id": 2477, "categories": ["Announcements", "Company Blog"], "dates": {"publishedOn": "2015-01-27", "tz": "UTC", "createdOn": "2015-01-27"}, "title": "Big data projects are hungry for simpler and more powerful tools: Survey validates Apache Spark is gaining developer traction!", "slug": "big-data-projects-are-hungry-for-simpler-and-more-powerful-tools-survey-validates-apache-spark-is-gaining-developer-traction", "content": "In partnership with <a href=\"https://typesafe.com/\">Typesafe</a>, we are excited to see the publication of the <a href=\"http://info.typesafe.com/COLL-20XX-Spark-Survey-Report_LP.html?lst=PR&amp;lsd=COLL-20XX-Spark-Survey-Trends-Adoption-Report\">survey report</a> representing the largest poll of Apache Spark developers to date. Spark is currently the most active open source project in big data and has been rapidly gaining traction over the past few years. This survey of over 2100 respondents further validates the wide variety of use cases and environments where it is being deployed.\n\nThe survey results indicate that 13% are already using Spark in production environments with 20% of the respondents with plans to deploy Spark in production environments in 2015, and 31% are currently in the process of evaluating it. In total, the survey covers over 500 enterprises that are using or planning to use Spark in production environments ranging from on-premise Hadoop clusters to public clouds, with data sources including key-value stores, relational databases, streaming data and file systems. Applications range from batch workloads to SQL queries, stream processing and machine learning, highlighting Spark\u2019s unique capability as a simple, unified platform for data processing.\n\nAt Databricks and within the Spark community, this type of feedback helps us enhance Spark for more use cases making big data simpler for enterprises of all sizes. As the creators of and most active contributors to Spark, we are happy to see widespread adoption of Spark across a wide range of industries.\n\nIn 2015, we are working on a number of Spark initiatives, including integrated SQL and data frame capabilities, improved stream processing capabilities, extending Spark's machine learning library (MLlib), and richer APIs in Python and R. We have also taken Spark to the cloud in our hosted service,<a href=\"https://databricks.com/product\"> Databricks Cloud</a>, that enables users to leverage Apache Spark directly in the cloud giving them a powerful platform to manage clusters, interactively explore and visualize data, and build production data pipelines. If you would like to try Databricks Cloud, <a href=\"https://databricks.com/registration\">sign up now</a>.\n\nWe are excited to partner with Typesafe in this effort and in our shared vision to bring a comprehensive suite of application development tools for developers that let enterprises operate with more agility and speed. The feedback from this survey will be extremely valuable for both companies to collectively enhance the Spark developer experience.\n\nTo learn more, don't forget to check out this<a href=\"http://www2.marketwire.com/mw/release_html_b1?release_id=1171473\"> Typesafe press release</a>."}
{"status": "publish", "description": null, "creator": "patrick", "link": "https://databricks.com/blog/2015/02/09/learning-spark-book-available-from-oreilly.html", "authors": ["Holden Karau", "Andy Konwinski", "Patrick Wendell", "Matei Zaharia"], "id": 2479, "categories": ["Announcements", "Company Blog"], "dates": {"publishedOn": "2015-02-09", "tz": "UTC", "createdOn": "2015-02-09"}, "title": "\"Learning Spark\" book available from O'Reilly", "slug": "learning-spark-book-available-from-oreilly", "content": "<a href=\"https://databricks.com/wp-content/uploads/2015/02/large-oreilly-book-cover.jpg\"><img class=\"size-medium wp-image-2486 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/02/large-oreilly-book-cover-228x300.jpg\" alt=\"large oreilly book cover\" width=\"228\" height=\"300\" /></a>\n\nToday we are happy to announce that the complete <a href=\"http://shop.oreilly.com/product/0636920028512.do\" target=\"_blank\"><i>Learning Spark</i></a> book is available from O\u2019Reilly in e-book form with the print copy expected to be available February 16th. At Databricks, as the creators behind Apache Spark, we have witnessed <a title=\"Big data projects are hungry for simpler and more powerful tools: Survey validates Apache Spark is gaining developer traction!\" href=\"https://databricks.com/blog/2015/01/27/big-data-projects-are-hungry-for-simpler-and-more-powerful-tools-survey-validates-apache-spark-is-gaining-developer-traction.html\" target=\"_blank\">explosive growth in the interest and adoption of Spark</a>, which has quickly become one of the most active software projects in Big Data. To continue fostering the developer and user communities around Spark we created a book to help engineers and data scientists learn Spark and use it to solve their most challenging problems.\n\nLearning Spark covers Spark\u2019s rich collection of data programming APIs and libraries (e.g., <a title=\"ML Pipelines: A New High-Level API for MLlib\" href=\"https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html\">MLlib</a>), which make it easy for data scientists to use cutting edge statistical approaches to tackle problems using data of unprecedented scale. Engineers, meanwhile, will learn how to write general-purpose distributed programs in Spark as well as configure and operate production deployments of Spark.\n\nThe Learning Spark book does not require any existing Spark or distributed systems knowledge, though some knowledge of Scala, Java, or Python might be helpful.\n\nThe topics covered include Spark\u2019s core general purpose distributed computing engine, as well as some of Spark\u2019s most popular components including <a title=\"Spark SQL Data Sources API: Unified Data Access for the Spark Platform\" href=\"https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html\" target=\"_blank\">Spark SQL</a>, Spark Streaming, and Spark's Machine Learning library <a title=\"ML Pipelines: A New High-Level API for MLlib\" href=\"https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html\" target=\"_blank\">MLlib</a>. Both new and existing Spark practitioners will be able to learn Spark best practices as well as important tuning tricks and debugging skills.\n\nThe book is available today from <a href=\"http://shop.oreilly.com/product/0636920028512.do\" target=\"_blank\">O\u2019Reilly</a>, <a href=\"http://amzn.to/1Cw1BTa\" target=\"_blank\">Amazon</a>, and <a href=\"https://books.google.com/books?id=tOptBgAAQBAJ&amp;dq=learning+spark&amp;source=gbs_navlinks_s\" target=\"_blank\">others</a> in e-book form<strong>, </strong>as well as print pre-order (expected availability of February 16th) from <a href=\"http://shop.oreilly.com/product/0636920028512.do\" target=\"_blank\">O\u2019Reilly</a>, <a href=\"http://amzn.to/1KsFSuR\" target=\"_blank\">Amazon</a>. \u00a0The code examples from the book are available on the books <a href=\"https://github.com/databricks/learning-spark\" target=\"_blank\">GitHub</a>\u00a0as well as notebooks in the \u201clearning_spark\u201d folder in <a title=\"Databricks Cloud Overview\" href=\"https://databricks.com/product\" target=\"_blank\">Databricks Cloud</a>.\n\nWe are also excited to share\u00a0the\u00a0discount code <strong><del>BWORM</del> AUTHD</strong>. This discount is for 40% off print or 50% off ebooks when you buy directly from O'Reilly.\n\nThe authors, Holden Karau, Andy Konwinski, Patrick Wendell, and Matei Zaharia will attend <a href=\"http://go.databricks.com/databricks-at-strata-hadoop-world-san-jose\" target=\"_blank\">Strata San Jose</a> (February 17 - 20th 2015). We will be giving talks and on Thursday morning we will be signing books. Please stop by <strong>booth 1021</strong> and visit us."}
{"status": "publish", "description": null, "creator": "kavitha", "link": "https://databricks.com/blog/2015/02/12/automatic-labs-selects-databricks-cloud-for-primary-real-time-data-processing.html", "authors": null, "id": 2566, "categories": ["Announcements", "Company Blog", "Customers"], "dates": {"publishedOn": "2015-02-13", "tz": "UTC", "createdOn": "2015-02-13"}, "title": "Automatic Labs Selects Databricks for Primary Real-Time Data Processing", "slug": "automatic-labs-selects-databricks-cloud-for-primary-real-time-data-processing", "content": "We're really excited to share that <a href=\"http://www.automatic.com\">Automatic Labs </a>has selected Databricks as its preferred big data processing platform.\n\nPress release:\u00a0<a href=\"http://www.marketwired.com/press-release/automatic-labs-turns-databricks-cloud-faster-innovation-dramatic-cost-savings-1991316.htm\" target=\"_blank\">http://www.marketwired.com/press-release/automatic-labs-turns-databricks-cloud-faster-innovation-dramatic-cost-savings-1991316.htm</a>\n\nAutomatic Labs needed to run large and complex queries against their entire data set to explore and come up with new product ideas. Their prior solution using Postgres impeded the ability of Automatic\u2019s team to efficiently explore data because queries took days to run and data could not be easily visualized, preventing Automatic Labs from bringing critical new products to market. They then deployed Databricks, our simple yet powerful unified big data processing platform on Amazon Web Services (AWS) and realized these key benefits:\n<ul>\n \t<li data-canvas-width=\"533.592\"><strong>Reduced time to bring product to market.</strong> Minimized the time to validate a product idea from months to days by speeding up the interactive exploration over Automatic\u2019s entire data set, and completing queries in minutes instead of days.</li>\n \t<li data-canvas-width=\"533.592\"><strong>Eliminated DevOps and non-core activities.</strong> Freed up one full-time data scientist from non-core activities such as DevOps and infrastructure maintenance to perform core data science activities.</li>\n \t<li data-canvas-width=\"533.592\"><strong>Infrastructure savings.</strong> Realized savings of ten thousand dollars in one month alone on AWS costs due to the ability to instantly set up and tear-down Apache Spark clusters</li>\n</ul>\nWith a mission to connect all cars on the road to the internet, Automatic Labs is now able to run large and complex production workloads with Databricks to explore new product ideas and bring them to market faster, such as custom driving reports, recommendations for users regarding fuel-efficient driving and more.\n\nDownload this <a href=\"https://databricks.com/wp-content/uploads/2015/04/Databricks_Case_Study_Automatic_Labs.pdf\">case study</a> learn more about how Automatic Labs is using Databricks.\n\n&nbsp;"}
{"status": "publish", "description": "Spark: A review of 2014 and looking ahead to 2015 priorities", "creator": "patrick", "link": "https://databricks.com/blog/2015/02/13/spark-a-review-of-2014-and-looking-ahead-to-2015-priorities.html", "authors": null, "id": 2576, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2015-02-14", "tz": "UTC", "createdOn": "2015-02-14"}, "title": "Apache Spark: A review of 2014 and looking ahead to 2015 priorities", "slug": "spark-a-review-of-2014-and-looking-ahead-to-2015-priorities", "content": "2014 has been a year of <a href=\"https://databricks.com/blog/2015/01/27/big-data-projects-are-hungry-for-simpler-and-more-powerful-tools-survey-validates-apache-spark-is-gaining-developer-traction.html\">tremendous growth</a> for Apache Spark. \u00a0It became the most active open source project in the Big Data ecosystem with over 400 contributors, and was adopted by many platform vendors - including all of the major Hadoop distributors. \u00a0Through our ecosystem of products, partners, and training at Databricks, we also saw over 200 enterprises deploying Spark in production.\n\nTo help Spark achieve this growth, Databricks has worked broadly throughout the project to improve functionality and ease of use. Indeed, while the community has grown a lot, about 75% of the code added to Spark last year came from Databricks. In this post, we would like to highlight some of the additions we made to Spark in 2014, and provide a preview of our priorities in 2015.\n\nIn general, our approach to developing Spark is two-fold: improving <i>usability and performance</i> for the core engine, and <i>expanding the functionality</i> of libraries on top, such as streaming, SQL, and machine learning. Because all these libraries use the same core engine, they benefit from the same improvements in deployability, performance, etc.\n<h2>Major Spark Additions in 2014</h2>\nOn the core engine, here are the major improvements we\u2019ve made in 2014:\n<ul>\n \t<li><b>Language support:</b> A major requirement for many enterprises was to make Spark available in languages their users were most familiar with, such as Java and Scala. Databricks <a href=\"https://databricks.com/blog/2014/04/14/spark-with-java-8.html\">led the work to integrate Spark with Java 8</a>, offering much simpler syntax to Java users, and led major additions to the Python API including performance improvements and the Python interfaces to MLlib, Spark Streaming and Spark SQL.</li>\n \t<li><b>Production management:</b> We helped to add high-availability features to the <a href=\"https://issues.apache.org/jira/browse/SPARK-610\">Spark standalone master</a> (allowing master recovery through ZooKeeper) and to <a href=\"https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html\">Spark Streaming</a> (allowing storing input reliably even from unreliable data sources to allow fault recovery later). We also worked with the community to make Spark scale dynamically on YARN, leading to better resource utilization, and to help integrate with Hadoop ecosystem features such as the Hadoop security model.</li>\n \t<li><b>Performance and stability:</b> We rewrote Spark\u2019s shuffle and network layers to provide significantly higher performance, and used this work to <a href=\"https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html\">break the world record in sorting</a> using Spark, beating the previous Hadoop-based record by 30x in per-node performance. More generally, we have worked broadly to make Spark operators run better on disk, allowing great performance at any scale from petabytes to megabytes.</li>\n</ul>\nOn the libraries side, we\u2019ve also had the fastest growth in Spark\u2019s standard library to date. Databricks contributed the following:\n<ul>\n \t<li><b>Spark SQL:</b> We contributed a new module for structured data makes it much easier to use Spark with data sources like Apache Hive, Parquet and <a title=\"An introduction to JSON support in Spark SQL\" href=\"https://databricks.com/blog/2015/02/02/an-introduction-to-json-support-in-spark-sql.html\">JSON</a>, and provides fast SQL connectivity to BI tools like <a title=\"MicroStrategy \u201cCertified on Spark\u201d\" href=\"https://databricks.com/blog/2014/06/04/microstrategy-certified-on-spark.html\">MicroStrategy</a>, <a title=\"Application Spotlight: Qlik\" href=\"https://databricks.com/blog/2014/06/24/application-spotlight-qlik.html\">Qlik</a> and <a title=\"Application Spotlight: Tableau Software\" href=\"https://databricks.com/blog/2014/10/15/application-spotlight-tableau-software.html\">Tableau</a>. Through Spark SQL, both developers and analysts can now more easily leverage Spark clusters.</li>\n \t<li><b>Machine learning library:</b> Databricks contributed multiple <a href=\"https://databricks.com/blog/2015/01/21/random-forests-and-boosting-in-mllib.html\">new</a> <a href=\"https://databricks.com/blog/2015/01/28/introducing-streaming-k-means-in-spark-1-2.html\">algorithms</a> and <a href=\"https://databricks.com/blog/2014/09/22/spark-1-1-mllib-performance-improvements.html\">optimizations</a> to MLlib, Spark\u2019s machine learning library, speeding up some tasks by as much as <a href=\"https://databricks.com/blog/2014/09/22/spark-1-1-mllib-performance-improvements.html\">5x</a>. We also contributed a <a href=\"https://databricks.com/blog/2014/08/27/statistics-functionality-in-spark.html\">statistics library</a> as a new, high-level <a href=\"https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html\">pipeline API</a> to make it easier to write complete machine learning applications.</li>\n \t<li><b>Graph processing:</b> We worked with UC Berkeley to add <a href=\"https://spark.apache.org/graphx/\">GraphX</a> as the standard graph analytics library in Spark, giving users access to a variety of graph processing algorithms.</li>\n \t<li><b>API stability in Spark 1.0:</b> On a more technical but very important level, we worked with the community to define <a href=\"https://cwiki.apache.org/confluence/display/SPARK/Spark+Versioning+Policy\">API stability guarantees</a> for Spark 1.x, which means that application written against Spark today will continue running on future versions. This is a crucial feature for enterprises and developers as it allows application portability across vendors and into future versions of Spark.</li>\n</ul>\nLooking back, it\u2019s a bit hard to imagine that a year ago, Spark didn\u2019t have built-in BI connectivity, rich monitoring, or about half of the higher-level libraries it contains today. Nonetheless, this is the rate at which fast-growing projects move. We\u2019re thrilled to continue working with the community to bring even more great features to Spark.\n<h2>What\u2019s Next</h2>\nEven though 2014 has been a great year for Spark, we know that we are only at the beginning of enterprise use of both Spark and big data in general. At Databricks, we\u2019re focused on a handful of major initiatives for Spark in 2015:\n<ul>\n \t<li><strong>Empowering large scale data science.</strong> In 2015, Spark will expand its focus on data scientists by providing higher level, powerful API\u2019s for statistical and analytical processing. The <a href=\"http://amplab-extras.github.io/SparkR-pkg/\">SparkR</a> project, which allows use of Spark from R, is quickly coming of age, and work to <a href=\"https://issues.apache.org/jira/browse/SPARK-5654\">merge SparkR into Spark</a> is already under way. We\u2019re also introducing a <a href=\"https://issues.apache.org/jira/browse/SPARK-5097\">data frame library</a> for use across all of Spark\u2019s language API\u2019s (Java, Scala, Python, and R) and a <a href=\"https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html\">machine learning pipeline API</a> in MLlib designed to inter-operate with data frames. The data frame library makes working with datasets, small or large, approachable for a wide range of users.</li>\n</ul>\n<ul>\n \t<li><strong>Rich data source integration.</strong> The data management ecosystem is home to a variety of data sources and sinks. Our work on a <a href=\"https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html\">pluggable data source API for Spark SQL will connect Spark </a>to many traditional enterprise data sources in addition to the new wave of big data / NoSQL storage systems. Work is already under way to connect to JDBC, HBase, and DBF files. To showcase data sources and other Spark integrations from the broader community, we have also recently founded <a href=\"http://spark-packages.org/\">Spark packages</a>, a community index to track third-party libraries available for Spark. Spark packages has over 30 libraries today; we expect it to grow substantially in 2015.</li>\n</ul>\n<ul>\n \t<li><strong>Simplifying deployment with Databricks Cloud.</strong> Our main goal at Databricks remains to make big data simple. This extends beyond designing concise, elegant API\u2019s available in Spark to providing a hassle-free runtime environment for our users. With <a href=\"https://databricks.com/product/databricks-cloud\">Databricks Cloud</a>, we make it easy for users to get started with Spark and big data within minutes, bypassing the many months of setup traditionally needed for a big data project.</li>\n</ul>\nOf course, you can also expect \u201cmore of the same\u201d, and continued work on performance and capabilities through Spark. If you\u2019d like to find out more about the latest Spark use cases and developments, sign up for <a href=\"http://spark-summit.org/east/2015\">Spark Summit East</a> in New York City in March. <a title=\"Spark Summit East 2015 Agenda is Now Available\" href=\"https://databricks.com/blog/2015/01/20/spark-summit-east-2015-agenda-is-now-available.html\">The agenda for the conference was recently posted</a>, and it\u2019s going to be our best community conference yet, with high-quality talks from industries including healthcare, finance and transportation."}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2015/02/19/extending-memsql-analytics-with-spark.html", "authors": null, "id": 2749, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2015-02-19", "tz": "UTC", "createdOn": "2015-02-19"}, "title": "Extending MemSQL Analytics with Apache Spark", "slug": "extending-memsql-analytics-with-spark", "content": "This is a guest blog from our one of our partners: <a href=\"http://www.memsql.com/\" target=\"_blank\">MemSQL</a>\n\n<hr />\n\n&nbsp;\n<h2>Summary</h2>\nCoupling operational data with the most advanced analytics puts data-driven business ahead. The MemSQL Apache Spark Connector enables such configurations.\n<h2>Meeting Transactional and Analytical Needs</h2>\nTransactional databases form the core of modern business operations. Whether that transaction is financial, physical in terms of inventory changes, or experiential in terms of a customer engagement, the transaction itself moves our business forward.\n\nBut while transactions represent the state of our business, analytics tell us patterns of the past, and help us predict patterns of the future. Analytics can tell us what levers influence profitability and put us ahead of the pack.\n\nSuccess in digital business requires both transactional and analytical prowess, including the foremost means to analyze data.\n<h2>Speed and Agility with MemSQL and Apache Spark</h2>\nAs a real-time database for transactions and analytics, MemSQL helps companies simultaneously ingest and query data with a focus on SQL operations. SQL is the lingua franca of business database operations and provides rich capabilities for complex queries, but there are some thing even SQL cannot accomplish.\n\nIn the cases where analysts and data scientists want the ability to manipulate and explore data in new ways, Apache Spark has emerged as the premier data processing framework that is fast, programmatic, and scalable. So that MemSQL users can take advantage of this functionality in Spark, MemSQL recently introduced the MemSQL Spark Connector.\n<h2>MemSQL Spark Connector Architecture</h2>\nThe MemSQL Spark Connector combines the memory-optimized and distributed architectures of both MemSQL and Spark to drive a high-throughput, highly parallelized, bi-directional link between two clusters.\n\nTwo primary components of the MemSQL Spark Connector enable Spark to query from and write to MemSQL.\n<ul>\n \t<li>A <em>MemSQLRDD</em> class for loading data from a MemSQL query</li>\n \t<li>A <em>saveToMemsql</em> function for persisting results to a MemSQL table</li>\n</ul>\n<a href=\"https://databricks.com/wp-content/uploads/2015/02/memSQL_1.png\"><img class=\"aligncenter size-large wp-image-2750\" src=\"https://databricks.com/wp-content/uploads/2015/02/memSQL_1-1024x461.png\" alt=\"memSQL_1\" width=\"1024\" height=\"461\" /></a>\n<p style=\"text-align: center;\"><b>Figure 1: MemSQL Spark Connector Architecture</b></p>\n\n<h2>Bringing Data to the Light of Day</h2>\nThe MemSQL Spark Connector takes the most current operational data and makes it accessible from Spark, expanding the analytics capabilities of MemSQL with the full range of Spark tools and libraries.\n\nMemSQL users can employ Spark\u2019s rich analytical functionality through the following steps.\n<ul>\n \t<li>Set up a replicated cluster providing clear demarcation between operations and analytics teams</li>\n \t<li>Give Spark access to live production data for the most recent and relevant results</li>\n \t<li>Allow Spark to write results set back to the primary MemSQL cluster to put new analyses into production</li>\n</ul>\n<a href=\"https://databricks.com/wp-content/uploads/2015/02/memSQL_2.png\"><img class=\"aligncenter size-large wp-image-2752\" src=\"https://databricks.com/wp-content/uploads/2015/02/memSQL_2-1024x359.png\" alt=\"memSQL_2\" width=\"1024\" height=\"359\" /></a>\n<p style=\"text-align: center;\"><b>Figure 2: Extend MemSQL Analytics</b></p>\n\n<h2>Twin Power of Memory Optimized Clusters</h2>\nWith both clusters operating at lightening fast memory optimized speeds and able to parallel process data transfers between Spark RDDs and MemSQL tables, the combination delivers top performance.\n\nGiven the native integration with Spark, data transfer is convenient as an advanced SQL query can be leveraged to push down computation to MemSQL and only transfer the data needed.\n\nFor more information on the MemSQL Spark Connector please visit:\n\n<a href=\"https://github.com/memsql/memsql-spark-connector\">Github Site for MemSQL Spark Connector</a>\n\n<a href=\"http://blog.memsql.com/memsql-spark-connector/\">MemSQL Technical Blog Post</a>\n\n<a href=\"http://www.memsql.com/download/\">MemSQL free 30 day trial</a>"}
{"status": "publish", "description": null, "creator": "rxin", "link": "https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html", "authors": null, "id": 2757, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2015-02-17", "tz": "UTC", "createdOn": "2015-02-17"}, "title": "Introducing DataFrames in Apache Spark for Large Scale Data Science", "slug": "introducing-dataframes-in-spark-for-large-scale-data-science", "content": "Today, we are excited to announce a new DataFrame API designed to make big data processing even easier for a wider audience.\n\nWhen we first open sourced Apache Spark, we aimed to provide a simple API for distributed data processing in general-purpose programming languages (Java, Python, Scala). Spark enabled distributed data processing through functional transformations on distributed collections of data (RDDs). This was an incredibly powerful API: tasks that used to take thousands of lines of code to express could be reduced to dozens.\n\nAs Spark continues to grow, we want to enable wider audiences beyond \u201cBig Data\u201d engineers to leverage the power of distributed processing. The new DataFrames API was created with this goal in mind. \u00a0This API is inspired by data frames in R and Python (Pandas), but designed from the ground-up to support modern big data and data science applications. As an extension to the existing RDD API, DataFrames feature:\n<ul>\n \t<li>Ability to scale from kilobytes of data on a single laptop to petabytes on a large cluster</li>\n \t<li>Support for a wide array of data formats and storage systems</li>\n \t<li>State-of-the-art optimization and code generation through the Spark SQL <a href=\"https://databricks.com/blog/2014/03/26/spark-sql-manipulating-structured-data-using-spark-2.html\">Catalyst</a> optimizer</li>\n \t<li>Seamless integration with all big data tooling and infrastructure via Spark</li>\n \t<li>APIs for Python, Java, Scala, and R (in development via <a href=\"http://amplab-extras.github.io/SparkR-pkg/\">SparkR</a>)</li>\n</ul>\nFor new users familiar with data frames in other programming languages, this API should make them feel at home. For existing Spark users, this extended API will make Spark easier to program, and at the same time improve performance through intelligent optimizations and code-generation.\n<h2>What Are DataFrames?</h2>\nIn Spark, a DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but\u00a0with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs.\n\nThe following example shows how to construct DataFrames in Python. A similar API is available in Scala and Java.\n\n[python]\n# Constructs a DataFrame from the users table in Hive.\nusers = context.table(\"users\")\n\n# from JSON files in S3\nlogs = context.load(\"s3n://path/to/data.json\", \"json\")\n[/python]\n<h2>How Can One Use DataFrames?</h2>\nOnce built, DataFrames provide a domain-specific language for distributed data manipulation. \u00a0Here is an example of using DataFrames to manipulate the\u00a0demographic data of a large population of users:\n\n[python]\n# Create a new DataFrame that contains \u201cyoung users\u201d only\nyoung = users.filter(users.age &lt; 21)\n\n# Alternatively, using Pandas-like syntax\nyoung = users[users.age &lt; 21]\n\n# Increment everybody\u2019s age by 1\nyoung.select(young.name, young.age + 1)\n\n# Count the number of young users by gender\nyoung.groupBy(\"gender\").count()\n\n# Join young users with another DataFrame called logs\nyoung.join(logs, logs.userId == users.userId, \"left_outer\")\n[/python]\n\nYou can also incorporate SQL while working with\u00a0DataFrames, using Spark SQL. This example counts the number of users in the <i>young</i> DataFrame.\n\n[python]\nyoung.registerTempTable(\"young\")\ncontext.sql(\"SELECT count(*) FROM young\")\n[/python]\n\nIn Python, you can also convert freely between Pandas DataFrame and Spark DataFrame:\n\n[python]\n# Convert Spark DataFrame to Pandas\npandas_df = young.toPandas()\n\n# Create a Spark DataFrame from Pandas\nspark_df = context.createDataFrame(pandas_df)\n[/python]\n\nSimilar to RDDs, DataFrames are evaluated lazily. That is to say, computation only happens when an action (e.g. display result, save output) is required. This allows their executions to be optimized, by applying techniques such as predicate push-downs and bytecode generation, as explained later in the section \"Under the Hood: Intelligent Optimization and Code Generation\". All DataFrame operations are also automatically parallelized and distributed on clusters.\n<h2>Supported Data Formats and Sources</h2>\nModern applications often need to collect and analyze data from a variety of sources. Out of the box, DataFrame supports reading data from the most popular formats, including JSON files, Parquet files, Hive tables. It can read from local file systems, distributed file systems (HDFS), cloud storage (S3), and external relational database systems via JDBC. In addition, through Spark SQL\u2019s <a href=\"https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html\">external data sources API</a>, DataFrames can be extended to support any third-party data formats or\u00a0sources. Existing third-party extensions already include Avro, CSV, ElasticSearch, and Cassandra.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/02/Introducing-DataFrames-in-Spark-for-Large-Scale-Data-Science1.png\"><img class=\"aligncenter size-full wp-image-2764\" src=\"https://databricks.com/wp-content/uploads/2015/02/Introducing-DataFrames-in-Spark-for-Large-Scale-Data-Science1.png\" alt=\"Introducing DataFrames in Spark for Large Scale Data Science\" width=\"779\" height=\"258\" /></a>\n\nDataFrames\u2019 support for data sources enables applications to easily combine data from disparate sources (known as federated query processing in database systems). For example, the following code snippet joins a site\u2019s textual traffic log stored in S3 with a PostgreSQL database to count the number of times each user has visited the site.\n\n[python]\nusers = context.jdbc(\"jdbc:postgresql:production\", \"users\")\nlogs = context.load(\"/path/to/traffic.log\")\nlogs.join(users, logs.userId == users.userId, \"left_outer\") \\\n.groupBy(\"userId\").agg({\"*\": \"count\"})\n[/python]\n<h2>Application: Advanced Analytics and Machine Learning</h2>\nData scientists are employing increasingly sophisticated techniques that go beyond joins and aggregations. To support this, DataFrames can be used directly in MLlib\u2019s <a href=\"https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html\">machine learning pipeline API</a>. In addition, programs can run arbitrarily complex user functions on DataFrames.\n\nMost common advanced analytics tasks can be specified using the new pipeline API in MLlib. For example, the following code creates a simple text classification pipeline consisting of a tokenizer, a hashing term frequency feature extractor, and logistic regression.\n\n[python]\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\nhashingTF = HashingTF(inputCol=\"words\", outputCol=\"features\")\nlr = LogisticRegression(maxIter=10, regParam=0.01)\npipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n[/python]\n\nOnce the pipeline is setup, we can use it to train on a DataFrame directly:\n\n[python]\ndf = context.load(\"/path/to/data\")\nmodel = pipeline.fit(df)\n[/python]\n\nFor more complicated tasks beyond what the machine learning pipeline API provides, applications can also apply arbitrarily complex functions on a DataFrame, which can also be manipulated using Spark\u2019s existing RDD API. The following snippet performs a word count, the \u201chello world\u201d of big data, on the \u201cbio\u201d column of a DataFrame.\n\n[python]\ndf = context.load(\"/path/to/people.json\")\n# RDD-style methods such as map, flatMap are available on DataFrames\n# Split the bio text into multiple words.\nwords = df.select(\"bio\").flatMap(lambda row: row.bio.split(\" \"))\n# Create a new DataFrame to count the number of words\nwords_df = words.map(lambda w: Row(word=w, cnt=1)).toDF()\nword_counts = words_df.groupBy(\"word\").sum()\n[/python]\n<h2>Under the Hood: Intelligent Optimization and Code Generation</h2>\nUnlike the eagerly evaluated data frames in R and Python, DataFrames in Spark have their execution automatically optimized by a query optimizer. Before any computation on a DataFrame starts, the <a href=\"https://databricks.com/blog/2014/03/26/spark-sql-manipulating-structured-data-using-spark-2.html\">Catalyst optimizer</a> compiles the operations that were used to build the DataFrame into a physical plan for execution. Because the optimizer understands the semantics of operations and structure of the data, it can make intelligent decisions to speed up computation.\n\nAt a high level, there are two kinds of optimizations. First, Catalyst applies logical optimizations such as predicate pushdown. The optimizer can push filter predicates down into the data source, enabling the physical execution to skip irrelevant data. In the case of Parquet files, entire blocks can be skipped and comparisons on strings can be turned into cheaper integer comparisons via dictionary encoding. In the case of relational databases, predicates are pushed down into the external databases to reduce the amount of data traffic.\n\nSecond, Catalyst compiles operations into physical plans for execution and generates <a href=\"https://databricks.com/blog/2014/06/02/exciting-performance-improvements-on-the-horizon-for-spark-sql.html\">JVM bytecode</a> for those plans that is often more optimized than hand-written code. For example, it can choose intelligently between broadcast joins and shuffle joins to reduce network traffic. It can also perform lower level optimizations such as eliminating expensive object allocations and reducing virtual function calls. As a result, we expect performance improvements for existing Spark programs when they migrate to DataFrames.\n\nSince the optimizer generates JVM bytecode for execution, Python users will experience the same high performance as Scala and Java users.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/02/Screen-Shot-2015-02-16-at-9.46.39-AM.png\"><img class=\"aligncenter size-large wp-image-2767\" src=\"https://databricks.com/wp-content/uploads/2015/02/Screen-Shot-2015-02-16-at-9.46.39-AM-1024x457.png\" alt=\"DataFrame performance\" width=\"1024\" height=\"457\" /></a>\n\nThe above chart compares the runtime performance of running group-by-aggregation on 10 million integer pairs on a single machine (<a href=\"https://gist.github.com/rxin/c1592c133e4bccf515dd\">source code</a>). Since both Scala and Python DataFrame operations are compiled into JVM bytecode for execution, there is little difference between the two languages, and both outperform the vanilla Python RDD variant by a factor of 5 and Scala RDD variant by a factor of 2.\n\nDataFrames were inspired by previous distributed data frame efforts, including Adatao's DDF and Ayasdi's BigDF. However, the main difference from these projects is that DataFrames go through the Catalyst optimizer, enabling optimized execution similar to that of Spark SQL queries. As we improve the Catalyst optimizer, the engine also becomes smarter, making applications faster with each new release of Spark.\n\nOur data science team at Databricks has\u00a0been using this new DataFrame API on our internal data pipelines. It has brought performance improvements to our Spark programs while making them more concise and easier to understand. We are very excited about it and believe it will make big data processing more accessible to a wider array of users.\n\nThis API will be released as part of Spark 1.3 in early March. If you can\u2019t wait, check out <a href=\"https://github.com/apache/spark/tree/branch-1.3\">Spark from GitHub</a> to try it out. If you are in the Bay Area at the Strata conference, please join us on <a href=\"http://www.meetup.com/spark-users/events/220031485/\">Feb 17 in San Jose for a meetup on this topic</a>.\n\nThis effort would not have been possible without the prior data frame implementations, and thus we would like to thank the developers of R, Pandas, DDF and BigDF for their work.\n\n<em>To try out DataFrames, <a href=\"https://databricks.com/try-databricks\">get a free trial of Databricks or use the Community Edition</a>.</em>"}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2015/02/24/databricks-at-strata-san-jose.html", "authors": null, "id": 2830, "categories": ["Company Blog", "Events"], "dates": {"publishedOn": "2015-02-24", "tz": "UTC", "createdOn": "2015-02-24"}, "title": "Databricks at Strata San Jose", "slug": "databricks-at-strata-san-jose", "content": "The Strata + Hadoop World Conference in San Jose last week was abuzz with \"putting data to work\" in keeping with this year's conference theme. This was a significant shift from last year's event where organizations\u00a0were highly focused on getting their arms around their big data projects and being steeped in evaluating the multitude of\u00a0tools of new technologies available. Last week's event highlighted what is top of mind for enterprises and developers alike - how to turn their big data initiatives and projects into real business results?\n\nOne\u00a0theme was loud and clear - Apache Spark's flame shone bright! \u00a0Derrick Harris from GigaOM summed this up aptly in his article \"<a href=\"https://gigaom.com/2015/02/20/for-now-spark-looks-like-the-future-of-big-data/\" target=\"_blank\">For now, Spark looks like the future of big data</a>\". To quote Derrick, <em>\"Titles can be misleading. For example, the O\u2019Reilly Strata + Hadoop World conference took place in San Jose, California, this week but Hadoop wasn\u2019t the star of the show. Based on the news I saw coming out of the event, it\u2019s another Apache project \u2014 Spark \u2014 that has people excited.\"</em>\n\nTo preface David Ramel from his article yesterday entitled <a href=\"http://adtmag.com/articles/2015/02/23/spark-continues-rise.aspx\" target=\"_blank\">Spark Continues Big Data Ascension</a>, <em>\"The flurry of Spark-related news and product releases further cements the project as the darling of the open source Big Data movement, showing a \"hockey stick-like\" growth in a chart measuring Spark awareness, according to a recent survey. It has been recognized as the most active Apache Software Foundation project and, indeed, most active Big Data open source project of any kind.\"</em>\n\nThe show was also jam-packed with announcements from our partners in the Spark ecosystem, with <a title=\"Extending MemSQL Analytics with Spark\" href=\"https://databricks.com/blog/2015/02/19/extending-memsql-analytics-with-spark.html\" target=\"_blank\">MemSQL</a> and <a href=\"http://www.tableau.com/about/press-releases/2015/tableau-delivers-additional-flexibility-hadoop-community-spark-sql-suppo-0\" target=\"_blank\">Tableau</a> announcing new Spark connectors. At the same time, we announced <a href=\"http://www.marketwired.com/press-release/update-databricks-intel-collaborate-optimize-apache-spark-based-analytics-intelr-architecture-1993678.htm\">a collaboration effort with Intel</a> to optimize Spark-based analytics on Intel architecture.\n\nLooking beyond the four walls of\u00a0the show, <a href=\"https://twitter.com/dberkholz/status/568561792751771648\" target=\"_blank\">Donnie Berkholz of Redmonk reflected</a> on the incredible hockey stick like surge\u00a0of Spark interest amongst users, as observed on\u00a0Stack Overflow:\n\n<img class=\"\" src=\"https://pbs.twimg.com/media/B-Pv0B-IAAAwjYz.png\" alt=\"Big Data activity on Stack Overflow\" width=\"394\" height=\"321\" />\n\nAs the team behind\u00a0Spark, we at Databricks are thrilled to have the opportunity to respond to this intense interest with Spark and to connect\u00a0with users.\u00a0 In line with this year's conference theme of turning their data initiatives into value, we had the opportunity to interact with enterprise users were keen to share with and also learn about running Spark in production on <a href=\"http://www.slideshare.net/databricks/introducing-databricks-cloud-strata-sj\">Databricks Cloud</a>.\n\nFor those of you who missed our sessions at Strata SJ last week, here are the pointers to some of the presentations and training material:\n\nThe material from the Spark Camp training class that was attended by over 300 students can be found <a href=\"http://www.slideshare.net/databricks/sparkcamp-strata-ca-intro-to-apache-spark-with-handson-tutorials\">here</a>. Info on future Spark training classes, can be found on the <a href=\"https://databricks.com/services/spark-training\">training page</a> of our website.\n\nOur Strata San Jose 2015 presentations can be found on our <a href=\"http://www.slideshare.net/databricks\" target=\"_blank\">slideshare account</a>:\n<ul>\n\t<li><a href=\"http://www.slideshare.net/databricks/large-scalesparktalk\" target=\"_blank\"><em>Lessons from Running Large Scale Spark Workloads -\u00a0</em>Reynold Xin, Matei Zaharia</a></li>\n\t<li><a href=\"http://www.slideshare.net/databricks/spark-streaming-state-of-the-union-strata-san-jose-2015\" target=\"_blank\"><em>Spark Streaming \u2014 The State of the Union, and Beyond \u00a0-\u00a0</em>Tathagata Das</a></li>\n\t<li><a href=\"http://www.slideshare.net/databricks/new-directions-for-apache-spark-in-2015\" target=\"_blank\"><em>New Directions for Spark in 2015 \u00a0-\u00a0</em>Matei Zaharia</a></li>\n\t<li><a href=\"http://www.slideshare.net/databricks/strata-debugging-talk\" target=\"_blank\"><em>Tuning and Debugging in Apache Spark -\u00a0</em>Patrick Wendell</a></li>\n\t<li><a href=\"http://www.slideshare.net/databricks/strata-sj-everyday-im-shuffling-tips-for-writing-better-spark-programs\" target=\"_blank\"><em>Everyday I\u2019m Shuffling \u2013 Tips for Writing Better Spark Programs -\u00a0</em>Vida Ha, Holden Karau</a></li>\n</ul>\nFor all of you east coast Spark enthusiasts, we will be holding the\u00a0inaugural <a href=\"http://spark-summit.org/east\" target=\"_blank\">Spark Summit East</a>\u00a0in New York City on March 18th through 19th.\u00a0 The <a title=\"Spark Summit East 2015 Agenda is Now Available\" href=\"http://spark-summit.org/east/2015/agenda\" target=\"_blank\">agenda has been released</a>,\u00a0and there will be many more informative sessions for all. \u00a0<a href=\"http://prevalentdesignevents.com/sparksummit2015/east/registration.aspx\" target=\"_blank\">Register today</a> if you have not done so!\n\nTo keep up with Databricks news, don't forget to sign up for our monthly newsletter <a href=\"http://go.databricks.com/newsletter-registration\" target=\"_blank\">here</a>."}
{"status": "publish", "description": null, "creator": "kavitha", "link": "https://databricks.com/blog/2015/03/04/databricks-cloud-from-raw-data-to-insights-and-data-products-in-an-instant.html", "authors": null, "id": 2871, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2015-03-04", "tz": "UTC", "createdOn": "2015-03-04"}, "title": "Databricks: From raw data, to insights and data products in an instant!", "slug": "databricks-cloud-from-raw-data-to-insights-and-data-products-in-an-instant", "content": "<div class=\"article-body\">\n\nEnterprises have been collecting ever-larger amounts of data with the goal of extracting insights and creating value. Yet despite a few innovative companies who are able to successfully exploit big data, the promised returns of big data remain elusive beyond the grasp of many enterprises.\n\nOne notable and rapidly growing open source technology that has emerged in the big data space is Apache Spark.\n\nSpark is an open source data processing framework that was built for speed, ease of use, and scale. Much of its benefits are due to how it unifies critical data analytics capabilities such as SQL, machine learning and streaming in a single framework. This enables enterprises to simultaneously achieve high performance computing at scale while simplifying their data processing infrastructure by avoiding the difficult integration of many disparate and difficult tools with a single powerful yet simple alternative.\n\nWhile Spark appears to have the potential to solve many big data challenges facing enterprises, many continue to struggle. Why? Because capturing value from big data requires capabilities beyond data processing; enterprises are finding out that there are many challenges in their journey to operationalize their data pipeline.\n\nFirst there is the infrastructure issue requiring data teams to pre-provision, setup and manage on-premise clusters that are both costly and time consuming. After solving the imminent infrastructure challenges, enterprises still have to contend with primitive tools that are difficult to use where working with data, code, and visualization requires switching between different software. These tools also force individuals to work in silos, stifling collaboration and making the sharing of work and communication of insights to the rest of the organization difficult.\n\nIn this typical scenario, enterprises are forced to take on the difficult task of building custom capabilities on top of Spark in order to operationalize it as an effective data platform. This severely reduces the productivity of data analytics teams, degrades their ability to focus on core tasks, and renders every big data project highly susceptible to failure. Indeed Gartner states that, \u201cthrough 2017, 60% of big-data projects will fail to go beyond piloting and experimentation and will be abandoned.\u201d\n\nInstead of attempting to operationalize Spark in-house, enterprises can benefit from obtaining Spark and the capabilities necessary to operationalize it in a single package that is easy to deploy, simple to learn, and provides the rich set of tools out-of-the-box. One of the key attributes of <a href=\"https://databricks.com/product/databricks-cloud\" target=\"_blank\" rel=\"nofollow\">Databricks Cloud</a> is its ability to provide Spark as-a-service to enterprises in a unified cloud-hosted data platform.\n\nDatabricks Cloud provides fully managed Spark clusters that can be dynamically scaled up and down in a matter of seconds. This frees enterprises to focus on extracting value out of their data instead of spending their valuable resources on operations. In addition to Spark as-a-service, Databricks Cloud includes other critical components required by enterprises to fully develop, test, deploy and manage their end-to-end data pipeline from prototype, all the way to production with no re-engineering required. These include:\n<ol>\n \t<li>An interactive workspace for exploration and visualization so teams can learn, work and collaborate in a single, easy to use environment;</li>\n \t<li>A production pipeline scheduler that helps projects go from prototype to production without re-engineering;</li>\n \t<li>An extensible platform that enables organizations to connect their existing data applications with Spark to disseminate the power of big data.</li>\n</ol>\nWith these critical components, enterprises could seamlessly transition from data ingest to exploration and production while leveraging the power of Spark. They will able to overcome the existing bottlenecks that impede their ability to operationalize Spark, and instead, focus on finding answers from their data, building data products, and ultimately capture the value promised by big data.\n\n</div>"}
{"status": "publish", "description": null, "creator": "kavitha", "link": "https://databricks.com/blog/2015/03/05/radius-intelligence-implements-databricks-cloud-for-real-time-insights-on-targeted-marketing-campaigns.html", "authors": null, "id": 2875, "categories": ["Announcements", "Company Blog", "Customers"], "dates": {"publishedOn": "2015-03-05", "tz": "UTC", "createdOn": "2015-03-05"}, "title": "Radius Intelligence implements Databricks for real-time insights on targeted marketing campaigns", "slug": "radius-intelligence-implements-databricks-cloud-for-real-time-insights-on-targeted-marketing-campaigns", "content": "We\u2019re thrilled to share that Radius Intelligence has selected Databricks as its preferred big data processing platform, to deliver real-time insights in support of targeted marketing campaigns.\n\nPress release:\u00a0<a href=\"http://www.marketwired.com/press-release/radius-intelligence-implements-databricks-cloud-maximize-data-processing-throughput-1997836.htm\" target=\"_blank\">http://www.marketwired.com/press-release/radius-intelligence-implements-databricks-cloud-maximize-data-processing-throughput-1997836.htm</a><a href=\"http://www2.marketwire.com/mw/release_html_b1?release_id=1179658\" target=\"_blank\">\n</a>\nRadius is a marketing intelligence platform that enables B2B marketers to acquire new customers intelligently. By matching customer intelligence data to Radius\u2019 weekly-updated data set of 25 million businesses in the US, marketers can deploy targeted campaigns of greats leads \u2014 net-new and existing opportunities \u2014 to their Salesforce.com instance. Radius provides two specific features, Segments and Insights, that use a combination of customer data and external data to allow CMOs to predict their future marketing and campaign success.\u00a0On a daily basis, Radius processes billions of data points from customers and external data sources.Previously, Radius used Amazon Elastic MapReduce to process data which hampered team effectiveness, code maintainability, and ability to test new methods. Radius then chose Apache Spark for their data processing framework to maximize throughput, maximize speed, and maximize engineer productivity. In order to fully harness the power of Apache Spark, Radius deployed Databricks to maintain their Spark infrastructure and to provide additional critical data processing components on top of Spark.\n\n<article class=\"blog-post-article\">Since implementing Spark on Databricks, Radius\u2019 core data index build now takes a few hours compared to more than a day with a MapReduce based system. Databricks has enabled the Radius teams to work together \u2014 data scientists and data engineers \u2014 on difficult problems that require a combination of quality development and\u00a0the scientific method. The process of testing hypothesis now can be done in a matter of minutes and in real-time rather than over the course of days.</article>Since implementing\u00a0 Databricks, Radius has seen the following company-wide benefits:\n<ul>\n \t<li>Dramatic increase in overall team effectiveness</li>\n \t<li>Improved codebase maintainability</li>\n \t<li>Focus on data science instead of performance optimizations</li>\n</ul>\nDownload this <a title=\"Customer Case Studies\" href=\"https://databricks.com/wp-content/uploads/2015/04/Databricks_Case_Study_Radius.pdf\" target=\"_blank\">case study</a>\u00a0to learn more about how Radius Intelligence is using Databricks."}
{"status": "publish", "description": "Sharethrough selects Databricks Cloud to discover hidden patterns in customer behavior data.", "creator": "kavitha", "link": "https://databricks.com/blog/2015/03/10/sharethrough-selects-databricks-to-discover-hidden-patterns-in-ad-serving-platform.html", "authors": null, "id": 2902, "categories": ["Announcements", "Company Blog", "Customers"], "dates": {"publishedOn": "2015-03-10", "tz": "UTC", "createdOn": "2015-03-10"}, "title": "Sharethrough Selects Databricks to Discover Hidden Patterns in Ad Serving Platform", "slug": "sharethrough-selects-databricks-to-discover-hidden-patterns-in-ad-serving-platform", "content": "We\u2019re really excited to announce\u00a0that Sharethrough\u00a0has selected Databricks to discover hidden patterns in customer behavior data.\n\nPress release:\u00a0<a href=\"http://www.marketwired.com/press-release/sharethrough-implements-databricks-cloud-discover-hidden-patterns-advertising-serving-1998953.htm\" target=\"_blank\">http://www.marketwired.com/press-release/sharethrough-implements-databricks-cloud-discover-hidden-patterns-advertising-serving-1998953.htm</a>\n\nSharethrough builds software for delivering ads into the natural flow of content sites and apps (also known as native advertising). Because Sharethrough serves ads on some of the most popular digital properties such as Forbes and People, the need for a high-performance big data scale processing platform permeates every aspect of their business.\n\nInitially, Sharethrough attempted to establish\u00a0a big data\u00a0platform with self-hosted Hadoop clusters, leveraging Hive as the ad hoc query tool. However, this initial\u00a0platform\u00a0severely impeded the productivity of the Sharethrough team because the self-hosted clusters\u00a0were too labor intensive to maintain and Hive was too slow for ad hoc querying.\n\nTo overcome these challenges, Sharethrough turned to\u00a0Databricks to implement a big data platform that is\u00a0simultaneously high performance and easy to maintain. They were able to deploy Databricks in Sharethrough\u2019s Virtual Private Cloud (VPC) in AWS within days. The cluster management interface in\u00a0Databricks was simple enough to enable their engineering team to create, scale, and terminate Spark clusters with a few clicks, instead of dedicating full-time engineers to this task, as was the case with the self-hosted Hadoop clusters.\n\nOnce the Apache Spark clusters were in place, Sharethrough was able to easily bring their clickstream data from AWS S3 into the interactive workspace of Databricks. The interactive workspace provides \u201cnotebooks\u201d, enabling users to work with the data in their preferred language - SQL, Python, Java, or Scala.\n\nAs a result of\u00a0deploying Databricks, Sharethrough gained a number of benefits:\n<ul>\n \t<li>Faster prototyping of new applications</li>\n \t<li>Easier debugging of complex pipelines</li>\n \t<li>Improved overall engineering team productivity.</li>\n</ul>\nDownload this <a title=\"Databricks customer case study\" href=\"https://databricks.com/wp-content/uploads/2015/03/Databricks_Case_Study_Sharethrough.pdf\" target=\"_blank\">case study</a> learn more about how Sharethrough\u00a0is using Databricks."}
{"status": "publish", "description": "Overview of Apache Spark 1.3 features, including DataFrames, MLlib, packages, and much more", "creator": "patrick", "link": "https://databricks.com/blog/2015/03/13/announcing-spark-1-3.html", "authors": null, "id": 2968, "categories": ["Apache Spark", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2015-03-13", "tz": "UTC", "createdOn": "2015-03-13"}, "title": "Announcing Apache Spark 1.3!", "slug": "announcing-spark-1-3", "content": "Today I\u2019m excited to announce the general availability of Apache Spark 1.3! Apache Spark 1.3 introduces the widely anticipated DataFrame API, an evolution of Spark\u2019s RDD abstraction designed to make crunching large datasets simple and fast. Apache Spark 1.3 also boasts a large number of improvements across the stack, from Streaming, to ML, to SQL. The release has been posted today on the Apache Spark website.\n\nWe\u2019ll be publishing in depth overview posts covering Spark\u2019s new features over the coming weeks. Some of the salient features of this release are:\n<h2>A new\u00a0DataFrame API</h2>\nThe DataFrame API that we <a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\" target=\"_blank\">recently announced</a> officially ships in Apache Spark 1.3. DataFrames evolve Spark\u2019s RDD model, making operations\u00a0with structured datasets even\u00a0faster and easier. They are inspired by, and fully interoperable with, Pandas and R data frames, and are available in Spark\u2019s Java, Scala, and Python API\u2019s as well as the upcoming (unreleased) R API. DataFrames introduce new simplified operators for filtering, aggregating, and projecting over large datasets. Internally, DataFrames leverage the Spark SQL logical optimizer to intelligently plan the physical execution of operations to work well on large datasets. This planning permeates all the way into physical storage, where optimizations such as predicate pushdown are applied based on analysis of user programs. Read more about the data frames API in the SQL programming guide.\n\n<pre># Constructs a DataFrame from a JSON dataset.\nusers = context.load(\"s3n://path/to/users.json\", \"json\")\n\n# Create a new DataFrame that contains \"young users\" only\nyoung = users.filter(users.age < 21)\n\n# Alternatively, using Pandas-like syntax\nyoung = users[users.age < 21]\n\n# DataFrame's support existing RDD operators\nprint(\"Young users: \" + young.count())</pre>\n\n<h2>Spark SQL Graduates from Alpha</h2>\nSpark SQL graduates from an alpha component in this release, guaranteeing compatibility for the SQL dialect and semantics in future releases. Spark SQL\u2019s data source API now fully interoperates with the new DataFrame component, allowing users to create DataFrames directly from Hive tables, Parquet files, and other sources. Users can also intermix SQL and data frame operators on the same data sets. New in 1.3 is the ability to read and write tables from a JDBC connection, with native support for Postgres and MySQL and other RDBMS systems. That API adds has write support for producing output tables as well, to JDBC or any other source.\n\n<pre>> CREATE TEMPORARY TABLE impressions\n  USING org.apache.spark.sql.jdbc\n  OPTIONS (\n    url \"jdbc:postgresql:dbserver\",\n    dbtable \"impressions\"\n  )\n\n> SELECT COUNT(*) FROM impressions</pre>\n\n<h2>Built-in Support for Spark Packages</h2>\nWe <a href=\"https://databricks.com/blog/2014/12/22/announcing-spark-packages.html\">earlier announced</a> an initiative to create a community package repository for Spark at the end of 2014. Today <a href=\"http://spark-packages.org/\" target=\"_blank\">Spark Packages</a> has 45 community projects catering to Spark developers, including data source integrations, testing utilities, and tutorials. To make packages easy for Spark users, Apache Spark 1.3 includes support for pulling published packages into the Spark shell or a program with a single flag.\n<pre># Launching Spark shell with a package\n./bin/spark-shell --packages databricks/spark-avro:0.2</pre>\nFor developers, Spark Packages has also created an <a href=\"https://github.com/databricks/sbt-spark-package\" target=\"_blank\">SBT plugin</a> to make publishing packages easy and introduced automatic Spark compatibility checks of new releases.\n<h2>Lower Level Kafka Support in Spark Streaming</h2>\nOver the last few releases, Kafka has become a popular input source for Spark streaming. Apache Spark 1.3 adds a new Kakfa streaming source that leverages Kafka\u2019s replay capabilities to provide reliable delivery semantics without the use of a write ahead log. It also provides primitives which enable exactly once guarantees for applications that have strong consistently requirements. Kafka support adds a Python API in this release, along with new primitives for creating Python API\u2019s in the future. For a full list of Spark streaming features see the upstream release notes.\n<h2>New Algorithms in MLlib</h2>\nApache Spark 1.3 provides a rich set of new algorithms. The latent Dirichlet allocation (LDA) is one of the first topic modeling algorithms to appear in MLlib. We\u2019ll be documenting LDA in more detail in a follow-up post. Spark\u2019s logistic regression has been generalized to multinomial logistic regression for multiclass classification. This release also adds improved clustering through Gaussian mixture models and power iteration clustering, and frequent itemsets mining through FP-growth. Finally, an efficient block matrix abstraction is introduced for distributed linear algebra. Several other algorithms and utilities are added and discussed in the full release notes.\n<h2>Related in-depth blog posts:</h2>\n<ul>\n \t<li><a title=\"What\u2019s new for Spark SQL in Spark 1.3\" href=\"https://databricks.com/blog/2015/03/24/spark-sql-graduates-from-alpha-in-spark-1-3.html\" target=\"_blank\">What\u2019s new for Spark SQL in Spark 1.3</a></li>\n \t<li><a href=\"https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html\" target=\"_blank\">Topic modeling with LDA: MLlib meets GraphX</a></li>\n \t<li><a title=\"Improvements to Kafka integration of Spark Streaming\" href=\"https://databricks.com/blog/2015/03/30/improvements-to-kafka-integration-of-spark-streaming.html\" target=\"_blank\">Improvements to Kafka integration of Spark Streaming</a></li>\n</ul>\n\n<hr />\n\nThis post only scratches the surface of interesting features in Apache Spark 1.3. Overall, this release contains more than 1000 patches from 176 contributors making it our largest yet. Head over to the <a href=\"http://spark.apache.org/releases/spark-release-1-3-0.html\" target=\"_blank\">official release notes</a> to learn more about this release, and watch the Databricks blog for more detailed posts about the major features in the next few weeks!"}
{"status": "publish", "description": "Big Data visualization with PanTera, leveraging the power of Apache Spark via Databricks", "creator": "kavitha", "link": "https://databricks.com/blog/2015/03/19/pantera-big-data-visualization-leverages-the-power-of-the-databricks-cloud.html", "authors": null, "id": 2994, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2015-03-19", "tz": "UTC", "createdOn": "2015-03-19"}, "title": "PanTera Big Data Visualization Leverages the Power of Databricks", "slug": "pantera-big-data-visualization-leverages-the-power-of-the-databricks-cloud", "content": "This is a guest blog from our one of our partners: <a href=\"http://uncharted.software/\">Uncharted</a> formerly known as Oculus Info, Inc.\n\n<hr />\n\n<strong>About PanTera<sup>TM</sup> </strong>\n\nPanTera was created with the fundamental guiding principles that visualization, interaction, and nuance are critical to understanding data. PanTera unlocks the specific opportunity and richness presented by large amounts of data by enabling interactive visual exploration at scale while preserving detail. These visualizations harness the power of human perception to rapidly identify patterns and form new hypotheses.\n\nCurrent tools have powerful libraries for data manipulation and analysis but largely rely on sampling or aggregation for visualization. This often means seeing the big picture at the expense of the whole picture. At Uncharted, we avoided this compromise by creating an application to allow the exploration of the entirety of large datasets with the ability to focus on any area of interest, all the way down to a single data point.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/03/BlogPost-NewYorkMagnify.png\"><img class=\"alignnone wp-image-2995 size-large\" src=\"https://databricks.com/wp-content/uploads/2015/03/BlogPost-NewYorkMagnify-1024x721.png\" alt=\"BlogPost-NewYorkMagnify\" width=\"1024\" height=\"721\" /></a>\n\n<em>Figure 1: 1.6 B Geolocated Instagram posts provide an overview of global usage. Zooming in on New York City, reveals detail showing quantization of some of the location data</em>\n\nPanTera and the Databricks make the visual exploration of big data accessible to any analyst, on a platform that is powerful enough for even the most experienced data scientists and developers. The application lets analysts work with multi-dimensional, high-resolution data without making assumptions for them.\n\n<strong>Built with the Speed and Flexibility of Spark</strong>\n\nPanTera allows users to quickly and iteratively explore the entirety of large datasets. To accomplish this, PanTera harnesses the power of Apache Spark to create a tile-based visualization system with parallels to browser-based, geographic maps such as Google Maps. Uncharted chose Spark because it gave us the most flexibility and speed.\n\nSpark allows us the flexibility to work with pre-existing or live streaming data, and to implement batch or on-demand processing, all with a single code base. Spark\u2019s ability to handle SQL queries as well as Scala and Java code let us quickly create a tool that can handle tiling any kind of data, numerical or unstructured text. Most importantly, Spark\u2019s memory-optimized computation caches entire datasets, allowing us to make deep exploration fast and easy.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/03/Bitcoin-Screen-Shot.png\"><img class=\"alignnone wp-image-2996 size-large\" src=\"https://databricks.com/wp-content/uploads/2015/03/Bitcoin-Screen-Shot-1024x684.png\" alt=\"Bitcoin Screen Shot\" width=\"1024\" height=\"684\" /></a>\n\n<em>Figure 2: Cross-Plot of 13M Bitcoin Transactions in PanTera</em>\n\n<strong>Discovery with the ease of Databricks</strong>\n\nThe real-time exploratory nature of PanTera is only possible with on-demand access to cluster computing resources. Databricks allows easy provisioning of the right cluster for the job, and enables flexibility as needs change. Essential elements of data capture and ETL are also handled by Databricks, allowing us to focus on creating great visualizations.\n\nFor our users, Databricks extends PanTera\u2019s capabilities by creating an iterative workflow where analysts visualize data, then perform data manipulation in the Databricks Notebook and visualize again. These critical Databricks features enable our users to perform great analysis and avoid infrastructure headaches.\n\nDon\u2019t forget to check out<a href=\"http://pantera.io\"> our website</a> to learn more about PanTera and experience how our visual interface for big data enhances statistical exploration on Databricks."}
{"status": "publish", "description": null, "creator": "kavitha", "link": "https://databricks.com/blog/2015/03/17/sparking-an-anti-money-laundering-revolution.html", "authors": null, "id": 3021, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2015-03-17", "tz": "UTC", "createdOn": "2015-03-17"}, "title": "Spark\u2019ing an Anti Money Laundering Revolution", "slug": "sparking-an-anti-money-laundering-revolution", "content": "This is a guest blog from our one of our partners: <a href=\"http://tresata.com/\">Tresata</a>\n\n<hr />\n\nTresata and Databricks announced a real-time, Apache Spark and Hadoop-powered Anti-Money Laundering solution earlier today. Tresata\u2019s predictive analytics application TEAK, offers for the first time in the market an at-scale, real-time AML investigation and resolution engine.\n\nThe performance, speed, predictive power and precision TEAK delivers would not have been possible without its Spark underpinnings.\n\nAdditionally, by being one of the first business applications to be certified to run on Databricks Cloud, Tresata\u2019s TEAK is breaking new ground and offering Banks, Retailers, Telcos &amp; Regulators the only quick start, rapidly scalable AML solution.\n\nTresata has always been at the leading edge of developing industry leading data analytics applications for complex, critical and crucial business processes. It was the first analytics application company powering its entire software with Hadoop, was the first to use cascading, the first to use scalding, and the first to use Spark. In keeping with the \u2018trend\u2019, we are proud to be the first to use the Databricks Cloud for deploying a core industry application.\n\nWhen our application engines became one of the <a href=\"https://databricks.com/blog/2014/03/18/Spark-certification.html\">first to be \u201cCertified on Spark\u201d</a>, we immediately saw the massive productivity boost our customers achieved leveraging a 100% Spark-powered analytics platform. This gave us an incentive to work closely with Databricks to also enable one of our recent analytics applications to work on the Databricks Cloud, especially given the enormity of the challenge.\n\n<em>Tresata has always believed that new big data technologies will unleash trillions of dollars of economic value. Money laundering is just one such area where it by itself is a trillion dollar problem\u2026annually. </em>\n\nThe illegal process of taking \u2018dirty\u2019 money and making it \u2018clean\u2019 requires passing funds through an intricate and interconnected network of people, places &amp; things and their inherent but otherwise unseen interconnections. Why is it that current AML solutions have been solely focused on entity-level (person, business, corporation) or transaction level risk scores, without viewing them within the context of the greater network risk score?\n\nThe short answer is a lack of technological prowess that unites all dimensions of accurately predicting AML. And in real-time.\n\nUntil now.\n\nRecognizing this fatal flaw in current AML solutions - taking an entity-level look at what is a massive network problem \u2013 Tresata incubated TEAK in its R&amp;D lab almost two years ago. Powered by the only real-time Spark certified network discovery, traversal and query engine (<a href=\"http://tresata.com/platform/\">Tresata ORION</a>), Tresata successfully deployed its advanced algorithms and capability to look at entire networks in real time to bring a new dimension to this problem \u2013 Tresata Network Scores.\n\nThese scores are at the heart of TEAK\u2019s success at precisely predicting fraudulent transactions from not just entity dynamics but based on the entire supply-chain of money movement.\n\nThis required some breakthrough technological innovations, which would not have been possible without Spark, namely:\n<ul>\n \t<li><strong>Interactive Real-Time Scoring Engine:</strong> in memory RDDs &amp; optimized data structures provide incredibly fast refresh response (typically few seconds, no more than 20 seconds for complex queries)</li>\n \t<li><strong>At-scale immediate response graph traversal:</strong> with an easy to use query language (QUE \u2013 our SQL-like graph query language) and a property based graph model one can do multi-hop traversals in network analysis</li>\n \t<li><strong>Graph Search Engine:</strong> Ability to scan the entire dataset and enabled to do more than just point queries</li>\n \t<li><strong>Go beyond a few thousand of nodes:</strong> at scale graph engine that works for &gt; 50MM entities, hundreds of millions of edges and doesn\u2019t skimp on performance</li>\n</ul>\nTresata is excited to partner with Databricks to provide an answer to one of the biggest challenges that governments, societies and institutions face today. It is our belief that with the rise of open-source, distributed computing, this collaboration with Databricks spurs a true business revolution \u2013 one that applies the massive power of Spark, Hadoop, clouds and algorithms to solving massive business problems.\n\nTresata\u2019s AML solution - powered by Spark and delivered in Databricks cloud \u2013 delivers that power today."}
{"status": "publish", "description": null, "creator": "ali", "link": "https://databricks.com/blog/2015/03/18/databricks-launches-jobs-feature-for-production-workloads.html", "authors": null, "id": 3028, "categories": ["Announcements", "Company Blog", "Product"], "dates": {"publishedOn": "2015-03-18", "tz": "UTC", "createdOn": "2015-03-18"}, "title": "Databricks Launches \"Jobs\" Feature for Production Workloads", "slug": "databricks-launches-jobs-feature-for-production-workloads", "content": "Databricks now includes a new feature called Jobs, enabling support for running production pipelines, consisting of standalone Spark applications. Jobs includes a scheduler that enables data scientists and engineers to specify a periodic schedule for their production jobs, which will be executed according to the specified schedule.\n\n<b>Notebooks as Jobs</b>\n\nIn addition to supporting running standalone Apache Spark applications, the Jobs feature provides a unique capability that allows running Databricks notebooks as jobs. That is, a job can be specified to use an existing Notebook, which will then be executed according to the specified schedule. This enables seamless transition between interactive exploration and production. Thus, data scientists can use notebooks as they did before, to perform their interactive data explorations. Once the notebook is sufficiently developed, it can then be transitioned to production use as a Job, without requiring any time consuming code rewrites. The output of every run of a job, including graphical output, is also stored as a notebook, which can be opened and used as any other notebook, allowing interactive debugging or further post-hoc exploration. This way, data scientists can repeatedly iterate and improve their jobs without having to spend time rewriting and moving code between different systems.\n\n&nbsp;\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/03/jobs-screenshot.png\"><img class=\"alignnone wp-image-3034 size-large\" src=\"https://databricks.com/wp-content/uploads/2015/03/jobs-screenshot-1024x433.png\" alt=\"jobs-screenshot\" width=\"1024\" height=\"433\" /></a>\n\n&nbsp;\n\n<b>Notebooks as Workflows</b>\n\nIn addition to running notebooks as jobs, users can run compiled applications and libraries as jobs. We have found users to often use a notebook to specify a workflow that calls other standalone jobs. Such workflows can conveniently be scripted in a language such as Python, using simple if-statements and exception-handling. Using notebooks in this way to specify production workflows is very powerful, as virtually any pattern can be expressed using a notebook.\n\n<b>Flexible Cluster Support</b>\n\nJobs integrate with Databricks\u2019 existing clusters. A job can be specified to use an existing Databricks cluster. Furthermore, a job can be specified to have its own dedicated cluster that is launched and torn down on every run. This will ensure that the job gets its own dedicated cluster, isolating it from errors caused by other users and jobs. Clusters can be launched on AWS on-demand instances, as well as the much cheaper spot instances. Furthermore, there is support for a hybrid mode called, fallback-on-demand, which tries to launch most of the cluster machines on spot instances, but will fallback on on-demand instances if the supply of spot instances is limited. This way, organizations can be sure to get the clusters they request, while cutting costs when possible, by using spot instances.\n\n<b>Notification Support</b>\n\nThe job feature comes with a notification system, which can be configured to send an email to a set of users whenever a production job completes or fails. This is particularly important as jobs run with no human-in-the-loop, requiring attention whenever something goes wrong.\n\nThe launch of the jobs feature is aimed at further improving the end-to-end user experience of Databricks. Notebooks can now be used in production workloads, in addition to being useful as Libraries (notebooks can call other notebooks), Dashboards, and online collaboration. Although this is the first official release of the Jobs feature, we have several customers already using it in production environments as part of our early adopter program.\n\nWe would love to hear your feedback - please let us know what you think about this new feature!"}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2015/03/20/using-mongodb-with-spark.html", "authors": null, "id": 3040, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2015-03-20", "tz": "UTC", "createdOn": "2015-03-20"}, "title": "Using MongoDB with Apache Spark", "slug": "using-mongodb-with-spark", "content": "[sidenote]<strong>Update August 4th 2016:</strong>\nSince this original post, MongoDB has released a new Databricks-certified connector for Apache Spark. See the <a href=\"https://databricks.com/blog/2016/07/21/the-new-mongodb-connector-for-apache-spark-in-action-building-a-movie-recommendation-engine.html\">updated blog post</a> for a tutorial and notebook on using the new MongoDB Connector for Apache Spark.[/sidenote]\n\n<hr/>\n\n[sidenote]This is a guest blog from Matt Kalan, a Senior\u00a0Solution Architect at\u00a0<a href=\"http://www.mongodb.com/\" target=\"_blank\">MongoDB</a>[/sidenote]\n\n<hr />\n\n<h2>Introduction</h2>\nThe broad spectrum of data management technologies available today makes it difficult for users to discern hype from reality. While I know the immense value of <a href=\"http://www.mongodb.com/\" target=\"_blank\">MongoDB</a> as a real-time, distributed operational database for applications, I started to experiment with Apache Spark because I wanted to understand the options available for analytics and batch operations.\n\nI started with a simple example of taking 1-minute time series intervals of stock prices with the opening (first) price, high (max), low (min), and closing (last) price of each time interval and turning them into 5-minute intervals (called OHLC bars).\u00a0\u00a0 The 1-minute data is stored in MongoDB and is then processed in Spark via the MongoDB Hadoop Connector, which allows MongoDB to be an input or output to/from Spark.\n\nOne might imagine that a more typical example is that you record this market data in MongoDB for real-time purposes but then potentially run the analytical models in another environment offline. Of course the models would be way more complicated \u2013 this is just as a Hello World level example. I chose OHLC bars just because that was the data I found easily.\n<h2>Summary</h2>\n<strong>Use case</strong>: aggregating 1-minute intervals of stock prices into 5-minute intervals\n\n<strong>Input</strong>: 1-minute stock prices intervals in a MongoDB database\n\n<strong>Simple Analysis:</strong> performed with Spark\n\n<strong>Output: </strong>5-minute stock price intervals in Spark and optionally write back into MongoDB\n\n<strong>Steps to set up the environment:</strong>\n<ul>\n \t<li><strong>Set up Spark environment</strong> \u2013 I installed Spark v1.2.0 in a VM on my Mac laptop</li>\n \t<li><strong>Download sample data</strong> \u2013 I acquired these data points in <a href=\"http://www.google.com/url?q=http%3A%2F%2Fwww.barchartmarketdata.com%2Fdata-samples%2Fmstf.csv&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNErKsHfV7l6PKvm6igSQIpuVbfabQ\">1 minute</a> increments from <a href=\"http://www.barchartmarketdata.com/sample-data-feeds\">this web page</a></li>\n \t<li><strong>Install MongoDB on the VM</strong> \u2013 I easily installed MongoDB with yum on CentOS with instructions from <a href=\"http://docs.mongodb.org/manual/tutorial/install-mongodb-on-red-hat-centos-or-fedora-linux/\">this page</a></li>\n \t<li><strong>Start MongoDB </strong>\u2013 a default configuration file is installed by yum so you can just run this to start on localhost and the default port <em>27017</em>\u00a0:</li>\n</ul>\n<pre>mongod -f /etc/mongod.conf</pre>\n<ul>\n \t<li><strong>Load sample data</strong> \u2013 <a href=\"http://docs.mongodb.org/manual/reference/program/mongoimport/\">mongoimport</a> allows you to load CSV files directly as a flat document in MongoDB. The command is simply this:</li>\n</ul>\n<pre>mongoimport equities-msft-minute-bars-2009.csv --type csv --headerline -d marketdata -c minibars</pre>\n<ul>\n \t<li><strong>Install MongoDB Hadoop Connector</strong> \u2013 You can download the Hadoop Connector jar at: <a href=\"https://github.com/mongodb/mongo-hadoop/wiki/Spark-Usage\">Using the MongoDB Hadoop Connector with Spark</a>. If you use the Java interface for Spark, you would also download the MongoDB Java Driver jar. Any jars that you download can be added to Spark using the --jars option to the PySpark command. I used Python with Spark below (called PySpark).</li>\n</ul>\nFor the following examples, here is what a document looks like in the MongoDB collection (via the Mongo shell). You start the Mongo shell simply with the command \u201cmongo\u201d from the /bin directory of the MongoDB installation.\n<pre>&gt; use marketdata \n&gt; db.minbars.findOne()\n{\n\t\"_id\" : ObjectId(\"54c00d1816526bc59d84b97c\"),\n \t\"Symbol\" : \"MSFT\",\n\t\"Timestamp\" : \"2009-08-24 09:30\",\n\t\"Day\" : 24,\n \t\"Open\" : 24.41,\n\t\"High\" : 24.42,\n\t\"Low\" : 24.31,\n \t\"Close\" : 24.31,\n\t\"Volume\" : 683713\n}\n</pre>\n<h2>Spark Example</h2>\nFor my initial foray into Spark, I opted to use Python with the interactive shell command \u201cPySpark\u201d. This gave me an interactive Python environment for leveraging Spark classes. Python appears to be popular among quants because it is a more natural language to use for interactive querying compared to Java or Scala. I was able to successfully read from MongoDB in Spark, but make sure you upgrade to Spark v1.2.2 or v1.3.0 to address a bug in earlier versions of PySpark.\n\nThe benefits of Spark were immediately evident, and in line with what you would expect in an interactive environment \u2013 queries return quickly, much faster than Hive, due in part to the fact they are not compiled to MapReduce. While the latency is still higher than MongoDB\u2019s internal querying and aggregation framework, there are more options for distributed, multi-threaded analysis with Spark, so it clearly has a role to play for data analytics.\n\n[python]\n# set up parameters for reading from MongoDB via Hadoop input format\nconfig = {&quot;mongo.input.uri&quot;: &quot;mongodb://localhost:27017/marketdata.minbars&quot;}\ninputFormatClassName = &quot;com.mongodb.hadoop.MongoInputFormat&quot;\n# these values worked but others might as well\nkeyClassName = &quot;org.apache.hadoop.io.Text&quot;\nvalueClassName = &quot;org.apache.hadoop.io.MapWritable&quot;\n \n# read the 1-minute bars from MongoDB into Spark RDD format\nminBarRawRDD = sc.newAPIHadoopRDD(inputFormatClassName, keyClassName, valueClassName, None, None, config)\n\n# configuration for output to MongoDB\nconfig[&quot;mongo.output.uri&quot;] = &quot;mongodb://localhost:27017/marketdata.fiveminutebars&quot;\noutputFormatClassName = &quot;com.mongodb.hadoop.MongoOutputFormat&quot;\n \n# takes the verbose raw structure (with extra metadata) and strips down to just the pricing data\nminBarRDD = minBarRawRDD.values()\n \nimport calendar, time, math\n \ndateFormatString = '%Y-%m-%d %H:%M'\n \n# sort by time and then group into each bar in 5 minutes\ngroupedBars = minBarRDD.sortBy(lambda doc: str(doc[&quot;Timestamp&quot;])).groupBy(lambda doc (doc[&quot;Symbol&quot;], math.floor(calendar.timegm(time.strptime(doc[&quot;Timestamp&quot;], dateFormatString)) / (5*60))))\n \n# define function for looking at each group and pulling out OHLC\n# assume each grouping is a tuple of (symbol, seconds since epoch) and a resultIterable of 1-minute OHLC records in the group\n \n# write function to take a (tuple, group); iterate through group; and manually pull OHLC\ndef ohlc(grouping):\n\tlow = sys.maxint\n\thigh = -sys.maxint\n\ti = 0\n\tgroupKey = grouping[0]\n\tgroup = grouping[1]\n\tfor doc in group:\n     \t#take time and open from first bar\n     \tif i == 0:\n          \topenTime = doc[&quot;Timestamp&quot;]\n          \topenPrice = doc[&quot;Open&quot;]\n\n     \t#assign min and max from the bar if appropriate\n     \tif doc[&quot;Low&quot;] &lt; low:\n          \tlow = doc[&quot;Low&quot;]\n\n     \tif doc[&quot;High&quot;] &gt; high:\n          \thigh = doc[&quot;High&quot;]\n\n     \ti = i + 1\n     \t# take close of last bar\n     \tif i == len(group):\n          \tclose = doc[&quot;Close&quot;]\n\t\toutputDoc = {&quot;Symbol&quot;: groupKey[0], \n    \t&quot;Timestamp&quot;: openTime,\n     \t&quot;Open&quot;: openPrice,\n     \t\t&quot;High&quot;: high,\n     \t\t&quot;Low&quot;: low,\n \t\t&quot;Close&quot;: close}\n\n\n\treturn (None, outputDoc)\n\n resultRDD = groupedBars.map(ohlc)\n \nresultRDD.saveAsNewAPIHadoopFile(&quot;file:///placeholder&quot;, outputFormatClassName, None, None, None, None, config)\n[/python]\n\nI saw the appeal of Spark from my first introduction. It was pretty easy to use. It is also especially nice in that it has operations that run on all elements in a list or a matrix of data. I can also see the appeal of having statistical capabilities like R, but in which the data can be distributed across many nodes easily (there is a Spark project for R as well).\n\nSpark is certainly new, and I had to use Spark v1.2.2 or later due to a bug (<a href=\"https://issues.apache.org/jira/browse/SPARK-5361\">SPARK-5361</a>) that initially prevented me from writing from PySpark to a Hadoop file (writing to Hadoop &amp; MongoDB in Java &amp; Scala should work). Another drawback I encountered was the difficulty to visualize data during an interactive session in PySpark. It reminded me of my college days being frustrated debugging matrices representing ray traces in Matlab, before they added better tooling. Likewise there are still challenges in displaying the data in the RDD structures; while there is a function collect() for creating lists that are more easily printable, some elements such as iterables remain difficult to display.\n<h2>Key Takeaways of Using MongoDB with Spark</h2>\n<h3>Spark is easy to integrate with MongoDB</h3>\nOverall it was useful to see how data in MongoDB can be accessed via Spark. In retrospect, I spent more time manipulating the data than I did integrating them with MongoDB, which is what I had hoped. I also started with a pre-configured VM on a single node instead of setting up the environment. I have since learned of the Databricks Cloud, which I expect would make a larger installation easy.\n<h3>Many real-world applications exist</h3>\nA real-life scenario for this kind of data manipulation is storing and querying real-time, intraday market data in MongoDB. Prices update throughout the current day, allowing users to querying them in real-time. Using Spark, after the end of day (even if the next day begins immediately like with FX), individual ticks can be aggregated into structures that are more efficient to access, such as these OHLC bars, or large documents with arrays of individual ticks for the day, by ticker symbol. This approach gives great write throughput during the day for capture, as well as blazing fast access to weeks, month, or years of prices. There are users of MongoDB whose systems follow this approach, and who have dramatically reduced latency for analytics, as well as reduced their hardware footprint. By storing the aggregated data back in MongoDB, you can index the data flexibly and retrieve it quickly.\n\nFor more information, you can visit:\n<ul>\n \t<li><a href=\"https://github.com/mongodb/mongo-hadoop/wiki/Spark-Usage\" rel=\"nofollow\">Documentation for Using MongoDB with Spark</a></li>\n \t<li><a href=\"http://www.mongodb.com/what-is-mongodb\">MongoDB Overview</a></li>\n \t<li><a href=\"http://www.mongodb.com/lp/white-paper/big-data-nosql\">MongoDB Big Data White Paper</a></li>\n \t<li><a href=\"https://www.mongodb.com/lp/download/mongodb-enterprise\">Download MongoDB Enterprise Edition</a></li>\n \t<li><a href=\"https://www.mongodb.org/downloads\">Download MongoDB Community edition</a></li>\n</ul>"}
{"status": "publish", "description": "Spark SQL new features in Apache Spark 1.3", "creator": "michael", "link": "https://databricks.com/blog/2015/03/24/spark-sql-graduates-from-alpha-in-spark-1-3.html", "authors": null, "id": 3069, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2015-03-24", "tz": "UTC", "createdOn": "2015-03-24"}, "title": "What's new for Spark SQL in Apache Spark 1.3", "slug": "spark-sql-graduates-from-alpha-in-spark-1-3", "content": "The Apache Spark 1.3 release represents a major milestone for Spark SQL. \u00a0In addition to several major features, we are very excited to announce that the project has officially graduated from Alpha, after being introduced only a little under a year ago. \u00a0In this blog post we will discuss exactly what this step means for compatibility moving forward, as well as highlight some of the major features of the release.\n<h2>Graduation from Alpha</h2>\nWhile we know many organizations (including all of Databricks' customers) have already begun using Spark SQL in production, the graduation from Alpha comes with a promise of stability for those building applications using this component. \u00a0Like the rest of the Spark stack, we now promise binary compatibility for all public interfaces through the Apache Spark 1.X release series.\n\nSince the SQL language itself and our interaction with Apache Hive represent a very large interface, we also wanted to take this chance to articulate our vision for how the project will continue to evolve. A large number of Spark SQL users have data in Hive metastores and legacy workloads which rely on Hive QL. As a result, Hive compatibility will remain a major focus for Spark SQL moving forward\n\nMore specifically, the HiveQL interface provided by the HiveContext remains the most complete dialect of SQL that we support and we are committed to continuing to maintain compatibility with this interface. \u00a0In places where our semantics differ in minor ways from Hive's (i.e. <a href=\"https://issues.apache.org/jira/browse/SPARK-5680\" target=\"_blank\">SPARK-5680</a>), we continue to aim to provide a superset of Hive's functionality. \u00a0Additionally, while we are excited about all of the new data sources that are available through the improved native Data Sources API (see more below), we will continue to support reading tables from the Hive Metastore using Hive's SerDes.\n\nThe new DataFrames API (also discussed below) is currently marked experimental. \u00a0Since this is the first release of this new interface, we wanted an opportunity to get feedback from users on\u00a0the API before it is set in stone. \u00a0That said, we do not anticipate making any major breaking changes to DataFrames, and hope to remove the experimental tag from this part of Spark SQL in Apache Spark 1.4. \u00a0You can track progress and report any issues at <a href=\"https://issues.apache.org/jira/browse/SPARK-6116\">SPARK-6116</a>.\n<h2>Improved Data Sources API</h2>\nThe Data Sources API was another major focus for this release, and provides a single interface for loading and storing data using Spark SQL. \u00a0In addition to the sources that come prepackaged with the Apache Spark distribution, this API provides an integration point for external developers to add support for custom data sources. \u00a0At Databricks, we have already contributed libraries for reading data stored in <a href=\"http://spark-packages.org/package/databricks/spark-avro\">Apache Avro</a> or <a href=\"http://spark-packages.org/package/databricks/spark-csv\">CSV</a> and we look forward to contributions from others in the community (check out <a href=\"http://spark-packages.org/\">spark packages</a> for a full list of sources that are currently available).\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-23-at-3.42.56-PM.png\"><img class=\"aligncenter size-large wp-image-3072\" src=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-23-at-3.42.56-PM-1024x305.png\" alt=\"Spark SQL data sources\" width=\"1024\" height=\"305\" /></a>\n<h2></h2>\n<h2>Unified Load/Save Interface</h2>\nIn this release we added a unified interface to SQLContext and DataFrame for loading and storing data using both the built-in and external data sources. \u00a0These functions provide a simple way to load and store data, independent of whether you are writing in Python, Scala, Java, R or SQL. \u00a0The examples below show how easy it is to both load data from Avro and convert it into parquet in different languages.\n<h3>Scala</h3>\n\n<pre>val df = sqlContext.load(\"/home/michael/data.avro\", \"com.databricks.spark.avro\")\ndf.save(\"/home/michael/data.parquet\", \"parquet\")</pre>\n\n<h3>Python</h3>\n\n<pre>df = sqlContext.load(\"/home/michael/data.avro\", \"com.databricks.spark.avro\")\ndf.save(\"/home/michael/data.parquet\", \"parquet\")</pre>\n\n<h3>Java</h3>\n\n<pre>DataFrame df = sqlContext.load(\"/home/michael/data.avro\", \"com.databricks.spark.avro\")\ndf.save(\"/home/michael/data.parquet\", \"parquet\")</pre>\n\n<h3>SQL</h3>\n\n<pre>> CREATE TABLE avroData\n  USING com.databricks.spark.avro\n  OPTIONS (\n    path \"/home/michael/data.avro\"\n  )\n\n> CREATE TABLE parquetData\n  USING parquet\n  OPTIONS (\n    path \"/home/michael/data/parquet\")\n  AS SELECT * FROM avroData</pre>\n\n<h2>Automatic Partition Discovery and Schema Migration for Parquet</h2>\nParquet has long been one of the fastest data sources supported by Spark SQL. \u00a0With its columnar format, queries against parquet tables can execute quickly by avoiding the cost of reading unneeded data.\n\nIn the Apache Spark 1.3 release we added two major features to this source. \u00a0First, organizations that store lots of data in parquet often find themselves evolving the schema over time by adding or removing columns. \u00a0With this release we add a new feature that will scan the metadata for all files, merging the schemas to come up with a unified representation of the data. \u00a0This functionality allows developers to read data where the schema has changed overtime, without the need to perform expensive manual conversions.\n\nAdditionally, the parquet datasource now supports auto-discovering data that has been partitioned into folders, and then prunes which folders are scanned based on predicates in queries made against this data. \u00a0This optimization means that you can greatly speed up may queries simply by breaking up your data into folders. \u00a0For example:\n\n<pre>/data/year=2014/file.parquet\n/data/year=2015/file.parquet\n...\n\nSELECT * FROM table WHERE year = 2015</pre>\n\nIn Apache Spark 1.4, we plan to provide an interface that will allow other formats, such as ORC, JSON and CSV, to take advantage of this partitioning functionality.\n<h2>Persistent Data Source Tables</h2>\nAnother feature that has been added in Apache Spark 1.3 is the ability to persist metadata about Spark SQL Data Source tables to the Hive metastore. \u00a0These tables allow multiple users to share the metadata about where data is located in a convenient manner. \u00a0Data Source tables can live alongside native Hive tables, which can also be read by Spark SQL.\n<h2>Reading from JDBC Sources</h2>\nFinally, a Data Source for reading from JDBC has been added as built-in source for Spark SQL. \u00a0Using this library, Spark SQL can extract data from any existing relational databases that supports JDBC. \u00a0Examples include mysql, postgres, H2, and more. \u00a0Reading data from one of these systems is as simple as creating a virtual table that points to the external table. \u00a0Data from this table can then be easily read in and joined with any of the other sources that Spark SQL supports.\n\n<pre>> CREATE TEMPORARY TABLE impressions\n  USING org.apache.spark.sql.jdbc\n  OPTIONS (\n    url \"jdbc:postgresql:dbserver\",\n    dbtable \"impressions\"\n  )\n\n> SELECT COUNT(*) FROM impressions</pre>\n\nThis functionality is a great improvement over Spark's earlier support for JDBC (i.e., <a href=\"https://spark.apache.org/docs/1.3.0/api/java/org/apache/spark/rdd/JdbcRDD.html\">JdbcRDD</a>). \u00a0Unlike the pure RDD implementation, this new DataSource supports automatically pushing down predicates, converts the data into a DataFrame that can be easily joined, and is accessible from Python, Java, and SQL in addition to Scala.\n<h2>Introducing DataFrames</h2>\nWhile we have already talked about the DataFrames in <a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\">other blog posts</a> and <a href=\"http://www.slideshare.net/databricks/spark-sqlsse2015public\">talks at the Spark Summit East</a>, any post about Apache Spark 1.3 would be remiss if it didn't mention this important new API. DataFrames evolve Spark\u2019s RDD model, making it faster and easier for Spark developers to work with structured data by providing simplified methods for filtering, aggregating, and projecting over large datasets. Our DataFrame implementation was inspired by Pandas' and R's data frames, and are fully interoperable with these implementations. \u00a0Additionally, Spark SQL DataFrames are available in Spark\u2019s Java, Scala, and Python API\u2019s as well as the upcoming (unreleased) R API.\n<a href=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-23-at-3.59.28-PM.png\"><img class=\"aligncenter size-large wp-image-3081\" src=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-23-at-3.59.28-PM-1024x236.png\" alt=\"Spark SQL blog - dataframes\" width=\"1024\" height=\"236\" /></a>Internally, DataFrames take advantage of the Catalyst query optimizer to intelligently plan the execution of your big data analyses. This planning permeates all the way into physical storage, where optimizations such as predicate pushdown are applied based on analysis of user programs. \u00a0Since this planning is happening at the logical level, optimizations can even occur across function calls, as shown in the example below.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-23-at-4.00.33-PM.png\"><img class=\"aligncenter size-large wp-image-3082\" src=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-23-at-4.00.33-PM-1024x577.png\" alt=\"dataframe optimizations\" width=\"1024\" height=\"577\" /></a>\n\nIn this example, Spark SQL is able to push the filtering of users by their location through the join, greatly reducing its cost to execute. \u00a0This optimization is possible even though the original author of the <code>add_demographics</code> function did not provide a parameter for specifying how to filter users!\n\nThis is only example of how Spark SQL DataFrames can make developers more efficient by providing a simple interface coupled with powerful optimization.\n\n&nbsp;\n\nTo learn more about Spark SQL, Dataframes, or Apache Spark 1.3, checkout the\u00a0<a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes\">SQL programming guide</a>\u00a0on the Apache Spark website. Stay tuned to\u00a0this blog for updates on other components of the <a title=\"Announcing Spark 1.3!\" href=\"https://databricks.com/blog/2015/03/13/announcing-spark-1-3.html\">Apache Spark 1.3</a> release!"}
{"status": "publish", "description": "Blog post from Databricks, describing updates to Apache Spark MLlib in release 1.3", "creator": "joseph", "link": "https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html", "authors": null, "id": 3135, "categories": ["Apache Spark", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2015-03-25", "tz": "UTC", "createdOn": "2015-03-25"}, "title": "Topic modeling with LDA: MLlib meets GraphX", "slug": "topic-modeling-with-lda-mllib-meets-graphx", "content": "<em>Topic models</em> automatically infer the topics discussed in a collection of documents. These topics can be used to summarize and organize documents, or used for featurization and dimensionality reduction in later stages of a Machine Learning (ML) pipeline.\n\nWith Apache Spark 1.3, MLlib now supports <em>Latent Dirichlet Allocation (LDA)</em>, one of the most successful topic models. LDA is also the first MLlib algorithm built upon GraphX. In this blog post, we provide an overview of LDA and its use cases, and we explain how GraphX was a natural choice for implementation.\n<h2>Topic Models</h2>\nAt a high level, topic modeling aims to find structure within an unstructured collection of documents. After learning this \u201cstructure,\u201d a topic model can answer questions such as: What is document X discussing? How similar are documents X and Y? If I am interested in topic Z, which documents should I read first?\n<h2>LDA</h2>\nTopic modeling is a very broad field. Apache Spark 1.3 adds Latent Dirichlet Allocation (LDA), arguably the most successful topic model to date. Initially developed for both text analysis and population genetics, LDA has since been extended and used in many applications from time series to image analysis. First, let\u2019s describe LDA in terms of text analysis.\n\nWhat are topics? LDA is not given topics, so it must infer them from raw text. LDA defines a topic as a distribution over words. For example, when we run MLlib\u2019s LDA on a <a href=\"http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.html\">dataset of articles from 20 newsgroups</a>, the first few topics are:\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/03/20newsgroups.png\"><img class=\"aligncenter wp-image-3146\" src=\"https://databricks.com/wp-content/uploads/2015/03/20newsgroups.png\" alt=\"20newsgroups\" width=\"675\" height=\"290\" /></a>\n\n&nbsp;\n\nLooking at these highest-weighted words in 3 topics, we can quickly understand what each topic is about: sports, space exploration, and computers. \u00a0LDA\u2019s success largely stems from its ability to produce interpretable topics.\n<h2>Use Cases</h2>\nIn addition to inferring these topics, LDA infers a distribution over topics for each document. \u00a0E.g., document X might be 60% about \u201cspace exploration,\u201d 30% about \u201ccomputers\u201d and 10% about other topics.\n\nThese topic distributions can be used in many ways:\n<ul>\n \t<li><b>Clustering</b>: Topics are cluster centers and documents are associated with multiple clusters (topics). \u00a0This clustering can help organize or summarize document collections.\n<ul>\n \t<li>See <a href=\"http://www.cs.cmu.edu/~lemur/science/\">this generated summary of <i>Science</i> articles</a> from Prof. Blei and Lafferty. \u00a0Click on a topic to see a list of articles about that topic.</li>\n</ul>\n</li>\n \t<li><b>Feature generation</b>: LDA can generate features for other ML algorithms to use. \u00a0As mentioned above, LDA infers a distribution over topics for each document; with K topics, this gives K numerical features. \u00a0These features can then be plugged into algorithms such as Logistic Regression or Decision Trees for prediction tasks.</li>\n \t<li><strong>Dimensionality reduction:</strong> Each document\u2019s distribution over topics gives a concise summary of the document. \u00a0Comparing documents in this reduced feature space can be more meaningful than comparing in the original feature space of words.</li>\n</ul>\n<h2>Using LDA in MLlib</h2>\nWe give a short example of using LDA. \u00a0We describe the process here and provide the actual code in <a href=\"https://gist.github.com/jkbradley/ab8ae22a8282b2c8ce33\">this Github gist</a>. \u00a0Our example first loads and pre-processes documents. \u00a0The most important part of preprocessing is choosing a vocabulary. \u00a0In our example, we split text into terms (words) and then remove (a) non-alphabetic terms, (b) short terms with &lt; 4 characters, and (c) the most common 20 terms (as \u201cstopwords\u201d). \u00a0In general, it is important to adjust this preprocessing for your particular dataset.\n\nWe then run LDA using 10 topics and 10 iterations. \u00a0It is often important to choose the number of topics based on your dataset. \u00a0Using other options as defaults, we train LDA on the Spark documentation Markdown files (spark/docs/*.md).\n\nWe end up with 10 topics. \u00a0Here are 5 hand-picked topics, each with its most important 5 terms. \u00a0Note how each corresponds nicely to a component of Spark! \u00a0(The quoted topic titles were added by hand to make this clear.)\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/03/Spark-docs.png\"><img class=\"aligncenter size-large wp-image-3149\" src=\"https://databricks.com/wp-content/uploads/2015/03/Spark-docs-1024x351.png\" alt=\"Spark docs\" width=\"1024\" height=\"351\" /></a>\n\nLDA has Scala and Java APIs in Spark 1.3. \u00a0The Python API will be added soon.\n<h2>Implementation: GraphX</h2>\nThere are many algorithms for learning an LDA model. \u00a0We chose Expectation-Maximization (EM) for its simplicity and fast convergence. \u00a0Because EM for LDA has an implicit graphical structure, building LDA on top of GraphX was a natural choice.\n\nLDA has 2 main types of data: terms (words) and documents. \u00a0We store this data on a bipartite graph (illustrated below) which has term vertices (left) and document vertices (right). \u00a0Each term vertex stores weights indicating which topics that term is relevant to; likewise, each document vertex stores its current estimate of the topics discussed in the document.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/03/graph.png\"><img class=\"aligncenter wp-image-3150\" src=\"https://databricks.com/wp-content/uploads/2015/03/graph-1024x554.png\" alt=\"graph\" width=\"770\" height=\"417\" /></a>\n\nWhenever a term appears in a document, the graph has an edge between the corresponding term vertex and document vertex. \u00a0E.g., in the figure above, Article 1 contains the terms \u201chockey\u201d and \u201csystem.\u201d\n\nThese edges also illustrate the algorithm\u2019s communication. \u00a0On each iteration, every vertex updates its data (topic weights) by collecting data from its neighbors. \u00a0Below, Article 2 updates its topic estimates by collecting data from connected term vertices.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/03/update.png\"><img class=\"aligncenter wp-image-3151\" src=\"https://databricks.com/wp-content/uploads/2015/03/update.png\" alt=\"update\" width=\"586\" height=\"294\" /></a>\n\nGraphX was thus a natural choice for LDA. \u00a0As MLlib grows, we expect more graph-structured learning algorithms in the future!\n<h2>Scalability</h2>\nParallelization of LDA is not straightforward, and there have been many research papers proposing different strategies. \u00a0The key problem is that all methods involve a large amount of communication. \u00a0This is evident in the graph description above: terms and documents need to update their neighbors with new data on each iteration, and there are <i>many</i> neighbors.\n\nWe chose the Expectation-Maximization algorithm partly because it converges to a solution in a small number of iterations. \u00a0Fewer iterations means less communication.\n\nBefore adding LDA to Spark, we ran tests on a large Wikipedia dataset. \u00a0Here are the numbers:\n<ul>\n \t<li>Training set size: 4.6 million documents</li>\n \t<li>Vocabulary size: 1.1 million terms</li>\n \t<li>Training set size: 1.1 billion tokens (~239 words/document)</li>\n \t<li>100 topics</li>\n \t<li>16-worker EC2 cluster</li>\n \t<li>Timing results: 176 sec/iteration on average over 10 iterations</li>\n</ul>\n<h2>What\u2019s Next?</h2>\nSpark contributors are currently developing additional LDA algorithms: online Variational-Bayes (a fast approximate algorithm) and Gibbs sampling (a slower but sometimes more accurate algorithm). \u00a0We are also adding helper infrastructure such as Tokenizers for automatic data preparation and more prediction functionality.\n\nTo get started using LDA, <a href=\"http://spark.apache.org/downloads.html\">download Spark 1.3</a> today!\n\nTo see examples and learn the API details, check out the <a href=\"http://spark.apache.org/docs/latest/mllib-clustering.html#latent-dirichlet-allocation-lda\">MLlib documentation</a>.\n<h2>Acknowledgements</h2>\nThe development of LDA has been a collaboration between many Spark contributors:\n\nJoseph K. Bradley, Joseph Gonzalez, David Hall, Guoqiang Li, Xiangrui Meng, Pedro Rodriguez, Avanesov Valeriy, and Xusen Yin.\n<h2>Additional resources</h2>\nLearn more about topic models and LDA with these overviews:\n<ul>\n \t<li>Overview of topic models: \u00a0<a href=\"http://www.cs.columbia.edu/~blei/papers/BleiLafferty2009.pdf\">D. Blei and J. Lafferty. \u00a0\u00a0\u201cTopic Models.\u201d \u00a0In A. Srivastava and M. Sahami, editors, Text Mining: Classification, Clustering, and Applications. Chapman &amp; Hall/CRC Data Mining and Knowledge Discovery Series, 2009.</a></li>\n \t<li><a href=\"http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\">Wikipedia on LDA</a>, with mathematical details</li>\n</ul>\nGet in-depth background from these research papers:\n<ul>\n \t<li>Original LDA papers\n<ul>\n \t<li><a href=\"http://www.cs.columbia.edu/~blei/papers/BleiNgJordan2003.pdf\">Blei, Ng, and Jordan. \u00a0\"Latent Dirichlet Allocation.\" \u00a0JMLR, 2003.</a>\n<ul>\n \t<li>Application: text document analysis</li>\n</ul>\n</li>\n \t<li><a href=\"http://www.ncbi.nlm.nih.gov/pubmed/10835412\">Pritchard et al. \u201cInference of population structure using multilocus genotype data.\u201d Genetics 155: 945--959, 2000.</a>\n<ul>\n \t<li>Application: population genetics analysis</li>\n</ul>\n</li>\n</ul>\n</li>\n \t<li>Paper which clearly explains several algorithms, including EM: \u00a0<a href=\"http://arxiv.org/pdf/1205.2662.pdf\">Asuncion, Welling, Smyth, and Teh. \"On Smoothing and Inference for Topic Models.\" \u00a0UAI, 2009.</a></li>\n</ul>"}
{"status": "publish", "description": "In Spark 1.3, we have focused on significant improvements to the Kafka integration of Spark Streaming.", "creator": "tdas", "link": "https://databricks.com/blog/2015/03/30/improvements-to-kafka-integration-of-spark-streaming.html", "authors": null, "id": 3172, "categories": ["Apache Spark", "Engineering Blog", "Streaming"], "dates": {"publishedOn": "2015-03-30", "tz": "UTC", "createdOn": "2015-03-30"}, "title": "Improvements to Kafka integration of Spark Streaming", "slug": "improvements-to-kafka-integration-of-spark-streaming", "content": "<a href=\"https://kafka.apache.org/\">Apache Kafka</a> is rapidly becoming one of the most popular open source stream ingestion platforms. We see the same trend among the users of Spark Streaming as well. Hence, in Apache Spark 1.3, we have focused on making significant improvements to the Kafka integration of Spark Streaming. This has resulted\u00a0the following additions:\n<ol>\n \t<li>New<b> Direct API</b> for Kafka - This allows each Kafka record to be processed exactly once despite failures, without using <a href=\"https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html\">Write Ahead Logs</a>. This makes Spark Streaming + Kafka pipelines more efficient while providing stronger fault-tolerance guarantees.</li>\n \t<li><b>Python API</b> for Kafka - So that you can start processing Kafka data purely from Python.</li>\n</ol>\nIn this article, we are going to discuss these improvements in more detail.\n<h2>Direct API for Kafka</h2>\n<i>[Primary Contributor - Cody]</i>\n\nSpark Streaming has supported Kafka since its inception, and Spark Streaming has been used with Kafka in production at many places\u00a0(see <a href=\"http://www.slideshare.net/databricks/spark-streaming-state-of-the-union-strata-san-jose-2015\">this</a> talk). However, the Spark community has demanded better fault-tolerance guarantees and stronger reliability semantics overtime. \u00a0To meet this\u00a0demand, Spark 1.2 introduced <a href=\"https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html\">Write Ahead Logs (WAL)</a>. It\u00a0ensures that no data received from any reliable data sources (i.e., transactional sources like Flume, Kafka, and Kinesis) will be lost due to failures (i.e., at-least-once semantics). Even for unreliable (i.e. non-transactional) sources like plain old sockets, it minimizes data loss.\n\nHowever, for sources that allow replaying of data streams from arbitrary positions in the streams (e.g. Kafka), we can achieve even stronger fault-tolerance semantics because\u00a0these sources let Spark Streaming have more control on the consumption of the data stream. Spark 1.3 introduces the concept of a <b>Direct API,</b> which can achieve exactly-once semantics even without using Write Ahead Logs. Let\u2019s look at the details of Spark\u2019s direct API for Apache Kafka.\n<h2>How did we build it?</h2>\nAt a high-level, the earlier Kafka integration worked with Write Ahead Logs (WALs) as follows:\n<ol>\n \t<li>The Kafka data is continuously received by Kafka Receivers running in the Spark workers/executors. This used the high-level consumer API of Kafka.</li>\n \t<li>The received data is stored in Spark\u2019s worker/executor memory as well as to the WAL (replicated on HDFS). The Kafka Receiver updated Kafka\u2019s offsets to Zookeeper only after the data has been persisted to the log.</li>\n \t<li>The information about the received data and its WAL locations is also stored reliably. On failure, this information is used to re-read the data and continue processing.</li>\n</ol>\n<a href=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-29-at-10.11.42-PM.png\"><img class=\"aligncenter wp-image-3174\" src=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-29-at-10.11.42-PM-300x250.png\" alt=\"Streaming blog figure 1\" width=\"500\" height=\"417\" /></a>\n\n&nbsp;\n\nWhile this approach ensures that no data from Kafka is lost, there is still a small chance that some records may get processed more than once due to failures (that is, at-least-once semantics). This can occur when some received data is saved reliably to WAL but the system fails before updating the corresponding Kafka offsets in Zookeeper. This leads to an inconsistency - Spark Streaming considers that data to have been received, but Kafka considers that the data was not successfully sent as the offset in Zookeeper was not updated. Hence, Kafka will send the data again after the system recovers from the failure.\n\nThis inconsistency arises because the two systems cannot be atomically updated with the information that describes\u00a0what has already been sent. To avoid this, only one system needs to maintain a consistent view of what has been sent or received. Additionally, that system needs to have complete control over the replay\u00a0of the data stream during the\u00a0recovery\u00a0from failures. Therefore, we decided to keep all the consumed offset information <i>only</i> in Spark Streaming, which can use Kafka\u2019s <a href=\"http://kafka.apache.org/documentation.html#simpleconsumerapi\">Simple Consumer API</a> to replay data from arbitrary offsets as required due to failures.\n\nTo build this (primary contributor was Cody), the new Direct Kafka API takes a completely different approach from Receivers and WALs. Instead of receiving the data continuously using Receivers and storing it in a WAL, we simply decide at the beginning of every batch interval what is the range of offsets to consume. Later, when each batch\u2019s jobs are executed, the data corresponding to the offset ranges is read from Kafka for processing (similar to how HDFS files are read). These offsets are also saved reliably (with <a href=\"http://spark.apache.org/docs/latest/streaming-programming-guide.html#checkpointing\">checkpoints</a>) and used to recompute the data to recover from failures.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-29-at-10.14.11-PM.png\"><img class=\"aligncenter wp-image-3177\" src=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-29-at-10.14.11-PM-300x206.png\" alt=\"streaming blog figure 2\" width=\"500\" height=\"344\" /></a>\n\nNote that Spark Streaming can reread and reprocess segments of the stream from Kafka to recover from failures. However, due to the exactly-once nature of RDD transformations, the final recomputed results are exactly same as that would have been without failures.\n\nThus this direct API eliminates the need for both WALs and Receivers for Kafka, while ensuring that each Kafka record is effectively received by Spark Streaming exactly once. This allows one to build a Spark Streaming + Kafka pipelines with end-to-end exactly-once semantics (if your updates to downstream systems are idempotent or transactional). Overall, it makes such streaming processing pipelines more fault-tolerant, efficient, and easier to use.\n<h2>How to use it?</h2>\nThe new API is simpler to use than the previous\u00a0one.\n\n[scala]\n// Define the Kafka parameters, broker list must be specified\nval kafkaParams = Map(&quot;metadata.broker.list&quot; -&gt; &quot;localhost:9092,anotherhost:9092&quot;)\n\n// Define which topics to read from\nval topics = Set(&quot;sometopic&quot;, &quot;anothertopic&quot;)\n\n// Create the direct stream with the Kafka parameters and topics\nval kafkaStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](streamingContext, kafkaParams, topics)\n[/scala]\n\nSince this direct approach does not have any receivers, you do not have to worry about creating multiple input DStreams to create more receivers. Nor do you have to configure the number of Kafka partitions to be consumed per receiver. Each Kafka partition will be automatically read in parallel. \u00a0Furthermore, each Kafka partition will correspond to a RDD partition, thus simplifying the parallelism model.\n\nIn addition to the new streaming API, we have also introduced <code>KafkaUtils.createRDD(),</code> which can be used to run batch jobs on Kafka data.\n\n[scala]\n// Define the offset ranges to read in the batch job\nval offsetRanges = Array(\n  OffsetRange(&quot;some-topic&quot;, 0, 110, 220),\n  OffsetRange(&quot;some-topic&quot;, 1, 100, 313),\n  OffsetRange(&quot;another-topic&quot;, 0, 456, 789)\n)\n\n// Create the RDD based on the offset ranges\nval rdd = KafkaUtils.createRDD[String, String, StringDecoder, StringDecoder](sparkContext, kafkaParams, offsetRanges)\n[/scala]\n\nIf you want to learn more about the API and the details of how it was implemented, take a look at the following.\n<ul>\n \t<li><a href=\"http://spark.apache.org/docs/latest/streaming-kafka-integration.html\">Spark Streaming + Kafka Integration Guide</a></li>\n \t<li>Cody\u2019s <a href=\"https://github.com/koeninger/kafka-exactly-once/blob/master/blogpost.md\">blog post</a> with more details</li>\n \t<li>Full word count example of the Direct API in <a href=\"https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala\">Scala</a> and <a href=\"https://github.com/apache/spark/blob/master/examples/scala-2.10/src/main/java/org/apache/spark/examples/streaming/JavaDirectKafkaWordCount.java\">Java</a></li>\n \t<li><a href=\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.kafka.KafkaUtils$\">Scala</a> and <a href=\"http://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/kafka/KafkaUtils.html\">Java</a> documentation of the Direct API</li>\n \t<li>Updated <a href=\"http://spark.apache.org/docs/latest/streaming-programming-guide.html#fault-tolerance-semantics\">Fault-tolerance Semantics in Spark Streaming Programming Guide</a></li>\n</ul>\n<h2>Python API for Kafka</h2>\n<i>[Primary Contributor - Davies]</i>\n\nIn Spark 1.2, the basic Python API of Spark Streaming was added so that developers could write distributed stream processing applications purely in Python. In Spark 1.3, we have extended the Python API to include Kafka (primarily contributed by Davies Liu). With this, writing stream processing applications in Python with Kafka becomes a breeze. Here is a sample code.\n\n[python]\nkafkaStream = KafkaUtils.createStream(streamingContext, \n&quot;zookeeper-server:2181&quot;, &quot;consumer-group&quot;, {&quot;some-topic&quot;: 1})\n\nlines = kafkaStream.map(lambda x: x[1])\n[/python]\n\nSee the full <a href=\"https://github.com/apache/spark/blob/master/examples/src/main/python/streaming/kafka_wordcount.py\">example</a> and the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark-streaming-kafka-module\">python docs</a>. Instructions to run the example can be found in the Kafka integration guide. Note that for running the example or any python applications using the Kafka API, you will have to add the Kafka Maven dependencies to the path. This is can be easily done in Spark 1.3 as you can directly add Maven dependencies to spark-submit (recommended way to launch Spark applications). See the \u201cDeploying\u201d section in the <a href=\"http://spark.apache.org/docs/latest/streaming-kafka-integration.html\">Kafka integration guide</a> for more details.\n\nAlso note that this is using the earlier Kafka API. Extending Python to the Direct API is <a href=\"https://issues.apache.org/jira/browse/SPARK-5946\">in progress</a> and expected to be available in Spark 1.4. Additionally, we want to add Python APIs for the rest of the built-in sources to the to achieve parity between the Scala, Java and Python streaming APIs.\n<h2>Future Directions</h2>\nWe will\u00a0continuously improve the stability and performance of Kafka integration. Some of the\u00a0improvements\u00a0we intend\u00a0to make\u00a0are as follows:\n<ul>\n \t<li>Automatically updating Zookeeper as batches complete successfully, to make Zookeeper based Kafka monitoring tools work - <a href=\"https://issues.apache.org/jira/browse/SPARK-6051\">SPARK-6051</a></li>\n \t<li>Python API for the Direct API - <a href=\"https://issues.apache.org/jira/browse/SPARK-5946\">SPARK-5946</a></li>\n \t<li>Extending this direct API approach to Kinesis - <a href=\"https://issues.apache.org/jira/browse/SPARK-6599\">SPARK-6599</a></li>\n \t<li>Connection pooling for Kafka Simple Consumer API across batches</li>\n</ul>\n&nbsp;"}
{"status": "publish", "description": "We present a new feature in Databricks Cloud called Integrated Search that helps our users find relevant information quickly and easily.", "creator": "dave_wang", "link": "https://databricks.com/blog/2015/04/02/learning-how-to-write-spark-applications-in-databricks-cloud-with-the-integrated-search-feature.html", "authors": null, "id": 3189, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2015-04-02", "tz": "UTC", "createdOn": "2015-04-02"}, "title": "Learning how to write Apache Spark applications in Databricks with the Integrated Search feature", "slug": "learning-how-to-write-spark-applications-in-databricks-cloud-with-the-integrated-search-feature", "content": "We built Databricks on top of the Apache Spark framework to make data analysis simple. In the same spirit, we want to make the adoption and usage of Databricks simple. Developers and data scientists need to answer questions about Databricks and Spark quickly in order to get their work done, while project leads need to minimize the time their teams spend on ramping up on a new platform. In this blog post, we present a new feature in Databricks called <i>Integrated Search</i> that helps our users find relevant information quickly and easily.\n<h2>What is <i>Integrated Search</i>?</h2>\nThe <i>integrated search</i> feature helps users find relevant information about Databricks and Apache Spark from three powerful resources:\n<ul>\n \t<li><b>Databricks Guide: </b>An extensive guide with tutorials, sample applications, reference code, and documentation built into Databricks</li>\n \t<li><b><b>Databricks Forum:</b> </b>An <a href=\"https://forums.databricks.com/\">online forum</a> where customers can ask questions and find answers provided by our expert field engineers and developers who work on Spark, as well as other customers</li>\n \t<li><b>Databricks Workspace:</b> The workspace contains code previously written by you and other users of your Databricks instance, a valuable resource for finding code examples where your team may have solved a similar problem before</li>\n</ul>\nFor questions about how to use Databricks features and Apache Spark, <i>Integrated Search</i> provides a simple interface that allows users to quickly find answers without leaving Databricks and interrupting their workflow. In the example below, one can easily locate example applications of processing Parquet files in Databricks Guide, or find more in-depth discussions about how to use Parquet in the Databricks forum:\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-30-at-11.06.18-AM.png\"><img class=\"aligncenter wp-image-3191\" src=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-30-at-11.06.18-AM.png\" alt=\"Integrated search blog figure 1\" width=\"400\" height=\"365\" /></a>\n\nThere are also times when users want to find code examples written by others in their team, or they may simply want to locate some code they wrote in the past. For this scenario, <i>Integrated Search</i> can search through code and results in <i>Notebooks</i>, which are interactive documents users create in the Databricks Workspace. In the example below, users can find examples of code where their team used Spark broadcast variables, so they can quickly determine how to do so in their own code:\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-30-at-11.07.17-AM.png\"><img class=\"aligncenter wp-image-3192\" src=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-30-at-11.07.17-AM-1024x694.png\" alt=\"Integrated search blog figure 2\" width=\"400\" height=\"271\" /></a>\n<h2>Benefits for Databricks users</h2>\nRather than wasting time crawling irrelevant or out-of-date help sources, and switching between multiple websites to search code, product documentation, and forums, <i>Integrated Search</i> provides a one-stop-shop for getting answers about the nuts-and-bolts of creating Spark applications using Databricks. We hope this feature will help our customers master Apache Spark more quickly, so they can spend their valuable time developing exciting applications in Databricks.\n\nTo learn more about Databricks, check out our <a href=\"https://databricks.com/product/databricks-cloud\">products page</a> or the <a href=\"https://www.youtube.com/watch?v=IBwYINZdQgQ\">overview video</a>\n\nIf you\u2019re interested in trying out Databricks, <a href=\"https://databricks.com/registration\">sign up for a trial</a>!\n\n&nbsp;"}
{"status": "publish", "description": "To celebrate Spark\u2019s fifth birthday, one thing I wanted to do was to highlight some of the key ideas behind how we built out the project that still apply today.", "creator": "matei", "link": "https://databricks.com/blog/2015/03/31/spark-turns-five-years-old.html", "authors": null, "id": 3200, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2015-03-31", "tz": "UTC", "createdOn": "2015-03-31"}, "title": "Apache Spark Turns Five Years Old!", "slug": "spark-turns-five-years-old", "content": "Today, we\u2019re celebrating an important milestone for the Apache Spark project -- it\u2019s now been five years since Spark was <a href=\"https://github.com/apache/spark/commit/df29d0ea4c8b7137fdd1844219c7d489e3b0d9c9\">first open sourced</a>. When we first decided to release our research code at UC Berkeley, none of us knew how far Spark would make it, but we believed we had built some really neat technology that we wanted to share with the world. In the five years since, we\u2019ve been simply awed by the numerous contributors and users that have made Spark the leading-edge computing framework it is today. Indeed, to our knowledge, Spark has now become the most active open source project in big data (looking at either contributors per month or commits per month). In addition to contributors, it has built up an array of <a href=\"https://databricks.com/blog/2015/01/27/big-data-projects-are-hungry-for-simpler-and-more-powerful-tools-survey-validates-apache-spark-is-gaining-developer-traction.html\">hundreds of production use cases</a> from batch analytics to stream processing.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-31-at-7.51.18-AM.png\"><img class=\"aligncenter wp-image-3221\" src=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-31-at-7.51.18-AM-1024x411.png\" alt=\"Screen Shot 2015-03-31 at 7.51.18 AM\" width=\"600\" height=\"241\" /></a>\n\nTo celebrate Spark\u2019s fifth birthday, one thing I wanted to do was to highlight some of the key ideas behind how we built out the project that still apply today. To do this, I took another look at the <a href=\"https://github.com/apache/spark/tree/df29d0ea4c8b7137fdd1844219c7d489e3b0d9c9\">first public version of Spark</a>.\n\nThe first thing to notice is that this version was quite small: it weighed in at 3900 lines of code, of which 1300 were the Scala interpreter, 600 were examples and 300 were tests. Since March 2010, I\u2019m happy to say that our test coverage has gone up substantially. However, the observation about size does reflect something important: since the beginning, we\u2019ve sought to keep the Spark engine small and compact, making it easier for many developers to understand and for us to change and improve. Even today, the core Spark engine is only about 50,000 lines of code. The main additions since that first version have been support for \u201cshuffle\u201d operations, which required new networking code and a DAG scheduler, as well as support for multiple backend schedulers, such as YARN. Nonetheless, even today we can regularly make large changes to the core engine that improve the performance or stability of all Spark applications. For example, during our work last year on <a href=\"https://databricks.com/blog/2014/10/10/spark-petabyte-sort.html\">large-scale sorting</a>, multiple developers at Databricks ended up rewriting almost all of Spark\u2019s networking layer.\n\nThe second thing to notice about Spark from 2010 is what it can do: even this ~2000 line engine could handle two of the most important workloads for Spark today, iterative algorithms and interactive queries. Back in 2010, we were the only cluster computing engine to support interactive use, by modifying the Scala interpreter to submit code to a Spark cluster. We\u2019ve constantly sought to improve this experience and enable truly interactive data science through features like Spark\u2019s <a href=\"http://spark.apache.org/examples.html\">Python API</a> and <a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\">DataFrames</a>. In addition, even the 2010 version of Spark was able to run iterative algorithms like <a href=\"https://github.com/apache/spark/blob/df29d0ea4c8b7137fdd1844219c7d489e3b0d9c9/src/examples/SparkHdfsLR.scala\">logistic regression</a> 20-30x faster than MapReduce (subsequent improvements brought this up to 100x).\n\nA final important element in how we think about the project is our focus on simple, stable APIs. The code examples that ship with Spark from 2010, like <a href=\"https://github.com/apache/spark/blob/df29d0ea4c8b7137fdd1844219c7d489e3b0d9c9/src/examples/SparkHdfsLR.scala\">logistic regression</a> and <a href=\"https://github.com/apache/spark/blob/df29d0ea4c8b7137fdd1844219c7d489e3b0d9c9/src/examples/SparkPi.scala\">computing pi</a>, are nearly identical to Spark code from today (see <a href=\"https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala\">logistic regression</a>, <a href=\"https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala\">pi</a>). We work very hard to define stable APIs that developers can build on years into the future, minimizing the work they must do to keep up with improvements in Spark. Starting in Apache Spark 1.0, these compatibility guarantees are now <a href=\"https://cwiki.apache.org/confluence/display/SPARK/Spark+Versioning+Policy\">formalized</a> for all major Spark components.\n\nThat\u2019s enough about Spark in 2010. How has the project grown since then? While there has been tremendous activity in all areas of Spark, including support for more programming languages (Java, Python and soon R), data sources, and optimizations, the single biggest addition to Spark has been its standard libraries. Over the years, Spark has acquired four high-level libraries -- <a href=\"http://spark.apache.org/streaming\">Spark Streaming</a>, <a href=\"http://spark.apache.org/mllib\">MLlib</a>, <a href=\"http://spark.apache.org/graphx\">GraphX</a> and <a href=\"http://spark.apache.org/sql\">Spark SQL</a> -- that all run on top of the core engine, and interoperate easily and efficiently with each other. Today these libraries are the bulk of the code in Spark -- about 200,000 lines compared to 50,000 in the core engine. They also represent the single largest standard library available for big data, making it easy to write applications that span all stages of the data lifecycle. Nevertheless, these libraries are still quite new, the majority of them having been added in the last two years. In future years I expect these libraries to grow significantly, with the aim to build as rich a toolset for big data as the libraries available for small data. You can find some of the areas where Databricks is working on these libraries in my <a href=\"http://spark-summit.org/east/2015/talk/matei-zaharia\">slides from Spark Summit 2015</a>.\n\nFinally, like any five-year-old, Spark is still sometimes able to get into trouble without supervision and sometimes hard to understand. At Databricks, we\u2019re working hard to make Spark easier to use and run than ever, through our efforts on both the Spark codebase and support materials around it. All of our work on Spark is open source and goes directly to Apache. In addition, we have put up a large array of free <a href=\"https://databricks.com/spark/developer-resources\">online training materials</a>, as well as <a href=\"https://databricks.com/spark/training\">training courses</a> and <a href=\"http://shop.oreilly.com/product/0636920028512.do\">books</a>. Finally, we have built a service to make it very easy to run Spark in a few clicks, <a href=\"https://databricks.com/product/databricks-cloud\">Databricks Cloud</a>. We hope that you enjoy using Spark, no matter which environment you run it in, as much as we enjoy building it."}
{"status": "publish", "description": null, "creator": "rxin", "link": "https://databricks.com/blog/2015/04/01/spark-2-rearchitecting-spark-for-mobile.html", "authors": null, "id": 3259, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2015-04-01", "tz": "UTC", "createdOn": "2015-04-01"}, "title": "Apache Spark 2.0: Rearchitecting Spark for Mobile Platforms", "slug": "spark-2-rearchitecting-spark-for-mobile", "content": "Yesterday, to celebrate Apache Spark\u2019s <a href=\"https://databricks.com/blog/2015/03/31/spark-turns-five-years-old.html\">5 year old</a> birthday, we looked back at the history of the project. Today, we are happy to announce the next major chapter of Spark development: an architectural overhaul designed to enable Spark on mobile devices. Mobile computing is quickly rising to dominance, and by the end of 2017, it is estimated that 90% of CPU cycles will be devoted to mobile hardware. Spark\u2019s project goal can be accomplished only when Spark runs efficiently for the growing population of mobile users. Already today, 100% of Spark\u2019s users have mobile phones.\n\nDesigned and optimized for modern data centers and Big Data applications, Spark is unfortunately not a good fit for mobile computing today. In the past few months, we have been prototyping the feasibility of a mobile-first Spark architecture, and today we would like to share with you our findings. This post outlines the technical design of Spark\u2019s mobile support, and shares results from several early prototypes. See also <a href=\"https://issues.apache.org/jira/browse/SPARK-6646\">SPARK-6646</a> for community discussion on the issue.\n\n<center><img src=\"https://databricks.com/wp-content/uploads/2015/03/image00.png\" alt=\"Spark on iPhone\" width=\"300\" /></center><strong>Requirements</strong>\n\nMust Have:\n\n<ul>\n    <li>Support running Spark on Android and iOS</li>\n    <li>Facilitate the development of SoLoMo (social, local, mobile) applications</li>\n    <li>Maintain source backward compatibility</li>\n    <li>Support heterogeneous mobile phones within a single Spark cluster</li>\n</ul>\n\nNice to Have:\n\n<ul>\n    <li>Support running Spark on Windows phones</li>\n    <li>Support feature phones through J2ME</li>\n</ul>\n\n<h2>Compilation and Runtime</h2>\n\nThe Android Runtime (ART) already supports developing applications using Scala, as evidenced by this <a href=\"http://scala-ide.org/docs/tutorials/play/index.html\">Scala IDE article on Android development</a>.\n\niOS, however, has no built-in JVM support. Luckily, we can leverage multiple community efforts here:\n\n<ol>\n    <li>Use <a href=\"http://robovm.com/\">RoboVM</a>, an ahead-of-time (AOT) compiler and library for developing iOS applications using Java.</li>\n    <li>Use <a href=\"http://greedy.github.io/scala-llvm/\">Scala+LLVM</a> to compile Spark\u2019s Scala code into LLVM bytecode. Given Apple\u2019s strong stand behind the LLVM project, we believe this can achieve the highest level of performance.</li>\n    <li>Since <a href=\"https://leverich.github.io/swiftislikescala/\">Swift looks very similar to Scala</a>, we can create a project to automatically translate Scala source code into Swift, and then build the iOS package using XCode.</li>\n    <li>Use <a href=\"http://www.scala-js.org/\">Scala.js</a> to compile Spark into JavaScript, and execute Spark in the Nitro JavaScript engine (Safari).</li>\n</ol>\n\nOption 4 is preferred because not only does it support iOS, but also all platforms using a single package. On iOS and Android, Spark can be executed by the JavaScript engines. On servers, Spark can be executed in Node.js.\n\n<h2>Performance Optimizations</h2>\n\nJavaScript engines are one of the hottest areas of innovation, and thus we fully expect JavaScript engines to improve their performance rapidly. However, mobile JavaScript engines seem to lag behind their desktop variants. In particular, there appears to be no SIMD support for mobile JavaScript.\n\nFor parts of Spark that might benefit tremendously from SIMD, such as low level matrix operations, we can selectively rewrite those parts to generate LLVM bytecode.\n\n<h2>Networking and Wire Protocol</h2>\n\nSpark\u2019s network transport builds on Netty, which in turn relies on java.nio or Linux epoll. Android ART appears to support java.nio out of the box, but we might need to rewrite Netty to use kqueue on iOS. Additionally, it is unclear whether low-level networking primitives such as zero-copy can be exposed in JavaScript. We will need to work more closely with Apple and Google to improve networking support in mobile JavaScript.\n\nA viable alternative is to leverage <a href=\"http://www.grpc.io/\">grpc</a>, an open-source high performance RPC library developed by Google. grpc provides out of the box support for all common platforms (Java, Objective C, etc) using HTTP/2.\n\nGiven the focus on debuggability, JSON should be the preferred wire serialization protocol over any existing binary formats.\n\n<h2>True Locality Scheduling and DAGScheduler</h2>\n\nTo better support local, social, and mobile features of Spark, we change the locality field of RDDs to GPS coordinates, and locality scheduling can be refactored to support true locality that was never possible on servers.\n\nIn order to maintain source compatibility, we keep the old interface, and introduce a new true locality interface.\n\n[scala]\nclass RDD {\n  @deprecate(&quot;2.0&quot;, &quot;use getPreferredTrueLocations&quot;)\n  def getPreferredLocations(p: Partition): Seq[String]\n\n/**\n   * Returns the preferred locations for executing task on partition\n   * <code>p</code>. Concrete implementations of RDD can use this to enable\n   * locality scheduling.\n   */\n  def getPreferredTrueLocations(p: Partition): Seq[LatLong]\n}\n[/scala]\n\nThe DAGScheduler needs to be updated to compute the geographical proximity of executors.\n\n<h2>Extending TaskContext to the Mobile Platform</h2>\n\nThe TaskContext provides the contextual information for Spark tasks (e.g., job IDs, attempt IDs, etc.). While this was sufficient for running on server, the mobile platform has many other contextual information like GPS location, ongoing calls, etc. Such contextual information may be useful to optimize the processing of tasks without affecting the user experience of the smartphone/tablet user. For example, if a phone call is active, a new task may be paused until the call is over so that the call quality is not affected.\n\n<h2>Basic Engine on iPhone and Androids</h2>\n\nWe have already built a few proof-of-concepts using iPhones to better understand the complexities of the mobile platform. Here are some screenshots from our prototype.\n\nThe screenshot at the beginning of the blog post shows a Spark Streaming NetworkWordCount example running on an iPhone. It is receiving data using sockets from a server running on Amazon EC2. We are also prototyping it on Android. Here is some early screenshots on the Android device emulator.\n\n<center><img src=\"https://databricks.com/wp-content/uploads/2015/03/image01-808x1024.png\" alt=\"Spark on Android\" width=\"200\" /></center>\n\n<h2>Prototyping Machine Learning Application</h2>\n\nIn real-world machine learning, labels are always hard and expensive to obtain. This won\u2019t be an issue with Spark on Mobile. In the pipeline API for machine learning, we will provide a transformer that produces human tags.\n\n[scala]\ntagger = HumanTagger(tags=[\u201cspam\u201d, \u201cham\u201d], retries=10)\nlabeled = tagger.transform(images)\nmodel = LogisticRegression().fit(labeled)\n[/scala]\n\nDuring transformation, images are displayed along with possible tags and users choose the correct tag for each image. To make sure the transformation is fault tolerant, we will randomize the ordering and retry 10 times for each record. Yes, RDDs are always deterministic. And of course, we support tagging with multi-labels, even on a wearable device:\n\n<center><img src=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-31-at-11.37.26-PM.png\" width=\"400\" /></center>In conclusion, this design doc proposes architectural changes to Spark to allow efficient execution on mobile platforms. Whether it is an octa-core x86 processor or a quad-core ARM processor, Spark should be the one unifying computing platform. We look forward to collaborating with the community to realize this effort, and hearing your feedback on the JIRA ticket: <a href=\"https://issues.apache.org/jira/browse/SPARK-6646\">SPARK-6646</a>.\n\n[<em>Just in case you didn't realize - it's April 1st!</em>]"}
{"status": "publish", "description": null, "creator": "kavitha", "link": "https://databricks.com/blog/2015/04/03/timeful-chooses-databricks-cloud-to-enable-intelligent-time-management.html", "authors": null, "id": 3266, "categories": ["Announcements", "Company Blog", "Customers"], "dates": {"publishedOn": "2015-04-03", "tz": "UTC", "createdOn": "2015-04-03"}, "title": "Timeful Chooses Databricks to Enable Intelligent Time Management", "slug": "timeful-chooses-databricks-cloud-to-enable-intelligent-time-management", "content": "We are thrilled to announce that\u00a0Timeful chose\u00a0Databricks to enable\u00a0intelligent time management with data analytics.\n\nPress release:\u00a0<a href=\"http://www.marketwired.com/press-release/timeful-chooses-databricks-cloud-enable-intelligent-time-management-with-data-2006609.htm\" target=\"_blank\">http://www.marketwired.com/press-release/timeful-chooses-databricks-cloud-enable-intelligent-time-management-with-data-2006609.htm</a>\n\nTimeful helps its users manage their time better by tracking commitments, categorizing to-do list items and assisting in the development of good lifestyle habits. Deployed as an application on smart phones devices, Timeful utilizes machine learning to recommend a personalized scheduled based on previous behavior, availability, and preferences.\n\nThe Timeful team relies heavily on data analytics to improve product design and monitor the personalized schedule recommendations. Both of these data analytics tasks proved to be challenging in Timeful\u2019s environment because Timeful stores its data in multiple Postgres databases, which proved to be too slow and cumbersome for Timeful's needs.\n\nUtilizing\u00a0Databricks as a centralized, high-performance data processing platform, Timeful was able to overcome\u00a0the limitations imposed by multiple Postgres databases. The speed and simplicity of Databricks enabled\u00a0the continuous monitoring of\u00a0Timeful's\u00a0production systems while providing\u00a0an easy way to access and explore terabyte scale production data\u00a0for non-engineers.\n\nAs a result of\u00a0deploying Databricks, Timeful\u00a0gained a number of benefits:\n<ul>\n \t<li>Improved key metrics monitoring by processing the entire production data set instead of sampling subsets</li>\n \t<li>More effective data-driven product design in a much shorter cycle</li>\n \t<li>Redirect one FTE in data analytics to focus on problem solving instead of data analysis for other teams</li>\n</ul>\nDownload this <a title=\"Customer Case Studies\" href=\"https://databricks.com/resources/customer-case-studies\" target=\"_blank\">case study</a> learn more about how Timeful\u00a0is using Databricks."}
{"status": "publish", "description": "Databricks organized the first Spark Summit East event in New York City. Learn more about the keynotes or Spark use cases here.", "creator": "scott", "link": "https://databricks.com/blog/2015/04/08/a-look-back-at-spark-summit-east.html", "authors": null, "id": 3361, "categories": ["Company Blog", "Events"], "dates": {"publishedOn": "2015-04-08", "tz": "UTC", "createdOn": "2015-04-08"}, "title": "A Look Back at Spark Summit East", "slug": "a-look-back-at-spark-summit-east", "content": "<a href=\"https://databricks.com/wp-content/uploads/2015/04/Spark-Summit.jpg\"><img class=\"aligncenter wp-image-3364\" src=\"https://databricks.com/wp-content/uploads/2015/04/Spark-Summit-1024x683.jpg\" alt=\"Spark summit keynote\" width=\"600\" height=\"400\" /></a>\n\nWe are delighted about the success of the first <a href=\"http://spark-summit.org/east/2015\">Spark Summit East</a>, held in New York City on March 18th. The summit was attended by a sold-out crowd of over 900 people from more than 300 organizations.\n\nDatabricks is proud to make all talk videos, slides, training talk videos, and training materials available online for free as a service to the Apache Spark community. Slides are already available on the <a href=\"http://spark-summit.org/east/2015\">Spark Summit East agenda page</a> and videos will be published there too as soon as we finish editing them.\n<h2>Keynotes</h2>\nMatei Zaharia, the creator of Spark, opened the summit with a talk titled <a href=\"http://www.slideshare.net/databricks/new-direction-for-spark-in-2015-spark-summit-east\">New Directions for Spark in 2015</a>. In it he presented work on higher level interfaces for machine learning, work enabling new external data sources to connect to Spark, and more.\n\nDuring the next keynote Ion Stoica, CEO of Databricks, highlighted two companies that are using Spark in Databricks Cloud to create innovative data products. MyFitnessPal is utilizing Spark in a variety of ways to understanding the behavior of their customers. Automatic Labs is using Spark to analyze data collected from sensors in vehicles.\n\nNext Brian Schimpf who is Director of Engineering at Palantir spoke about some of the data challenges Palantir faces related to trader oversight in the financial industry and the technology they\u2019ve built in response with Spark at the core.\n\nIn the final two keynotes Matthew Glickman, managing director at Goldman Sachs, spoke of Spark as the \u201clingua franca\u201d platform for scalable Big Data computation and Peter Wang from Continuum, a creator of the PyData Conference, spoke about the advantages of using Python and Spark together.\n<h2>Community Talks</h2>\nBeyond the keynotes, the summit showcased dozens of community talks across three parallel tracks covering a broad array of use cases from startups and large enterprises. Gauged by the diversity of talk topics, Spark is being used in a wide range of industries and organizations. Some highlights include:\n<ul>\n \t<li><b>Finance:</b> <a href=\"http://spark-summit.org/wp-content/uploads/2015/03/SSE15-24-Levans-Kuipers.pdf\">Tresata is revolutionizing the detection of money laundering</a></li>\n \t<li><b>Retail:</b> <a href=\"http://spark-summit.org/wp-content/uploads/2015/03/SSE15-27-ZacharyCohn.pdf\">Gilt</a> and <a href=\"http://spark-summit.org/wp-content/uploads/2015/03/SSE15-39-Solmaz-Shahalizadeh.pdf\">Shopify</a> are using Spark to improve overall customer experience through better personalization</li>\n \t<li><b>Media: </b><a href=\"http://spark-summit.org/wp-content/uploads/2015/03/SSE15-18-Neumann-Alla.pdf\">Comcast is offering real-time recommendation to their viewers</a></li>\n \t<li><b>Health and wellness:</b> <a href=\"http://spark-summit.org/wp-content/uploads/2015/03/SSE15-36-Hesamoddin-Salehian.pdf\">MyFitnessPal is providing more accurate food information to its 80 million users</a></li>\n \t<li><b>Pharma:</b> <a href=\"http://spark-summit.org/wp-content/uploads/2015/03/SSE15-30-David-Tester.pdf\">Norvartis is accelerating Genomics research with Spark</a></li>\n</ul>\n<h2>Training</h2>\nThe day following the Summit we trained over 500 students to use Spark in three parallel classes. You can download the course material for free via the links below:\n<ul>\n \t<li><a href=\"http://training.databricks.com/workshop/sparkcamp.pdf\">Intro to Apache Spark</a></li>\n \t<li><a href=\"http://training.databricks.com/workshop/datasci.pdf\">Data Science of Apache Spark</a></li>\n \t<li><a href=\"http://www.slideshare.net/databricks/spark-summit-east-2015-advdevopsstudentslides\">DevOps with Apache Spark Workshop</a></li>\n</ul>\nLearn more about Spark training classes run by Databricks on the <a href=\"https://databricks.com/services/spark-training\">training portion</a> of our website.\n<h2>Learn More</h2>\nFor Spark enthusiasts on the West Coast, the next <a href=\"http://spark-summit.org/2015\">Spark Summit</a> will be in San Francisco from June 15th to 17th. <a href=\"http://prevalentdesignevents.com/sparksummit2015/registration.aspx\">Register now</a> before it sells out.\n\nTo keep up with Spark and Databricks news, sign up for <a href=\"https://databricks.com/resources/newsletters\">our monthly newsletter</a>."}
{"status": "publish", "description": null, "creator": "michael", "link": "https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html", "authors": null, "id": 3375, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2015-04-13", "tz": "UTC", "createdOn": "2015-04-13"}, "title": "Deep Dive into Spark SQL's Catalyst Optimizer", "slug": "deep-dive-into-spark-sqls-catalyst-optimizer", "content": "Spark SQL is one of the newest and most technically involved components of Spark. It powers both SQL queries and the new <a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\">DataFrame API</a>. At the core of Spark SQL is the Catalyst optimizer, which leverages advanced programming language features (e.g. Scala's <a href=\"http://docs.scala-lang.org/tutorials/tour/pattern-matching.html\">pattern matching</a> and <a href=\"http://docs.scala-lang.org/overviews/quasiquotes/intro.html\">quasiquotes</a>) in a novel way to build an extensible query optimizer.\n\nWe recently published a <a href=\"http://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf\">paper</a> on Spark SQL that will appear in <a href=\"http://www.sigmod2015.org/\">SIGMOD 2015</a> (co-authored with Davies Liu, Joseph K. Bradley, Xiangrui Meng, Tomer Kaftan, Michael J. Franklin, and Ali Ghodsi). In this blog post we are republishing a section in the paper that explains the internals of the Catalyst optimizer for broader consumption.\n\nTo implement Spark SQL, we designed a new extensible optimizer, Catalyst, based on functional programming constructs in Scala. Catalyst\u2019s extensible design had two purposes. First, we wanted to make it easy to add new optimization techniques and features to Spark SQL, especially for the purpose of\u00a0tackling various problems we were seeing with big data (e.g., semistructured data and advanced analytics). Second, we wanted to enable external developers to extend the optimizer -- for example, by adding data source specific rules that can push filtering or aggregation into external storage systems, or support for new data types. Catalyst supports both rule-based and cost-based optimization.\n\nAt its core, Catalyst contains a general library for representing trees and applying rules to manipulate them. On top of this framework, we have built libraries specific to relational query processing (e.g., expressions, logical query plans), and several sets of rules that handle different phases of query execution: analysis, logical optimization, physical planning, and code generation to compile parts of queries to Java bytecode. For the latter, we use another Scala feature, <a href=\"http://docs.scala-lang.org/overviews/quasiquotes/intro.html\">quasiquotes</a>, that makes it easy to generate code at runtime from composable expressions. Finally, Catalyst offers several public extension points, including external data sources and user-defined types.\n<h2>Trees</h2>\nThe main data type in Catalyst is a tree composed of node objects. Each node has a node type and zero or more children. New node types are defined in Scala as subclasses of the TreeNode class. These objects are immutable and can be manipulated using functional transformations, as discussed in the next subsection.\n\nAs a simple example, suppose we have the following three node classes for a very simple expression language:\n<ul>\n\t<li><code>Literal(value: Int)</code>: a constant value</li>\n\t<li><code>Attribute(name: String):</code> an attribute from an input row, e.g.,\u201cx\u201d</li>\n\t<li><code>Add(left: TreeNode, right: TreeNode):</code> sum of two expressions.</li>\n</ul>\nThese classes can be used to build up trees; for example, the tree for the expression\u00a0<code>x+(1+2)</code>, would be represented in Scala code as follows:\n\n[scala]\nAdd(Attribute(x), Add(Literal(1), Literal(2)))\n[/scala]\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-12-at-8.11.11-AM.png\"><img class=\"aligncenter wp-image-3382 size-medium\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-12-at-8.11.11-AM-300x174.png\" alt=\"Catalyst blog figure 1\" width=\"300\" height=\"174\" /></a>\n\n&nbsp;\n<h2>Rules</h2>\nTrees can be manipulated using rules, which are functions from a tree to another tree. While a rule can run arbitrary code on its input tree (given that this tree is just a Scala object), the most common approach is to use a set of pattern matching functions that find and replace subtrees with a specific structure.\n\nPattern matching is a feature of many functional languages that allows extracting values from potentially nested structures of algebraic data types. In Catalyst, trees offer a transform method that applies a pattern matching function recursively on all nodes of the tree, transforming the ones that match each pattern to a result. For example, we could implement a rule that folds Add operations between constants as follows:\n\n[scala]\ntree.transform {\n  case Add(Literal(c1), Literal(c2)) =&gt; Literal(c1+c2)\n}\n[/scala]\n\nApplying this to the tree for <code>x+(1+2)</code> would yield the new tree <code>x+3</code>. The <code>case</code> keyword here is Scala\u2019s standard pattern matching syntax, and can be used to match on the type of an object as well as give names to extracted values (<code>c1</code> and <code>c2</code> here).\n\nThe pattern matching expression that is passed to transform is a partial function, meaning that it only needs to match to a subset of all possible input trees. Catalyst will tests which parts of a tree a given rule applies to, automatically skipping over and descending into subtrees that do not match. This ability means that rules only need to reason about the trees where a given optimization applies and not those that do not match. Thus, rules do not need to be modified as new types of operators are added to the system.\n\nRules (and Scala pattern matching in general) can match multiple patterns in the same transform call, making it very concise to implement multiple transformations at once:\n\n[scala]\ntree.transform {\n  case Add(Literal(c1), Literal(c2)) =&gt; Literal(c1+c2)\n  case Add(left, Literal(0)) =&gt; left\n  case Add(Literal(0), right) =&gt; right\n}\n[/scala]\n\nIn practice, rules may need to execute multiple times to fully transform a tree. Catalyst groups rules into batches, and executes each batch until it reaches a fixed point, that is, until the tree stops changing after applying its rules. Running rules to fixed point means that each rule can be simple and self-contained, and yet still eventually have larger global effects on a tree. In the example above, repeated application would constant-fold larger trees, such as <code>(x+0)+(3+3)</code>. As another example, a first batch might analyze an expression to assign types to all of the attributes, while a second batch might use these types to do constant folding. After each batch, developers can also run sanity checks on the new tree (e.g., to see that all attributes were assigned types), often also written via recursive matching.\n\nFinally, rule conditions and their bodies can contain arbitrary Scala code. This gives Catalyst more power than domain specific languages for optimizers, while keeping it concise for simple rules.\n\nIn our experience, functional transformations on immutable trees make the whole optimizer very easy to reason about and debug. They also enable parallelization in the optimizer, although we do not yet exploit this.\n<h2>Using Catalyst in Spark SQL</h2>\n<strong>\n</strong>We use Catalyst\u2019s general tree transformation framework in four phases, as shown below: (1) analyzing a logical plan to resolve references, (2) logical plan optimization, (3) physical planning, and (4) code generation to compile parts of the query to Java bytecode. In the physical planning phase, Catalyst may generate multiple plans and compare them based on cost. All other phases are purely rule-based. Each phase uses different types of tree nodes; Catalyst includes libraries of nodes for expressions, data types, and logical and physical operators. We now describe each of these phases.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-12-at-8.41.26-AM.png\"><img class=\"aligncenter wp-image-3385\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-12-at-8.41.26-AM-1024x235.png\" alt=\"Catalyst blog figure 2\" width=\"800\" height=\"184\" /></a>\n<h2>Analysis</h2>\nSpark SQL begins with a relation to be computed, either from an abstract syntax tree (AST) returned by a SQL parser, or from a DataFrame object constructed using the API. In both cases, the relation may contain unresolved attribute references or relations: for example, in the SQL query <code>SELECT col FROM sales</code>, the type of col, or even whether it is a valid column name, is not known until we look up the table sales. An attribute is called unresolved if we do not know its type or have not matched it to an input table (or an alias). Spark SQL uses Catalyst rules and a Catalog object that tracks the tables in all data sources to resolve these attributes. It starts by building an \u201cunresolved logical plan\u201d tree with unbound attributes and data types, then applies rules that do the following:\n<ul>\n\t<li>Looking up relations by name from the catalog.</li>\n\t<li>Mapping named attributes, such as col, to the input provided given operator\u2019s children.</li>\n\t<li>Determining which attributes refer to the same value to give them a unique ID (which later allows optimization of expressions such as <code>col = col</code>).</li>\n\t<li>Propagating and coercing types through expressions: for example, we cannot know the return type of <code>1 + col</code> until we have resolved col and possibly casted its subexpressions to a compatible types.</li>\n</ul>\nIn total, the rules for the analyzer are about <a href=\"https://github.com/apache/spark/blob/fedbfc7074dd6d38dc5301d66d1ca097bc2a21e0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala\">1000 lines of code</a>.\n<h2>Logical Optimizations</h2>\nThe logical optimization phase applies standard rule-based optimizations to the logical plan. (Cost-based optimization is performed by generating multiple plans using rules, and then computing their costs.) These include constant folding, predicate pushdown, projection pruning, null propagation, Boolean expression simplification, and other rules. In general, we have found it extremely simple to add rules for a wide variety of situations. For example, when we added the fixed-precision DECIMAL type to Spark SQL, we wanted to optimize aggregations such as sums and averages on DECIMALs with small precisions; it took 12 lines of code to write a rule that finds such decimals in SUM and AVG expressions, and casts them to unscaled 64-bit LONGs, does the aggregation on that, then converts the result back. A simplified version of this rule that only optimizes SUM expressions is reproduced below:\n\n[scala]\nobject DecimalAggregates extends Rule[LogicalPlan] {\n  /** Maximum number of decimal digits in a Long */\n  val MAX_LONG_DIGITS = 18\n  def apply(plan: LogicalPlan): LogicalPlan = {\n    plan transformAllExpressions {\n      case Sum(e @ DecimalType.Expression(prec, scale))\n          if prec + 10 &lt;= MAX_LONG_DIGITS =&gt;\n        MakeDecimal(Sum(UnscaledValue(e)), prec + 10, scale) }\n}\n[/scala]\n\nAs another example, a 12-line rule optimizes LIKE expressions with simple regular expressions into String.startsWith or String.contains calls. The freedom to use arbitrary Scala code in rules made these kinds of optimizations, which go beyond pattern-matching the structure of a subtree, easy to express.\n\nIn total, the logical optimization rules are <a href=\"https://github.com/apache/spark/blob/fedbfc7074dd6d38dc5301d66d1ca097bc2a21e0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala\">800 lines of code</a>.\n<h2>Physical Planning</h2>\nIn the physical planning phase, Spark SQL takes a logical plan and generates one or more physical plans, using physical operators that match the Spark execution engine. It then selects a plan using a cost model. At the moment, cost-based optimization is only used to select join algorithms: for relations that are known to be small, Spark SQL uses a broadcast join, using a peer-to-peer broadcast facility available in Spark. The framework supports broader use of cost-based optimization, however, as costs can be estimated recursively for a whole tree using a rule. We thus intend to implement richer cost-based optimization in the future.\n\nThe physical planner also performs rule-based physical optimizations, such as pipelining projections or filters into one Spark map operation. In addition, it can push operations from the logical plan into data sources that support predicate or projection pushdown. We will describe the API for these data sources in a later section.\n\nIn total, the physical planning rules are about <a href=\"https://github.com/apache/spark/blob/fedbfc7074dd6d38dc5301d66d1ca097bc2a21e0/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala\">500 lines of code</a>.\n<h2>Code Generation</h2>\nThe final phase of query optimization involves generating Java bytecode to run on each machine. Because Spark SQL often operates on in-memory datasets, where processing is CPU-bound, we wanted to support code generation to speed up execution. Nonetheless, code generation engines are often complicated to build, amounting essentially to a compiler. Catalyst relies on a special feature of the Scala language, quasiquotes, to make code generation simpler. Quasiquotes allow the programmatic construction of abstract syntax trees (ASTs) in the Scala language, which can then be fed to the Scala compiler at runtime to generate bytecode. We use Catalyst to transform a tree representing an expression in SQL to an AST for Scala code to evaluate that expression, and then compile and run the generated code.\n\nAs a simple example, consider the Add, Attribute and Literal tree nodes introduced in Section 4.2, which allowed us to write expressions such as <code>(x+y)+1</code>. Without code generation, such expressions would have to be interpreted for each row of data, by walking down a tree of Add, Attribute and Literal nodes. This introduces large amounts of branches and virtual function calls that slow down execution. With code generation, we can write a function to translate a specific expression tree to a Scala AST as follows:\n\n[scala]\ndef compile(node: Node): AST = node match {\n  case Literal(value) =&gt; q&quot;$value&quot;\n  case Attribute(name) =&gt; q&quot;row.get($name)&quot;\n  case Add(left, right) =&gt; q&quot;${compile(left)} + ${compile(right)}&quot;\n}\n[/scala]\n\nThe strings beginning with <code>q</code> are quasiquotes, meaning that although they look like strings, they are parsed by the Scala compiler at compile time and represent ASTs for the code within. Quasiquotes can have variables or other ASTs spliced into them, indicated using <code>$</code> notation. For example, <code>Literal(1)</code> would become the Scala AST for 1, while <code>Attribute(\"x\")</code> becomes <code>row.get(\"x\")</code>. In the end, a tree like <code>Add(Literal(1), Attribute(\"x\"))</code> becomes an AST for a Scala expression like <code>1+row.get(\"x\")</code>.\n\nQuasiquotes are type-checked at compile time to ensure that only appropriate ASTs or literals are substituted in, making them significantly more useable than string concatenation, and they result directly in a Scala AST instead of running the Scala parser at runtime. Moreover, they are highly composable, as the code generation rule for each node does not need to know how the trees returned by its children are constructed. Finally, the resulting code is further optimized by the Scala compiler in case there are expression-level optimizations that Catalyst missed. The following figure shows that quasiquotes let us generate code with performance similar to hand-tuned programs.\n<img class=\"aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-12-at-8.45.27-AM-300x129.png\" alt=\"\" width=\"400\" />\n\nWe have found quasiquotes very straightforward to use for code generation, and we observed that even new contributors to Spark SQL could quickly add rules for new types of expressions. Quasiquotes also work well with our goal of running on native Java objects: when accessing fields from these objects, we can code-generate a direct access to the required field, instead of having to copy the object into a Spark SQL Row and use the Row\u2019s accessor methods. Finally, it was straightforward to combine code-generated evalua- tion with interpreted evaluation for expressions we do not yet generate code for, since the Scala code we compile can directly call into our expression interpreter.\n\nIn total, Catalyst\u2019s code generator is about <a href=\"https://github.com/apache/spark/blob/fedbfc7074dd6d38dc5301d66d1ca097bc2a21e0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala\">700 lines of code</a>.\n\nThis blog post covered the internals of Spark SQL\u2019s Catalyst optimizer. It\u2019s novel, simple design has enabled the Spark community to rapidly prototype, implement, and extend the engine. You can read through rest of the <a href=\"http://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf\">paper here</a>. If you are attending SIGMOD this year, please drop by our session!<strong><strong>\n</strong></strong>\n\nYou can also find more information about Spark SQL from the following:\n<ul>\n\t<li><a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html\">Spark SQL and DataFrame Programming Guide</a> from Apache Spark</li>\n\t<li><a href=\"http://www.slideshare.net/databricks/yin-huai-20150325meetupwithdemos\">Data Source API in Spark</a> presentation by Yin Huai</li>\n\t<li><a href=\"http://www.slideshare.net/databricks/introducing-dataframes-in-spark-for-large-scale-data-science\">Introducing DataFrames in Spark for Large Scale Data Science</a> by Reynold Xin</li>\n\t<li><a href=\"http://www.slideshare.net/databricks/spark-sqlsse2015public\">Beyond SQL: Speeding up Spark with DataFrames</a> by Michael Armbrust</li>\n</ul>"}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2015/04/14/running-spark-graphx-algorithms-on-library-of-congress-subject-heading-skos.html", "authors": null, "id": 3407, "categories": ["Apache Spark", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2015-04-14", "tz": "UTC", "createdOn": "2015-04-14"}, "title": "Running Apache Spark GraphX algorithms on Library of Congress subject heading SKOS", "slug": "running-spark-graphx-algorithms-on-library-of-congress-subject-heading-skos", "content": "<div class=\"asset-body\">This is a guest post from Bob DuCharme.</div>\n<div class=\"asset-body\">Original article\u00a0appeared in:\u00a0<a href=\"http://www.snee.com/bobdc.blog/2015/04/running-spark-graphx-algorithm.html\" target=\"_blank\">http://www.snee.com/bobdc.blog/2015/04/running-spark-graphx-algorithm.html</a></div>\n<div class=\"asset-body\">\n\n<hr />\n\n</div>\n<div class=\"asset-body\"><b>Well, one algorithm, but a very cool one.</b></div>\n<div id=\"more\" class=\"asset-more\">\n<div id=\"id116252\">\n\n<img id=\"id116247\" src=\"http://www.snee.com/bobdc.blog/img/GraphXLoCSKOS.png\" alt=\"GraphX LoC SKOS logos\" width=\"160\" align=\"right\" border=\"0\" hspace=\"30px\" vspace=\"30px\" />\n<p id=\"id116269\">Last month, in <a id=\"id116271\" href=\"http://www.snee.com/bobdc.blog/2015/03/spark-and-sparql-rdf-graphs-an.html\">Apache Spark and SPARQL; RDF Graphs and GraphX</a>, I described how Apache Spark has emerged as a more efficient alternative to MapReduce for distributing computing jobs across clusters. I also described how Spark's GraphX library lets you do this kind of computing on graph data structures and how I had some ideas for using it with RDF data. My goal was to use RDF technology on GraphX data and vice versa to demonstrate how they could help each other, and I demonstrated the former with a Scala program that output some GraphX data as RDF and then showed some SPARQL queries to run on that RDF.</p>\n<p id=\"id116277\">Today I'm demonstrating the latter by reading in a well-known RDF dataset and executing GraphX's Connected Components algorithm on it. This algorithm collects nodes into groupings that connect to each other but not to any other nodes. In classic Big Data scenarios, this helps applications perform tasks such as the identification of subnetworks of people within larger networks, giving clues about which products or cat videos to suggest to those people based on what their friends liked.</p>\n<p id=\"id116281\">The US Library of Congress has been working on their <a id=\"id116283\" href=\"http://id.loc.gov/authorities/subjects.html\">Subject Headings</a> metadata since 1898, and it's available in SKOS RDF. Many of the subjects include \"related\" values; for example, you can see that the subject <a id=\"id116288\" href=\"http://id.loc.gov/authorities/subjects/sh85027617.html\">Cocktails</a> has related values of <a id=\"id116293\" href=\"http://id.loc.gov/authorities/subjects/sh85027615.html\">Cocktail parties</a> and <a id=\"id116298\" href=\"http://id.loc.gov/authorities/subjects/sh2009010761.html\">Happy hours</a>, and that Happy hours has related values of <a id=\"id116304\" href=\"http://id.loc.gov/authorities/subjects/sh93000452.html\">Bars (Drinking establishments)</a>, <a id=\"id116139\" href=\"http://id.loc.gov/authorities/subjects/sh85113249.html\">Restaurants</a>, and Cocktails. So, while it includes skos:related triples that indirectly link Cocktails to Restaurants, it has none that link these to the subject of <a id=\"id116126\" href=\"http://id.loc.gov/authorities/subjects/sh85125961.html\">Space stations</a>, so the Space stations subject is not part of the same Connected Components subgraph as the Cocktails subject.</p>\n<p id=\"id116111\">After reading the Library of Congress Subject Header RDF into a GraphX graph and running the Connected Components algorithm on the skos:related connections, here are some of the groupings I found near the beginning of the output:</p>\n\n<pre id=\"id116102\">\"Hiding places\" \n\"Secrecy\" \n\"Loneliness\" \n\"Solitude\" \n\"Privacy\" \n--------------------------\n\"Cocktails\" \n\"Bars (Drinking establishments)\" \n\"Cocktail parties\" \n\"Restaurants\" \n\"Happy hours\" \n--------------------------\n\"Space stations\" \n\"Space colonies\" \n\"Large space structures (Astronautics)\" \n\"Extraterrestrial bases\" \n--------------------------\n\"Inanna (Sumerian deity)\" \n\"Ishtar (Assyro-Babylonian deity)\" \n\"Astarte (Phoenician deity)\" \n--------------------------\n\"Cross-cultural orientation\" \n\"Cultural competence\" \n\"Multilingual communication\" \n\"Intercultural communication\" \n\"Technical assistance--Anthropological aspects\" \n--------------------------\n</pre>\n<p id=\"id116094\">(You can find the <a id=\"id116090\" href=\"http://snee.com/bobdc.blog/files/readLoCSH.out\">complete output here</a>, a 565K file.) People working with RDF-based applications already know that this kind of data can help to enhance search. For example, someone searching for media about \"Space stations\" will probably also be interested in media filed under \"Space colonies\" and \"Extraterrestrial bases\". This data can also help other applications, and now, it can help distributed applications that use Spark.</p>\n\n<div id=\"id116082\">\n<h2 id=\"id116235\">Storing RDF in GraphX data structures</h2>\n<p id=\"id116227\">First, as I mentioned in the earlier blog entry, GraphX development currently means coding with the Scala programming language, so I have been learning Scala. My old friend from XML days <a id=\"id116222\" href=\"http://www.contakt.org/\">Tony Coates</a> wrote <a id=\"id116208\" href=\"http://www.contakt.org/Blog/Post/13/A-Scala-API-for-RDF-Processing\">A Scala API for RDF Processing</a>, which takes better advantage of native Scala data structures than I ever could, and the <a id=\"id116198\" href=\"https://github.com/w3c/banana-rdf\">banana-rdf Scala library</a> also looks interesting, but although I was using Scala my main interest was storing RDF in Spark GraphX data structures, not in Scala particularly.</p>\n<p id=\"id116186\">The basic Spark data structure is the Resilient Distributed Dataset, or RDD. The graph data structure used by GraphX is a combination of an RDD for vertices and one for edges. Each of these RDDs can have additional information; the Spark website's <a id=\"id116176\" href=\"https://spark.apache.org/docs/1.1.1/graphx-programming-guide.html#example-property-graph\">Example Property Graph</a> includes (name, role) pairs with its vertices and descriptive property strings with its edges. The obvious first step for storing RDF in a GraphX graph would be to store predicates in the edges RDD, subjects and resource objects in the vertices RDD, and literal properties as extra information in these RDDs like the (name, role) pairs and edge description strings in the Spark website's Example Property Graph.</p>\n\n<div id=\"id116082\">\n<p id=\"id118585\">But, as I also wrote last time, a hardcore RDF person would ask <a id=\"id118588\" href=\"http://www.snee.com/bobdc.blog/2015/03/spark-and-sparql-rdf-graphs-an.html#id106263\">these questions</a>:</p>\n\n<ul id=\"id118594\">\n \t<li id=\"id118596\">\n<p id=\"id118597\">What about properties of edges? For example, what if I wanted to say that an <tt id=\"id118600\">xp:advisor</tt>property was an <tt id=\"id118603\">rdfs:subPropertyOf</tt> the Dublin Core property <tt id=\"id118607\">dc:contributor</tt>?</p>\n</li>\n \t<li id=\"id118612\">\n<p id=\"id118613\">The ability to assign properties such as a name of \"rxin\" and a role of \"student\" to a node like 3L is nice, but what if I don't have a consistent set of properties that will be assigned to every node\u2014for example, if I've aggregated person data from two different sources that don't use all the same properties to describe these persons?</p>\n</li>\n</ul>\n<p id=\"id118618\">The Example Property Graph can store these (name, role) pairs with the vertices because that RDD is declared as <tt id=\"id118620\">RDD[(VertexId, (String, String))]</tt>. Each vertex will have two strings stored with it; no more and no less. It's a data structure, but you can also think of it as a proscriptive schema, and the second bullet above is asking how to get around that.</p>\n<p id=\"id118625\">I got around both issues by storing the data in three data structures\u2014the two RDDs described above and one more:</p>\n\n<ul id=\"id118628\">\n \t<li id=\"id118631\">\n<p id=\"id118632\">For the vertex RDD, along with the required long integer that must be stored as each vertex's identifier, I only stored one extra piece of information: the URI associated with that RDF resource. I did this for the subjects, the predicates (which may not be \"vertices\" in the GraphX sense of the word, but damn it, they're resources that can be the subjects or objects of triples if I want them to), and the relevant objects. After reading the triple {<tt id=\"id118634\">&lt;http://id.loc.gov/authorities/subjects/sh85027617&gt; &lt;http://www.w3.org/2004/02/skos/core#related&gt; &lt;http://id.loc.gov/authorities/subjects/sh2009010761&gt;</tt>} from the Library of Congress data, the program will create three vertices in this RDD whose node identifiers might be 1L, 2L, and 3L, with each of the triple's URIs stored with one of these RDD vertices.</p>\n</li>\n \t<li id=\"id118639\">\n<p id=\"id118640\">For the edge RDD, along with the required two long integers identifying the vertices at the start and end of the edge, each of my edges also stores the URI of the relevant predicate as the \"description\" of the edge. The edge for the triple above would be (1L, 3L, http://www.w3.org/2004/02/skos/core#related).</p>\n</li>\n \t<li id=\"id118644\">\n<p id=\"id118645\">To augment the graph data structure created from the two RDDs above, I created a third RDD to store literal property values. Each entry stores the long integer representing the vertex of the resource that has the property, a long integer representing the property (the integer assigned to that property in the vertex RDD), and a string representing the property value. For the triple { <tt id=\"id118648\">&lt;http://id.loc.gov/authorities/subjects/sh2009010761&gt; &lt;http://www.w3.org/2004/02/skos/core#prefLabel&gt; \"Happy hours\"</tt>} it might store (3L, 4L, \"Happy hours\"), assuming that 4L had been stored as the internal identifier for the skos:prefLabel property. To run the Connected Components algorithm and then output the preferred label of each member of each subgraph, I didn't need this RDD, but it does open up many possibilities for what you can do with RDF in an a Spark GraphX program.</p>\n</li>\n</ul>\n</div>\n<div id=\"id118734\">\n<h2 id=\"id118737\">Creating a report on Library of Congress Subject Heading connecting components</h2>\n<p id=\"id118740\">After loading up these data structures (plus another one that allows quick lookups of preferred labels) my program below applies the GraphX Connected Components algorithm to the subset of the graph that uses the skos:related property to connect vertices such as \"Cocktails\" and \"Happy hours\". Iterating through the results, it uses them to load a hash map with a list for each subgraph of connected components. Then, it goes through each of these lists, printing the label associated with each member of each subgraph and a string of hyphens to show where each list ends, as you can see in the excerpt above.</p>\n<p id=\"id118744\">I won't go into more detail about what's in my program because I commented it pretty heavily. (I do have to thank my friend Tony, mentioned above, for helping me past one point where I was stuck on a Scala scoping issue. Also, as I've warned before, my coding style will probably make experienced Scala programmers choke on their Red Bull. I'd be happy to hear about suggested improvements.)</p>\n<p id=\"id118748\">After getting the program to run properly with a small subset of the data, I ran it on the 1 GB subjects-skos-2014-0306.nt file that I downloaded from the Library of Congress with its 7,705,147 triples. Spark lets applications scale up by giving you an infrastructure to distribute program execution across multiple machines, but the 8GB on my single machine wasn't enough to run this, so I used two grep commands to create a version of the data that only had the skos:related and skos:prefLabel triples. At this point I had a total of 439,430 triples. Because my code didn't account for blank nodes, I removed the 385 triples that used them, leaving 439,045 to work with in a 60MB file. This ran successfully and you can follow the link shown earlier to see the complete output.</p>\n\n</div>\n<div id=\"id118752\">\n<h2 id=\"id118755\">Other GraphX algorithms to run on your RDF data</h2>\n<p id=\"id118758\"><a id=\"id118760\" href=\"https://spark.apache.org/docs/latest/graphx-programming-guide.html#graph-algorithms\">Other GraphX algorithms</a> besides Connected Components include Page Rank and Triangle Counting. <a id=\"id118764\" href=\"http://en.wikipedia.org/wiki/Graph_theory\">Graph theory</a> is an interesting world, in which my favorite phrase so far is \"<a id=\"id118769\" href=\"http://en.wikipedia.org/wiki/Strangulated_graph\">strangulated graph</a>\".</p>\n<p id=\"id118775\">One of the greatest things about RDF and Linked Data technology is the <a id=\"id118778\" href=\"http://linkeddata.org/\">growing amount</a> of interesting data being made publicly available, and with new tools such as these algorithms to work with this data\u2014tools that can be run on inexpensive, scalable clusters faster than typical Hadoop MapReduce jobs\u2014there are a lot of great possibilities.</p>\n\n\n[scala]\n//////////////////////////////////////////////////////////////////\n// readLoCSH.scala: read Library of Congress Subject Headings into\n// Spark GraphX graph and apply connectedComponents algorithm to those\n// connected by skos:related property.\n\nimport scala.io.Source \nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.graphx._\nimport org.apache.spark.rdd.RDD\nimport scala.collection.mutable.ListBuffer\nimport scala.collection.mutable.HashMap\n\nobject readLoCSH {\n\nval componentLists = HashMap[VertexId, ListBuffer[VertexId]]()\nval prefLabelMap =  HashMap[VertexId, String]()\n\ndef main(args: Array[String]) {\nval sc = new SparkContext(&quot;local&quot;, &quot;readLoCSH&quot;, &quot;127.0.0.1&quot;)\n\n// regex pattern for end of triple\nval tripleEndingPattern = &quot;&quot;&quot;\\s*\\.\\s*$&quot;&quot;&quot;.r    \n// regex pattern for language tag\nval languageTagPattern = &quot;@[\\\\w-]+&quot;.r    \n\n// Parameters of GraphX Edge are subject, object, and predicate\n// identifiers. RDF traditionally does (s, p, o) order but in GraphX\n// it's (edge start node, edge end node, edge description).\n\n// Scala beginner hack: I couldn't figure out how to declare an empty\n// array of Edges and then append Edges to it (or how to declare it\n// as a mutable ArrayBuffer, which would have been even better), but I\n// can append to an array started like the following, and will remove\n// the first Edge when creating the RDD.\n\nvar edgeArray = Array(Edge(0L,0L,&quot;http://dummy/URI&quot;))\nvar literalPropsTriplesArray = new Array[(Long,Long,String)](0)\nvar vertexArray = new Array[(Long,String)](0)\n\n// Read the Library of Congress n-triples file\n//val source = Source.fromFile(&quot;sampleSubjects.nt&quot;,&quot;UTF-8&quot;)  // shorter for testing\nval source = Source.fromFile(&quot;PrefLabelAndRelatedMinusBlankNodes.nt&quot;,&quot;UTF-8&quot;)\n\nval lines = source.getLines.toArray\n\n// When parsing the data we read, use this map to check whether each\n// URI has come up before.\nvar vertexURIMap = new HashMap[String, Long];\n\n// Parse the data into triples.\nvar triple = new Array[String](3)\nvar nextVertexNum = 0L\nfor (i &lt;- 0 until lines.length) {\n    // Space in next line needed for line after that. \n    lines(i) = tripleEndingPattern.replaceFirstIn(lines(i),&quot; &quot;)  \n    triple = lines(i).mkString.split(&quot;&gt;\\\\s+&quot;)       // split on &quot;&gt; &quot;\n    // Variables have the word &quot;triple&quot; in them because &quot;object&quot; \n    // by itself is a Scala keyword.\n    val tripleSubject = triple(0).substring(1)   // substring() call\n    val triplePredicate = triple(1).substring(1) // to remove &quot;&lt;&quot;\n    if (!(vertexURIMap.contains(tripleSubject))) {\n        vertexURIMap(tripleSubject) = nextVertexNum\n        nextVertexNum += 1\n    }\n    if (!(vertexURIMap.contains(triplePredicate))) {\n        vertexURIMap(triplePredicate) = nextVertexNum\n        nextVertexNum += 1\n    }\n    val subjectVertexNumber = vertexURIMap(tripleSubject)\n    val predicateVertexNumber = vertexURIMap(triplePredicate)\n\n    // If the first character of the third part is a &lt;, it's a URI;\n    // otherwise, a literal value. (Needs more code to account for\n    // blank nodes.)\n    if (triple(2)(0) == '&lt;') { \n        val tripleObject = triple(2).substring(1)   // Lose that &lt;.\n        if (!(vertexURIMap.contains(tripleObject))) {\n            vertexURIMap(tripleObject) = nextVertexNum\n            nextVertexNum += 1\n        }\n        val objectVertexNumber = vertexURIMap(tripleObject)\n        edgeArray = edgeArray :+\n            Edge(subjectVertexNumber,objectVertexNumber,triplePredicate)\n    }\n    else {\n        literalPropsTriplesArray = literalPropsTriplesArray :+\n            (subjectVertexNumber,predicateVertexNumber,triple(2))\n    }\n}\n\n// Switch value and key for vertexArray that we'll use to create the\n// GraphX graph.\nfor ((k, v) &lt;- vertexURIMap) vertexArray = vertexArray :+  (v, k)   \n\n// We'll be looking up a lot of prefLabels, so create a hashmap for them. \nfor (i &lt;- 0 until literalPropsTriplesArray.length) {\n    if (literalPropsTriplesArray(i)._2 ==\n        vertexURIMap(&quot;http://www.w3.org/2004/02/skos/core#prefLabel&quot;)) {\n        // Lose the language tag.\n        val prefLabel =\n            languageTagPattern.replaceFirstIn(literalPropsTriplesArray(i)._3,&quot;&quot;)\n        prefLabelMap(literalPropsTriplesArray(i)._1) = prefLabel;\n    }\n}\n\n// Create RDDs and Graph from the parsed data.\n\n// vertexRDD Long: the GraphX longint identifier. String: the URI.\nval vertexRDD: RDD[(Long, String)] = sc.parallelize(vertexArray)\n\n// edgeRDD String: the URI of the triple predicate. Trimming off the\n// first Edge in the array because it was only used to initialize it.\nval edgeRDD: RDD[Edge[(String)]] =\n    sc.parallelize(edgeArray.slice(1,edgeArray.length))\n\n// literalPropsTriples Long, Long, and String: the subject and predicate\n// vertex numbers and the the literal value that the predicate is\n// associating with the subject.\nval literalPropsTriplesRDD: RDD[(Long,Long,String)] =\n    sc.parallelize(literalPropsTriplesArray)\n\nval graph: Graph[String, String] = Graph(vertexRDD, edgeRDD)\n\n// Create a subgraph based on the vertices connected by SKOS &quot;related&quot;\n// property.\nval skosRelatedSubgraph =\n    graph.subgraph(t =&gt; t.attr ==\n                   &quot;http://www.w3.org/2004/02/skos/core#related&quot;)\n\n// Find connected components  of skosRelatedSubgraph.\nval ccGraph = skosRelatedSubgraph.connectedComponents() \n\n// Fill the componentLists hashmap.\nskosRelatedSubgraph.vertices.leftJoin(ccGraph.vertices) {\ncase (id, u, comp) =&gt; comp.get\n}.foreach\n{ case (id, startingNode) =&gt; \n  {\n      // Add id to the list of components with a key of comp.get\n      if (!(componentLists.contains(startingNode))) {\n          componentLists(startingNode) = new ListBuffer[VertexId]\n      }\n      componentLists(startingNode) += id\n  }\n}\n\n// Output a report on the connected components. \nprintln(&quot;------  connected components in SKOS \\&quot;related\\&quot; triples ------\\n&quot;)\nfor ((component, componentList) &lt;- componentLists){\n    if (componentList.size &gt; 1) { // don't bother with lists of only 1\n        for(c &lt;- componentList) {\n            println(prefLabelMap(c));\n        }\n        println(&quot;--------------------------&quot;)\n    }\n}\n\nsc.stop\n}\n[/scala]\n\n</div>\n</div>\n</div>\n</div>"}
{"status": "publish", "description": "Celtra provides advertisers with a solution for brand advertising. Databricks Cloud enabled Celtra to improve product design and problem resolution through analytics.", "creator": "kavitha", "link": "https://databricks.com/blog/2015/04/15/celtra-scales-big-data-analysis-projects-six-fold-with-databricks-cloud.html", "authors": null, "id": 3426, "categories": ["Announcements", "Company Blog", "Customers", "Product"], "dates": {"publishedOn": "2015-04-15", "tz": "UTC", "createdOn": "2015-04-15"}, "title": "Celtra Scales Big Data Analysis Projects Six-Fold with Databricks", "slug": "celtra-scales-big-data-analysis-projects-six-fold-with-databricks-cloud", "content": "We are thrilled to announce that Celtra\u00a0selected\u00a0Databricks to scale its\u00a0big data analysis projects, increasing the amount of ad-hoc analysis done, six-fold.\n\nPress release:\u00a0<a href=\"http://www.marketwired.com/press-release/celtra-scales-big-data-analysis-projects-six-fold-with-databricks-cloud-2009995.htm\" target=\"_blank\">http://www.marketwired.com/press-release/celtra-scales-big-data-analysis-projects-six-fold-with-databricks-cloud-2009995.htm</a>\n\n<a href=\"http://www.celtra.com/\">Celtra</a> provides agencies, media suppliers and brand leaders alike with an integrated, scalable HTML5 technology for brand advertising on smartphones, tablets and desktop.\u00a0The platform, <i>AdCreator 4</i>, gives clients such as MEC, Kargo, Pepsi and Macy\u2019s the ability to easily create, manage, and traffic sophisticated data-driven dynamic ads, optimize them on the go, and track their performance with insightful analytics.\n\nA wide variety of data is collected by Celtra, including data related to internal company processes, data based on the usage of the product by clients and, most importantly, data focused on the engagements of consumers with their clients\u2019 ads.\u00a0In addition to providing analytics to its clients, Celtra is constantly exploring new ways to leverage this information to improve their offering.\n\nAs Celtra\u2019s business grew, it was challenged to meet the corresponding increase in demand for analytics because of its\u00a0diverse data sources, terabyte scale data, and small analytics team. It\u00a0needed a powerful data platform that was capable of integrating data from disparate data sources while being fast enough to support interactive analysis at terabyte scale. This platform must also be user-friendly enough to empower teams outside of analytics to perform analysis themselves, and to remove the bottleneck created by their small analytics team.\n\nWith the adoption of Databricks, Celtra has enabled teams from Engineering, Product Management, and QA to perform complex data analysis on their own, leveraging the massive production data to improve product design, address anomalies rapidly, and fine-tune the performance of production systems.\n\nDatabricks provided Celtra with a number of critical business benefits:\n<ul>\n \t<li>Increased the amount of ad-hoc analysis done six-fold, leading to better informed product design and quicker issue detection and resolution.</li>\n \t<li>Reduced the load on the analytics engineering team by expanding access to the number of people able to work with the data directly by a factor of four.</li>\n \t<li>Increased collaboration and improved reproducibility and repeatability of analyses.</li>\n \t<li>Reduced the cost of cloud infrastructure through faster and easier management of Apache Spark clusters.</li>\n</ul>\nDownload this <a title=\"Customer Case Studies\" href=\"https://databricks.com/resources/customer-case-studies\" target=\"_blank\">case study</a> learn more about how Celtra\u00a0is using Databricks."}
{"status": "publish", "description": "Recently, Databricks added a new feature, Jobs, to our cloud service. This feature allows one to programmatically run Spark jobs on Amazon\u2019s EC2 easier than ever before. In this blog, I will provide a quick tour of this feature.", "creator": "ion", "link": "https://databricks.com/blog/2015/04/16/the-easiest-way-to-run-spark-jobs.html", "authors": null, "id": 3471, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2015-04-16", "tz": "UTC", "createdOn": "2015-04-16"}, "title": "The Easiest Way to Run Apache Spark Jobs", "slug": "the-easiest-way-to-run-spark-jobs", "content": "Recently, Databricks added a new feature, Jobs, to our cloud service. You can find a detailed overview of this feature <a href=\"https://databricks.com/blog/2015/03/18/databricks-launches-jobs-feature-for-production-workloads.html\">here</a>.\n\nThis feature allows one to programmatically run Apache Spark jobs on Amazon\u2019s EC2 easier than ever before. In this blog, I will provide a quick tour of this feature.\n<h2>What is a Job?</h2>\nThe job feature is very flexible. A user can run a job not only as any Spark JAR, but also notebooks you have created with Databricks Cloud. In addition, notebooks can be used as scripts to create sophisticated pipelines.\n<h2>How to run a Job?</h2>\nAs shown below, Databricks Cloud offers an intuitive, easy to use interface to create a job.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-6.16.29-PM.png\"><img class=\"aligncenter wp-image-3472\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-6.16.29-PM-1024x293.png\" alt=\"Jobs how-to blog figure 1\" width=\"600\" height=\"171\" /></a>\n\nWhen creating a job, you will need to specify the name and the size of the cluster which will run the job. Since typically with Spark the amount of memory determines its performance, you will then be asked to enter the memory capacity of the cluster. Databricks Cloud will automatically instantiate a cluster of the specified capacity when running the job either by reusing your existing cluster or by creating a new one, and then subsequently tears down the cluster, once the job completes.\n\nNext, you need to specify the notebook or the JAR\u00a0you intend to run as a job, the input arguments of the job (both JARs\u00a0and notebooks can take input arguments), and the job\u2019s configuration parameters: <i>schedule</i>, <i>timeout</i>, <i>alerts</i>, and the <i>type of EC2 instances</i> you would like the job to use. Next, we consider each of these configuration parameters in turn.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-6.18.17-PM.png\"><img class=\"aligncenter wp-image-3473\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-6.18.17-PM-300x216.png\" alt=\"Jobs howto blog figure 2\" width=\"250\" height=\"180\" /></a>\n\n<b>Scheduling</b>: The user can run any job periodically, by simply specifying the starting time and the interval, as shown below.\n\n<strong><strong><a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-6.21.47-PM.png\"><img class=\"aligncenter wp-image-3475\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-6.21.47-PM-300x88.png\" alt=\"Jobs how-to blog figure 3\" width=\"400\" height=\"118\" /></a></strong></strong>\n\n<b>Timeout</b>: Optionally the user can set a timeout which specifies the time the job is allowed to run before being terminated. This feature is especially useful when handling runaway jobs, and to make sure that an instance of a periodic job terminates before the next instance begins. If no timeout is specified and a job instance takes more than the scheduling period, no new instances are started before the current one terminates.\n\n<strong><strong><a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-6.25.57-PM.png\"><img class=\"aligncenter size-medium wp-image-3476\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-6.25.57-PM-300x126.png\" alt=\"Jobs how-to blog figure 4\" width=\"300\" height=\"126\" /></a></strong></strong><strong><strong>\u00a0</strong></strong>\n\n<b>Alerts</b>: When running production jobs, it is critical to alert the user when any significant event occurs. Databricks Cloud allows a user to specify the events they would like to be alerted on via e-mail: when <i>job starts</i>, when it <i>successfully finishes</i>, or when it <i>finishes with error</i>.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-6.27.39-PM.png\"><img class=\"aligncenter size-medium wp-image-3477\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-6.27.39-PM-300x193.png\" alt=\"Jobs how-to figure 5\" width=\"300\" height=\"193\" /></a>\n\n<b>Resource type</b>: Finally, the user can specify whether they would want to use spot or on-demand instances to run the job.\n\n<strong><strong> <a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-6.28.54-PM.png\"><img class=\"aligncenter size-medium wp-image-3478\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-6.28.54-PM-300x151.png\" alt=\"Jobs how-to figure 6\" width=\"300\" height=\"151\" /></a>\u00a0</strong></strong>\n<h2>History and Results<strong><strong>\u00a0</strong></strong></h2>\nThe Job UI provides an easy way to inspect the status of each run of a given job. The figure below shows the status of multiple runs of the same job. i.e., when each run starts, how long it takes, and if it has terminated successfully.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-6.33.59-PM.png\"><img class=\"aligncenter size-medium wp-image-3481\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-6.33.59-PM-300x152.png\" alt=\"Jobs how-to figure 6\" width=\"300\" height=\"152\" /></a>\n\nBy clicking on any of the \u201cRun x\u201d links you can immediately see the output of the corresponding run including its output logs and errors, if any. The picture below shows the output of \u201cRun 6\u201d above.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-7.36.50-PM.png\"><img class=\"aligncenter size-medium wp-image-3484\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-7.36.50-PM-300x234.png\" alt=\"Jobs how-to blog figure 7\" width=\"300\" height=\"234\" /></a>\n\nSimilarly, the figure below shows the output of running a notebook as a job. Incidentally, the output is the same as running the notebook manually.\n\n<strong><strong><a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-7.37.30-PM.png\"><img class=\"aligncenter size-medium wp-image-3485\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-7.37.30-PM-300x296.png\" alt=\"Jobs how-to figure 8\" width=\"300\" height=\"296\" /></a></strong></strong><strong><strong>\u00a0</strong></strong>\n<h2>Summary</h2>\nAs I hope this short tour has convinced you, Databricks Cloud provides a powerful, yet easy to use feature to run not only arbitrary Spark jobs, but also notebooks created with Databricks Cloud. If you\u2019d like to run your own jobs with Databricks Cloud, you can <a href=\"https://databricks.com/registration\">register</a> here for an account.\n<h2>Additional\u00a0Resources</h2>\n<p style=\"text-align: left;\">Other Databricks Cloud how-tos can be found at:</p>\n\n<ul>\n \t<li style=\"text-align: left;\"><a href=\"https://databricks.com/?p=3558\" target=\"_blank\">Analyzing Apache Access Logs with Databricks Cloud</a></li>\n</ul>"}
{"status": "publish", "description": "This blog post describes two new MLlib algorithms contributed from Huawei in Spark 1.3 and their use cases: FP-growth for frequent pattern mining and Power Iteration Clustering for graph clustering.", "creator": "dave_wang", "link": "https://databricks.com/blog/2015/04/17/new-mllib-algorithms-in-spark-1-3-fp-growth-and-power-iteration-clustering.html", "authors": null, "id": 3523, "categories": ["Apache Spark", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2015-04-17", "tz": "UTC", "createdOn": "2015-04-17"}, "title": "New MLlib Algorithms in Apache Spark 1.3: FP-Growth and Power Iteration Clustering", "slug": "new-mllib-algorithms-in-spark-1-3-fp-growth-and-power-iteration-clustering", "content": "This is a guest blog post from Huawei\u2019s big data global team.\n\nHuawei, a Fortune Global 500 private company, has put together a global team since 2013 to work on Apache Spark community projects and contribute back to the community. This blog post describes two new MLlib algorithms contributed from Huawei in Apache Spark 1.3 and their use cases: FP-growth for frequent pattern mining and Power Iteration Clustering for graph clustering.\n\n<hr />\n\n&nbsp;\n<h2>FP-Growth: Scalable Frequent Itemset Mining</h2>\nAs smartphones and the mobile internet become more and more popular, a huge amount of data traffic is transmitted each second on the global internet. In a typical network of millions subscribers, traffic rates can reach terabytes per second, which drives gigabytes of event logs generated per second from the underneath network equipment. At Huawei, we are often interested in analyzing traffic patterns from these logs, so we can leverage usage information to make the network more efficient. A common technique for analyzing network data is <i>frequent pattern mining</i>. Frequent pattern mining can reveal the most frequently visited site in a particular period or find popular routing paths that generate most traffic in a particular region. Finding these patterns allows us to improve utilization of the network; for instance, information on routing hotspots can influence the placement of gateway and routers in the network.\n<h2>FP-Growth</h2>\nThe FP-growth mining problem models its input as a set of <i>transactions</i>. Each transaction is simply a set of <i>items</i> and the algorithm looks for common subsets of items that appear across transactions. For a subset to be considered a pattern, it must appear in some minimum proportion of all transactions, termed the <i>support</i>. In the case of a telco network, items would be individual network nodes, and a transaction could represent one path of nodes. Then the algorithm would return sub-paths of the network that are frequently traversed.\n\nA naive way to do this is to generate all possible itemsets and count their occurrence, which is not scalable because it quickly becomes a combinatorial explosion problem as the input data size increases. To solve this problem, we chose FP-growth, a classic algorithm that finds all frequent itemsets without generating and testing all candidates. And to make FP-growth work on large-scale datasets, we at Huawei has implemented a parallel version of FP-growth, as described in <a href=\"http://dx.doi.org/10.1145/1454008.1454027\">Li et al., PFP: Parallel FP-growth for query recommendation</a>, and contributed it to Apache Spark 1.3.\n\nHere is a brief description of the algorithm. The algorithm takes an RDD of transactions from user, and works in two steps to output frequent itemsets. In the first step, item frequency is calculated and infrequent items are filtered (because frequent itemsets must consist of frequent items). In the second step, suffix trees (FP-trees) are constructed and grown from the filtered transactions, and then frequent itemsets can be extracted from the suffix trees. The work is distributed based on the suffixes of the filtered transactions, and <code>combineByKey</code> is used to reduce the amount of shuffle data.\n<h2>Scalability</h2>\nWe have compared MLlib\u2019s FP-growth implementation against Mahout on our production datasets. The results are plotted as below.\n<p style=\"text-align: center;\"><img class=\"wp-image-3526\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-17-at-8.16.01-AM1.png\" alt=\"Experiment 1: Running times for different support levels using a 1.5GB data set.\" width=\"500\" height=\"305\" /></p>\n<p style=\"text-align: center;\">Experiment 1: Running times for different support levels using a 1.5GB data set.</p>\n<p style=\"text-align: left;\"><a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-17-at-8.20.13-AM.png\"><img class=\"aligncenter wp-image-3529\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-17-at-8.20.13-AM.png\" alt=\"Huawei guest blog figure 2\" width=\"500\" height=\"305\" /></a></p>\n<p style=\"text-align: center;\">Experiment 2: Running times for different data sizes (GB).</p>\nAs shown in the figures, MLlib is about 7~9 times faster than Mahout on a 1.5GB dataset, and MLlib achieves good scalability as the dataset grows 10 times and 100 times. In the largest test, MLlib is about 11 times faster than Mahout.\n<h2>Examples</h2>\n<p style=\"text-align: left;\">MLlib\u2019s FP-growth is available in Scala/Java in Apache Spark 1.3. Its Python API was merged recently and it will be available in 1.4. Following example code demonstrates its API usage:</p>\n\n\n[scala]\nimport org.apache.spark.mllib.fpm.FPGrowth\n\n// the input data set containing all transactions\nval transactions = sc.textFile(&quot;...&quot;).map(_.split(&quot; &quot;)).cache()\n\n// run the FP-growth algorithm\nval model = new FPGrowth()\n  .setMinSupport(0.5)\n  .setNumPartitions(10)\n  .run(transactions)\n\n// print the frequent itemset result\nmodel.freqItemsets.collect().foreach { itemset =&gt;\n  println(itemset.items.mkString(&quot;[&quot;, &quot;,&quot;, &quot;]&quot;) + &quot;, &quot; + itemset.freq)\n}\n[/scala]\n\nFor more information about MLlib\u2019s FP-growth, please visit its <a href=\"https://spark.apache.org/docs/latest/mllib-frequent-pattern-mining.html\">user guide</a> and check out full examples in <a href=\"https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/mllib/FPGrowthExample.scala\">Scala</a> and in <a href=\"https://github.com/apache/spark/blob/master/examples/src/main/java/org/apache/spark/examples/mllib/JavaFPGrowthExample.java\">Java</a> on GitHub.\n<h2>Power Iteration Clustering: Spectral Clustering on GraphX</h2>\nCommunication service providers like Huawei must manage, operate, and optimize increasingly dynamic traffic workloads on heterogeneous networks. Among various algorithms being used in this effort, unsupervised learning including clustering plays an important role, for example, in identifying similar behaviors among users or network clusters. Graph clustering algorithms are commonly used in the telecom industry for this purpose, and can be applied to data center management and operation.\n<h2>Power Iteration Clustering</h2>\nWe have implemented Power Iteration Clustering (PIC) in MLlib, a simple and scalable graph clustering method described in <a href=\"http://www.icml2010.org/papers/387.pdf\">Lin and Cohen, Power Iteration Clustering</a>. PIC takes an undirected graph with similarities defined on edges and outputs clustering assignment on nodes. PIC uses truncated <a href=\"http://en.wikipedia.org/wiki/Power_iteration\">power iteration</a> to find a very low-dimensional embedding of the nodes, and this embedding leads to effective graph clustering.\n\nPIC is a graph algorithm and it can be easily described in a graph language. So it was natural to implement PIC using GraphX in Spark and take advantage of GraphX\u2019 graph processing APIs and optimization. MLlib\u2019s PIC is among the first MLlib algorithms built upon GraphX. In particular, we store the normalized similarity matrix as a graph with normalized similarities defined as edge properties. The edge properties are cached and remain static during the power iterations. The embedding of nodes is defined as node properties on the same graph topology. We update the embedding through power iterations, where aggregateMessages is used to compute matrix-vector multiplications, the essential operation in a power iteration method. Finally, k-means is used to cluster nodes using the embedding.\n<h2>Examples</h2>\nMLlib\u2019s PIC is available in Scala/Java in Apache Spark 1.3. Its Python support will be added in a future release. The following example code demonstrates its API usage:\n\n[scala]\nimport org.apache.spark.mllib.clustering.PowerIterationClustering\n\n\n// pairwise similarities\nval similarities: RDD[(Long, Long, Double)] = ...\n\nval pic = new PowerIteartionClustering()\n  .setK(3)\n  .setMaxIterations(20)\nval model = pic.run(similarities)\n\nmodel.assignments.collect().foreach { a =&gt;\n  println(s&quot;${a.id} -&gt; ${a.cluster}&quot;)\n}\n[/scala]\n\nA more concrete example can be found at <a href=\"https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/mllib/PowerIterationClusteringExample.scala\">PowerIterationClusteringExample</a>, and the following is a clustering assignment produced by it with five circles:\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-17-at-8.25.54-AM.png\"><img class=\"aligncenter wp-image-3533\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-17-at-8.25.54-AM.png\" alt=\"Huawei guest blog figure 3\" width=\"500\" height=\"356\" /></a>\n\nWhat we notice is that PIC is able to distinguish clearly the degree of similarity \u2013 as represented by the Euclidean distance among the points \u2013 even though their relationship is non-linear. For more information about PIC in MLlib, please visit its <a href=\"https://spark.apache.org/docs/latest/mllib-clustering.html#power-iteration-clustering-pic\">user guide</a>.\n<h2>Summary</h2>\nBoth FP-growth and PIC are included in Apache Spark 1.3. So you can <a href=\"http://spark.apache.org/downloads.html\">download it</a> now and try them out. At Huawei, our team is working on improving MLlib\u2019s FP-growth implementation further and exploring possible enhancements to PIC. In addition, we plan to work on MLlib\u2019s pipeline API, such as model persistence and re-deployment of the models, and share use cases of MLlib algorithms from our customers.\n<h2>Acknowledgement</h2>\nXiangrui Meng at Databricks provided tremendous help to us, including design discussions and code reviews. We also want to thank all community members who helped code reviews and expanded the work, e.g., adding Python support and model import/export."}
{"status": "publish", "description": "Databricks Cloud provides a powerful platform to process, analyze, and visualize big and small data in one place. In this blog, we will illustrate how to analyze access logs of an Apache HTTP web server using Notebooks.", "creator": "ion", "link": "https://databricks.com/blog/2015/04/21/analyzing-apache-access-logs-with-databricks-cloud.html", "authors": null, "id": 3558, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2015-04-21", "tz": "UTC", "createdOn": "2015-04-21"}, "title": "Analyzing Apache Access Logs with Databricks", "slug": "analyzing-apache-access-logs-with-databricks-cloud", "content": "Databricks provides a powerful platform to process, analyze, and visualize big and small data in one place. In this blog, we will illustrate how to analyze access logs of an Apache HTTP web server using Notebooks.\u00a0Notebooks allow users to write and run arbitrary Apache Spark code and interactively visualize the results. Currently, notebooks support three languages: Scala, Python, and SQL. In this blog, we will be using Python for illustration.\n\nThe analysis presented in this blog and much more is available in Databricks as part of the Databricks Guide. Find this notebook in your Databricks workspace at \u201c<code>databricks_guide/Sample Applications/Log Analysis/Log Analysis in Python</code>\u201d\u00a0- it will also show you how to create a data frame of access logs with Python using the new Spark SQL 1.3 API. \u00a0Additionally, there are also Scala &amp; SQL notebooks in the same folder with similar analysis available.\n<h2>Getting Started</h2>\nFirst we need to locate the log file. In this example, we are using synthetically generated logs which are stored in the <code>\u201c/dbguide/sample_log\u201d</code> file. The command below (typed in the notebook) assigns the log file pathname to the <code>DBFS_SAMPLE_LOGS_FOLDER</code> variable, which will be used throughout the rest of this analysis.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-2.21.13-PM.png\"><img class=\"aligncenter wp-image-3559\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-2.21.13-PM-1024x84.png\" alt=\"Apache Log how-to figure 1\" width=\"700\" height=\"58\" /></a>\n<p style=\"text-align: center;\"><em>Figure 1: Location of the synthetically generated logs in your instance of Databricks\u00a0Cloud</em></p>\n\n<h2>Parsing the Log File</h2>\nEach line in the log file corresponds to an Apache web server access request. To parse the log file, we define <code>parse_apache_log_line()</code>, a function that takes a log line as an argument and returns the main fields of the log line. The return type of this function is a PySpark SQL Row object which models the web log access request. \u00a0For this we use the \u201cre\u201d module which implements regular expression operations. The <code>APACHE_ACCESS_LOG_PATTERN</code> variable contains the regular expression used to match an access log line. In particular, <code>APACHE_ACCESS_LOG_PATTERN</code> matches client IP address (<code>ipAddress</code>) and identity (<code>clientIdentd</code>), user name as defined by HTTP authentication (<code>userId</code>), time when the server has finished processing the request (<code>dateTime</code>), the HTTP command issued by the client, e.g., GET (<code>method</code>), protocol, e.g., HTTP/1.0 (<code>protocol</code>), response code (<code>responseCode</code>), and the size of the response in bytes (<code>contentSize</code>).\n<p style=\"text-align: left;\"><a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-2.26.06-PM.png\"><img class=\"aligncenter wp-image-3562\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-2.26.06-PM-1024x528.png\" alt=\"Apache log how-to blog figure 2\" width=\"700\" height=\"361\" /></a></p>\n<p style=\"text-align: center;\"><em>Figure 2: Example function to parse the log file in a Databricks Cloud notebook</em></p>\n\n<h2>Loading the Log File</h2>\nNow we are ready to load the logs into a <a href=\"https://spark.apache.org/docs/latest/quick-start.html#basics\">Resilient Distributed Dataset (RDD)</a>. An RDD is a partitioned collection of tuples (rows), and is the primary data structure in Spark. Once the data is stored in an RDD, we can easily analyze and process it in parallel. To do so, we launch a Spark job that reads and parses each line in the log file using the <code>parse_apache_log_line()</code> function defined earlier, and then creates an RDD, called access_logs. Each tuple in access_logs contains the fields of a corresponding line (request) in the log file, <code>DBFS_SAMPLE_LOGS_FOLDER</code>. Note that once we create the <code>access_logs</code> RDD, we cache it into memory, by invoking the <code>cache()</code> method. This will dramatically speed up subsequent operations we will perform on <code>access_logs</code>.\n<p style=\"text-align: left;\"><a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-2.29.26-PM.png\"><img class=\"aligncenter wp-image-3564\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-2.29.26-PM-1024x229.png\" alt=\"Apache log how-to figure 3\" width=\"700\" height=\"157\" /></a></p>\n<p style=\"text-align: center;\"><em>Figure 3: Example code to load the log file in Databricks Cloud notebook</em></p>\nAt the end of the above code snippet, notice that we count the number of tuples in <code>access_logs</code> (which returns 100,000 as a result).\n<h2>Log Analysis</h2>\nNow we are ready to analyze the logs stored in the <code>access_logs</code> RDD. Below we give two simple examples:\n<ol>\n \t<li>Computing the average content size</li>\n \t<li>Computing and plotting the frequency\u00a0of each response code</li>\n</ol>\n<h3 style=\"text-align: left;\">1. Average Content Size</h3>\n<p style=\"text-align: left;\">We compute the average content size in two steps. First, we create another RDD, <code>content_sizes</code>, that contains only the \u201c<code>contentSize</code>\u201d field from <code>access_logs</code>, and cache this RDD:</p>\n<p style=\"text-align: left;\"><a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-2.34.14-PM.png\"><img class=\"aligncenter wp-image-3567\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-2.34.14-PM-1024x133.png\" alt=\"Apache log how-to figure 4\" width=\"700\" height=\"91\" /></a></p>\n<p style=\"text-align: center;\"><em>Figure 4:\u00a0Create the content size RDD in Databricks notebook</em></p>\nSecond, we use the <code>reduce()</code> operator to compute the sum of all content sizes and then divide it into the total number of tuples to obtain the average:\n<p style=\"text-align: left;\"><a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-2.34.21-PM.png\"><img class=\"aligncenter wp-image-3568\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-2.34.21-PM-1024x87.png\" alt=\"Apache log how-to figure 5\" width=\"700\" height=\"59\" /></a></p>\n<p style=\"text-align: center;\"><em>Figure 5: Computing the average content size with the <code>reduce()</code> operator</em></p>\nThe result is 249 bytes. Similarly we can easily compute the min and max, as well as other statistics of the content size distribution.\n<p style=\"text-align: left;\">An important point to note is that both commands above run in parallel. Each RDD is partitioned across a set of workers, and each operation invoked on an RDD is shipped and executed in parallel at each worker on the corresponding RDD partition. For example the lambda function passed as the argument of <code>reduce()</code> will be executed in parallel at workers on each partition of the <code>content_sizes</code> RDD. This will result in computing the partial sums for each partition. Next, these partial sums are aggregated at the driver to obtain the total sum. The ability to cache RDDs and process them in parallel are the two of the main features of Spark that allows us to perform large scale, sophisticated analysis.</p>\n\n<h3 style=\"text-align: left;\">2. Computing and Plotting the Frequency of Each Response Code</h3>\n<p style=\"text-align: left;\">We compute these counts using a map-reduce pattern. In particular, the code snippet returns an RDD (<code>response_code_to_count_pair_rdd</code>) of tuples, where each tuple associates a response code with its count.</p>\n<p style=\"text-align: left;\"><a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-8.07.31-PM.png\"><img class=\"aligncenter wp-image-3570\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-8.07.31-PM-1024x186.png\" alt=\"Apache how-to blog figure 6\" width=\"700\" height=\"127\" /></a></p>\n<p style=\"text-align: center;\"><em>Figure 6: Counting the response codes using a map-reduce pattern</em></p>\nNext, we take the first 100 tuples from <code>response_code_to_count_pair_rdd</code> to filter out possible bad data, and store the result in another RDD, <code>response_code_to_count_array</code>.\n<p style=\"text-align: left;\"><a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-8.11.20-PM.png\"><img class=\"aligncenter wp-image-3571\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-8.11.20-PM-1024x163.png\" alt=\"Apache log how-to figure 7\" width=\"700\" height=\"112\" /></a></p>\n<p style=\"text-align: center;\"><em>Figure 7: Filter out possible bad data with take()</em></p>\nTo plot data we convert the <code>response_code_to_count_array</code> RDD into a DataFrame. A DataFrame\u00a0is basically a table, and it is very similar to the DataFrame abstraction in the popular <a href=\"http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.html\">Python\u2019s pandas package</a>. The resulting DataFrame\u00a0(<code>response_code_to_count_data_frame</code>) has two columns \u201cresponse code\u201d and \u201ccount\u201d.\n<p style=\"text-align: left;\"><a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-8.13.41-PM.png\"><img class=\"aligncenter wp-image-3572\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-8.13.41-PM-1024x133.png\" alt=\"Apache log how-to figure 8\" width=\"700\" height=\"91\" /></a></p>\n<p style=\"text-align: center;\"><em>Figure 8: Converting RDD to DataFrame for easy data manipulation and visualization</em></p>\nNow we can plot the count of response codes by simply invoking<code> display()</code> on our data frame.\n<p style=\"text-align: left;\"><a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-8.15.56-PM.png\"><img class=\"aligncenter wp-image-3573\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-8.15.56-PM-1024x484.png\" alt=\"Apache log how-to figure 9\" width=\"700\" height=\"331\" /></a></p>\n<p style=\"text-align: center;\"><em>Figure 9: Visualizing response codes with display()</em></p>\nIf you want to change the plot size you can do so interactively by just clicking on the down arrow below the plot, and select another plot type. To illustrate this capability, below we show the same data using a pie-chart.\n<p style=\"text-align: left;\"><em><a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-8.17.25-PM.png\"><img class=\"aligncenter wp-image-3574\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-8.17.25-PM-1024x505.png\" alt=\"Apache log how-to figure 10.1\" width=\"700\" height=\"345\" /></a></em></p>\n<p style=\"text-align: center;\"><em>Figure 10: Changing the visualization\u00a0of response codes to a pie chart</em></p>\n\n<h2>Additional\u00a0Resources</h2>\n<p style=\"text-align: left;\">If you\u2019d like to analyze your Apache access logs with Databricks Cloud, you can<a href=\"https://databricks.com/registration\"> register</a> here for an account.\u00a0You can also find the source code on Github <a href=\"https://github.com/databricks/reference-apps/tree/master/logs_analyzer/chapter1/python/databricks/apps/logs\" target=\"_blank\">here</a>.</p>\n<p style=\"text-align: left;\">Other Databricks Cloud how-tos can be found at:</p>\n\n<ul>\n \t<li style=\"text-align: left;\"><a title=\"The Easiest Way to Run Spark Jobs\" href=\"https://databricks.com/blog/2015/04/16/the-easiest-way-to-run-spark-jobs.html\" target=\"_blank\">Easiest way to run Spark jobs</a></li>\n</ul>"}
{"status": "publish", "description": "Due to the enormous volume of data we needed a scalable solution to perform exploratory, interactive graph data analysis. We leveraged the power of Apache Spark to develop the LynxKite graph analytics platform.", "creator": "dave_wang", "link": "https://databricks.com/blog/2015/04/23/big-graph-analytics-with-lynxkite-spark.html", "authors": null, "id": 3583, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2015-04-23", "tz": "UTC", "createdOn": "2015-04-23"}, "title": "Big Graph Analytics with LynxKite & Apache Spark", "slug": "big-graph-analytics-with-lynxkite-spark", "content": "This is a guest blog from our one of our partners:\u00a0<a href=\"http://www.lynxanalytics.com/\" target=\"_blank\">Lynx Analytics</a>\n\n<hr />\n\n<h2>\u00a0About Lynx Analytics</h2>\nLynx Analytics is a data analytics consultancy firm with a focus on graph analytics and proprietary big graph analytics software development. We augment classical data mining methods with our expertise in graph analytics, and apply these methods against large datasets such as call data records, bank transactions, and cell tower usage.\n\nApplying the graph analytics can reveal unexpected patterns about human behavior, emergent properties of customer interactions, untapped market opportunities, to name just a few examples. Graph analysis is also often the only way to establish relationships between diverse datasets, leading to complex insights that were unattainable by analyzing standalone datasets.\n\nOur clients are large multinational telecommunications and financial corporations. Due to the enormous volume of data we needed a scalable solution to perform exploratory, interactive graph data analysis. The existing solutions did not fulfill the needs of our analysts and clients, so we leveraged the power of Apache Spark to develop the LynxKite graph analytics platform.\n<h2>Why choose Spark?</h2>\nThe LynxKite graph analytics platform is a web application with a rich, clean UI for exploring and manipulating graphs. One critical requirement is to allow the users to work with extremely large dataset sizes interactively in real-time. After evaluating several distributed computation frameworks we found Apache Spark to best fulfill our needs for low latency, ease of use, and production readiness.\n\nLeveraging the power of Spark, LynxKite takes just a few clicks to bucket the customers by age and gender, and visualize the number of calls within and between the buckets. Within a minute, the overlapping communities can be identified in the graph, and for each customer we can find the average age of the most homogeneous community they belong to.\n<h2>The Architecture &amp; Benefits to Users</h2>\nThe LynxKite frontend is an AngularJS web application running in the browser. It is served by a Play Framework web server, which also receives the AJAX requests from the frontend. The web server process is also the Apache Spark driver application and is connected to our Apache Spark cluster. When the frontend requests new data, such as an aggregate view of the graph, the computations run on the Apache Spark cluster.\n<h2>The highlights of this technical solution are:</h2>\n<ul>\n \t<li><b>Latency is low. </b>Many computations complete in less than a second, which is a dream come true for our users. The more computationally intensive operations can be sped up by increasing the cluster size. When the cluster is hosted by a cloud provider, its size can be easily adjusted to fit the needs of the moment.</li>\n \t<li><b>Simple is easy. </b>The clean and flexible Apache Spark Scala API allows us to implement graph analytics methods in a very simple and natural way. To quantify this, we compared our solution to open \u00adsource implementations (on other frameworks) of common graph algorithms, to see a ten\u00adfold advantage of the Spark solution with respect to code complexity.</li>\n \t<li><b>Complex is possible. </b>The ease of developing with the Apache Spark Scala API has enabled us to create a great variety of more complex analytic methods. Viral modeling can estimate unobserved properties based on a small proportion of nodes with observed properties, using the link structure between them. Time and space can be visualized to explore the diffusion of product usage or understand geographic data on a map.</li>\n \t<li><b>Deployments are smooth.</b> We are deploying LynxKite into the private Hadoop clusters of a number of clients. This is a surprisingly straightforward process thanks to the integration of Apache Spark into the Hadoop ecosystem.</li>\n</ul>\n<h2>What\u2019s Next</h2>\nBetting big on a pioneering technology such as Apache Spark, we really relied on its developer community. We found the Spark community to be extremely smart, professional, and responsive to our questions, tickets, and pull requests. We are very thankful and hope to contribute more.\n\nWe look forward to our presence\u00a0 at<a href=\"http://spark-summit.org/2015\" target=\"_blank\"> Spark Summit West 2015</a> (San Francisco, June 15\u201317) where we will talk about the technical challenges of running Apache Spark in an interactive setting.\n\nFind out more at <a href=\"http://www.lynxanalytics.com/\" target=\"_blank\">www.lynxanalytics.com</a>\n\n&nbsp;"}
{"status": "publish", "description": "2014 Spark made significant major improvements across the entire engine. In this post, we look back and cover recent performance efforts in Spark.", "creator": "rxin", "link": "https://databricks.com/blog/2015/04/24/recent-performance-improvements-in-apache-spark-sql-python-dataframes-and-more.html", "authors": null, "id": 3610, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2015-04-24", "tz": "UTC", "createdOn": "2015-04-24"}, "title": "Recent performance improvements in Apache Spark: SQL, Python, DataFrames, and More", "slug": "recent-performance-improvements-in-apache-spark-sql-python-dataframes-and-more", "content": "In this post, we look back and cover recent performance efforts in Apache Spark. In a follow-up blog post next week, we will look forward and share with you our thoughts on the future evolution of Spark's performance.\n\n2014 was the most active year of Spark development to date, with major improvements across the entire engine. One particular area where it made great strides was performance: Spark <a href=\"https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html\">set a new world record in 100TB sorting</a>, beating the previous record held by Hadoop MapReduce by three times, using only one-tenth of the resources; it received a new <a href=\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\">SQL query engine</a> with a state-of-the-art optimizer; and many of its built-in algorithms became <a href=\"https://databricks.com/blog/2014/09/22/spark-1-1-mllib-performance-improvements.html\">five times faster</a>.\n\nBack in 2010, we at the AMPLab at UC Berkeley designed Spark for interactive queries and iterative algorithms, as these were two major use cases not well served by batch frameworks like MapReduce. As a result, early users were drawn to Spark because of the significant performance improvements in these workloads. However, performance optimization is a never-ending process, and as Spark\u2019s use cases have grown, so have the areas looked at for further improvement. User feedback and detailed measurements helped the Apache Spark developer community to prioritize areas to work in. Starting with the core engine, I will cover some of the recent optimizations that have been made.\n\n[caption id=\"attachment_2822\" align=\"aligncenter\" width=\"600\"]<a href=\"https://databricks.com/wp-content/uploads/2014/11/Spark_Ecosystem_Chart11.jpg\"><img class=\"wp-image-2822\" src=\"https://databricks.com/wp-content/uploads/2014/11/Spark_Ecosystem_Chart11-1024x414.jpg\" alt=\"The Spark ecosystem\" width=\"600\" height=\"243\" /></a> The Spark ecosystem[/caption]\n<h2>Core engine</h2>\nOne unique thing about Spark is its user-facing APIs (SQL, streaming, machine learning, etc.) run over a common core execution engine. Whenever possible, specific workloads are sped up by making optimizations in the core engine. As a result, these optimizations speed up <em>all</em> components. We\u2019ve often seen very surprising results this way: for example, when core developers decreased latency to introduce Spark Streaming, we also saw SQL queries become two times faster.\n\nIn the core engine, the major improvements in 2014 were in communication. First, <em>shuffle</em> is the operation that moves data point-to-point across machines. It underpins almost all workloads. For example, a SQL query joining two data sources uses shuffle to move tuples that should be joined together onto the same machine, and product recommendation algorithms such as ALS use shuffle to send user/product weights across the network.\n\nThe last two releases of Spark featured a new sort-based shuffle layer and a new network layer based on <a href=\"http://en.wikipedia.org/wiki/Netty_%28software%29\">Netty</a> with zero-copy and explicit memory management. These two make Spark more robust in very large-scale workloads. In our own experiments at Databricks, we have used this to run petabyte shuffles on 250,000 tasks. These two changes were also the key to Spark <a href=\"https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html\">setting the current world record in large-scale sorting</a>, beating the previous Hadoop-based record by 30 times in per-node performance.\n\nIn addition to shuffle, core developers rewrote Spark\u2019s <em>broadcast</em> primitive to use a BitTorrent-like protocol to reduce network traffic. This speeds up workloads that need to send a large parameter to multiple machines, including SQL queries and many machine learning algorithms. We have seen more than <a href=\"https://github.com/apache/spark/pull/3417\">five times performance improvements</a> for these workloads.\n<h2>Python API (PySpark)</h2>\nPython is perhaps the most popular programming language used by data scientists. The Spark community views Python as a first-class citizen of the Spark ecosystem. When it comes to performance, Python programs historically lag behind their JVM counterparts due to the more dynamic nature of the language.\n\nSpark\u2019s core developers have worked extensively to bridge the performance gap between JVM languages and Python. In particular, PySpark can now run on <em>PyPy</em> to leverage the just-in-time compiler, in some cases <a href=\"https://github.com/apache/spark/pull/2144\">improving performance by a factor of 50</a>. The way Python processes communicate with the main Spark JVM programs have also been redesigned to enable <em>worker reuse</em>. In addition, broadcasts are handled via a more optimized serialization framework, enabling PySpark to broadcast data larger than 2GB. The latter two have made general Python program performance two to 10 times faster.\n<h2>SQL</h2>\nOne year ago, Shark, an earlier SQL on Spark engine based on Hive, was deprecated and we at Databricks built a new query engine based on a new query optimizer, <a href=\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\">Catalyst</a>, designed to run natively on Spark. It was a controversial decision, within the Apache Spark developer community as well as internally within Databricks, because building a brand new query engine necessitates astronomical engineering investments. One year later, more than 115 open source contributors have joined the project, making it one of the most active open source query engines.\n\n[caption id=\"attachment_3624\" align=\"aligncenter\" width=\"570\"]<img class=\"size-full wp-image-3624\" src=\"https://databricks.com/wp-content/uploads/2015/04/sparksql-tpcds-perf.jpg\" alt=\"Shark vs. Spark SQL\" width=\"570\" height=\"197\" /> Shark vs. Spark SQL[/caption]\n\nDespite being less than a year old, Spark SQL is outperforming Shark on almost all benchmarked queries. In TPC-DS, a decision-support benchmark, Spark SQL is outperforming Shark often by an order of magnitude, due to <a href=\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\">better optimizations and code generation</a>.\n<h2>Machine learning (MLlib) and Graph Computation (GraphX)</h2>\nFrom early on, Spark was packaged with powerful standard libraries that can be optimized along with the core engine. This has allowed for a number of rich optimizations to these libraries. For instance, Spark 1.1 featured a new communication pattern for aggregating machine learning models using <a href=\"https://databricks.com/blog/2014/09/22/spark-1-1-mllib-performance-improvements.html\">multi-level aggregation trees</a>. This has reduced the model aggregation time by an order of magnitude. This new communication pattern, coupled with the more efficient broadcast implementation in core, results in speeds 1.5 to five times faster across all algorithms.\n\n<img class=\"aligncenter size-full wp-image-3623\" src=\"https://databricks.com/wp-content/uploads/2015/04/mllib-perf.jpg\" alt=\"mllib-perf\" width=\"570\" height=\"258\" />\n\nIn addition to optimizations in communication, <em>Alternative Least Squares (ALS)</em>, a common collaborative filtering algorithm, was also re-implemented 1.3, which provided another factor of two speedup for ALS over what the above chart shows. In addition, all the built-in algorithms in GraphX have also seen 20% to 50% runtime performance improvements, due to a new optimized API.\n<h2>DataFrames: Leveling the Field for Python and JVM</h2>\nIn Spark 1.3, we introduced a <a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\">new <em>DataFrame</em> API</a>. This new API makes Spark programs more concise and easier to understand, and at the same time exposes more application semantics to the engine. As a result, Spark can use Catalyst to optimize these programs.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/02/Screen-Shot-2015-02-16-at-9.46.39-AM.png\"><img class=\"aligncenter wp-image-2767\" src=\"https://databricks.com/wp-content/uploads/2015/02/Screen-Shot-2015-02-16-at-9.46.39-AM-1024x457.png\" alt=\"DataFrame performance\" width=\"600\" height=\"268\" /></a>\n\nThrough the new DataFrame API, Python programs can achieve the same level of performance as JVM programs because the Catalyst optimizer compiles DataFrame operations into JVM bytecode. Indeed, performance sometimes beats hand-written Scala code.\n\nThe Catalyst optimizer will also become smarter over time, picking better logical optimizations and physical execution optimizations. For example, in the future, Spark will be able to leverage schema information to create a custom physical layout of data, improving cache locality and reducing garbage collection. This will benefit both Spark SQL and DataFrame programs. As more libraries are converting to use this new DataFrame API, they will also automatically benefit from these optimizations.\n\nThe goal of Spark is to offer a single platform where users can get the best distributed algorithms for any data processing task. We will continue to push the boundaries of performance, making Spark faster and more powerful for more users.\n\nNote: An earlier version of this blog post appeared on <a href=\"http://radar.oreilly.com/2015/02/recent-performance-improvements-in-apache-spark.html\">O'Reilly Radar</a>."}
{"status": "publish", "description": "Project Tungsten focuses on improving the performance of Spark applications, pushing it closer to the limits of hardware.", "creator": "rxin", "link": "https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html", "authors": null, "id": 3638, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2015-04-28", "tz": "UTC", "createdOn": "2015-04-28"}, "title": "Project Tungsten: Bringing Apache Spark Closer to Bare Metal", "slug": "project-tungsten-bringing-spark-closer-to-bare-metal", "content": "In a previous <a href=\"https://databricks.com/blog/2015/04/24/recent-performance-improvements-in-apache-spark-sql-python-dataframes-and-more.html\">blog post</a>, we looked back and surveyed performance improvements made to Apache Spark in the past year. In this post, we look forward and share with you the next chapter, which we are calling <i>Project Tungsten. </i>2014 witnessed Spark setting the world record in large-scale sorting and saw major improvements across the entire engine from Python to SQL to machine learning. Performance optimization, however, is a never ending process.\n\nProject Tungsten will be the largest change to Spark\u2019s execution engine since the project\u2019s inception. It focuses on substantially improving the efficiency of <i>memory and CPU</i> for Spark applications, to push performance closer to the limits of modern hardware. This effort includes three initiatives:\n<ol>\n \t<li><i>Memory Management and Binary Processing:</i> leveraging application semantics to manage memory explicitly and eliminate the overhead of JVM object model and garbage collection</li>\n \t<li><i>Cache-aware computation</i>: algorithms and data structures to exploit memory hierarchy</li>\n \t<li><i>Code generation</i>: using code generation to exploit modern compilers and CPUs</li>\n</ol>\nThe focus on CPU efficiency is motivated by the fact that Spark workloads are increasingly bottlenecked by CPU and memory use rather than IO and network communication. This trend is shown by recent research on the performance of big data workloads (<a href=\"https://kayousterhout.github.io/trace-analysis/\">Ousterhout et al</a>) and we\u2019ve arrived at similar findings as part of our ongoing tuning and optimization efforts for <a href=\"https://databricks.com/product/databricks-cloud\">Databricks Cloud</a> customers.\n\nWhy is CPU the new bottleneck? There are many reasons for this. One is that hardware configurations offer increasingly large aggregate IO bandwidth, such as 10Gbps links in networks and high bandwidth SSD\u2019s or striped HDD arrays for storage. From a software perspective, Spark\u2019s optimizer now allows many workloads to avoid significant disk IO by pruning input data that is not needed in a given job. In Spark\u2019s shuffle subsystem, serialization and hashing (which are CPU bound) have been shown to be key bottlenecks, rather than raw network throughput of underlying hardware. All these trends mean that Spark today is often constrained by CPU efficiency and memory pressure rather than IO.\n<h2>1. Memory Management and Binary Processing</h2>\nApplications on the JVM typically rely on the JVM\u2019s garbage collector to manage memory. The JVM is an impressive engineering feat, designed as a general runtime for many workloads. However, as Spark applications push the boundary of performance, the overhead of JVM objects and GC becomes non-negligible.\n\nJava objects have a large inherent memory overhead. Consider a simple string \u201cabcd\u201d that would take 4 bytes to store using UTF-8 encoding. JVM\u2019s native String implementation, however, stores this differently to facilitate more common workloads. It encodes each character using 2 bytes with UTF-16 encoding, and each String object also contains a 12 byte header and 8 byte hash code, as illustrated by the following output from the the <a href=\"http://openjdk.java.net/projects/code-tools/jol/\">Java Object Layout</a> tool.\n<pre>java.lang.String object internals:\nOFFSET \u00a0SIZE \u00a0\u00a0TYPE DESCRIPTION \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0VALUE\n\u00a0\u00a0\u00a0\u00a0\u00a00 \u00a0\u00a0\u00a0\u00a04 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0(object header) \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0...\n\u00a0\u00a0\u00a0\u00a0\u00a04 \u00a0\u00a0\u00a0\u00a04 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0(object header) \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0...\n\u00a0\u00a0\u00a0\u00a0\u00a08 \u00a0\u00a0\u00a0\u00a04 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0(object header) \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0...\n\u00a0\u00a0\u00a0\u00a012 \u00a0\u00a0\u00a0\u00a04 char[] String.value \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0[]\n\u00a0\u00a0\u00a0\u00a016 \u00a0\u00a0\u00a0\u00a04 \u00a0\u00a0\u00a0int String.hash \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a00\n\u00a0\u00a0\u00a0\u00a020 \u00a0\u00a0\u00a0\u00a04 \u00a0\u00a0\u00a0int String.hash32 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a00\nInstance size: 24 bytes (reported by Instrumentation API)</pre>\nA simple 4 byte string becomes over 48 bytes in total in the JVM object model!\n\nThe other problem with the JVM object model is the overhead of garbage collection. At a high level, generational garbage collection divides objects into two categories: ones that have a high rate of allocation/deallocation (the young generation) ones that are kept around (the old generation). Garbage collectors exploit the transient nature of young generation objects to manage them efficiently. This works well when GC can reliably estimate the life cycle of objects, but falls short if the estimation is off (i.e. some transient objects spill into the old generation). Since this approach is ultimately based on heuristics and estimation, eeking out performance can require the \u201cblack magic\u201d of GC tuning, with <a href=\"http://www.oracle.com/technetwork/java/javase/gc-tuning-6-140523.html\">dozens of parameters</a> to give the JVM more information about the life cycle of objects.\n\nSpark, however, is not just a general-purpose application. Spark understands how data flows through various stages of computation and the scope of jobs and tasks. As a result, Spark knows much more information than the JVM garbage collector about the life cycle of memory blocks, and thus should be able to manage memory more efficiently than the JVM.\n\nTo tackle both object overhead and GC\u2019s inefficiency, we are introducing an explicit memory manager to convert most Spark operations to operate directly against binary data rather than Java objects. This builds on <code>sun.misc.Unsafe</code>, an advanced functionality provided by the JVM that exposes C-style memory access (e.g. explicit allocation, deallocation, pointer arithmetics). Furthermore, Unsafe methods are <i>intrinsic</i>, meaning each method call is compiled by JIT into a single machine instruction.\n\nIn certain areas, Spark has already started using explicitly managed memory. Last year, Databricks contributed a new Netty-based network transport that explicitly manages all network buffers using a jemalloc like memory manager. That was critical in scaling up Spark\u2019s shuffle operation and winning the Sort Benchmark.\n\nThe first pieces of this will appear in Spark 1.4, which includes a hash table that operates directly against binary data with memory explicitly managed by Spark. Compared with the standard Java <code>HashMap</code>, this new implementation much less indirection overhead and is invisible to the garbage collector.\n\n<img class=\" wp-image-3639 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-27-at-6.08.39-PM-1024x684.png\" alt=\"Screen Shot 2015-04-27 at 6.08.39 PM\" width=\"800\" height=\"534\" />\n\nThis is still work-in-progress, but initial performance results are encouraging. As shown above, we compare the throughput of aggregation operations using different hash map: one with our new hash map\u2019s heap mode, one with offheap, and one with java.util.HashMap. The new hash table supports over 1 million aggregation operations per second in a single thread, about 2X the throughput of java.util.HashMap. More importantly, without tuning any parameters, it has almost no performance degradation as memory utilization increases, while the JVM default one eventually thrashes due to GC.\n\nIn Spark 1.4, this hash map will be used for aggregations for DataFrames and SQL, and in 1.5 we will have data structures ready for most other operations, such as sorting and joins. This will in many cases eliminating the need to tune GC to achieve high performance.\n<h2>2. Cache-aware Computation</h2>\nBefore we explain cache-aware computation, let\u2019s revisit \u201cin-memory\u201d computation. Spark is widely known as an in-memory computation engine. What that term really means is that Spark can leverage the memory resources on a cluster efficiently, processing data at a rate much higher than disk-based solutions. However, Spark can also process data orders magnitude larger than the available memory, transparently spill to disk and perform external operations such as sorting and hashing.\n\nSimilarly, cache-aware computation improves the speed of data processing through more effective use of L1/ L2/L3 CPU caches, as they are orders of magnitude faster than main memory. When profiling Spark user applications, we\u2019ve found that a large fraction of the CPU time is spent waiting for data to be fetched from main memory. As part of Project Tungsten, we are designing cache-friendly algorithms and data structures so Spark applications will spend less time waiting to fetch data from memory and more time doing useful work.\n\nConsider sorting of records as an example. A standard sorting procedure would store an array of pointers to records and use quicksort to swap pointers until all records are sorted. Sorting in general has good cache hit rate due to the sequential scan access pattern. Sorting a list of pointers, however, has a poor cache hit rate because each comparison operation requires dereferencing two pointers that point to randomly located records in memory.\n\n<img class=\"aligncenter wp-image-3640\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-27-at-6.12.51-PM.png\" alt=\"\" width=\"400\" height=\"249\" />\n\nSo how do we improve the cache locality of sorting? A very simple approach is to store the sort key of each record side by side with the pointer. For example, if the sort key is a 64-bit integer, then we use 128-bit (64-bit pointer and 64-bit key) to store each record in the pointers array. This way, each quicksort comparison operation only looks up the pointer-key pairs in a linear fashion and requires no random memory lookup. Hopefully the above illustration gives you some idea about how we can redesign basic operations to achieve higher cache locality.\n\nHow does this apply to Spark? Most distributed data processing can be boiled down to a small list of operations such as aggregations, sorting, and join. By improving the efficiency of these operations, we can improve the efficiency of Spark applications as a whole. We have already built a version of sort that is cache-aware that is 3X faster than the previous version. This new sort will be used in sort-based shuffle, high cardinality aggregations, and sort-merge join operator. By the end of this year, most Spark\u2019s lowest level algorithms will be upgraded to be cache-aware, increasing the efficiency of all applications from machine learning to SQL.\n<h2>3. Code Generation</h2>\nAbout a year ago Spark introduced <a href=\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\">code generation for expression evaluation</a> in SQL and DataFrames. Expression evaluation is the process of computing the value of an expression (say \u201c<code>age &gt; 35 &amp;&amp; age &lt; 40</code>\u201d) on a particular record. At runtime, Spark dynamically generates bytecode for evaluating these expressions, rather than stepping through a slower interpreter for each row. Compared with interpretation, code generation reduces the boxing of primitive data types and, more importantly, avoids expensive <a href=\"http://shipilev.net/blog/2015/black-magic-method-dispatch/\">polymorphic function dispatches</a>.\n\nIn an <a href=\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\">earlier blog post</a>, we demonstrated that code generation could speed up many TPC-DS queries by almost an order of magnitude. We are now broadening the code generation coverage to most built-in expressions. In addition, we plan to increase the level of code generation from <i>record-at-a-time</i> expression evaluation to <i>vectorized</i> expression evaluation, leveraging JIT\u2019s capabilities to exploit better instruction pipelining in modern CPUs so we can process multiple records at once.\n\nWe\u2019re also applying code generation in areas beyond expression evaluations to optimize the CPU efficiency of internal components. One area that we are very excited about applying code generation is to speed up the conversion of data from in-memory binary format to wire-protocol for shuffle. As mentioned earlier, shuffle is often bottlenecked by data serialization rather than the underlying network. With code generation, we can increase the throughput of serialization, and in turn increase shuffle network throughput.\n\n<img class=\" wp-image-3641 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-27-at-6.15.12-PM-1024x383.png\" alt=\"\" width=\"400\" />\n\nThe above chart compares the performance of shuffling 8 million complex rows in one thread using the Kryo serializer and a code generated custom serializer. The code generated serializer exploits the fact that all rows in a single shuffle have the same schema and generates specialized code for that. This made the generated version over 2X faster to shuffle than the Kryo version.\n<h2>Tungsten and Beyond</h2>\nProject Tungsten is a broad initiative that will influence the design of Spark\u2019s core engine over the next several releases. The first pieces will land in Spark 1.4, which includes explicitly managed memory for <a href=\"https://github.com/apache/spark/pull/5725\">aggregation operations</a> in Spark\u2019s DataFrame API as well as <a href=\"https://github.com/apache/spark/pull/5497\">customized serializers</a>. Expanded coverage of binary memory management and cache-aware data structures will appear in Spark 1.5. Several parts of project Tungsten leverage the DataFrame model. We will also retrofit the improvements onto Spark\u2019s RDD API whenever possible.\n\nThere are also a handful of longer term possibilities for Tungsten. In particular, we plan to investigate compilation to LLVM or OpenCL, so Spark applications can leverage SSE/SIMD instructions out of modern CPUs and the wide parallelism in GPUs to speed up operations in machine learning and graph computation.\n\nThe goal of Spark has always been to offer a single platform where users can get the best distributed algorithms for any data processing task. Performance is a key part of that goal, and Project Tungsten aims to let Spark applications run at the speed offered by bare metal. Stay tuned for the Databricks blog for longer term articles on the components of Project Tungsten as they ship. We\u2019ll also be reporting details about the project at the upcoming <a href=\"http://spark-summit.org/2015\">Spark Summit in San Francisco in June</a>.\n\n* Note: hand-drawing diagrams inspired by our friends at Confluent (Martin Kleppmann)"}
{"status": "publish", "description": "We\u2019re proud to announce that the new Spark Summit website is live! This includes the full list of community talks along with the first set of keynotes! With over 260 submissions this year, the Program Committee had its work cut out narrowing the list to 54 talks.", "creator": "scott", "link": "https://databricks.com/blog/2015/05/11/spark-summit-2015-in-san-francisco-is-just-around-the-corner.html", "authors": null, "id": 3754, "categories": ["Announcements", "Company Blog", "Events"], "dates": {"publishedOn": "2015-05-11", "tz": "UTC", "createdOn": "2015-05-11"}, "title": "Spark Summit 2015 in San Francisco is just around the corner!", "slug": "spark-summit-2015-in-san-francisco-is-just-around-the-corner", "content": "<a href=\"https://spark-summit.org/\"><img class=\"alignnone wp-image-3755\" src=\"https://databricks.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-11-at-9.50.26-AM-1024x488.png\" alt=\"Spark Summit - new website\" width=\"800\" height=\"381\" /></a>\n\n&nbsp;\n\nWe\u2019re proud to announce that the new <a href=\"http://spark-summit.org\">Spark Summit website</a> is live! This includes the full list of <a href=\"https://spark-summit.org/2015/schedule/\">community talks</a> along with the first set of <a href=\"https://spark-summit.org/2015/speakers/\">keynotes</a>.\u00a0With over 260 submissions this year, the Program Committee had its work cut out narrowing the list to 54 talks. We would like to thank everyone who submitted and invite everyone to submit a presentation for a future Spark Summit (just in case you have not heard as yet, we are taking the Spark Summit to Amsterdam this fall.).\n\nThis year, join us at the <a href=\"https://spark-summit.org/2015/venue\">Hilton Union Square</a> where you will hear from thought leaders and practitioners in Data Science, the Spark community and the enterprise. Those looking to get hands-on with Spark are encouraged to sign up for one of our <a href=\"https://spark-summit.org/2015/training/\">workshops</a>.\n\nThe tracks for Spark Summit 2015 are organized by the following three themes:\n<ul>\n\t<li>Developer</li>\n\t<li>Use Cases</li>\n\t<li>Data Science</li>\n</ul>\nTraining tracks include:\n<ul>\n\t<li>Intro to Apache Spark</li>\n\t<li>Advanced: DevOps with Spark</li>\n\t<li>Data Science with Spark</li>\n</ul>\n<a href=\"http://prevalentdesignevents.com/sparksummit2015/registration.aspx\" target=\"_blank\">Register now</a> before the tickets sell out! Use promo code \"<strong>Databricks20</strong>\" to receive\u00a020% off your registration fee.\n<h3>Our stellar keynotes line-up</h3>\n<ul>\n\t<li>Ben Horowitz, Co-founder of Andreessen Horowitz</li>\n\t<li>Doug Wolfe, CIO of the CIA</li>\n\t<li>Ion Stoica, CEO of Databricks</li>\n\t<li>Tim O'Reilly, Founder of O'Reilly Media</li>\n\t<li>Mike Olson, Chief Strategy Officer of Cloudera</li>\n\t<li>Matt Wood, Data Scientist at Amazon Web Services</li>\n\t<li>Matei Zaharia, CTO of Databricks</li>\n\t<li>Beth Smith, General Manager, Analytics Platform at IBM</li>\n\t<li>Brian Kursar, Data Scientist at Toyota</li>\n\t<li>Anil Gadre, SVP of Product Management at MapR</li>\n\t<li>Gloria lau, Data Science at Timeful</li>\n\t<li>Reynold Xin, Co-founder of Databricks</li>\n</ul>\n<h3>Community Talk Highlights</h3>\n<ul>\n\t<li><a href=\"https://spark-summit.org/2015/events/sparkr-the-past-the-present-and-the-future/\">SparkR: The Past, the Present and the Future</a> - Shivaram Venkataraman (UC Berkeley AMPLAB), Rui Sun (Intel Asia Pacific R&amp;D)</li>\n\t<li><a href=\"https://spark-summit.org/2015/events/Solving-low-latency-query-over-big-data-with-Spark-SQL\">Solving Low Latency Query Over Big Data with Spark SQL</a> - Julien Pierre (Microsoft)</li>\n\t<li><a href=\"https://spark-summit.org/2015/events/tagging-and-processing-data-in-real-time-using-spark-streaming/\">Tagging and Processing Data in Real-Time Using Spark Streaming</a> - Hari Shreedharan (Cloudera Inc.), Siddhartha Jain (Salesforce Inc.)</li>\n\t<li><a href=\"https://spark-summit.org/2015/events/spark-and-spark-streaming-at-netflix\">Spark and Spark Streaming at Netflix</a> - Kedar Sadekar (Netflix), Monal Daxini (Netflix)</li>\n\t<li><a href=\"https://spark-summit.org/2015/events/hybrid-community-detection-for-web-scale-e-commerce-using-spark-streaming-and-graphx/\">Hybrid Community Detection for Web-scale E-commerce Using Spark Streaming and GraphX</a> - Ming Huang (Taobao)</li>\n\t<li><a href=\"https://spark-summit.org/2015/events/use-of-spark-mllib-for-predicting-the-offlining-of-digital-media/\">Use of Spark MLlib for Predicting the Offlining of Digital Media</a> - Christopher Burdorf (NBC Universal)</li>\n\t<li><a href=\"https://spark-summit.org/2015/events/appraiser-how-airbnb-generates-complex-models-in-spark-for-demand-prediction/\">Appraiser : How Airbnb Generates Complex Models in Spark for Demand Prediction</a> - Hector Yee (Airbnb)</li>\n\t<li><a href=\"https://spark-summit.org/2015/events/lessons-learned-with-spark-at-the-us-patent-trademark-office/\">Lessons Learned with Spark at the US Patent &amp; Trademark Office</a> - Christopher Bradford (OpenSource Connections)</li>\n</ul>\n<h3>Sponsors</h3>\nThank you to all of our wonderful <a href=\"http://spark-summit.org/2015/sponsors\">sponsors</a> who are helping in numerous ways to bring Spark Summit 2015 to life. You\u2019ve heard this before but without our sponsors, the Summits wouldn\u2019t happen."}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2015/05/14/ntt-data-operating-spark-clusters-at-thousands-core-scale-and-use-cases-for-telco-and-iot.html", "authors": null, "id": 3775, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2015-05-14", "tz": "UTC", "createdOn": "2015-05-14"}, "title": "NTT DATA: Operating Apache Spark clusters at thousands-core scale and use cases for Telco and IoT", "slug": "ntt-data-operating-spark-clusters-at-thousands-core-scale-and-use-cases-for-telco-and-iot", "content": "This is a guest blog from our one of our partners: <a href=\"http://www.nttdata.com/global/en/\" target=\"_blank\">NTT DATA Corporation</a>\n\n<hr />\n\n<h2>About NTT DATA Corporation</h2>\nNTT DATA Corporation is a Japanese IT solution provider and the global IT services arm of NTT (Nippon Telegraph and Telephone Corporation), which ranks among the top 10 telecommunication companies in the world by revenue.\n\nAt NTT DATA, the OSS (Open Source Software) Professional Services Team has the responsibility of providing our customers with consulting, designing, integrating, and supporting services for various OSS products including Apache Hadoop. For these 7+ years we've been integrating dozens of Hadoop systems, which include 1000+ nodes cluster for production use of our customer.\n\nRecently, Apache Spark has become a core component of our development platform and been included in the support service we provide.\n<h2>Why Spark</h2>\nThere are several reasons we make use of Spark. The first is that Spark can be effectively integrated with our existing Hadoop ecosystem, such as HDFS and YARN. \u00a0The companies who are well-experienced with Hadoop are able to use Spark on the same Hadoop cluster. The second is that Spark has features useful for data analysts, which traditional Hadoop doesn\u2019t have, such as interactive shell for on demand analysis and high level APIs for complex data analysis.\n\nIn order to validate that Spark has a good balance of high throughput and low latency, we ran some tests on a large cluster and found that Spark could scale to processing tens of TBs of data without unpredictable decrease of performance or stoppages. The detailed result of this validation is shown in the footnoted URL.\n<h2>Our\u00a0Spark use cases</h2>\nThe followings are some examples of our Spark use cases in production:\n\n<b>Use case 1: The first case is for the analysis of system infrastructures of a Telecom company.</b>\n\n5 years ago, we implemented an on-premises Hadoop cluster consisting of 1000 nodes with NTT DOCOMO, the leading mobile carrier company in Japan. Our emphasis was on achieving high fault tolerance and scalability while computing vast amount of system operation data in the mobile carrier. We were able to achieve our goals without any data loss for 4+ years nevertheless it was difficult to operate Hadoop at this scale at that time.\n\nHowever, we still needed more speed and flexibility for present day requirements. In other words, the demand for a newer parallel distributed processing framework, based on the computational model other than MapReduce was steadily increasing. \u00a0To satisfy this condition, we have launched a Spark on YARN cluster with 4000+ cores, evaluated Spark's features and successfully migrated some data processing models to Spark environment.\n\n<b>Use case 2: The second case is for the numerical analysis of massive IoT data gathered from machineries and public infrastructures.</b>\n\nWe are supporting this customer to establish the platform for data analysis, using a statistical approach. One important requirement of this project is to iterate trial-and-error workflows rapidly in order to find algorithms optimized for dynamically changing situations. In general, Spark excels in this type of use case.\n\nIn this project, Spark applications are combined with Hadoop HDFS, the stable data storage, and YARN, the resource management service for distributed computing. One important advantage of using YARN is that we can use multiple versions of Spark on the same cluster at the same time. For example, we can try an application built for the latest version of Spark, which has many useful new features, while another application built for older version of Spark is already running on the cluster.\n<h2>Our future with Spark</h2>\nNTT DATA has become the No 1 Spark contributing company in Japan. Based on the above mentioned production cases and our long experiences as a Hadoop integrator, we regularly provide input and feedback to the Spark development community. Our main focus of contribution is to improve usability. For example, we are developing a Web-based debugging tool \"Timeline Viewer\", which has been contributed to the community. We can easily understand when/where tasks are assigned in chronological order and how long it took with Timeline Viewer.\n<h2>Conclusion</h2>\nWe found that Spark has flexible features and good performance for production use from our deployments. When technical issues arise, NTT DATA has been able to resolve them quickly and contribute the solutions back to the Spark community. \u00a0Based on our experiences for distributed computing and open source software, NTT DATA has begun to actively deploy Spark for our customers.\n\nBy taking advantage of open source software, we provide capability to establish variety of data processing systems, such as strict data management, batch processing, data analysis, stream processing, visualization, etc. We believe that using open source software actively triggers \u201cthe open innovation\u201d and Spark is one core component for this concept.\n<h2>Additional Resources</h2>\nFor more information of our validation of Spark, please refer to Masaru Dobashi's <a href=\"http://spark-summit.org/2014/talk/spark-on-large-hadoop-cluster-and-evaluation-from-the-view-point-of-enterprise-hadoop-user-and-developer\" target=\"_blank\">recent presentation video and slides</a> at the Spark Summit 2014.\n\nAnd also please refer to Satoshi Tanaka's (from NTT DOCOMO) <a href=\"http://www.slideshare.net/hadoopconf/hadoopspark-hadoop-conference-japan-2014\" target=\"_blank\">slides</a> in Japanese at Hadoop Conference Japan 2014.\n\nTimeline Viewer is proposed in <a href=\"https://issues.apache.org/jira/browse/SPARK-3468\" target=\"_blank\">this ticket</a>."}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2015/05/28/tuning-java-garbage-collection-for-spark-applications.html", "authors": null, "id": 3813, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2015-05-28", "tz": "UTC", "createdOn": "2015-05-28"}, "title": "Tuning Java Garbage Collection for Apache Spark Applications", "slug": "tuning-java-garbage-collection-for-spark-applications", "content": "This is a guest post from our friends in the SSG STO Big Data Technology group at Intel.\n\n<a href=\"https://spark-summit.org/2015/\" target=\"_blank\">Join us at the Spark Summit</a> to hear from Intel and other companies deploying Apache Spark in production. \u00a0Use the code <em>Databricks20</em> to receive a\u00a020% discount!\n\n<hr />\n\nApache Spark is gaining wide industry adoption due to its superior performance, simple interfaces, and a rich library for analysis and calculation. Like many projects in the big data ecosystem, Spark runs on the Java Virtual Machine (JVM). Because Spark can store large amounts of data in memory, it has a major reliance on Java\u2019s memory management and garbage collection (GC). New initiatives like <a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\" target=\"_blank\">Project Tungsten</a> will simplify and optimize memory management in future Spark versions. But today, users who understand Java\u2019s GC options and parameters can tune them to eek out the best the performance of their Spark applications. This article describes how to configure the JVM's garbage collector for Spark, and gives actual use cases that explain how to tune GC in order to improve Spark\u2019s performance. We look at key considerations when tuning GC, such as collection throughput and latency.\n<h2>Introduction to Spark and Garbage Collection</h2>\nWith Spark being widely used in industry, Spark applications\u2019 stability and performance tuning issues are increasingly a topic of interest. Due to Spark\u2019s memory-centric approach, it is common to use 100GB or more memory as heap space, which is rarely seen in traditional Java applications. In working with large companies using Spark, we receive plenty of concerns about the various challenges surrounding GC during execution of Spark applications. For example, garbage collection takes a long time, causing program to experience long delays, or even crash in severe cases. In this article, we use real examples, combined with the specific issues, to discuss GC tuning methods for Spark applications that can alleviate these problems.\n\nJava applications typically use one of two garbage collection strategies: Concurrent Mark Sweep (CMS) garbage collection and ParallelOld garbage collection. The former aims at lower latency, while the latter is targeted for higher throughput. Both strategies have performance bottlenecks: CMS GC does not do compaction[1], while Parallel GC performs only whole-heap compaction, which results in considerable pause times. At Intel, we advise our customers to choose the strategy which best suits a given application\u2019s requirements. For applications with real-time response, we generally recommend CMS GC; for off-line analysis programs, we use Parallel GC.\n\nSo for a computing framework such as\u00a0Spark that supports both streaming computing and traditional batch processing, can we find an optimal collector? The Hotspot JVM version 1.6 introduced a third option for garbage collections: the Garbage-First GC (G1 GC). The G1 collector is planned by Oracle as the long term replacement for the CMS GC. Most importantly, the G1 collector aims to achieve both high throughput and low latency. Before we go into details on using the G1 collector with Spark, let\u2019s go over some background on Java GC fundamentals.\n<h2>How Java\u2019s Garbage Collectors Work</h2>\nIn traditional JVM memory management, heap space is divided into Young and Old generations. The young generation consists of an area called Eden along with two smaller survivor spaces, as shown in Figure 1. Newly created objects are initially allocated in Eden. Each time a <i>minor GC</i> occurs, the JVM copies live objects in Eden to an empty survivor space and also copies live objects in the other survivor space that is being used to that empty survivor space. This approach leaves one of the survivor spaces holding objects, and the other empty for the next collection. Objects that have survived some number of minor collections will be copied to the old generation. When the old generation fills up, a <i>major GC</i> will suspend all threads to perform full GC, namely organizing or removing objects in the old generation. This execution pause when all threads are suspended is called Stop-The-World (STW), which sacrifices performance in most GC algorithms. [2]\n\n<em>Figure 1 Generational Hotspot Heap Structure [2] **</em>\n\n<img class=\"aligncenter wp-image-3815\" src=\"https://databricks.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-11.35.50-AM-1024x302.png\" alt=\"Figure 1Generational Hotspot Heap Structure [2] ** \" width=\"600\" height=\"177\" />\n\nJava\u2019s newer G1 GC completely changes the traditional approach. The heap is partitioned into a set of equal-sized heap regions, each a contiguous range of virtual memory (Figure 2). Certain region sets are assigned the same roles (Eden, survivor, old) as in the older collectors, but there is not a fixed size for them. This provides greater flexibility in memory usage. When an object is created, it is initially allocated in an available region. When the region fills up, JVM creates new regions to store objects. When minor GC occurs, G1 copies live objects from one or more regions of the heap to a single region on the heap, and select a few free new regions as Eden regions. Full GC occurs only when all regions hold live objects and no full-empty region can be found. G1 uses the Remembered Sets (RSets) concept when marking live objects. RSets track object references into a given region by external regions. There is one RSet per region in the heap. The RSet avoids whole-heap scan, and enables the parallel and independent collection of a region. In this context, we can see that G1 GC not only greatly improves heap occupancy rate when full GC is triggered, but also makes the minor GC pause times more controllable, thereby is very friendly for large memory environment. How do these disruptive improvements change GC performance? Here we use the easiest way to observe the performance changes, i.e. by migrating from old GC settings to G1 GC settings. [3]\n\n<em>Figure 2 Illustration for G1 Heap Structure [3]**</em>\n\n<img class=\"aligncenter wp-image-3817\" src=\"https://databricks.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-11.38.37-AM.png\" alt=\"Figure 2Illustration for G1 Heap Structure [3]** \" width=\"400\" height=\"218\" />\n\nSince G1 gives up the approach of using fixed heap partitions for young/aged objects, we have to adjust GC configuration options accordingly to safeguard the smooth running of the application with G1 collector. Unlike with older garbage collectors, we\u2019ve generally found that a good starting point with the G1 collector is not to perform <i>any</i> tuning. So we recommend beginning only with the default settings and simply enabling G1 via the <code>-XX:+UseG1GC </code>option. One tweak we have found sometimes useful is that, when an application uses multiple threads, it is best to use <code>-XX: -ResizePLAB</code> to close <code>PLAB()</code> resize and avoid performance degradation caused by a large number of thread communications.\n\nFor a complete list of GC parameters supported by Hotspot JVM, you can use the parameter <code>-XX: +PrintFlagsFinal</code> to print out the list, or refer to the Oracle official documentation for explanations on part of the parameters.\n<h2>Understanding Memory Management in Spark</h2>\nA Resilient Distributed Dataset (RDD) is the core abstraction in Spark. Creation and caching of RDD\u2019s closely related to memory consumption. Spark allows users to persistently cache data for reuse in applications, thereby avoid the overhead caused by repeated computing. One form of persisting RDD is to cache all or part of the data in JVM heap. Spark\u2019s executors divide JVM heap space into two fractions: one fraction is used to store data persistently cached into memory by Spark application; the remaining fraction is used as JVM heap space, responsible for memory consumption during RDD transformation. We can adjust the ratio of these two fractions using the <code>spark.storage.memoryFraction</code> parameter to let Spark control the total size of the cached RDD by making sure it doesn\u2019t exceed RDD heap space volume multiplied by this parameter\u2019s value. The unused portion of the RDD cache fraction can also be used by JVM. Therefore, GC analysis for Spark applications should cover memory usage of both memory fractions.\n\nWhen an efficiency decline caused by GC latency is observed, we should first check and make sure the Spark application uses the limited memory space in an effective way. The less memory space RDD takes up, the more heap space is left for program execution, which increases GC efficiency; on the contrary, excessive memory consumption by RDDs leads to significant performance loss due to a large number of buffered objects in the old generation. Here we expand on this point with a use case:\n\nFor example, the user has an application based on the Bagel component of Spark, which performs simple iterative computing. The result of one superstep (iteration) depends on that of the previous superstep, so the result of each superstep will be persisted in memory space. During program execution, we observed that when the number of iterations increase, the memory space used by progress grows rapidly, causing GC to get worse. When we looked closely at Bagel, we discovered that it caches the RDDs of each superstep in memory without freeing them up over time, even though they are not used after a single iteration. This leads to a memory consumption growth that triggers more GC attempts. We removed this unnecessary caching in <a href=\"https://issues.apache.org/jira/i#browse/SPARK-2661?issueKey=SPARK-2661&amp;serverRenderedViewIssue=true\">SPARK-2661</a>. After this modification cache, RDD size stabilizes after three iterations and cache space is now effectively controlled (as shown in Table 1). As a result, GC efficiency has been greatly improved, with the total running time of the program shortened by 10%~20%.\n\n<em>Table 1: Comparison of Bagel Application\u2019s RDD Cache Sizes before and after Optimization</em>\n<table class=\"table\">\n<tbody>\n<tr>\n<th>Iteration Number</th>\n<th>Cache Size of each Iteration</th>\n<th>Total Cache Size (Before Optimization)</th>\n<th>Total Cache Size (After Optimization)</th>\n</tr>\n<tr>\n<td>Initialization</td>\n<td>4.3GB</td>\n<td>4.3GB</td>\n<td>4.3GB</td>\n</tr>\n<tr>\n<td>1</td>\n<td>8.2GB</td>\n<td>12.5GB</td>\n<td>8.2GB</td>\n</tr>\n<tr>\n<td>2</td>\n<td>98.8GB</td>\n<td>111.3 GB</td>\n<td>98.8GB</td>\n</tr>\n<tr>\n<td>3</td>\n<td>90.8GB</td>\n<td>202.1 GB</td>\n<td>90.8GB</td>\n</tr>\n</tbody>\n</table>\n&nbsp;\n\n<b><i>Conclusion:</i></b>\n\nWhen GC is observed as too frequent or long lasting, it may indicate that memory space is not used efficiently by Spark process or application. You can improve performance by explicitly cleaning up cached RDD\u2019s after they are no longer needed.\n<h2>Choosing a Garbage Collector</h2>\nIf our application is using memory as efficiently as possible, the next step is to tune our choice of garbage collector. After implementing <a href=\"https://issues.apache.org/jira/i#browse/SPARK-2661?issueKey=SPARK-2661&amp;serverRenderedViewIssue=true\">SPARK-2661</a>, we set up a four-node cluster, assigned an 88GB heap to each executor, and launched Spark in Standalone mode to conduct our experiments. We started with the default Spark Parallel GC, and found that because the Spark application\u2019s memory overhead is relatively large and most of the objects cannot be reclaimed in a reasonably short life cycle, the Parallel GC is often trapped in full GC, which brings a decline to performance every time it occurs. To make it worse, Parallel GC provides very limited options for performance tuning, so we can only use some basic parameters to adjust performance, such as the size ratio of each generation, and the number of copies before objects are promoted to the old generation. Since these tuning strategies only postpone full GC, the Parallel GC tuning helps little to long-running applications. Therefore, in this article we do not proceed with the Parallel GC tuning. Table 2 shows the operation of the Parallel GC, and obviously when the full GC is executed the lowest CPU utilization rates occur.\n\n<em>Table 2: Parallel GC Running Status (Before Tuning)</em>\n<table class=\"table\">\n<tbody>\n<tr>\n<td><b>Configuration Options </b></td>\n<td><code>-XX:+UseParallelGC -XX:+UseParallelOldGC -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -Xms88g -Xmx88g</code></td>\n</tr>\n<tr>\n<td><b>Stage*</b></td>\n<td>\u00a0<img class=\"alignnone wp-image-3833\" src=\"https://databricks.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-1.42.13-PM1.png\" alt=\"Screen Shot 2015-05-26 at 1.42.13 PM\" width=\"500\" height=\"218\" /></td>\n</tr>\n<tr>\n<td><b>Task*</b></td>\n<td>\u00a0<img class=\"alignnone wp-image-3837\" src=\"https://databricks.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-1.56.05-PM.png\" alt=\"Screen Shot 2015-05-26 at 1.56.05 PM\" width=\"502\" height=\"220\" /></td>\n</tr>\n<tr>\n<td><b>CPU*</b></td>\n<td>\u00a0<img class=\"alignnone wp-image-3838\" src=\"https://databricks.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-1.57.56-PM.png\" alt=\"Screen Shot 2015-05-26 at 1.57.56 PM\" width=\"493\" height=\"215\" /></td>\n</tr>\n<tr>\n<td><b>Mem*</b></td>\n<td>\u00a0<img class=\"alignnone wp-image-3839\" src=\"https://databricks.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-2.00.31-PM.png\" alt=\"Screen Shot 2015-05-26 at 2.00.31 PM\" width=\"493\" height=\"218\" /></td>\n</tr>\n</tbody>\n</table>\nCMS GC cannot do anything to eliminate full GC in this Spark application. Moreover, CMS GC has much longer full GC pause times than Parallel GC, taking a big bite out of the application\u2019s throughput.\n\nNext, we ran our application with default G1 GC configuration. To our surprise, G1 GC also gave unacceptable full GC (see \u201cCPU Utilization\u201d in Table 3, obviously Job 3 paused nearly 100 seconds), and a long pause time significantly dragged down the entire application operation. As shown in Table 4, although the total running time is slightly longer than the Parallel GC, G1 GC\u2019s performance was slightly better than the CMS GC.\n\n<em>Table 3: G1 GC Running Status (Before Tuning)</em>\n<table class=\"table\">\n<tbody>\n<tr>\n<td><b>Configuration Options</b></td>\n<td>-XX:+UseG1GC -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -Xms88g -Xmx88g</td>\n</tr>\n<tr>\n<td><b>Stage*</b></td>\n<td>\u00a0<img class=\"alignnone wp-image-3842\" src=\"https://databricks.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-3.02.59-PM.png\" alt=\"Screen Shot 2015-05-26 at 3.02.59 PM\" width=\"500\" height=\"223\" /></td>\n</tr>\n<tr>\n<td><b>Task*</b></td>\n<td>\u00a0<img class=\"alignnone wp-image-3843\" src=\"https://databricks.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-3.03.42-PM.png\" alt=\"Screen Shot 2015-05-26 at 3.03.42 PM\" width=\"500\" height=\"222\" /></td>\n</tr>\n<tr>\n<td><b>CPU*</b></td>\n<td>\u00a0<img class=\"alignnone wp-image-3844\" src=\"https://databricks.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-3.04.24-PM.png\" alt=\"Screen Shot 2015-05-26 at 3.04.24 PM\" width=\"509\" height=\"222\" /></td>\n</tr>\n<tr>\n<td><b>Mem*</b></td>\n<td>\u00a0<img class=\"alignnone wp-image-3845\" src=\"https://databricks.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-3.05.59-PM.png\" alt=\"Screen Shot 2015-05-26 at 3.05.59 PM\" width=\"505\" height=\"215\" /></td>\n</tr>\n</tbody>\n</table>\n&nbsp;\n\n<em>Table 4 Comparison of Three Garbage Collectors\u2019 Program Running Time (88GB Heap before tuning)</em>\n<table class=\"table\">\n<tbody>\n<tr>\n<th>Garbage Collector</th>\n<th>Running Time for 88GB Heap</th>\n</tr>\n<tr>\n<td>Parallel GC</td>\n<td>6.5min</td>\n</tr>\n<tr>\n<td>CMS GC</td>\n<td>9min</td>\n</tr>\n<tr>\n<td>G1 GC</td>\n<td>7.6min</td>\n</tr>\n</tbody>\n</table>\n<h2>Tuning The G1 Collector Based on Logs[4][5]</h2>\nAfter we set up G1 GC, the next step is to further tune the collector performance based on GC log.\n\nFirst of all, we want JVM to record more details in GC log. So for Spark, we set \u201cspark.executor.extraJavaOptions\u201d to include additional flags. In general, we need to set such options:\n\n<code>-XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark</code>\n\nWith these options defined, we keep track of detailed GC log and effective GC options in Spark's executer log (output to <code>$SPARK_HOME/work/$ app_id/$executor_id/stdout</code> at each <i>worker</i> node). Next, we can analyze root cause of the problems according to GC log and learn how to improve the program performance.\n\nLet's take a look at the structure of a G1 GC log as follows, which takes a mixed GC in G1 GC for example.\n<pre>251.354: [G1Ergonomics (Mixed GCs) continue mixed GCs, reason: candidate old regions available, candidate old regions: 363 regions, reclaimable: 9830652576 bytes (10.40 %), threshold: 10.00 %]\n\n[Parallel Time: 145.1 ms, GC Workers: 23]\n\n[GC Worker Start (ms): Min: 251176.0, Avg: 251176.4, Max: 251176.7, Diff: 0.7]\n\n[Ext Root Scanning (ms): Min: 0.8, Avg: 1.2, Max: 1.7, Diff: 0.9, Sum: 28.1]\n\n[Update RS (ms): Min: 0.0, Avg: 0.3, Max: 0.6, Diff: 0.6, Sum: 5.8]\n\n[Processed Buffers: Min: 0, Avg: 1.6, Max: 9, Diff: 9, Sum: 37]\n\n[Scan RS (ms): Min: 6.0, Avg: 6.2, Max: 6.3, Diff: 0.3, Sum: 143.0]\n\n[Object Copy (ms): Min: 136.2, Avg: 136.3, Max: 136.4, Diff: 0.3, Sum: 3133.9]\n\n[Termination (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.3]\n\n[GC Worker Other (ms): Min: 0.0, Avg: 0.1, Max: 0.2, Diff: 0.2, Sum: 1.9]\n\n[GC Worker Total (ms): Min: 143.7, Avg: 144.0, Max: 144.5, Diff: 0.8, Sum: 3313.0]\n\n[GC Worker End (ms): Min: 251320.4, Avg: 251320.5, Max: 251320.6, Diff: 0.2]\n\n[Code Root Fixup: 0.0 ms]\n\n[Clear CT: 6.6 ms]\n\n[Other: 26.8 ms]\n\n[Choose CSet: 0.2 ms]\n\n[Ref Proc: 16.6 ms]\n\n[Ref Enq: 0.9 ms]\n\n[Free CSet: 2.0 ms]\n\n[Eden: 3904.0M(3904.0M)-&gt;0.0B(4448.0M) Survivors: 576.0M-&gt;32.0M Heap: 63.7G(88.0G)-&gt;58.3G(88.0G)]\n\n[Times: user=3.43 sys=0.01, real=0.18 secs]\n</pre>\nFrom this log, we can see that G1 GC log has a very clear hierarchy. The log lists when and why the pause occurs, and grades time consumption of various threads as well as average and maximum CPU time. Finally, G1 GC lists the cleanup results after this pause, and the total time consumption.\n\nIn our current G1 GC running log, we find a special block like this:\n<pre>(to-space exhausted), 1.0552680 secs]\n\n[Parallel Time: 958.8 ms, GC Workers: 23]\n\n[GC Worker Start (ms): Min: 759925.0, Avg: 759925.1, Max: 759925.3, Diff: 0.3]\n\n[Ext Root Scanning (ms): Min: 1.1, Avg: 1.4, Max: 1.8, Diff: 0.6, Sum: 33.0]\n\n[SATB Filtering (ms): Min: 0.0, Avg: 0.0, Max: 0.3, Diff: 0.3, Sum: 0.3]\n\n[Update RS (ms): Min: 0.0, Avg: 1.2, Max: 2.1, Diff: 2.1, Sum: 26.9]\n\n[Processed Buffers: Min: 0, Avg: 2.8, Max: 11, Diff: 11, Sum: 65]\n\n[Scan RS (ms): Min: 1.6, Avg: 2.5, Max: 3.0, Diff: 1.4, Sum: 58.0]\n\n[Object Copy (ms): Min: 952.5, Avg: 953.0, Max: 954.3, Diff: 1.7, Sum: 21919.4]\n\n[Termination (ms): Min: 0.0, Avg: 0.1, Max: 0.2, Diff: 0.2, Sum: 2.2]\n\n[GC Worker Other (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.6]\n\n[GC Worker Total (ms): Min: 958.1, Avg: 958.3, Max: 958.4, Diff: 0.3, Sum: 22040.4]\n\n[GC Worker End (ms): Min: 760883.4, Avg: 760883.4, Max: 760883.4, Diff: 0.0]\n\n[Code Root Fixup: 0.0 ms]\n\n[Clear CT: 0.4 ms]\n\n[Other: 96.0 ms]\n\n[Choose CSet: 0.0 ms]\n\n[Ref Proc: 0.4 ms]\n\n[Ref Enq: 0.0 ms]\n\n[Free CSet: 0.1 ms]\n\n[Eden: 160.0M(3904.0M)-&gt;0.0B(4480.0M) Survivors: 576.0M-&gt;0.0B Heap: 87.7G(88.0G)-&gt;87.7G(88.0G)]\n\n[Times: user=1.69 sys=0.24, real=1.05 secs]\n\n760.981: [G1Ergonomics (Heap Sizing) attempt heap expansion, reason: allocation request failed, allocation request: 90128 bytes]\n\n760.981: [G1Ergonomics (Heap Sizing) expand the heap, requested expansion amount: 33554432 bytes, attempted expansion amount: 33554432 bytes]\n\n760.981: [G1Ergonomics (Heap Sizing) did not expand the heap, reason: heap expansion operation failed]\n\n760.981: [Full GC 87G-&gt;36G(88G), 67.4381220 secs]\n</pre>\nAs we can see, the largest performance degradation was caused by such a full GC, and was output in the log as To-space Exhausted, To-space Overflow or the similar (for various JVM versions, the output may look slightly different). The cause is that when the G1 GC collector tries to collect garbage for certain regions, it fails to find free regions which it can copy the live objects to. This situation is called Evacuation Failure and often leads to full GC. And apparently, full GC in G1 GC is even worse than in Parallel GC, so we must try to avoid full GC in order to achieve better performance. To avoid full GC in G1 GC, there are two commonly-used approaches:\n<ol>\n \t<li>Decrease the <code>InitiatingHeapOccupancyPercent</code> option\u2019s value (the default value is 45), to let G1 GC starts initial concurrent marking at an earlier time, so that we are more likely to avoid full GC.</li>\n \t<li>Increase the <code>ConcGCThreads</code> option\u2019s value, to have more threads for concurrent marking, thus we can speed up the concurrent marking phase. Take caution that this option could also take up some effective worker thread resources, depending on your workload CPU utilization.</li>\n</ol>\nTuning these two options minimized the possibility of a full GC occurrences. After full GC was eliminated, performance was increased dramatically. However, we still found long pauses during GC. On further investigation, we found the following occurrence in our logs:\n<pre>280.008: [G1Ergonomics (Concurrent Cycles) request concurrent cycle initiation, reason: occupancy higher than threshold, occupancy: 62344134656 bytes, allocation request: 46137368 bytes, threshold: 42520176225 bytes (45.00 %), source: concurrent humongous allocation]\n</pre>\nHere we see <i>humongous objects</i> (objects that are 50% the size of a standard region or larger). G1 GC would put each of these objects in contiguous set of regions. And since copying these objects would consume a lot of resources, humongous objects are directly allocated out of the old generation (bypassing all young GCs) and then categorized into humongous regions [4]. Before 1.8.0_u40, a complete heap liveness analysis is required to reclaim humongous regions [<a href=\"https://bugs.openjdk.java.net/browse/JDK-8027959\">JDK-8027959</a>]. If there are many objects of this kind, the heap would be filled up very quickly, and to reclaim them is too expensive. Even with the fixes (they do increase the efficiency of reclaiming humongous objects greatly), the allocation of contiguous regions is still more expensive (especially when meeting serious heap fragmentations), so we want to avoid creating objects of this size. We can increase the value of <code>G1HeapRegionSize</code> to reduce possibility of creating humongous regions, but the default value is already at its maximum size of 32M if we are using a comparatively large heap. This means we can only analyze the program to find these objects and to minimize their creation. Otherwise, it likely leads to more concurrent marking phase, and after that, you need to carefully tune mix GC related knobs (e.g., <code>-XX:G1HeapWastePercent -XX:G1MixedGCLiveThresholdPercent</code>) to avoid long mix GC pauses (caused by lots of humongous objects).\n\nNext, we can analyze the interval of a single GC cycle from cycle start until end of mixed GC. If the time is too long, you can consider increasing the value of <code>ConcGCThreads</code>, but note that this will take up more CPU resources.\n\nThe G1 GC also has ways to decrease STW pause length in return for doing more work in the concurrent stage of garbage collection. As mentioned above, G1 GC maintains an Remembered Set(RSet) for each region to track object references into a given region by external regions, and G1 collector updates RSets both at the STW stage and at the concurrent stage. If you are seeking to decrease the length of STW pauses with the G1 GC, you can decrease the value of <code>G1RSetUpdatingPauseTimePercent</code> while increasing the value of <code>G1ConcRefinementThreads</code>. The option <code>G1RSetUpdatingPauseTimePercent</code> is used to specify a desired ratio of RSets update time in total STW time, which is 10-percent by default, and <code>G1ConcRefinementThreads</code> is used to define the number of threads for maintaining RSets during program running. With these two options tuned, we can shift more workloads of RSets updating from STW stage to concurrent stage.\n\nIn addition, for long-running applications, we use the <code>AlwaysPreTouch</code> option, so JVM applies all the memory needed to OS at startup and avoids dynamic applications. This improves runtime performance at the cost of extending the start time.\n\nEventually, after several rounds of GC parameters tuning, we arrived at the results in Table 5. Compared with the previous results, we finally obtained a more satisfactory running efficiency.\n\n<em>Table 5 G1 GC Running Status (after Tuning)</em>\n<table>\n<tbody>\n<tr>\n<td><b>Configuration Options</b></td>\n<td><code>-XX:+UseG1GC -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -Xms88g -Xmx88g -XX:InitiatingHeapOccupancyPercent=35 -XX:ConcGCThread=20</code></td>\n</tr>\n<tr>\n<td><b>Stage*</b></td>\n<td>\u00a0<img class=\"alignnone wp-image-3852\" src=\"https://databricks.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-3.20.31-PM.png\" alt=\"Screen Shot 2015-05-26 at 3.20.31 PM\" width=\"500\" height=\"188\" /></td>\n</tr>\n<tr>\n<td><b>Task*</b></td>\n<td>\u00a0<img class=\"alignnone wp-image-3853\" src=\"https://databricks.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-3.21.36-PM.png\" alt=\"Screen Shot 2015-05-26 at 3.21.36 PM\" width=\"497\" height=\"193\" /></td>\n</tr>\n<tr>\n<td><b>CPU*</b></td>\n<td>\u00a0<img class=\"alignnone wp-image-3854\" src=\"https://databricks.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-3.22.13-PM.png\" alt=\"Screen Shot 2015-05-26 at 3.22.13 PM\" width=\"500\" height=\"196\" /></td>\n</tr>\n<tr>\n<td><b>Mem*</b></td>\n<td>\u00a0<img class=\"alignnone wp-image-3855\" src=\"https://databricks.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-3.22.45-PM.png\" alt=\"Screen Shot 2015-05-26 at 3.22.45 PM\" width=\"500\" height=\"188\" /></td>\n</tr>\n</tbody>\n</table>\n<h4></h4>\n<b><i>Conclusion:</i></b>\n\nWe recommend trying the G1 GC compared with alternatives for Spark applications. Finer-grained optimizations can be obtained through GC log analysis. After tuning, we successfully shortened the running time of the application to 4.3 minutes. Compared with the running time before tuning, we achieved a performance increase of 1.7 times; compared with Parallel GC, an increase by 1.5 times more or less.\n<h2>Summary</h2>\nFor Spark applications which rely heavily on memory computing, GC tuning is particularly important. When problems emerge with GC, do not rush into debugging the GC itself. First consider inefficiency in Spark program\u2019s memory management, such as persisting and freeing up RDD in cache. When tuning garbage collectors, we first recommend using G1 GC to run Spark applications. The G1 collector is well poised to handle growing heap sizes often seen with Spark. With G1, fewer options will be needed to provide both higher throughput and lower latency. Of course, there is no fixed pattern for GC tuning. The various applications have different characteristics, and in order to tackle unpredictable situations, one must master the art of GC tuning according to logs and other forensics. Finally, we cannot forget optimizing through program\u2019s logic and code, such as reducing intermediate object creation or replication, controlling creation of large objects, storing long-lived objects in off-heap, and so on.\n\nBy using the G1 GC we achieved major performance improvements in Spark applications. <a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\" target=\"_blank\">Future work in Spark</a> will move memory management responsibility away from Java\u2019s garbage collectors and into Spark itself. This will alleviate much of the tuning requirements for Spark applications. Nonetheless, today garbage collector choice can increase performance for mission critical Spark applications.\n<h2>Acknowledgement</h2>\nDuring the tuning practice and writing of this article, we received guidance and assistance from Ms. Yanping Wang, senior engineer from Intel's Java Runtime team.\n\n<b>* Indicates graphs generated using internal Performance Analysis tools developed by Intel Big Data team.</b>\n\n<b>** Indicates images from Oracle documentation. For details, see reference [2] [3]</b>\n<h2>References</h2>\n[1]\u00a0<a href=\"https://docs.oracle.com/cd/E13150_01/jrockit_jvm/jrockit/geninfo/diagnos/garbage_collect.html#wp1086917\">https://docs.oracle.com/cd/E13150_01/jrockit_jvm/jrockit/geninfo/diagnos/garbage_collect.html#wp1086917</a>\n\n[2] <a href=\"http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/index.html\">http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/index.html</a>\n\n[3] <a href=\"http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/G1GettingStarted/index.html\" target=\"_blank\">http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/G1GettingStarted/index.html</a>\n\n[4] <a href=\"http://www.infoq.com/articles/tuning-tips-G1-GC\" target=\"_blank\">http://www.infoq.com/articles/tuning-tips-G1-GC</a>\n\n[5] <a href=\"https://blogs.oracle.com/poonam/entry/understanding_g1_gc_logs\">https://blogs.oracle.com/poonam/entry/understanding_g1_gc_logs</a>\n<h2>About the Authors:</h2>\nDaoyuan Wang, Software Engineer from SSG STO Big Data Technology, Intel Asia-Pacific Research &amp;\u00a0Development Ltd., who is also an active Spark contributor in the Apache community.\n\nJie Huang, engineering manager of SSG STO Big Data Technology, Intel Asia-Pacific Research &amp; Development Ltd."}
{"status": "publish", "description": null, "creator": "kavitha", "link": "https://databricks.com/blog/2015/06/01/databricks-launches-mooc-data-science-on-spark.html", "authors": null, "id": 3911, "categories": ["Announcements", "Company Blog"], "dates": {"publishedOn": "2015-06-01", "tz": "UTC", "createdOn": "2015-06-01"}, "title": "Databricks Launches MOOC: Data Science on Apache Spark", "slug": "databricks-launches-mooc-data-science-on-spark", "content": "For the past several months, we have been working in collaboration with professors from the University of California Berkeley and University of California Los Angeles to produce two freely available Massive Open Online Courses (MOOCs). We are proud to announce that both MOOCs will launch in June on the edX platform!\n\nThe first course, called Introduction to Big Data with Apache Spark, begins today and teaches students about Apache Spark and performing data analysis. The second course, called Scalable Machine Learning, will begin on June 29th and will introduce the underlying statistical and algorithmic principles required to develop scalable machine learning pipelines, and provides hands-on experience using Spark.\u00a0 Both courses will be freely available on the edX MOOC platform, and edX Verified Certificates are also available for a fee.\n\nStudents have shown an overwhelming interest in these courses, as exhibited by the following enrollment numbers (as of 5/28/15):\n<ul>\n \t<li>over 80K enrolled students</li>\n \t<li>over 4K new enrollments in the past week</li>\n \t<li>nearly 1K Verified Certificate enrollments</li>\n</ul>\nWe would also like to thank the Spark community for their support.\u00a0 Several community members are serving as teaching assistants and beta testers, and multiple study groups have been organized by community members in anticipation of these courses.\n\nBoth courses are available for free on the edX website, and you can sign up for them today:\n<ol>\n \t<li><a href=\"https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x\">Introduction to Big Data with Apache Spark</a></li>\n \t<li><a href=\"https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x\">Scalable Machine Learning</a></li>\n</ol>\nIt is our mission to enable data scientists and engineers around the world to leverage the power of Big Data, and an important part of this mission is to educate the next generation."}
{"status": "publish", "description": "We are happy to announce improved support for statistical and mathematical functions in the upcoming 1.4 release.", "creator": "rxin", "link": "https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html", "authors": null, "id": 3920, "categories": ["Apache Spark", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2015-06-02", "tz": "UTC", "createdOn": "2015-06-02"}, "title": "Statistical and Mathematical Functions with DataFrames in Apache Spark", "slug": "statistical-and-mathematical-functions-with-dataframes-in-spark", "content": "We <a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\" target=\"_blank\">introduced DataFrames</a> in Apache Spark 1.3 to make Apache Spark much easier to use. Inspired by data frames in R and Python, DataFrames in Spark expose an API that\u2019s similar to the single-node data tools that data scientists are already familiar with. Statistics is an important part of everyday data science. We are happy to announce improved support for statistical and mathematical functions in the upcoming 1.4 release.\n\nIn this blog post, we walk through some of the important functions, including:\n<ol>\n \t<li>Random data generation</li>\n \t<li>Summary and descriptive statistics</li>\n \t<li>Sample covariance and correlation</li>\n \t<li>Cross tabulation (a.k.a. contingency table)</li>\n \t<li>Frequent items</li>\n \t<li>Mathematical functions</li>\n</ol>\nWe use Python in our examples. However, similar APIs exist for Scala and Java users as well.\n<h2>1. Random Data Generation</h2>\nRandom data generation is useful for testing of existing algorithms and implementing randomized algorithms, such as random projection. We provide methods under sql.functions for generating columns that contains i.i.d. values drawn from a distribution, e.g., uniform (<code>rand</code>), \u00a0and standard normal (<code>randn</code>).\n<pre>In [1]: from pyspark.sql.functions import rand, randn\nIn [2]: # Create a DataFrame with one int column and 10 rows.\nIn [3]: df = sqlContext.range(0, 10)\nIn [4]: df.show()\n+--+\n|id|\n+--+\n| 0|\n| 1|\n| 2|\n| 3|\n| 4|\n| 5|\n| 6|\n| 7|\n| 8|\n| 9|\n+--+\n\nIn [4]: # Generate two other columns using uniform distribution and normal distribution.\nIn [5]: df.select(\"id\", rand(seed=10).alias(\"uniform\"), randn(seed=27).alias(\"normal\")).show()\n+--+-------------------+--------------------+\n|id| \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0uniform| \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0normal|\n+--+-------------------+--------------------+\n| 0| 0.7224977951905031| -0.1875348803463305|\n| 1| 0.2953174992603351|-0.26525647952450265|\n| 2| 0.4536856090041318| -0.7195024130068081|\n| 3| 0.9970412477032209| \u00a00.5181478766595276|\n| 4|0.19657711634539565| \u00a00.7316273979766378|\n| 5|0.48533720635534006| 0.07724879367590629|\n| 6| 0.7369825278894753| -0.5462256961278941|\n| 7| 0.5241113627472694| -0.2542275002421211|\n| 8| 0.2977697066654349| -0.5752237580095868|\n| 9| 0.5060159582230856| \u00a01.0900096472044518|\n+--+-------------------+--------------------+\n</pre>\n<h2>2. Summary and Descriptive Statistics</h2>\nThe first operation to perform after importing data is to get some sense of what it looks like. For numerical columns, knowing the descriptive summary statistics can help a lot in understanding the distribution of your data. The function <code>describe</code> returns a DataFrame containing information such as number of non-null entries (count), mean, standard deviation, and minimum and maximum value for each numerical column.\n<pre>In [1]: from pyspark.sql.functions import rand, randn\nIn [2]: # A slightly different way to generate the two random columns\nIn [3]: df = sqlContext.range(0, 10).withColumn('uniform', rand(seed=10)).withColumn('normal', randn(seed=27))\n\nIn [4]: df.describe().show()\n+-------+------------------+-------------------+--------------------+\n|summary| \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0id| \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0uniform| \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0normal|\n+-------+------------------+-------------------+--------------------+\n| \u00a0count| \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a010| \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a010| \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a010|\n| \u00a0\u00a0mean| \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a04.5| 0.5215336029384192|-0.01309370117407197|\n| stddev|2.8722813232690143| \u00a00.229328162820653| \u00a00.5756058014772729|\n| \u00a0\u00a0\u00a0min| \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a00|0.19657711634539565| -0.7195024130068081|\n| \u00a0\u00a0\u00a0max| \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a09| 0.9970412477032209| \u00a01.0900096472044518|\n+-------+------------------+-------------------+--------------------+\n</pre>\nIf you have a DataFrame with a large number of columns, you can also run describe on a subset of the columns:\n<pre>In [4]: df.describe('uniform', 'normal').show()\n+-------+-------------------+--------------------+\n|summary|            uniform|              normal|\n+-------+-------------------+--------------------+\n|  count|                 10|                  10|\n|   mean| 0.5215336029384192|-0.01309370117407197|\n| stddev|  0.229328162820653|  0.5756058014772729|\n|    min|0.19657711634539565| -0.7195024130068081|\n|    max| 0.9970412477032209|  1.0900096472044518|\n+-------+-------------------+--------------------+\n</pre>\nOf course, while describe works well for quick exploratory data analysis, you can also control the list of descriptive statistics and the columns they apply to using the normal select on a DataFrame:\n<pre>In [5]: from pyspark.sql.functions import mean, min, max\nIn [6]: df.select([mean('uniform'), min('uniform'), max('uniform')]).show()\n+------------------+-------------------+------------------+\n|      AVG(uniform)|       MIN(uniform)|      MAX(uniform)|\n+------------------+-------------------+------------------+\n|0.5215336029384192|0.19657711634539565|0.9970412477032209|\n+------------------+-------------------+------------------+\n</pre>\n<h2>3. Sample covariance and correlation</h2>\n<a href=\"http://en.wikipedia.org/wiki/Covariance\">Covariance</a> is a measure of how two variables change with respect to each other. A positive number would mean that there is a tendency that as one variable increases, the other increases as well. A negative number would mean that as one variable increases, the other variable has a tendency to decrease. The sample covariance of two columns of a DataFrame can be calculated as follows:\n<pre>In [1]: from pyspark.sql.functions import rand\nIn [2]: df = sqlContext.range(0, 10).withColumn('rand1', rand(seed=10)).withColumn('rand2', rand(seed=27))\n\nIn [3]: df.stat.cov('rand1', 'rand2')\nOut[3]: 0.009908130446217347\n\nIn [4]: df.stat.cov('id', 'id')\nOut[4]: 9.166666666666666\n</pre>\nAs you can see from the above, the covariance of the two randomly generated columns is close to zero, while the covariance of the id column with itself is very high.\n\nThe covariance value of 9.17 might be hard to interpret. <a href=\"http://en.wikipedia.org/wiki/Correlation\">Correlation</a> is a normalized measure of covariance that is easier to understand, as it provides quantitative measurements of the statistical dependence between two random variables.\n<pre>In [5]: df.stat.corr('rand1', 'rand2')\nOut[5]: 0.14938694513735398\n\nIn [6]: df.stat.corr('id', 'id')\nOut[6]: 1.0\n</pre>\nIn the above example, id correlates perfectly with itself, while the two randomly generated columns have low correlation value.\n<h2>4. Cross Tabulation (Contingency Table)</h2>\n<a href=\"http://en.wikipedia.org/wiki/Contingency_table\">Cross Tabulation</a> provides a table of the frequency distribution for a set of variables. Cross-tabulation is a powerful tool in statistics that is used to observe the statistical significance (or independence) of variables. In Spark 1.4, users will be able to cross-tabulate two columns of a DataFrame in order to obtain the counts of the different pairs that are observed in those columns. Here is an example on how to use crosstab to obtain the contingency table.\n<pre>In [1]: # Create a DataFrame with two columns (name, item)\nIn [2]: names = [\"Alice\", \"Bob\", \"Mike\"]\nIn [3]: items = [\"milk\", \"bread\", \"butter\", \"apples\", \"oranges\"]\nIn [4]: df = sqlContext.createDataFrame([(names[i % 3], items[i % 5]) for i in range(100)], [\"name\", \"item\"])\n\nIn [5]: # Take a look at the first 10 rows.\nIn [6]: df.show(10)\n+-----+-------+\n| name|   item|\n+-----+-------+\n|Alice|   milk|\n|  Bob|  bread|\n| Mike| butter|\n|Alice| apples|\n|  Bob|oranges|\n| Mike|   milk|\n|Alice|  bread|\n|  Bob| butter|\n| Mike| apples|\n|Alice|oranges|\n+-----+-------+\n\nIn [7]: df.stat.crosstab(\"name\", \"item\").show()\n+---------+----+-----+------+------+-------+\n|name_item|milk|bread|apples|butter|oranges|\n+---------+----+-----+------+------+-------+\n|      Bob|   6|    7|     7|     6|      7|\n|     Mike|   7|    6|     7|     7|      6|\n|    Alice|   7|    7|     6|     7|      7|\n+---------+----+-----+------+------+-------+\n</pre>\nOne important thing to keep in mind is that the cardinality of columns we run crosstab on cannot be too big. That is to say, the number of distinct \u201cname\u201d and \u201citem\u201d cannot be too large. Just imagine if \u201citem\u201d contains 1 billion distinct entries: how would you fit that table on your screen?!\n<h2>5. Frequent Items</h2>\nFiguring out which items are frequent in each column can be very useful to understand a dataset. In Spark 1.4, users will be able to find the frequent items for a set of columns using DataFrames. We have implemented an <a href=\"http://dx.doi.org/10.1145/762471.762473\">one-pass algorithm</a> proposed by Karp et al. This is a fast, approximate algorithm that always return all the frequent items that appear in a user-specified minimum proportion of rows. Note that the result might contain false positives, i.e. items that are not frequent.\n<pre>In [1]: df = sqlContext.createDataFrame([(1, 2, 3) if i % 2 == 0 else (i, 2 * i, i % 4) for i in range(100)], [\"a\", \"b\", \"c\"])\n\nIn [2]: df.show(10)\n+-+--+-+\n|a| b|c|\n+-+--+-+\n|1| 2|3|\n|1| 2|1|\n|1| 2|3|\n|3| 6|3|\n|1| 2|3|\n|5|10|1|\n|1| 2|3|\n|7|14|3|\n|1| 2|3|\n|9|18|1|\n+-+--+-+\n\nIn [3]: freq = df.stat.freqItems([\"a\", \"b\", \"c\"], 0.4)\n</pre>\nGiven the above DataFrame, the following code finds the frequent items that show up 40% of the time for each column:\n<pre>In [4]: freq.collect()[0]\nOut[4]: Row(a_freqItems=[11, 1], b_freqItems=[2, 22], c_freqItems=[1, 3])\n</pre>\nAs you can see, \u201c11\u201d and \u201c1\u201d are the frequent values for column \u201ca\u201d. You can also find frequent items for column combinations, by creating a composite column using the struct function:\n<pre>In [5]: from pyspark.sql.functions import struct\n\nIn [6]: freq = df.withColumn('ab', struct('a', 'b')).stat.freqItems(['ab'], 0.4)\n\nIn [7]: freq.collect()[0]\nOut[7]: Row(ab_freqItems=[Row(a=11, b=22), Row(a=1, b=2)])\n</pre>\nFrom the above example, the combination of \u201ca=11 and b=22\u201d, and \u201ca=1 and b=2\u201d appear frequently in this dataset. Note that \u201ca=11 and b=22\u201d is a false positive.\n<h2>6. Mathematical Functions</h2>\nSpark 1.4 also added a suite of mathematical functions. Users can apply these to their columns with ease. The list of math functions that are supported come from <a href=\"https://github.com/apache/spark/blob/efe3bfdf496aa6206ace2697e31dd4c0c3c824fb/python/pyspark/sql/functions.py#L109\" target=\"_blank\">this file</a> (we will also post pre-built documentation once 1.4 is released). The inputs need to be columns functions that take a single argument, such as <code>cos</code>, <code>sin</code>, <code>floor</code>, <code>ceil</code>. For functions that take two arguments as input, such as <code>pow</code>, <code>hypot</code>, either two columns or a combination of a double and column can be supplied.\n<pre>In [1]: from pyspark.sql.functions import *\nIn [2]: df = sqlContext.range(0, 10).withColumn('uniform', rand(seed=10) * 3.14)\n\nIn [3]: # you can reference a column or supply the column name\nIn [4]: df.select(\n   ...:   'uniform',\n   ...:   toDegrees('uniform'),\n   ...:   (pow(cos(df['uniform']), 2) + pow(sin(df.uniform), 2)). \\\n   ...:     alias(\"cos^2 + sin^2\")).show()\n\n+--------------------+------------------+------------------+\n|             uniform|  DEGREES(uniform)|     cos^2 + sin^2|\n+--------------------+------------------+------------------+\n|  0.7224977951905031| 41.39607437192317|               1.0|\n|  0.3312021111290707|18.976483133518624|0.9999999999999999|\n|  0.2953174992603351|16.920446323975014|               1.0|\n|0.018326130186194667| 1.050009914476252|0.9999999999999999|\n|  0.3163135293051941|18.123430232075304|               1.0|\n|  0.4536856090041318| 25.99427062175921|               1.0|\n|   0.873869321369476| 50.06902396043238|0.9999999999999999|\n|  0.9970412477032209| 57.12625549385224|               1.0|\n| 0.19657711634539565| 11.26303911544332|1.0000000000000002|\n|  0.9632338825504894| 55.18923615414307|               1.0|\n+--------------------+------------------+------------------+\n</pre>\n<h2>What\u2019s Next?</h2>\nAll the features described in this blog post will be available in Spark 1.4 for Python, Scala, and Java, to be released in the next few days. If you can\u2019t wait, you can also build Spark from the 1.4 release branch yourself: <a href=\"https://github.com/apache/spark/tree/branch-1.4\">https://github.com/apache/spark/tree/branch-1.4</a>\n\nStatistics support will continue to increase for DataFrames through better integration with Spark MLlib in future releases. Leveraging the existing Statistics package in MLlib, support for feature selection in pipelines, Spearman Correlation, ranking, and aggregate functions for covariance and correlation.\n\nAt the end of the blog post, we would also like to thank Davies Liu, Adrian Wang, and rest of the Spark community for implementing these functions."}
{"status": "publish", "description": null, "creator": "denny", "link": "https://databricks.com/blog/2015/06/04/simplify-machine-learning-on-spark-with-databricks.html", "authors": null, "id": 3938, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2015-06-04", "tz": "UTC", "createdOn": "2015-06-04"}, "title": "Simplify Machine Learning on Apache Spark with Databricks", "slug": "simplify-machine-learning-on-spark-with-databricks", "content": "Join us at <a href=\"https://spark-summit.org/2015/\">Spark Summit</a> to hear more about new functionalities of Apache Spark. \u00a0Use the code <em>Databricks20</em> to receive a 20% discount!\n\n<hr />\n\nAs many data scientists and engineers can attest, the majority of the time is spent not on the models themselves but on the supporting infrastructure. \u00a0Key issues include on the ability to easily visualize, share, deploy, and schedule jobs. \u00a0More disconcerting is the need for data engineers to re-implement the models developed by data scientists for production. \u00a0With Databricks, data scientists and engineers can simplify these logistical issues and spend more of their time focusing on their data problems.\n\n<h2>Simplify Visualization</h2>\n\nAn important perspective for data scientists and engineers is the ability to quickly visualize the data and the model that is generated. \u00a0For example, a common issue when working with linear regression is to determine the model\u2019s goodness of fit. \u00a0While statistical evaluations such as Mean Squared Error are fundamental, the ability to view the data scatterplot in relation to the regression model is just as important.\n\n&nbsp;\n\n<h3>Training the models</h3>\n\nUsing a dataset comparing the population (x) with label data of median housing prices (y), we can build a linear regression model using Spark MLlib\u2019s Linear Regression with Stochastic Gradient Descent (LinearRegressionWithSGD). \u00a0\u00a0Spark MLlib is a core component of Apache Spark that allows data scientists and data engineers to quickly experiment and build data models - and bring them to production. \u00a0Because we are experimenting with SGD, we will need to try out different iterations and learning rates (i.e. alpha or step size).\n\nAn easy way to start experimenting with these models is to create a Databricks notebook in your language of choice (python, scala, Spark SQL) and provide contextual information via markdown text. \u00a0The screenshot below is two cells from an example DBC notebook where the top cell contains markdown comments while the bottom cell contains pyspark code to train two models.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/06/Figure-1a.png\"><img class=\"alignnone wp-image-4003\" src=\"https://databricks.com/wp-content/uploads/2015/06/Figure-1a-1024x321.png\" alt=\"Figure 1a\" width=\"701\" height=\"220\" /></a>\n\n<em><strong>Figure 1:</strong> Screenshot of Databricks Notebook training two models with Linear Regression with SGD</em>\n\n&nbsp;\n\n<h3>Evaluating the models</h3>\n\nOnce the models are trained, with some additional pyspark code, you can quickly calculate the mean squared error of these two models:\n\n<pre>valuesAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\nMSE = valuesAndPreds.(lambda (v, p): (v - p)**2).mean()\nprint(\"Mean Squared Error = \" + str(MSE))</pre>\n\n&nbsp;\n\nThe definition of the models and MSE results are in the table below.\n\n<table style=\"height: 112px;\" width=\"565\">\n<tbody>\n<tr>\n<td></td>\n<td><b># of iterations</b></td>\n<td><b>Step Size</b></td>\n<td><b>MSE</b></td>\n</tr>\n<tr>\n<td><b>Model A</b></td>\n<td>100</td>\n<td>0.01</td>\n<td>1.25095190484</td>\n</tr>\n<tr>\n<td><b>Model B</b></td>\n<td>1500</td>\n<td>0.1</td>\n<td>0.205298649734</td>\n</tr>\n</tbody>\n</table>\n\nWhile the evaluation of statistics most likely indicates that Model B has a better goodness of fit, the ability to visually inspect the data will make it easier to validate these results.\n\n&nbsp;\n\n<h3>Visualizing the models</h3>\n\nWith Databricks, there are numerous visualization options that you can use with your Databricks notebooks. \u00a0In addition to the default visualizations automatically available when working with Spark DataFrames, you can also use matplotlib, ggplot, and d3.js - all embedded with the same notebook.\n\nIn our example, we are using ggplot (the python code is below) so we can not only provide a scatter plot of the original dataset (in blue), but also graph line plots of the two models where Model A is in red and Model B is in green.\n\n<pre>p = ggplot(pydf, aes('x','y')) + \\\n    geom_point(color='blue') + \\\n    geom_line(pydf, aes('x','y2'), color='red') + \\\n    geom_line(pydf, aes('x','y3'), color='green')\ndisplay(p)</pre>\n\nEmbedded within the same notebook is the median housing prices ggplot scatterplot figure where the x-axis is the normalized population and y-axis is the normalized median housing price; Model A \u00a0is in red while Model B is in green.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/06/Figure-2a.png\"><img class=\"alignnone wp-image-4004\" src=\"https://databricks.com/wp-content/uploads/2015/06/Figure-2a-1024x745.png\" alt=\"Figure 2a\" width=\"700\" height=\"509\" /></a>\n\n<em><strong>Figure 2:</strong> <i>Screenshot of a ggplot scatterplot embedded within a Databricks\u00a0notebook</i></em>\n\nAs you can see from the above figure, the green line (Model B) has a better goodness of fit compared to the red line (Model A). \u00a0While the evaluation statistics pointed toward this direction, the ability to quickly visualize the data and the models within the same notebook allows the data scientist to spend more time understanding and optimizing their models.\n\n&nbsp;\n\n<h2>Simplify Sharing</h2>\n\nAnother crucial aspect of data sciences is the collaborative effort needed to solve data problems. \u00a0With many developers, engineers, and data scientists often working in different time zones, schedules, and/or locations, it is important to have an environment that is designed for collaboration.\n\n&nbsp;\n\n<h3>Portability</h3>\n\nWith Databricks, you can make it easier to collaborate with your team. \u00a0You can share your Databricks notebooks by sharing its URL so that any web browser on any device can view your notebooks.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/06/IMG_1596.png\"><img class=\"alignnone wp-image-4006\" src=\"https://databricks.com/wp-content/uploads/2015/06/IMG_1596-1024x576.png\" alt=\"IMG_1596\" width=\"600\" height=\"337\" /></a>\n\n<i><strong>Figure 3</strong>: Databricks notebook view of a the same linear regression SGD model via matplotlib on an iPhone 6.</i>\n\n&nbsp;\n\n<h3>Non-proprietary</h3>\n\nWhile these\u00a0notebooks are optimized for Databricks, you can export these notebooks to python, scala, and SQL files so you can use them in your own environments. \u00a0\u00a0A common use-case for this approach is that data scientists and engineers will collaborate and experiment in Databricks and then apply their resulting code into their on-premises environment.\n\n&nbsp;\n\n<h3>Share Definitions</h3>\n\nAs a data scientist or data engineer working with many different datasets, keeping up with all of the changes in schema and locations itself can be a full time job. \u00a0To help keep this under control, Databricks includes centralized table definitions. Instead of searching for include files that contain the schema, go the tables tab within Databricks \u00a0and you can define all of your tables in one place. \u00a0\u00a0This way as a data engineer updates the schema or source location for these table, these changes are immediately available to all notebooks.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/06/Figure-41.png\"><img class=\"alignnone wp-image-3958\" src=\"https://databricks.com/wp-content/uploads/2015/06/Figure-41-1024x512.png\" alt=\"Figure 4\" width=\"800\" height=\"400\" /></a>\n\n<i><strong>Figure 4:</strong> View of table definitions (schema and sample data) all from one place.</i>\n\n&nbsp;\n\n<h3>Collaborate</h3>\n\nAs notebooks are being created and shared, users can comment on the code or figures so they can provide input to the notebooks without making any changes to them. \u00a0This way you can lock the notebooks to prevent accidental changes and still accept feedback.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/06/Figure-5.png\"><img class=\"alignnone wp-image-3960\" src=\"https://databricks.com/wp-content/uploads/2015/06/Figure-5-1024x595.png\" alt=\"Figure 5\" width=\"700\" height=\"407\" /></a>\n\n<i><strong>Figure 5:</strong> Users commenting on a Databricks\u00a0notebook to more easily facilitate feedback</i>\n\n&nbsp;\n\n<h2>Simplify Deployment</h2>\n\nOne of the key advantages of Databricks is that the model developed by data scientists can be run in production. This is a huge advantage as it reduces the development cycle and tremendously simplifies the maintenance. In contrast, today data scientists develop the model using single machine tools such as R or Python and then have data engineers re-implement the model for production.\n\n&nbsp;\n\n<h3>Simplify Infrastructure</h3>\n\nAs a data engineer, there are many steps and configurations to deploy Apache Spark in production. \u00a0Some examples include (but are not limited to):\n\n<ul>\n    <li>Configuring High Availability and Disaster Recovery for your Spark clusters</li>\n    <li>Building the necessary manifests to spin up and down clusters</li>\n    <li>Configuring Spark to utilize local SSDs for fast retrieval</li>\n    <li>Upgrading or patching your Spark clusters to the latest version of the OS or Apache Spark</li>\n</ul>\n\nWith Databricks, the management of your Spark clusters are taken care by dedicated Databricks engineers who are supported by the developers and committers of the Apache Spark open source project. \u00a0These clusters are configured for optimal performance and balance the issues surrounding resource scheduling, caching, and garbage collection.\nOnce deployed, you can quickly view what clusters are available and their current state including the libraries and notebooks that are attached to the cluster(s). \u00a0Concerns around high availability, disaster recovery, manifests to build and deploy clusters, service management, configurations, patching, and upgrades are all managed on your behalf using your own (or your company\u2019s) AWS account.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/06/Figure-6.png\"><img class=\"alignnone wp-image-3968\" src=\"https://databricks.com/wp-content/uploads/2015/06/Figure-6-1024x640.png\" alt=\"Figure 6\" width=\"700\" height=\"438\" /></a>\n\n<em><strong>Figure 6:</strong> Databricks Cluster view for easier management of your Databricks infrastructure</em>\n\n&nbsp;\n\n<h3>Simplify Job Scheduling</h3>\n\nTraditionally, transitioning from code development to production is a complicated task. \u00a0It typically involves separate personnel and processes to build the code and push it into production. \u00a0But Databricks has a powerful Jobs feature for running applications in production. \u00a0You can take the notebook you had just created and run it as a periodic job - scheduling it minute, hourly, daily, weekly, or monthly intervals. \u00a0It also has a smart cluster allocation feature that allows you to run your notebook on an existing cluster or on an on-demand cluster. \u00a0You can also receive email notifications for your job as well as configure retries and timeouts.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/06/Figure-7.png\"><img class=\"alignnone wp-image-3962\" src=\"https://databricks.com/wp-content/uploads/2015/06/Figure-7-1024x640.png\" alt=\"Figure 7\" width=\"700\" height=\"438\" /></a>\n\n<strong>Figure 7</strong>: View of the Population vs. Price Multi-Chart Notebook Nightly Job\n\n&nbsp;\n\nAs well, you can upload and execute any Spark JAR compiled against any Spark installation within the Jobs feature. \u00a0Therefore any previous work can be used immediately instead of recreating and rebuilding the code-base.\n\n&nbsp;\n\n<h2>Try out Databricks</h2>\n\nWe created Databricks to make it easier for data scientists and data engineers to focus on experimenting and training their models, quickly deploy and schedule jobs against those models, easily collaborate and share their learnings, and easily share the schema and definitions for their datasets. \u00a0\u00a0Let us manage the cluster, configure it for optimal performance, perform upgrades and patches, and ensure high availability and disaster recovery.\n\nMachine Learning with Spark MLlib is a lot more fun when you get to spend most of your time doing Machine Learning!"}
{"status": "publish", "description": "The upcoming Spark Summit will feature open office hours with experts on both Spark and Databricks Cloud.", "creator": "patrick", "link": "https://databricks.com/blog/2015/06/03/join-us-for-engineer-office-hours-at-the-spark-summit.html", "authors": null, "id": 3976, "categories": ["Company Blog", "Events"], "dates": {"publishedOn": "2015-06-03", "tz": "UTC", "createdOn": "2015-06-03"}, "title": "Join us for Engineer Office Hours at the Spark Summit", "slug": "join-us-for-engineer-office-hours-at-the-spark-summit", "content": "<i>This post is about the upcoming <a href=\"https://spark-summit.org/2015/\" target=\"_blank\">Spark Summit in San Francisco</a>. Tickets are selling fast, so register today to join us!\u00a0Use the code <em>Databricks20</em> to receive a\u00a020% discount!</i>\n\n<hr />\n\nWe\u2019re happy to announce that the upcoming Spark Summit will feature open office hours with experts on both Apache Spark and Databricks. The committer leads of every major Spark component (MLlib, Core, SQL, and Streaming) will be hosting hours along with several core engineers on the Databricks platform.\n\nAll office hours are hosted at the Databricks booth (A1). There is no need to sign up, just show up and ask questions! We\u2019re looking forward to seeing you there!\n\n<b>Day 1, June 15th </b>(at Databricks Booth, A1)\n<table class=\"table\">\n<tbody>\n<tr>\n<td></td>\n<td><b><i>Expert</i></b></td>\n<td><b><i>Topic Area</i></b></td>\n</tr>\n<tr>\n<td>1:00-1:45</td>\n<td>Andrew Or</td>\n<td>Spark Core, YARN</td>\n</tr>\n<tr>\n<td></td>\n<td>Tathagata Das</td>\n<td>Spark Streaming</td>\n</tr>\n<tr>\n<td>1:45-2:30</td>\n<td>Michael Armbrust</td>\n<td>Spark SQL</td>\n</tr>\n<tr>\n<td></td>\n<td>Hossein Falaki</td>\n<td>Databricks</td>\n</tr>\n<tr>\n<td><i>2:30-3:00 break</i></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>3:00-3:40</td>\n<td>Parviz Deyhim</td>\n<td>Databricks, Spark Ops</td>\n</tr>\n<tr>\n<td></td>\n<td>Ahir Reddy</td>\n<td>Databricks</td>\n</tr>\n<tr>\n<td>3:40-4:15</td>\n<td>Vida Ha</td>\n<td>Databricks, Spark Ops</td>\n</tr>\n<tr>\n<td></td>\n<td>Yin Huai</td>\n<td>Spark SQL</td>\n</tr>\n<tr>\n<td><i>4:15-4:30 break</i></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>4:30-5:15</td>\n<td>Josh Rosen</td>\n<td>Spark Core, PySpark</td>\n</tr>\n<tr>\n<td></td>\n<td>Burak Yavuz</td>\n<td>Spark MLlib</td>\n</tr>\n<tr>\n<td>5:15-6:00</td>\n<td>Joseph Bradley</td>\n<td>Spark MLlib</td>\n</tr>\n</tbody>\n</table>\n&nbsp;\n\n<b>Day 2, June 16 </b>(at Databricks Booth, A1)\n<table class=\"table\">\n<tbody>\n<tr>\n<td></td>\n<td><b><i>Expert</i></b></td>\n<td><b><i>Topic Area</i></b></td>\n</tr>\n<tr>\n<td>1:00-1:45</td>\n<td>Andrew Or</td>\n<td>Spark Core, YARN</td>\n</tr>\n<tr>\n<td></td>\n<td>Pat McDonough</td>\n<td>Databricks, Spark Ops</td>\n</tr>\n<tr>\n<td>1:45-2:30</td>\n<td>Reynold Xin</td>\n<td>Spark SQL, Spark Core</td>\n</tr>\n<tr>\n<td></td>\n<td>Hossein Falaki</td>\n<td>Databricks</td>\n</tr>\n<tr>\n<td><i>2:30-3:00 break</i></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>3:00-3:40</td>\n<td>Tathagata Das</td>\n<td>Spark Streaming</td>\n</tr>\n<tr>\n<td></td>\n<td>Jeff Pang</td>\n<td>Databricks</td>\n</tr>\n<tr>\n<td>3:40-4:15</td>\n<td>Davies Liu</td>\n<td>PySpark, Spark Core, SparkR</td>\n</tr>\n<tr>\n<td></td>\n<td>Josh Rosen</td>\n<td>Spark Core, PySpark</td>\n</tr>\n<tr>\n<td><i>4:15-4:30 break</i></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>4:30-5:15</td>\n<td>Xiangrui Meng</td>\n<td>Spark MLlib</td>\n</tr>\n<tr>\n<td></td>\n<td>Burak Yavuz</td>\n<td>Spark MLlib</td>\n</tr>\n<tr>\n<td>5:15-6:00</td>\n<td>Matei Zaharia</td>\n<td>Creator of Spark</td>\n</tr>\n</tbody>\n</table>\n&nbsp;"}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2015/06/05/making-databricks-cloud-better-for-developers-ide-integration.html", "authors": null, "id": 3982, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2015-06-05", "tz": "UTC", "createdOn": "2015-06-05"}, "title": "Making Databricks better for developers: IDE Integration", "slug": "making-databricks-cloud-better-for-developers-ide-integration", "content": "We have been working hard at Databricks to make our product more user-friendly for developers. Recently, we have added two new features that will allow developers easily use\u00a0external libraries - both their own and 3rd party packages - in Databricks. We will showcase these features in a two-part series. Here is part 1, introducing how to upload your own libraries to Databricks. Stay tuned for the second installment on how to upload Apache Spark Packages and other 3rd party libraries!\n<h2>Using your favorite IDE with Databricks</h2>\nSometimes you prefer to stick to the development environment you are most familiar with. However, you also want to harness the power of Apache Spark with\u00a0Databricks. We now offer the option to upload the libraries you wrote in your favorite\u00a0IDE to Databricks with a single click.\n\nTo provide this functionality, we have created an SBT plugin (for more information on SBT, see <a href=\"http://www.scala-sbt.org/\" target=\"_blank\">http://www.scala-sbt.org/</a>). This plugin, sbt-databricks, (<a href=\"https://github.com/databricks/sbt-databricks\" target=\"_blank\">https://github.com/databricks/sbt-databricks</a>) provides Databricks users the ability to upload their libraries to Databricks within an IDE, like IntelliJ IDEA, or from the terminal. This means that anyone who has SBT can seamlessly upload their custom libraries to Databricks in a single click. This greatly simplifies the iteration time during development and provides users the freedom to develop in the environment that they are most comfortable with.\n<h2>Uploading your own libraries to Databricks in 4 simple steps</h2>\nHere is a simple example of how this\u00a0works with IntelliJ IDEA:\n\n<strong>0. Install the SBT\u00a0plugin. (IntelliJ IDEA -&gt; Preferences)</strong>\n\n<img class=\"alignnone wp-image-3987\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-03-at-2.09.35-PM.png\" alt=\"Screen Shot 2015-06-03 at 2.09.35 PM\" width=\"600\" height=\"375\" />\n\n<strong>1. Import the sbt-databricks plugin\u00a0</strong>\n\n<img class=\"alignnone wp-image-3989\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-03-at-2.09.48-PM-1024x537.png\" alt=\"Screen Shot 2015-06-03 at 2.09.48 PM\" width=\"600\" height=\"314\" />\n\n<strong>2.\u00a0Set up configurations in your build file</strong>\n\n<img class=\"alignnone wp-image-3990\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-03-at-2.10.01-PM-1024x499.png\" alt=\"Screen Shot 2015-06-03 at 2.10.01 PM\" width=\"600\" height=\"292\" />\n\n<strong>3. Open up the SBT console (through IDE or terminal)</strong>\n\n<img class=\"alignnone wp-image-3991\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-03-at-2.10.12-PM-1024x624.png\" alt=\"Screen Shot 2015-06-03 at 2.10.12 PM\" width=\"600\" height=\"366\" />\n\n<strong>4. Execute\u00a0\"dbcDeploy\" and hit \"Enter\"!</strong>\n\n<img class=\"alignnone wp-image-3994\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-03-at-2.10.24-PM-1024x644.png\" alt=\"Screen Shot 2015-06-03 at 2.10.24 PM\" width=\"600\" height=\"378\" />\n\n<strong>Congratulations! Your library is now in Databricks</strong>\n<h2>An example of using a custom library in Databricks</h2>\nNow your libraries are imported to Databricks,\n\nYou can use them in Notebooks during an interactive data exploration session...\n\n<img class=\"alignnone wp-image-3995\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-03-at-2.10.49-PM-1024x336.png\" alt=\"Screen Shot 2015-06-03 at 2.10.49 PM\" width=\"600\" height=\"197\" />\n\n...or you can\u00a0also use them in a production setting with Jobs! (both in Notebook Jobs and Jar Jobs, <a href=\"https://databricks.com/blog/2015/03/18/databricks-launches-jobs-feature-for-production-workloads.html\" target=\"_blank\">see our Jobs blog for more details on how this works</a>)\n\n<img class=\"alignnone wp-image-3996\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-03-at-2.10.58-PM-1024x347.png\" alt=\"Screen Shot 2015-06-03 at 2.10.58 PM\" width=\"600\" height=\"203\" />\n<h2>Summary</h2>\nIn this blog post we introduced <a href=\"https://github.com/databricks/sbt-databricks\" target=\"_blank\">sbt-databricks</a>, an SBT plugin that allows users to easily deploy their own libraries to Databricks straight from their IDEs (SBT support for different\u00a0IDEs can be found <a href=\"http://www.scala-sbt.org/0.13/docs/Community-Plugins.html\" target=\"_blank\">here</a>). At Databricks, our goal is to keep simple things simple, and make complex things possible. This includes providing developers\u00a0with the flexibility\u00a0to work\u00a0in the environments they prefer\u00a0- IDEs or\u00a0Notebooks. We are developers ourselves after all!\n\nIf you have more questions, please check out the additional resources for more detailed information on how to use this plugin.\n\nStay tuned for the next installment, where we will show how to search for, and import 3rd Party Libraries from Spark Packages and/or Maven Central!\n<h2>Additional resources</h2>\n<ul>\n \t<li>Download <a href=\"https://github.com/databricks/sbt-databricks\">sbt-databricks from github</a></li>\n \t<li><a href=\"https://github.com/databricks/sbt-databricks/blob/master/README.md\">Get Documentation and sbt tips and tricks</a></li>\n \t<li><a href=\"http://www.scala-sbt.org/0.13/docs/Community-Plugins.html\">Find SBT Plugins for other IDEs</a></li>\n \t<li><a href=\"https://databricks.com/product/databricks-cloud\" target=\"_blank\">Learn more about Databricks</a></li>\n \t<li><a href=\"http://go.databricks.com/register-for-dbc\" target=\"_blank\">Sign-up for a 14-day free trial of Databricks</a></li>\n</ul>\n&nbsp;"}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2015/06/09/announcing-sparkr-r-on-spark.html", "authors": null, "id": 4098, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2015-06-09", "tz": "UTC", "createdOn": "2015-06-09"}, "title": "Announcing SparkR: R on Apache Spark", "slug": "announcing-sparkr-r-on-spark", "content": "I am excited to announce that the upcoming Apache Spark 1.4 release will include SparkR, an R package that allows data scientists to analyze large datasets and interactively run jobs on them from the R shell.\n\nR is a popular statistical programming language with a number of extensions that support data processing and machine learning tasks. However, interactive data analysis in R is usually limited as the runtime is single-threaded and can only process data sets that fit in a single machine\u2019s memory. \u00a0SparkR, an R package initially developed at the AMPLab, provides an R frontend to Apache Spark and using Spark\u2019s distributed computation engine allows us to run large scale data analysis from the R shell.\n<h2>Project History</h2>\nThe SparkR project was initially started in the <a href=\"https://amplab.cs.berkeley.edu/\" target=\"_blank\">AMPLab</a>\u00a0as an effort to explore different techniques to integrate the usability of R with the scalability of Spark. Based on these efforts, an initial developer preview of SparkR was <a href=\"http://amplab-extras.github.io/SparkR-pkg\" target=\"_blank\">first open sourced in January 2014</a>. The project was then developed in the AMPLab for the next year and we made many performance and usability improvements through open source contributions to SparkR. SparkR was recently merged into the Apache Spark project and will be released as an alpha component of Apache Spark in the 1.4 release.\n<h2>SparkR DataFrames</h2>\nThe central component in the SparkR 1.4 release is the SparkR DataFrame, a distributed data frame implemented on top of <a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\" target=\"_blank\">Spark</a>. \u00a0Data frames are a fundamental data structure used for data processing in R and the concept of data frames has been extended to other languages with libraries like Pandas etc. Projects like <a href=\"https://github.com/hadley/dplyr\" target=\"_blank\">dplyr</a>\u00a0have further simplified expressing complex data manipulation tasks on data frames. SparkR DataFrames present an API similar to dplyr and local R data frames but can scale to large data sets using support for distributed computation in Spark.\n\nThe following example shows some of the aspects of the DataFrame API in SparkR. (You can see the full example at <a href=\"https://gist.github.com/shivaram/d0cd4aa5c4381edd6f85\">https://gist.github.com/shivaram/d0cd4aa5c4381edd6f85</a>)\n\n<pre># flights is a SparkR data frame. We can first print the column \n# names, types, flights\n#DataFrame[year:string, month:string, day:string, dep_time:string, dep_delay:string, #arr_time:string, arr_delay:string, carrier:string, tailnum:string, flight:string, origin:string, #dest:string, air_time:string, distance:string, hour:string, minute:string]\n# Print the first few rows using `head`\nhead(flights)\n# Filter all the flights leaving from JFK\njfk_flights <- filter(flights, flights$origin == \"JFK\")\n# Collect the DataFrame into a local R data frame (for plotting etc.)\nlocal_df <- collect(jfk_flights)\n</pre>\n\nFor a more comprehensive introduction to DataFrames you can see the SparkR programming guide at <a href=\"http://people.apache.org/~pwendell/spark-releases/latest/sparkr.html\" target=\"_blank\">http://people.apache.org/~pwendell/spark-releases/latest/sparkr.html</a>\n<h2>Benefits of Spark integration</h2>\nIn addition to having an easy to use API, SparkR inherits many benefits from being tightly integrated with Spark. These include:\n\n<b>Data Sources API</b>: By tying into Spark SQL\u2019s <a href=\"https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html\" target=\"_blank\">data sources API</a>\u00a0SparkR can read in data from a variety of sources include Hive tables, JSON files, Parquet files etc.\n\n<b>Data Frame Optimizations</b>: SparkR DataFrames also inherit all of the optimizations made to the computation engine in terms of <a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\" target=\"_blank\">code generation, memory management</a>. For example, the following chart compares the runtime performance of running group-by aggregation on 10 million integer pairs on a single machine in R, Python and Scala (it uses the same dataset as <a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\">https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html</a>). From the graph we can see that using the optimizations in the computation engine makes SparkR performance similar to that of Scala / Python.\n\n<img class=\" wp-image-4099 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-08-at-7.11.27-PM2-1024x359.png\" alt=\"Screen-Shot-2015-06-08-at-7.11.27-PM2\" width=\"500\" height=\"175\" />\n\n<b>Scalability to many cores and machines: </b>Operations executed on SparkR DataFrames get automatically distributed across all the cores and machines available on the Spark cluster. As a result SparkR DataFrames <a href=\"http://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html\" target=\"_blank\">can be used on terabytes of data</a>\u00a0and run on clusters with thousands of machines.\n<h2>Looking forward</h2>\nWe have many other features planned for SparkR in upcoming releases: these include support for <a href=\"https://issues.apache.org/jira/browse/SPARK-6805\" target=\"_blank\">high level machine learning algorithms</a>\u00a0and making SparkR DataFrames a stable component of Spark.\n\nThe SparkR package represents the work of many contributors from various organizations including AMPLab, Databricks, Alteryx and Intel. We\u2019d like to thank our contributors and users who tried out early versions of SparkR and provided feedback. \u00a0If you are interested in SparkR, do check out our talks at the upcoming <a href=\"https://spark-summit.org/2015/schedule/\" target=\"_blank\">Spark Summit 2015</a>."}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2015/06/09/huawei-embraces-open-source-apache-spark.html", "authors": null, "id": 4118, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2015-06-10", "tz": "UTC", "createdOn": "2015-06-10"}, "title": "Huawei Embraces Open-Source Apache Spark", "slug": "huawei-embraces-open-source-apache-spark", "content": "This is a guest blog from one of our partners: Huawei\n\n<a href=\"http://spark-summit.org/2015\">Join us at the Spark Summit</a> to hear from Intel and other companies deploying Apache Spark in production. \u00a0Use the code <i>Databricks20</i> to receive a 20% discount!\n\n<hr />\n\nIt\u2019s not unusual that one or more terabytes data flows in a telco network every second - this translates to roughly exabytes every month. \u00a0In fact, the challenges go beyond the speed and volume of network flow data. \u00a0For example, the location data is in original wireless coding format with complex nested structure, and leaves little room for compression; the signaling data, derived from multi-interfaces device of multi vendors in real-time and batch mode, requires complex association rules to make it meaningful and easily interpretable. \u00a0Finally, the dynamic relationships across those data layers and among data entities of each horizontal layer create an exceedingly complex analytical problem. An effective and inherently unified data processing framework is the key to address this set of challenges.\n\n<img class=\" wp-image-4119 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-09-at-10.32.31-PM.png\" alt=\"Screen Shot 2015-06-09 at 10.32.31 PM\" width=\"400\" height=\"359\" />\n\n<b><i>Why We Chose Spark </i></b>\n\nTo solve telco data issues and meet data analytics needs in a cost effective manner, two factors matter the most: First, a scale-out, parallel data flow model based platform that can simultaneously handle different processing modes while efficiently supporting diverse workloads on the same execution engine - from SQL to running machine learning algorithms, from streaming to graph computing.\n\nSecond, an open framework can support diverse complex data sources in a consistent way, support multiple APIs intuitively, and have rich libraries in easy extension. \u00a0In this way, the IT, business, data science and network users can continue using their existing skills without tackling steep learning curves. \u00a0It significantly shortens the lifecycle of application development and onsite deployment.\n\nApache Spark allows us to address both of these needs with a single powerful platform, without the burden of coding, managing and integrating with multiple processing frameworks.\n\n<b><i>How Huawei leverages Spark</i></b>\n\nSpark is core to the data processing and analytics platform of Huawei\u2019s big data solution, FusionInsight, which is used by more than 100 enterprise customers globally.\n\nWith Spark, the raw data from multi-systems of multi-vendors (e.g., \u00a0CRM, billing, OSS and network) can be easily loaded into a single data processing layer. \u00a0Data scientists and data engineers can also use Spark SQL to explore the data, extract and group features, and develop models by leveraging MLlib algorithms. \u00a0Application developers can leverage the output of these models or features to build specific applications (e.g. base station investment optimization), and publish dashboards or reports for subscriber profiling and network monitoring. \u00a0Finally, business users can use Spark SQL for ad-hoc query, or continue to use existing BI systems or tools like SAS, R or Python with Spark\u2019s powerful APIs.\n\nAs Huawei continues to build cutting-edge telecom solutions, we will increasingly adopt Spark as the core framework of our solutions since it provides a robust programming framework, rich set of APIs and libraries, vibrant ecosystem, and unparalleled pace of technology innovation.\n\n<b>Business Value Realized</b>\n\nIn one of top 5 mobile carriers in the world (who has more than 300 million subscribers) Huawei deployed Spark in its operating branch across mission-critical business areas. The system supports near real time analysis, ad-hoc query, especially over multiple data sources of CRM, billing, OSS (Operational Support System), and wireless network. \u00a0It also allows analysts and data scientists to build models over large data set more effectively, in some cases improving the time to deliver a product from months to mere weeks.\n\nWe have also had success in leveraging Spark to plan recommendation and churn prediction. \u00a0The conversion rate from pre-paid to post-paid customers improved by 10-20% in each month after the project going live the prediction for top K churned customers enhanced by ~30%, and each month it helped retain over 30,000 subscribers. \u00a0It translates into multi-million dollars business benefit to this flagship branch.\n\nHuawei and this customer are working together to further expand Spark into other operating branches, and to unlock the potential of data in other new business areas (e.g. providing site recommendation to leading ads agencies and retailers).\n\n<b><i>Huawei\u2019s Commitment to Spark </i></b>\n\nHuawei\u2019s relationship with Spark can be traced back to 2011 when AMP Lab was founded. \u00a0Huawei was convinced by the vision of AMP Lab and became corporate sponsor in early stage. Over years, Huawei has put together a global team to actively participate in the community and contribute things back. \u00a0In Spark 1.2 release, there\u2019re 10 contributors from Huawei and 11 contributors in 1.3 release.\n\nTo further the adoption of Spark in vertical industries, we have developed <a href=\"https://github.com/Huawei-Spark/hbase/tree/master\">Spark SQL on HBase</a>, a community package project, designed to accelerate online data query and analytics for large data sets, and contributed thousands of lines code back. \u00a0Huawei team has also contributed <a href=\"https://databricks.com/blog/2015/04/17/new-mllib-algorithms-in-spark-1-3-fp-growth-and-power-iteration-clustering.html\">two new features into Spark 1.3 release</a>: The FP-growth algorithm is utilized to solve the frequent pattern mining problem and Power Iteration Clustering algorithm to identify similar behaviors among subscribers, network clusters or other combinations.\n\nHuawei will continue to contribute to Spark and work on community projects, some of our planned efforts include: adding co-processor and custom filter into Spark SQL on HBase; participating on Project Tungsten while exploring the possibility to bring vectorized processing and compilation on LLVM; bringing business case driven new algorithms into MLlib under pipeline API and support MLlib feature transformer; planning to support CEP processing in Spark streaming. \u00a0In short, Huawei is deeply committed to Spark and intends participate extensively in joint community and industry efforts."}
{"status": "publish", "description": null, "creator": "patrick", "link": "https://databricks.com/blog/2015/06/11/announcing-apache-spark-1-4.html", "authors": null, "id": 4160, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2015-06-11", "tz": "UTC", "createdOn": "2015-06-11"}, "title": "Announcing Apache Spark 1.4", "slug": "announcing-apache-spark-1-4", "content": "Today I\u2019m excited to announce the general availability of Apache Spark 1.4! Spark 1.4 introduces SparkR, an R API targeted towards data scientists. It also evolves Spark\u2019s DataFrame API with a large number of new features. Spark's ML pipelines API first introduced in Spark 1.3 graduates from an alpha component. Finally, Spark Streaming and Core add visualization and monitoring to aid in production debugging. \u00a0We\u2019ll be publishing in-depth posts covering Spark\u2019s new features over the coming weeks. Here I\u2019ll briefly outline some of the major themes and features in this release.\n<h2>SparkR ships in Spark</h2>\nSpark 1.4 introduces <a href=\"http://databricks.com/blog/2015/06/09/announcing-sparkr-r-on-spark.html\">SparkR, an R API for Spark</a> and Spark's first new language API since PySpark was added in 2012. SparkR is based on Spark\u2019s <a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\">parallel DataFrame abstraction</a>. Users can create SparkR DataFrames from \u201clocal\u201d R data frames, or from any Spark data source such as Hive, HDFS, Parquet or JSON. SparkR DataFrames support all Spark DataFrame operations including aggregation, filtering, grouping, summary statistics, and other analytical functions. They also supports mixing-in SQL queries, and converting query results to and from DataFrames. Because SparkR uses the Spark\u2019s parallel engine underneath, operations take advantage of multiple cores or multiple machines, and can scale to data sizes much larger than standalone R programs.\n<pre>people <- read.df(sqlContext, \"./examples/src/main/resources/people.json\", \"json\")\nhead(people)\n##  age    name\n##1  NA Michael\n##2  30    Andy\n##3  19  Justin\n# SparkR automatically infers the schema from the JSON file\nprintSchema(people)\n# root\n#  |-- age: integer (nullable = true)\n#  |-- name: string (nullable = true)</pre>\n<h2>Window functions and other DataFrame improvements</h2>\nThis release adds window functions to Spark SQL and in Spark\u2019s DataFrame library. Window functions are popular for data analysts and allow users to compute statistics over window ranges.\n<pre>val w = Window.partitionBy(\"name\").orderBy(\"id\")\ndf.select(\n  sum(\"price\").over(w.rangeBetween(Long.MinValue, 2)),\n  avg(\"price\").over(w.rowsBetween(0, 4))\n)</pre>\nIn addition, we have also implemented many new features for DataFrames, including enriched support for <a href=\"http://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html\">statistics and mathematical functions</a> (random data generation, descriptive statistics and correlations, and contingency tables), as well as functionalities for working with missing data.\n\nTo make Dataframe operations execute quickly, this release also ships the initial pieces of <a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\">Project Tungsten</a>, a broad performance initiative which will be a central theme in Spark's upcoming 1.5 release. Spark 1.4 adds improvements to serializer memory use and options to enable fast binary aggregations.\n<h2>ML pipelines graduates from alpha</h2>\nSpark introduced <a href=\"http://www.databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html\">a machine learning (ML) pipelines API</a> in Spark 1.2. Pipelines enable production ML workloads that include many steps, such as data pre-processing, feature extraction and transformation, model fitting, and validation stages. Pipelines have added many components in the 1.3 and 1.4 releases, and in Spark 1.4, they officially graduates from an alpha component meaning API\u2019s will be stable going forward. As part of graduation this release brings the Python API into parity with the Java and Scala interfaces. Pipelines also add a variety of new feature transformers such as <code>RegexTokenizer</code>, <code>OneHotEncoder</code>, and <code>VectorAssembler</code>, and new algorithms like linear models with elastic-net and tree models are now available within the pipeline API.\n<h2>Visualization and monitoring across the stack</h2>\nProduction Spark programs can be complex, with long workflows comprised of many different stages. Spark 1.4 adds visual debugging and monitoring utilities to understand the runtime behavior of Spark applications. An application timeline viewer profiles the completion of stages and tasks inside a running program. Spark 1.4 also exposes a visual representation of the underlying computation graph (or \u201cDAG\u201d) that is tied directly to metrics of physical execution. Spark streaming adds visual monitoring over data streams, to continuously track the latency and throughput. Finally, Spark SQL's JDBC server adds its own monitoring UI to list and track the progress of user-submitted queries.\n\n<img class=\" wp-image-4161 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/06/DAG-visualization-930x1024.png\" alt=\"DAG visualization\" width=\"600\" height=\"661\" />\n\nThis post only scratches the surface of all the new features in Spark 1.4. Stay tuned to the Databricks blog, where we\u2019ll be writing posts about each of the major features in this release.\n\nTo download Spark 1.4, head on over to the <a href=\"http://spark.apache.org/downloads.html\">Apache Spark download</a> page. For a list of major patches in this release, visit the <a href=\"http://spark.apache.org/releases/spark-release-1-4-0.html\">release notes</a>."}
{"status": "publish", "description": null, "creator": "denny", "link": "https://databricks.com/blog/2015/06/15/databricks-and-ibm-collaborate-to-enhance-apache-spark-machine-learning.html", "authors": null, "id": 4257, "categories": ["Announcements", "Company Blog", "Partners"], "dates": {"publishedOn": "2015-06-15", "tz": "UTC", "createdOn": "2015-06-15"}, "title": "Databricks and IBM Collaborate to Enhance Apache Spark Machine Learning", "slug": "databricks-and-ibm-collaborate-to-enhance-apache-spark-machine-learning", "content": "At today\u2019s <a href=\"https://spark-summit.org/2015/\">Spark Summit</a>, Databricks and IBM announced a joint effort to contribute key machine learning capabilities to the Apache Spark Project. \u00a0Over the course of the next few months, Databricks and IBM will collaborate to expand Spark\u2019s machine learning capabilities. The companies plan to introduce new domain specific algorithms to the Spark ecosystem and add new machine learning primitives in the Apache Spark Project. IBM and Databricks will also collaborate to integrate IBM\u2019s SystemML \u2013 a robust machine-learning engine for large-scale data, with the Spark platform.\n\n\u201cThe size and scale of companies that are partnering with Databricks to support the Spark movement is both inspiring and validating,\u201d said Ion Stoica, CEO at Databricks. \u201cWe are looking forward to IBM becoming a key member of the Spark community, as seen by their investment in a Spark Technology Center in San Francisco. This collaboration will help Spark continue to gain mainstream adoption and deliver next-generation big data analytics and applications.\u201d\n\n<hr />\n\nPress release:\u00a0<a href=\"http://www2.marketwire.com/mw/release_html_b1?release_id=1200794\" target=\"_blank\">Databricks and IBM Collaborate to Advance Machine Learning to the Apache Spark Project</a>\n\n<hr />\n\n&nbsp;\n<h3>Learn More</h3>\nFor Spark enthusiasts abroad, the first <a href=\"http://spark-summit.org/eu-2015\">Spark Summit Europe</a> will be in Amsterdam from October 27th to 29th. <a href=\"http://spark-summit.org/eu-2015/cfp\">Submit a presentation</a> by June 23 and <a href=\"https://www.prevalentdesignevents.com/sparksummit2015/europe/registration.aspx?source=SummitBlog615\">register now</a> to get a discount.\n\nTo keep up with Spark and Databricks news, don\u2019t forget to sign up for <a href=\"https://databricks.com/resources/newsletters\">our monthly newsletter</a>.\n\n&nbsp;\n\n&nbsp;"}
{"status": "publish", "description": null, "creator": "ion", "link": "https://databricks.com/blog/2015/06/15/databricks-is-now-generally-available.html", "authors": null, "id": 4263, "categories": ["Announcements", "Company Blog", "Product"], "dates": {"publishedOn": "2015-06-15", "tz": "UTC", "createdOn": "2015-06-15"}, "title": "Databricks is now Generally Available", "slug": "databricks-is-now-generally-available", "content": "We are excited to announce today, at <a href=\"https://spark-summit.org/2015/\" target=\"_blank\">Spark Summit 2015</a>, the general availability of the Databricks \u2013 a hosted data platform from the team that created Apache Spark. With Databricks, you can effortlessly launch Spark clusters, explore data interactively, run production jobs, and connect third-party applications. We believe Databricks is the easiest way to use big data.\n\n<hr />\n\n<a href=\"http://www2.marketwire.com/mw/release_html_b1?release_id=1201002\" target=\"_blank\">Press Release</a> |\u00a0<a href=\"https://databricks.com/registration\" target=\"_blank\">Sign up for a 14-day free trial</a>\n\n<hr />\n\nOur vision at Databricks is to\u00a0<b>make big data simple</b> and enable\u00a0<b>every</b>\u00a0organization to turn its data into value. We first unveiled Databricks at Spark Summit 2014, and launched it in limited availability in November. The excitement for the platform has been fantastic, with thousands of people requesting access and tremendous feedback from users. We\u2019re now delighted to take the next step towards this vision by making Databricks available to everyone.\n\nWe want to enable that instant productivity for all of your data problems.\n\n[embed]https://vimeo.com/130273206[/embed]\n\n&nbsp;\n<h3>Why Databricks?</h3>\nAs many data scientists and engineers can attest, the majority of their time is spent not on the data analysis itself but on the supporting infrastructure.\u00a0Key issues include deploying software, keeping production jobs up, and connecting disparate tools to process and visualize data.\u00a0Equally problematic is the need for data engineers to re-implement the models developed by data scientists for production.\u00a0With Databricks, data scientists and engineers can eliminate these issues and just spend their time focusing on their data.\n\n&nbsp;\n<h3>Instant Spark Clusters</h3>\nInstead of taking weeks to months to provision hardware, instantly launch and manage optimized Spark clusters on Amazon EC2. You can scale from a few nodes to hundreds, all with just a few clicks. You can also use Amazon spot instances to save on costs.\n\n<a href=\"https://databricks.com/wp-content/uploads/2014/10/cluster-screen.png\"><img class=\"alignnone wp-image-4016\" src=\"https://databricks.com/wp-content/uploads/2014/10/cluster-screen-1024x362.png\" alt=\"cluster-screen\" width=\"600\" height=\"212\" /></a>\n\n&nbsp;\n<h3>Interactively Explore and Visualize Your Data</h3>\nDatabricks includes <i>notebooks</i>, an interactive and collaborative multi-user environment for exploration and visualization. You can combine text, code execution, visualization, and advanced analytics such as machine learning (MLlib) and graphs (GraphX) \u2013 all within the same notebook. You can write notebooks in SQL, Python, Scala, Java, and R. This way Databricks allows you to be instantly productive in your language of choice.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/06/mobile-devices-by-geo.png\"><img class=\"alignnone wp-image-4290\" src=\"https://databricks.com/wp-content/uploads/2015/06/mobile-devices-by-geo-1024x862.png\" alt=\"mobile devices by geo\" width=\"850\" height=\"715\" /></a>\n\n&nbsp;\n<h3>Easily Deploy Production Pipelines</h3>\nDatabricks has a powerful Jobs feature for taking applications from prototype to production. With Jobs, you can run both notebooks and standalone Spark or Spark Streaming programs. Jobs include a flexible scheduler, email alerts, automatic retries, run history, and cluster reuse. Furthermore, Databricks Jobs are the first Software as a Service platform to support Spark Streaming, making it easy to deploy scalable, fault-tolerant streaming applications. Databricks Jobs are simply the easiest way to run Spark applications.\n\n<a href=\"https://databricks.com/wp-content/uploads/2014/11/databricks_jobs_screenshot.png\"><img class=\"alignnone wp-image-4025\" src=\"https://databricks.com/wp-content/uploads/2014/11/databricks_jobs_screenshot-1024x323.png\" alt=\"databricks_jobs_screenshot\" width=\"748\" height=\"236\" /></a>\n\n&nbsp;\n<h3>New Features for GA</h3>\nWith the launch of General Availability today, we\u2019re also releasing three new features that users have been requesting. These are immediately available in Databricks deployments:\n<ul>\n\t<li><b>Spark 1.4 support: \u00a0</b>Choose <a href=\"https://databricks.com/blog/2015/06/11/announcing-apache-spark-1-4.html\">Apache Spark 1.4</a> when provisioning a Databricks cluster.</li>\n\t<li><b>Spark Streaming in notebooks:</b> Experiment with Spark Streaming interactively in notebooks, or deploy it in production jobs.</li>\n\t<li><b>Improved commenting:</b> Comment on individual text selections within a notebook and respond to comments via the new sidebar.</li>\n</ul>\n<img class=\"aligncenter wp-image-4268\" src=\"https://databricks.com/wp-content/uploads/2015/06/new-cluster-version-300x273.png\" alt=\"new-cluster-version\" width=\"200\" height=\"182\" />\n\n&nbsp;\n<h3>What\u2019s Coming Next?</h3>\nIn addition to general availability, at Spark Summit we have also announced several major new features that we are rolling out over the next few months. Expect to see these in your Databricks deployments soon:\n<ul>\n\t<li><b>R-language notebooks:</b> Analyze data using R and <a href=\"https://databricks.com/blog/2015/06/09/announcing-sparkr-r-on-spark.html\">SparkR</a>, including all of R\u2019s standard visualization and statistics packages.</li>\n\t<li><b>Access control and private notebooks: </b>Manage permissions to view and execute code at an individual level.</li>\n\t<li><b>Version control and GitHub: </b>Track changes to source code in Databricks, and store notebooks in GitHub to work with them from outside the platform.</li>\n</ul>\n&nbsp;\n<h3>How to Get Started</h3>\nDatabricks runs in your own Amazon Web Services account or Virtual Private Cloud. To try it out, <a href=\"https://accounts.cloud.databricks.com/registration.html#signup\">sign up for a 14-day free trial today</a>.\n\nFor more information on Databricks, check out:\n<ul>\n\t<li><a href=\"http://dbricks.co/intro-to-db\" target=\"_blank\">Product demo video</a></li>\n\t<li><a href=\"https://databricks.com/resources/videos\" target=\"_blank\">Customer videos</a></li>\n\t<li><a href=\"http://dbricks.co/prod-brochure\" target=\"_blank\">Product brochure</a></li>\n\t<li><a href=\"http://dbricks.co/feature-primer\" target=\"_blank\">Feature primer</a></li>\n\t<li><a href=\"http://dbricks.co/prod-datasheet\" target=\"_blank\">Data sheet</a></li>\n\t<li><a href=\"https://databricks.com/resources/briefs\" target=\"_blank\">Spark and Databricks primers</a></li>\n\t<li><a href=\"http://go.databricks.com/ovum_otr_report_download\" target=\"_blank\">Ovum On the Radar: Databricks</a></li>\n\t<li><a href=\"http://venturebeat.com/2015/06/15/more-data-more-complexity-making-big-data-simple\" target=\"_blank\">Venturebeat article: More data, more complexity? Making big data simple</a></li>\n</ul>"}
{"status": "publish", "description": null, "creator": "Wayne Chan", "link": "https://databricks.com/blog/2015/06/16/zen-and-the-art-of-spark-maintenance-with-cassandra.html", "authors": null, "id": 4292, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2015-06-16", "tz": "UTC", "createdOn": "2015-06-16"}, "title": "Guest blog: Zen and the Art of Apache Spark Maintenance with Cassandra", "slug": "zen-and-the-art-of-spark-maintenance-with-cassandra", "content": "This is a guest post from our friends at DataStax.\n\n<hr />\n\nApache Cassandra\u2122 is a fully distributed, highly scalable database that allows users to create online applications that are always-on and can process large amounts of data. \u00a0Apache Spark\u2122 is a processing engine that enables applications in Hadoop clusters to run up to 100X faster in memory, and even 10X faster when running on disk. So it was just a matter of time until the two technologies found each other to deliver ridiculously fast analytics on real-time, operational data stored in a high-performance transactional database.\n\nThis blog post will go into details on the inner workings of Spark and how you can shape your application to take advantage of interactions between Spark and Cassandra, and answers to of the most commonly asked questions on the Cassandra + Spark topic.\n<h2>Spark Architecture Basics</h2>\nSpark is centered around 4 processes, we can view them on a running system by using the <code>jps</code> command.\n<pre>9:57:59 /~ jps # Java PS, lists all running java processes\n15687 DseSparkMaster # Spark Master (May be incorporated in DseDaemon)\n22232 DseSparkWorker # Spark Worker\n22652 CoarseGrainedExecutorBackend # Spark Executor\n22653 Jps\n22415 SparkSubmit # Spark Driver (Your Application)</pre>\n\n<img class=\"aligncenter wp-image-4293\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-15-at-7.24.49-PM-1024x619.png\" alt=\"Screen Shot 2015-06-15 at 7.24.49 PM\" width=\"500\" height=\"302\" />\n<h3>Spark Master JVM</h3>\nAnalogous to Hadoop Job Tracker, doesn\u2019t need a lot of RAM since all it does is distribute work to the cluster.\n<h3>Spark Worker JVM</h3>\nAnalogous to Hadoop Task Tracker, also doesn\u2019t need a lot of RAM since its main responsibility is starting up executor processes.\n\nIn DSE/StandAlone mode, a Worker will only start a single executor JVM per application, but this does not mean that you will use only 1 core on the machine. The single executor JVM will use up to the \u201cmax cores\u201d set as available on the worker (spark.cores.max). Having more than 1 executor JVM is only possible with multiple workers (a DSE 4.7 feature)\n<h3>Spark Executor JVM</h3>\nThe most important part of Spark performance. Basically this is going to be a set of processes that do nothing but process RDD tasks.\n<h4>CPU Requirements</h4>\nEach core that is allocated to this executor will be able to act on one task at a time. That means a cluster with 20 cores defined by Spark will be able to act on 20 tasks concurrently. If the RDD being processed has less than 20 partitions, then the cluster will not be fully utilized. There will be more details on how tasks/partitions are generated below, but in general the number of tasks should be greater than the number of cores.\n\nYou can set more cores available on a worker than there are physical cores. This can have benefits for I/O bound tasks, but it is most likely a bad idea to oversubscribe CPUs on a node also running Cassandra. If oversubscribed, the system will be relying on the OS to decide when Cassandra gets to respond to requests or send out heartbeats. There are some interesting ideas to mitigate this using cgroups to limit cluster resources but I don\u2019t know enough about these strategies to recommend them.\n<h4>RAM Requirements</h4>\nNow let\u2019s imagine within this cluster we have 4 physical nodes with 5 cores on each. This means that every machine will have an executor JVM (most likely named<i>CoarseGrainedExecutorBackend</i>.) This JVM\u2019s heap will be shared between the executors and will have all the same caveats that any other JVM-based application has. A large heap will cause longer garbage collections and extremely large heaps are untenable. The size restriction is of less importance in batch applications like Spark since a 1-second stop the world GC doesn\u2019t mean too much in a 30 minute task. Databricks has a recommended size of 55 GB in the heap. When setting your executor JVM size remember that you are taking away memory from C* and the Operating System. Be sure to leave enough for C* to run and for page cache to help out with C* operations.\n<h5><b>RDD Storage Fraction</b></h5>\nThe largest portion is the cache which will be used for keeping RDD partitions in memory. Since actually retrieving data from Cassandra is most likely going to be a bottleneck for most users since most applications will pull a sizable amount of data from C* and then work on it in memory. The default is 60% of heap and is set with (<i>spark.storage.memoryFraction</i>). Feel free to adjust this but keep in mind the other two portions of the heap. <i>Note: as per the Spark documentation, this fraction should be roughly the same as the old generation size in your JVM</i>.\n<h5><b>Application Code and Shuffle Storage</b></h5>\nSpark shuffles are organized and performed through a shuffle management service which uses the space in the shuffle.storage portion of the executor to actually move around data (and sort it) before writing it to files and shipping it across the network. Any operations that require a full <i>sort</i>, <i>groupBy</i>, or <i>join</i> will trigger a full shuffle so be careful when reducing this setting (<i>spark.shuffle.memoryFraction</i>) from the default of 0.2. The remaining portion of the heap is for your application code and can be scaled depending on what code and jars are required for your application.\n\nThere must be at least the requested amount of RAM available on each worker to create an executor for the Spark Application. In heterogeneous clusters, this means the executor memory can be no larger than the smallest worker if you wish tasks to run on all the machines.\n<h5><b>Networking</b></h5>\nAmong these components the following connections will be established:\n<ul>\n \t<li>Driver &lt;-&gt; Master</li>\n \t<li>Master &lt;-&gt; Worker</li>\n \t<li>Driver &lt;-&gt; Executor</li>\n</ul>\n<img class=\" wp-image-4294 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-15-at-7.24.49-PM1-1024x619.png\" alt=\"Screen Shot 2015-06-15 at 7.24.49 PM\" width=\"500\" height=\"302\" />\n\nOne of the key things to troubleshoot here is the connection between the Driver and the Executors. Usually there is an issue with this communication if jobs start but then terminate prematurely with \u201cunexpected exception in \u2026.\u201d stemming from timed out futures. Most network connectivity issues are because that last link (between the driver and executor) is not successfully being established. Remember that this means that not only must the driver be able to communicate with the executor, but the executor must be able to connect with the driver. If you are having difficulty make sure that the Spark config option <i>spark.driver.host</i> (set in <i>spark-defaults.conf</i>) matches a reachable IP address on the machine running the driver application. In some situations we have found that using an IP address works when having a resolvable domain name does not.\n<h2>The Anatomy of an RDD</h2>\nThis is where we get real deep real fast. Let\u2019s start about talking about what an RDD is at its very base.\n\nAn RDD has several main components:\n<h3>A Dependency Graph</h3>\nThis details what RDDs must be computed before the current RDD can be successfully executed. This can be empty for an RDD coming from nowhere (like <i>sc.cassandraTable</i>) or a long chain of operations and dependencies (like with <i>rdd.map.filter.shuffle.join.map</i>).\n\nThe graph for any RDD can be viewed with <i>toDebugString</i>\n<pre>scala&gt; println(sc.cassandraTable(\"test\",\"tab\").toDebugString)\n(1) CassandraRDD[3] at RDD at CassandraRDD.scala:48\nscala&gt; println(sc.parallelize(1 to 10).map(_*2).map(_*2).map(_*2).toDebugString\n(6) MappedRDD[7] at map at :60\n| MappedRDD[6] at map at :60\n| MappedRDD[5] at map at :60\n| ParallelCollectionRDD[4] at parallelize at :60</pre>\n<h3>Partitions</h3>\nA description of how the RDD is partitioned and associated metadata describing the properties of each partition. Each partition should be thought of as a discrete chunk of the data represented by the RDD.\n<h3>Compute Method</h3>\nA compute method takes a piece of partition metadata (and the task context) and does something to that partition returning an iterator. This is the lazy method which will be executed when an action is called on the RDD.\n\nFor example, in the CassandraRDD this method reads metadata for each partition to get Cassandra token ranges and returns an iterator that yields C* data from that range. The Map RDD on the other hand uses the partition to retrieve an iterator from the previous RDD and then applies the given function to that iterator. (For more information see the video <a href=\"https://academy.datastax.com/demos/how-spark-cassandra-connector-reads-data\">How the Cassandra Connector Reads Data</a>.)\n<h3>Preferred Location Method</h3>\nA method which describes the preferred location where a particular partition should be computed. This location is defined by the RDD but most RDD types delegate this to the previous RDD in the chain. In all cases, this will be ignored if the partition has been check-pointed since the computed partition already exists. In CassandraRDD this method uses information from the custom partition class to see which node actually contains the ranges specified in the partition. Note that this is a \u201cpreferred\u201d not \u201cguaranteed\u201d location. Whether or not a partition will be streamed to another node or computed locally is dependent on the <i>spark.locality.wait</i> parameters. This parameter can be set to 0 to force all partitions to only be computed on local nodes.\n\nWhen an action is performed on an RDD the dependency tree is analyzed and separated into independent subtrees. Each independent subtree becomes a stage. All of the stages are processed until results can be provided to the user.\n<h2>Keeping the Dependency Graph Narrow</h2>\nMany things you do in Spark will only require one partition from the previous RDD (for example: <i>map</i>, <i>flatMap</i>, <i>keyBy</i>). Since computing a new partition in an RDD generated from one of these transforms only requires a single previous partition we can build them quickly and in place. These are the most efficient and reliable operations you can do in Spark. Since each new partition relies only on a single past partition they can be retried independently and should require no network operations.\n\n&nbsp;\n\n<img class=\" wp-image-4295 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-15-at-7.31.24-PM-1024x820.png\" alt=\"Screen Shot 2015-06-15 at 7.31.24 PM\" width=\"500\" height=\"401\" />\n\nOn the other hand some transforms require data being moved about the cluster because they require knowledge of all of the previous RDD\u2019s partitions to work. Transforms such as shuffles, groupBy, join, and sort all require a shuffle under the hood and thus are dependent on all of the previous RDD\u2019s partitions to do their work. You should attempt to keep these transformations to a minimum and push them as far down in your graph as possible (after any filtering you are doing.) It\u2019s also a great idea to cache the result of these expensive actions so that any further references to it will not require a recompute of the entire dependency tree.\n\n<img class=\" wp-image-4296 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-15-at-7.32.17-PM-1024x788.png\" alt=\"Screen Shot 2015-06-15 at 7.32.17 PM\" width=\"500\" height=\"385\" />\n\n<b>Where Operations should be in your Chain of RDD Operations</b>\n<table class=\"table\">\n<tbody>\n<tr>\n<td>Placement</td>\n<td>&lt;- Earliest</td>\n<td></td>\n<td></td>\n<td></td>\n<td>Latest -&gt;</td>\n</tr>\n<tr>\n<td>Type of Operation</td>\n<td>Cassandra RDD Specific</td>\n<td>Filters on the\nSpark Side</td>\n<td>Independent\nTransforms</td>\n<td>Per Partition Combinable\nTransforms</td>\n<td>Full Shuffle\nOperations</td>\n</tr>\n<tr>\n<td>Examples</td>\n<td>where\nselect</td>\n<td>filter\nsample</td>\n<td>map\nmapByPartition\nkeyBy</td>\n<td>reduceByKey\naggregateByKey</td>\n<td>groupByKey\njoin\nsort\nshuffle</td>\n</tr>\n</tbody>\n</table>\n<i>**Note if two RDDs share the same partitioner some of these operations become much cheaper. For example a join between RDDs with the same partitioner is essentially a 1 to 1 partition transform. **</i>\n\nThere are several ways to keep your graph narrow and minimize shuffles:\n<h3>Never let Spark Sort</h3>\nYou almost never *actually* want to sort all your data because every sort has a shuffle and shuffles are not your friend. (What are you going to do with 10 PB of sorted records anyway?) Every shuffle basically erases any data locality you once had as data will be randomly moved throughout your Spark cluster. If the actual goal is a TopX or BottomX consider the <i>top</i> and <i>takeOrdered</i> operations. These operations keep local TopX records on each partition so they don\u2019t require a full shuffle to get the results and locality is preserved. If you need the data to be sorted within subgroup check out the Let Cassandra Sort section below.\n\nOne exception to this rule is when writing; there may be occasions when having the data sorted by C* partition key may will allow the Spark Cassandra Connector to write faster. This hasn\u2019t been benchmarked yet and there is bound to be a balance between the time it takes to sort and the amount of data to be written.\n<h3>Do joins, groupBys, etc. on filtered data sets</h3>\nThe spark dependency graph doesn\u2019t know the internal schema of your data (unless you are using SparkSQL) so that means that spark can\u2019t optimize the execution path. This means the onus is on the developer to ensure that when you eventually do a <i>join</i> or <i>groupBy</i> you use the smallest dataset possible.\n\nThis means you want to ensure you never do this:\n<pre>rdd1.join(rdd2).filter(rdd1value = 30)\n</pre>\nWhen you could do this:\n<pre>rdd1.filter(rdd1value = 30).join(rdd2)\n</pre>\nOr even better if you are able to push the filter down to Cassandra:\n<pre>rdd1.where(rdd1value = 30).join(rdd2)</pre>\n<h3>Cache AFTER Hard Work</h3>\nAny time you have actually done a task which takes a long time (like a shuffle or reading data from C*), that might be a good time to cache your RDD. This tells Spark to make sure that the result of the compute from this RDD is stored. You can specify the storage_level with several parameters; see <a href=\"http://spark.apache.org/docs/1.2.0/programming-guide.html\">Spark Programming Guide</a> under RDD Persistence. A cached partition will not have to be recomputed if used multiple times.\n<pre>val hardWork = sc.cassandraTable(BigTable).map(expensiveFunction()).groupBy(someVal)\nhardWork.cache\nhardWork.operationOne().saveToCassandra\nhardWork.operationTwo().saveToCassandra\nhardWork.operationThree().saveToCassandra\n</pre>\n<h3>Never Collect and then Parallelize: Keep Data On The Cluster</h3>\nA huge anti-pattern is to <i>collect</i> an RDD, do some work on the driver, then <i>parallelize</i> it back to the cluster. Regardless of which language you are using with Spark, there is no excuse for ever doing work on the driver that could be done on the cluster instead. In practical terms, this means keeping your data as RDDs for the complete duration of the operation. The reason that this is so important is twofold; First, every time you perform a collect you have to serialize the contents of the RDD to the driver application (which may be a small JVM or running on small machine). Second, the client driver isn\u2019t taking advantage of the cluster resources so you are almost guaranteed that driver code will be less performant than similar distributed code.\n\nExample:\n\nNever\n<pre>val array = sc.cassandraTable().filter().collect\nval newArray = someFunction(array)\nval rdd = sc.parallelize(newArray)\n</pre>\nInstead:\n<pre>val array =3D sc.cassandraTable()\n.filter()\n.mapPartitions( someFunction(_))\n.collect()\n</pre>\n<h2>Take Advantage of Cassandra</h2>\nThe fusion of Spark and Cassandra is more than just availability and durability. Cassandra is a tried a true OLTP solution and we can leverage its advantages within Spark as well!\n<h3>Let Cassandra Sort</h3>\nMost of the time if you want your records sorted by some field within a grouping there is no need to have Spark do this work for you. For example, consider a situation where you have incoming scores streaming in for a game which need to be ordered per user. Instead of sorting the data in Spark you can have Cassandra sort the data as it writes it into Cassandra Partitions. This will remove the need for a Spark-side shuffle and it will be quickly retrievable.\n<h3>Use Cassandra Specific RDD Functions</h3>\nRemember how I said to be careful with <i>groupBy</i> or <i>Join</i>? Well there are some specific <i>groupBy</i>s and <i>joins</i> which are actually very performant and you can (and should) use them as often as you like. These special operations are those which are only acting on data that resides within the same Cassandra partition. Cassandra collocates all data that resides within a Cassandra partition so we won\u2019t need to shuffle and can take advantage of data locality.\n\nSince the raw Spark methods (<i>groupBy</i>, <i>Join</i>) will be shuffling your data, the connector provides new methods which do not require a repartitioning and when doing very specific operations. These methods are <i>spanBy</i> and <i>spanByKey</i>. With these methods you can quickly group up logical rows that share a common primary key without the pain that comes with a shuffle. Once you have grouped your partitions you can perform intra-partition operations on the newly made collections without fear of shuffles.\n\n**Note this functionality currently only works if the span is defined in the clustering key in C*. This means a table with a Primary Key (id, time, type) can be spanned by (id), (id,time), (id,time,type) but not (type) or (time,type). **\n<h3>Push Down Column Selection</h3>\nOne key way to save on network and serialization costs is to use the <i>select</i> method of the CassandraRDD. This method will push down column selections to Cassandra so the data retrieved and stored in spark is only what you actually want to act on.\n\nFor example, instead of:\n<pre>sc.cassandraTable(\"keyspace\",\"table\").map(row =&gt; (row.getInt(\"element1\"),row.getInt(\"element2\")))\n</pre>\nUse:\n<pre>sc.cassandraTable(\"keyspace\",\"table\").select(SomeColumns(\"element1\",\"element2\")\n</pre>\n<h3><b>Push Down Where Clauses</b></h3>\nThe Spark Cassandra Connector lets you push down where clauses to the Cassandra database. This will end up letting you do filtering on clustering columns and utilize secondary indexes. Some users may notice that this ends up putting \u201cALLOW FILTERING\u201d on the underlying Cassandra queries. These same users will most likely be quick to point out that \u201cALLOW FILTERING\u201d is a known Cassandra smell. This is correct, and you should normally never be using ALLOW FILTERING in a standard Cassandra OLTP application but here we are doing some quite different. In Spark, we won\u2019t be executing these queries very often (hopefully just at RDD generation) and we are already going to hit every node in the cluster from the get go. The major ill effects of secondary indexes revolve around their need to hit all nodes in the cluster and since we are going to be doing this anyway, we can only improve our performance by taking advantage of pushing down this index if it exists. This is especially true as the ratio of \u201cData That You Want\u201d / \u201cData In Your Table\u201d shrinks.\n<h3><b>RDD/Cassandra Inner joins</b></h3>\nWhen you are already aware of the keys that you want to retrieve from a Cassandra table you can avoid doing a full table scan by using the inner join functionality in the Connector. This functionality is accessed by calling <i>joinWithCassandraTable(keyspace,table)</i> on any RDD writable to Cassandra. This method is most useful when you have a large subset of data from your Cassandra Table which can be specified with partition keys. The additional function <i>repartitionByCassandraReplica</i>(keyspace,table) can be used in cases when the RDD is not already partitioned in a way which is data local with Cassandra. This places all of the requests which will access the same Cassandra node in the same Spark partition.\n\nWorst: Filter on the Spark Side\n<pre>sc.cassandraTable().filter(partitionkey in keys)\n</pre>\nBad: Filter on the C* Side in a Single Operation\n<pre>sc.cassandraTable().where(keys in veryLargeListOfKeys)\n</pre>\nBest: Filter on the C* side in a distributed and concurrent fashion\n<pre>sc.parallelize(keys).joinWithCassandraTable()</pre>\n<h3>Spark Cassandra Connector Metrics</h3>\nThe Spark Cassandra Connector now includes metrics on the throughput to and from Cassandra. These metrics are on by default but can be disabled. They are integrated into the Spark UI so you can now see exactly how many bytes are being serialized to and from Cassandra. To view these metrics go to the stage detail section of the Spark UI and you will see the Input and Output columns populated with the number of bytes read from and written to C*. (Note: Due to the way metrics are implemented in Spark the input and output will be shown as \u201cHadoop\u201d.)\n\n<img class=\"aligncenter wp-image-4297\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-15-at-7.39.44-PM-1024x145.png\" alt=\"Screen Shot 2015-06-15 at 7.39.44 PM\" width=\"800\" height=\"113\" />\n<h2>DataStax at Spark Summit 2015</h2>\nTo learn more about DataStax and our support for Spark, visit our booth at Spark Summit 2015! You can also let us know about what more features you would like in the Spark Cassandra Connector at <a href=\"https://github.com/datastax/spark-cassandra-connector\">github.com/datastax/spark-cassandra-connector</a>.\n\nAlso, Russ Spitzer, Software Engineer at DataStax, will be sharing his strategies on how to optimize Cassandra and Spark to perform lightning fast analytics on the world\u2019s most scalable OLTP database. \u00a0This session will be on Monday, June 15th from 4:00 - 4:15pm in Room 1.\n\nHere\u2019s the abstract for more details: <a href=\"https://spark-summit.org/2015/events/cassandra-and-spark-optimizing-for-data-locality/\">https://spark-summit.org/2015/events/cassandra-and-spark-optimizing-for-data-locality/</a>"}
{"status": "publish", "description": null, "creator": "nitin.bandugula", "link": "https://databricks.com/blog/2015/06/17/how-customers-win-with-spark-on-hadoop.html", "authors": null, "id": 4317, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2015-06-17", "tz": "UTC", "createdOn": "2015-06-17"}, "title": "Guest blog: How Customers Win with Apache Spark on Hadoop", "slug": "how-customers-win-with-spark-on-hadoop", "content": "This is a guest post from our friends at MapR.\n\n<hr />\n\n&nbsp;\n<p class=\"Normal1\">This blog summarizes my conversations over the last few months with users who have deployed Apache Spark in production on the MapR Distribution including Hadoop. My key observations overall are that Spark is indeed making inroads into our user community, which is leveraging not just the rapid application development and performance capabilities of Spark, but also the power of a complete Spark stack that the MapR platform uniquely supports.</p>\n\n<h3>Why Spark?</h3>\nWe asked our users what they learned after deploying Spark, and here is what they had to share:\n<ol>\n \t<li>Traditional MapReduce is definitely hard to code and maintain. Users want to build a number of applications as quickly as possible, and Spark now allows them to cut down on the development and maintenance time. This trend is in line with a survey that we conducted recently, and found that <a href=\"http://www.google.com/url?q=http%3A%2F%2Fwww.techvalidate.com%2Ftvid%2F432-D51-48F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNGrCAuPcK-xEKecBqNjIwABMX95fg\">18% of MapR customers have deployed over 50 use cases on a single cluster</a>. Users mentioned that platform capabilities such as multi-tenancy, high availability and data protection are even more critical when deploying so many applications, so rapidly.</li>\n \t<li>Although Scala provides good advantages for Spark app development, there are enough developers out there who are using Java APIs to build Spark applications. Java 8, with support for Lambda expressions, is expected to make their life considerably easier. Python APIs are mostly being used by a smaller subset of users\u2014the data scientist community\u2014 mainly for initial data modeling purposes.</li>\n</ol>\n&nbsp;\n<h3>Use Cases Overview</h3>\nThere are many different use cases that have been deployed combining Spark with MapR. Here are a few:\n<ol>\n \t<li><u>Faster batch applications</u>: Spark in-memory speeds are a definite plus point, especially for customer-facing applications. Many users have figured out that if their datasets can easily fit into memory based on the number of nodes they have, and if latency matters for that particular use case, then they need to quickly move towards converting those apps to Spark to gain performance advantages. A leading sales performance management company has done exactly this for their production application, originally written using traditional MapReduce.</li>\n \t<li><u>ETL data pipelines</u>: Given the full Spark stack support on MapR, a number of users are merging complex ETL pipelines into simpler programs that include feeding MLLib/Spark Streaming output to Spark SQL and GraphX applications. <a href=\"http://www.datanami.com/2015/01/19/creating-flexible-big-data-solutions-drug-discovery/\">Novartis does this</a> for drug discovery, using Spark for graph manipulations at scale.Several large financial services customers of MapR are doing ETL on streaming data from web clickstream and loading into transactional applications for call center applications so that customer service reps have all the latest information about what customers have been researching online.</li>\n \t<li><u>OLAP Cubes</u>: An emerging Spark use case across our customer base is one of an OLAP cube, where the end user can slice and dice an OLAP cube based on preconfigured datasets and filters. Predefined data loaded within a Spark context can be altered in real time by end users via predefined filters that kick off on-the-fly aggregations and simple linear regressions in the background. This solution is being used to deploy customer-facing services for real-time multidimensional OLAP analysis. As an example, <a href=\"https://www.mapr.com/customers/quantium\">Quantium</a>, one of the largest analytics services provider in Australia, has implemented this solution for its end users.</li>\n \t<li><u>Operational Analytics</u>: Yet another use case is real-time dashboarding and alerting systems based on streaming data, time-series data or operational data such as web clickstreams where a NoSQL store such as MapR-DB is being deployed as a durable, high-throughput persistence layer. A large retail analytics firm, a prominent financial services firm as well as a Fortune 100 healthcare company are implementing such solutions in production.</li>\n</ol>\n&nbsp;\n<h3>Platform Capabilities Still Matter</h3>\nIt may not come as a surprise, but the same enterprise-grade features that MapR customers have traditionally enjoyed continue to be applicable for Spark apps on Hadoop. NFS ingestion, high availability, a great option for an in-Hadoop NoSQL database, disaster recovery, and cross-datacenter replication still continue to matter and complete the story for production deployments.\n\n&nbsp;\n<h3>Want to Learn More?</h3>\nRead customer case studies for <a href=\"https://www.mapr.com/products/apache-spark\">Spark on Hadoop</a>.\n\nReview the <a href=\"https://www.mapr.com/apache-spark-dzone-ref-card\">Essential Apache Spark Cheat Sheet</a>.\n\nIf you are new to big data, check out our Spark-based <a href=\"https://www.mapr.com/solutions/big-data-and-hadoop-quick-start-solutions\">Quick Start Solutions </a>for Hadoop.\n\n&nbsp;"}
{"status": "publish", "description": null, "creator": "scott", "link": "https://databricks.com/blog/2015/06/19/a-look-back-at-spark-summit-2015.html", "authors": null, "id": 4344, "categories": ["Company Blog", "Events"], "dates": {"publishedOn": "2015-06-19", "tz": "UTC", "createdOn": "2015-06-19"}, "title": "A Look Back at Spark Summit 2015", "slug": "a-look-back-at-spark-summit-2015", "content": "<a href=\"https://databricks.com/wp-content/uploads/2015/06/image1.jpg\"><img class=\"alignnone size-full wp-image-4345\" src=\"https://databricks.com/wp-content/uploads/2015/06/image1.jpg\" alt=\"image1\" width=\"640\" height=\"480\" /></a>\n\nUPDATE: Slides and videos from the Summit are now available! <a href=\"https://spark-summit.org/2015/\" target=\"_blank\">Check them out now!</a>\n\n<hr />\n\nWe are delighted about the success of \u00a0<a href=\"https://spark-summit.org/2015/\">Spark Summit 2015</a> in San Francisco on June 15th and 16th, with three different sold-out <a href=\"https://spark-summit.org/2015/training/\">Spark Training</a> sessions on June 17th. \u00a0\u00a0This is the largest Spark Summit to date with more than 2000 attendees! \u00a0\u00a0Databricks is proud to make all talk videos, slides, training talk videos, and training materials available online for free as a service to the Apache Spark community. Slides will be available on the <a href=\"https://spark-summit.org/2015/schedule/\">Spark Summit 2015 agenda page</a> and videos will be published there too as soon as we finish editing them.\n<h3>Key Announcements</h3>\nMatei Zaharia, the creator of Spark, and Patrick Wendell - both co-founders of Databricks - opened the summit with a talk about the <a href=\"https://spark-summit.org/2015/events/keynote-1/\">Spark Community Update</a>. In it they described how Apache Spark continues to grow quickly, with new features including data frames, R support, and machine learning pipelines added in the past few releases.\n\nIn the next keynote, Ion Stoica, CEO of Databricks, and Ali Ghodsi, VP Engineering and Product Management of Databricks, talked about <a href=\"https://spark-summit.org/2015/events/keynote-2/\">Powering Data Science with Spark</a>. \u00a0In it, they talked about how Databricks makes big data simple by enabling data professionals to easily solve their data challenges and by leveraging the power of Spark. \u00a0\u00a0In it, they announced that <a href=\"https://databricks.com/blog/2015/06/15/databricks-is-now-generally-available.html\">Databricks is generally available</a>!\n\nIn a great partnership milestone, Databricks and IBM had also <a href=\"http://www2.marketwire.com/mw/release_html_b1?release_id=1200794\">announced a joint effort</a> to contribute key machine learning capabilities to the <a href=\"https://spark.apache.org/\">Apache Spark</a> Project. \u00a0Over the course of the next few months, Databricks and IBM will collaborate to expand Spark\u2019s machine learning capabilities.\n<h3>Keynotes</h3>\nWith Spark Summit being a community event focused on data science and data engineering at scale, some of our keynote highlights included:\n<ul>\n \t<li><a href=\"https://spark-summit.org/2015/events/tim-oreilly/\">Software Above the Level of a Single Device: The Implications</a> \u2013 Tim O'Reilly (O'Reilly Media)</li>\n \t<li><a href=\"https://spark-summit.org/2015/events/spark-at-nasa-jpl/\">Spark at NASA/JPL</a> \u2013 Chris Mattmann (NASA)</li>\n \t<li><a href=\"https://spark-summit.org/2015/events/keynote-4/\">Perspectives on Big Data &amp; Analytics</a> - Doug Wolfe (Central Intelligence Agency)</li>\n \t<li><a href=\"https://spark-summit.org/2015/events/keynote-5/\">Fireside chat</a> with Ben Horowitz (Andreessen Horowitz)</li>\n \t<li><a href=\"https://spark-summit.org/2015/events/keynote-8/\">Field Notes from Expeditions in the Cloud</a> \u2013 Matt Wood (Amazon Web Services)</li>\n \t<li><a href=\"https://spark-summit.org/2015/events/keynote-10/\">How Spark Fits into Baidu's Scale</a> \u2013 James Peng (Baidu)</li>\n \t<li><a href=\"https://spark-summit.org/2015/events/a-tale-of-a-data-driven-culture/\">A Tale of a Data-Driven Culture</a> \u2013 Gloria Lau (Timeful/Google)</li>\n</ul>\n<h3>Community Talks</h3>\nWith more than 260 submissions, this year\u2019s Spark Summit had one of the most amazing <a href=\"https://spark-summit.org/2015/schedule/\">schedules</a> to date, with some session highlights including:\n<ul>\n \t<li><a href=\"https://spark-summit.org/2015/events/appraiser-how-airbnb-generates-complex-models-in-spark-for-demand-prediction/\">Appraiser : How Airbnb Generates Complex Models in Spark for Demand Prediction</a> \u2013 Hector Yee (Airbnb)</li>\n \t<li><a href=\"https://spark-summit.org/2015/events/spark-and-spark-streaming-at-netflix/\">Spark and Spark Streaming at Netflix</a> \u2013 Kedar Sadekar (Netflix), Monal Daxini (Netflix)</li>\n \t<li><a href=\"https://spark-summit.org/2015/events/hybrid-community-detection-for-web-scale-e-commerce-using-spark-streaming-and-graphx/\">Dynamic Community Detection for Large-scale e-Commerce data with Spark Streaming and GraphX</a> \u2013 Ming Huang (Taobao Inc, Alibaba Group)</li>\n \t<li><a href=\"https://spark-summit.org/2015/events/lessons-learned-with-spark-at-the-us-patent-trademark-office/\">Lessons Learned with Spark at the US Patent &amp; Trademark Office</a> \u2013 Christopher Bradford (OpenSource Connections)</li>\n \t<li><a href=\"https://spark-summit.org/2015/events/Solving-low-latency-query-over-big-data-with-Spark-SQL/\">Solving Low Latency Query Over Big Data with Spark SQL</a> \u2013 Julien Pierre (Microsoft)</li>\n \t<li><a href=\"https://spark-summit.org/2015/events/sparkr-the-past-the-present-and-the-future/\">SparkR: The Past, the Present and the Future</a> \u2013 Shivaram Venkataraman (UC Berkeley AMPLAB), Rui Sun (Intel Asia Pacific R&amp;D)</li>\n \t<li><a href=\"https://spark-summit.org/2015/events/use-of-spark-mllib-for-predicting-the-offlining-of-digital-media/\">Use of Spark MLlib for Predicting the Offlining of Digital Media</a> \u2013 Christopher Burdorf (NBC Universal)</li>\n</ul>\n<h3>Training</h3>\nWe had <a href=\"https://spark-summit.org/2015/training/\">Spark Training</a> the day following Spark Summit, we trained over 500 students to use Spark in three parallel classes.\n<ul>\n \t<li><a href=\"https://spark-summit.org/2015/training/#intro\">Intro to Apache Spark</a></li>\n \t<li><a href=\"https://spark-summit.org/2015/training/#datasci\">Advanced: Data Science with Apache Spark</a></li>\n \t<li><a href=\"https://spark-summit.org/2015/training/#devops\">Advanced: Devops with Apache Spark</a></li>\n</ul>\nLearn more about Spark training classes run by Databricks on the <a href=\"https://databricks.com/services/spark-training\">training portion</a> of our website.\n<h3>Learn More</h3>\nFor Spark enthusiasts abroad, the first <a href=\"http://spark-summit.org/eu-2015\">Spark Summit Europe</a> will be in Amsterdam from October 27th to 29th. <a href=\"http://spark-summit.org/eu-2015/cfp\">Submit a presentation</a> by June 23 and <a href=\"https://www.prevalentdesignevents.com/sparksummit2015/europe/registration.aspx?source=SummitBlog615\">register now</a> to get a discount.\n\nTo keep up with Spark and Databricks news, don\u2019t forget to sign up for <a href=\"https://databricks.com/resources/newsletters\">our monthly newsletter</a>.\n\n&nbsp;"}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2015/06/22/understanding-your-spark-application-through-visualization.html", "authors": null, "id": 4356, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2015-06-22", "tz": "UTC", "createdOn": "2015-06-22"}, "title": "Understanding your Apache Spark Application Through Visualization", "slug": "understanding-your-spark-application-through-visualization", "content": "<p style=\"text-align: center;\"><i>The greatest value of a picture is when it forces us to notice what we never expected to see.\n</i><i>- John Tukey</i></p>\nIn the past, the Apache Spark UI has been instrumental in helping users debug their applications. <a href=\"https://databricks.com/blog/2015/06/11/announcing-apache-spark-1-4.html\" target=\"_blank\">In the latest Spark 1.4 release</a>, we are happy to announce that the data visualization wave has found its way to the Spark UI. The new visualization additions in this release includes\u00a0three main components:\n<ul>\n \t<li>Timeline view of Spark events</li>\n \t<li>Execution DAG</li>\n \t<li>Visualization of Spark Streaming statistics</li>\n</ul>\nThis blog post will be the first in a two-part series. This post will cover the first two components and save the last for a future post in the upcoming week.\n<h2>Timeline View of Spark Events</h2>\nSpark events have been part of the user-facing API since early versions of Spark. In the latest release, the Spark UI displays these events in a timeline such\u00a0that the relative ordering and interleaving of the events are evident at a glance.\n\nThe timeline view is available on three levels: <i>across all jobs</i>, <i>within one job</i>, and <i>within one stage</i>. On the landing page, the timeline displays all Spark events in an application across all jobs. Consider the following example:\n\n<img class=\" wp-image-4358 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-19-at-1.55.07-PM-1024x481.png\" alt=\"Screen Shot 2015-06-19 at 1.55.07 PM\" width=\"600\" height=\"282\" />\n\nThe sequence of events here is fairly straightforward. Shortly after all executors have registered, the application runs 4 jobs in parallel, one of which failed while the rest succeeded. Then, when all jobs have finished and the application exits, the executors are removed with it. Now let\u2019s click into one of the jobs.\n\n<img class=\"aligncenter wp-image-4360\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-19-at-1.56.30-PM-1024x426.png\" alt=\"Screen Shot 2015-06-19 at 1.56.30 PM\" width=\"599\" height=\"249\" />\n\nThis job runs word count on 3 files and joins the results at the end. From the timeline, it\u2019s clear that the the 3 word count stages run in parallel as they do not depend on each other. However, the join at the end does depend on the results from the first 3 stages, and so the corresponding stage (the collect at the end) does not begin until all preceding stages have finished. Let\u2019s look further inside one of the stages.\n\n<img class=\"aligncenter wp-image-4362\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-19-at-1.57.36-PM-1024x823.png\" alt=\"Screen Shot 2015-06-19 at 1.57.36 PM\" width=\"600\" height=\"483\" />\n\nThis stage has 20 partitions (not all are shown) spread out across 4 machines. Each bar represents a single task within the stage. From this timeline view, we can gather several insights about this stage. First, the partitions are fairly well distributed across the machines. Second, a majority of the task execution time comprises of raw computation rather than network or I/O overheads, which is not surprising because we are shuffling very little data. Third, the level of parallelism can be increased if we allocate the executors more cores; currently it appears that each executor can execute no more than two tasks at once.\n\nI would like to take the opportunity to showcase another feature in Spark using this timeline: <i>dynamic allocation</i>. This feature allows Spark to scale the number of executors dynamically based on the workload such that cluster resources are shared more efficiently. Let\u2019s see it in action through a timeline.\n\n<img class=\"aligncenter wp-image-4365\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-19-at-1.59.30-PM-1024x424.png\" alt=\"Screen Shot 2015-06-19 at 1.59.30 PM\" width=\"600\" height=\"248\" />\n\nThe first thing to note is that the application acquires executors over the course of a job rather than reserving them in advance. Then, shortly after the first job finishes, the set of executors used for the job becomes idle and is returned to the cluster. This allows other applications running in the same cluster to use our resources in the meantime, thereby increasing cluster utilization. Only when a new job comes in does our Spark application acquire a fresh set of executors to run it.\n\nThe ability to view Spark events in a timeline is useful for identifying the bottlenecks in an application. The next step in debugging the application is to map a particular task or stage to the Spark operation that gave rise to it.\n<h2>Execution DAG</h2>\nThe second visualization addition to the latest Spark release displays the execution DAG for each job. In Spark, a job is associated with a chain of RDD dependencies organized in a direct acyclic graph (DAG) that looks like the following:\n\n<img class=\"aligncenter wp-image-4366\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-19-at-2.00.59-PM.png\" alt=\"Screen Shot 2015-06-19 at 2.00.59 PM\" width=\"300\" height=\"395\" />\n\nThis job performs a simple word count. First, it performs a <i>textFile </i>operation to read an input file in HDFS, then a <i>flatMap</i> operation to split each line into words, then a <i>map</i> operation to form (word, 1) pairs, then finally a <i>reduceByKey</i> operation to sum the counts for each word.\n\nThe blue shaded boxes in the visualization refer to the Spark operation that the user calls in his / her code. The dots in these boxes represent RDDs created in the corresponding operations. The operations themselves are grouped by the stage they are run in.\n\nThere are a few observations that can be garnered from this visualization. First, it reveals the Spark optimization of pipelining operations that are not separated by shuffles. In particular, after reading from an input partition from HDFS, each executor directly applies the subsequent <i>flatMap </i>and <i>map</i> functions to the partition in the same task, obviating the need to trigger another stage.\n\nSecond, one of the RDDs is cached in the first stage (denoted by the green highlight). Since the enclosing operation involves reading from HDFS, caching this RDD means future computations on this RDD can access at least a subset of the original file from memory instead of from HDFS.\n\nThe value of the DAG visualization is most pronounced in complex jobs. As an example, the Alternating Least Squares (ALS) implementation in MLlib computes an approximate product of two factor matrices iteratively. This involves a series of <i>map</i>, <i>join</i>, <i>groupByKey</i> operations under the hood.\n\n<img class=\"aligncenter wp-image-4368\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-19-at-2.02.25-PM-1024x727.png\" alt=\"Screen Shot 2015-06-19 at 2.02.25 PM\" width=\"600\" height=\"426\" />\n\nIt is worth noting that, in ALS, caching at the correct places is critical to the performance because the algorithm reuses previously computed results extensively in each iteration. With the DAG visualization, users and developers alike can now pinpoint whether certain RDDs are cached correctly at a glance and, if not, understand quickly why an implementation is slow.\n\nAs with the timeline view, the DAG visualization allows the user to click into a stage and expand on details within the stage. The following depicts the DAG visualization for a single stage in ALS.\n\n<img class=\" wp-image-4369 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-19-at-2.03.21-PM-553x1024.png\" alt=\"Screen Shot 2015-06-19 at 2.03.21 PM\" width=\"300\" height=\"555\" />\n\nIn the stage view, the details of all RDDs belonging to this stage are expanded automatically. The user can now find information about specific RDDs quickly without having to resort to guess and check by hovering over individual dots on the job page.\n\nLastly, I would like to highlight a preliminary integration between the DAG visualization and Spark SQL. Since Spark SQL users are more familiar with higher level physical operators than with low level Spark primitives, the former should be displayed instead. The result is something that resembles a SQL query plan mapped onto the underlying execution DAG.\n\n<img class=\" wp-image-4370 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-19-at-2.04.05-PM-1024x879.png\" alt=\"Screen Shot 2015-06-19 at 2.04.05 PM\" width=\"600\" height=\"515\" />\n\nIntegration with Spark Streaming is also implemented in Spark 1.4 but will be showcased in a separate post.\n\nIn the near future, the Spark UI will be even more aware of the semantics of higher level libraries to provide more relevant details. Spark SQL will be given its own tab analogous to the existing Spark Streaming one. Within Spark Core, additional information such as number of partitions, call site, and cached percentages will be displayed on the DAG when the user hovers over an RDD.\n<h2>Summary</h2>\nThe latest Spark 1.4.0 release introduces several major visualization additions to the Spark UI. This effort stems from the project\u2019s recognition that presenting details about an application in an intuitive manner is just as important as exposing the information in the first place. Future releases will continue the trend of making the Spark UI more accessible to users of both Spark Core and the higher level libraries built on top of it.\n\nStay tuned for the second half of this two-part series about UI improvements in Spark Streaming!\n<h2>Acknowledgment</h2>\nThe features showcased in this post are the fruits of labor of several contributors in the Spark community. In particular, <i>@sarutak</i> of <i>NTT Data</i> is the main author of the timeline view feature."}
{"status": "publish", "description": "We have been working in collaboration with professors at UC Berkeley and UCLA to produce two freely available Massive Open Online Courses (MOOCs).", "creator": "dave_wang", "link": "https://databricks.com/blog/2015/06/29/databricks-launches-second-mooc-scalable-machine-learning.html", "authors": null, "id": 4396, "categories": ["Announcements", "Company Blog"], "dates": {"publishedOn": "2015-06-29", "tz": "UTC", "createdOn": "2015-06-29"}, "title": "Databricks Launches Second MOOC: Scalable Machine Learning", "slug": "databricks-launches-second-mooc-scalable-machine-learning", "content": "We have been working in collaboration with professors at UC Berkeley and UCLA to produce two freely available Massive Open Online Courses (MOOCs). The first MOOC was released earlier this month and has been a tremendous success, with over 60K students enrolled and a large number of active students. \u00a0We are excited to announce that the second MOOC is launching today!\n\nThis new course, called <em>CS190.1x: Scalable Machine Learning</em>, introduces the underlying statistical and algorithmic principles required to develop scalable machine learning pipelines, and provides hands-on experience using Apache Spark. \u00a0This course is freely available on the edX MOOC platform, and edX Verified Certificates are also available for a fee. \u00a0Moreover, this course was developed in conjunction with the first Databricks-sponsored MOOC, called <em>CS100.1x: Introduction to Big Data with Apache Spark</em>. Students who complete both courses will receive a BerkeleyX Big Data <a href=\"https://www.edx.org/xseries\" target=\"_blank\">XSeries</a> Certificate.\n\nStudents have shown an overwhelming interest in these courses, as exhibited by the following statistics (as of 6/26/15):\n<ul>\n \t<li>100K+ enrolled students</li>\n \t<li>4K+ Verified Certificate enrollments</li>\n \t<li>24.2% active students in CS100.1x</li>\n \t<li>11.3% completion rate for CS100.1x</li>\n</ul>\nIt is our mission to enable data scientists and engineers around the world to leverage the power of big data\u00a0by making it simple, and an important part of this mission is to educate the next generation.\n\nYou can still sign up for both courses today:\n<ol>\n \t<li><a href=\"https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x\" target=\"_blank\">Introduction to Big Data with Apache Spark</a></li>\n \t<li><a href=\"https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x\" target=\"_blank\">Scalable Machine Learning</a></li>\n</ol>"}
{"status": "publish", "description": null, "creator": "kavitha", "link": "https://databricks.com/blog/2015/07/02/myfitnesspal-delivers-new-feature-speeds-up-pipeline-and-boosts-team-productivity-with-databricks.html", "authors": null, "id": 4412, "categories": ["Announcements", "Company Blog", "Customers"], "dates": {"publishedOn": "2015-07-02", "tz": "UTC", "createdOn": "2015-07-02"}, "title": "MyFitnessPal Delivers New Feature, Speeds up Pipeline, and Boosts Team Productivity with Databricks", "slug": "myfitnesspal-delivers-new-feature-speeds-up-pipeline-and-boosts-team-productivity-with-databricks", "content": "To learn more about how Databricks helped MyFitnessPal with analytics, check out an <a href=\"http://blogs.wsj.com/cio/2015/06/03/spark-a-tool-at-big-datas-cutting-edge-helps-under-armour-perform-faster-analytics/\" target=\"_blank\">earlier article in Wall Street Journal</a> (log-in required) or <a href=\"https://databricks.com/wp-content/uploads/2015/07/Databricks_Case_Study_MyFitnessPal.pdf\" target=\"_blank\">download the case study</a>.\n\n<hr />\n\nWe are excited to announce that MyFitnessPal (An Under Armour company) uses Databricks to build the production pipeline for its new \u201cVerified Foods\u201d feature, gaining many performance and productivity benefits in the process.\n\nMyFitnessPal aims to build the largest health and fitness community online, by helping people to achieve healthier lifestyles through better diet and more exercise. Health-conscious people can use the MyFitnessPal website or the smartphone app to track their diet and exercise patterns and use the information to reach their fitness goals. MyFitnessPal wanted to further streamline the diet tracking functionality by offering a feature called \u201cVerified Foods\u201d, where one can get accurate and up-to-date nutritional information of food items by simply typing the name of the food in the MyFitnessPal application.\n\nTo deliver the functionality of \u201cVerified Foods\u201d, MyFitnessPal needed to create an accurate food database with a set of sophisticated algorithms. Prior attempts to implement these algorithms without Databricks proved to be not scalable, nor fast enough: They took weeks to run due to the enormous volume of data and their extreme complexity.\n\nMyFitnessPal chose Databricks to implement these algorithms in a production pipeline based on Apache Spark because Databricks delivers the speed and flexibility of Apache Spark in a simple-to-use, zero management platform. Because of the high reliability and fast performance of the data pipeline powered by Databricks, the \u201cVerified Foods\u201d database now includes a comprehensive list of items with readily available and highly accurate nutritional information.\n\nIn addition to powering the \u201cVerified Foods\u201d feature, Databricks also delivered a number of key benefits to the Data Engineering &amp; Science team at MyFitnessPal:\n<ul>\n \t<li>10X speed improvement, reducing the algorithm run time from weeks to mere hours.</li>\n \t<li>Dramatically higher team productivity as measured by the number of projects completed in the past quarter.</li>\n \t<li>Improved team efficiency due to the availability of mature libraries in Spark, and the ability to easily share and re-use code in the Databricks platform.</li>\n</ul>\nDownload the <a href=\"https://databricks.com/wp-content/uploads/2015/07/Databricks_Case_Study_MyFitnessPal.pdf\" target=\"_blank\">case study</a> to learn more about Databricks.\n\n<a href=\"https://accounts.cloud.databricks.com/registration.html#signup\">Sign up for a 14-day free trial to try Databricks today.</a>"}
{"status": "publish", "description": "The recently released Apache Spark 1.4 introduces PMML support to MLlib for linear models and k-means clustering.", "creator": "dave_wang", "link": "https://databricks.com/blog/2015/07/02/pmml-support-in-apache-spark-mllib.html", "authors": null, "id": 4423, "categories": ["Apache Spark", "Ecosystem", "Engineering Blog"], "dates": {"publishedOn": "2015-07-02", "tz": "UTC", "createdOn": "2015-07-02"}, "title": "Guest blog: PMML Support in Apache Spark's MLlib", "slug": "pmml-support-in-apache-spark-mllib", "content": "This is a guest blog from our friend Vincenzo Selvaggio who contributed this feature. He is a Senior Java Technical Architect and Project Manager, focusing on delivering advanced business process solutions for investment banks.\n\n<hr />\n\nThe recently released Apache Spark 1.4 introduces PMML support to MLlib for linear models and k-means clustering. This achievement is the result of active discussions from the community on JIRA (<a href=\"https://issues.apache.org/jira/browse/SPARK-1406\">https://issues.apache.org/jira/browse/SPARK-1406</a>) and GitHub (<a href=\"https://github.com/apache/spark/pull/3062\">https://github.com/apache/spark/pull/3062</a>) and embraces interoperability between Apache Spark and other platforms when it comes to predictive analytics.\n<h2>What is PMML?</h2>\nPredictive Model Markup Language (PMML) is the leading data mining standard developed by The Data Mining Group (DMG), an independent consortium, and it has been adopted by major \u00a0vendors and organizations (<a href=\"http://www.dmg.org/products.html\">http://www.dmg.org/products.html</a>). PMML uses XML to represent data mining models. A PMML document is an XML document with the following components:\n<ul>\n\t<li>a <b>Header</b> giving general information such as a description of the model and the application used to generate it</li>\n\t<li>a <b>DataDictionary</b> containing the definition of fields used by the model</li>\n\t<li>a <b>Model</b> defining the structure and the parameters of the data mining model</li>\n</ul>\n<h2>Why use PMML?</h2>\nPMML allows users to build a model in one system, export it and deploy it in a different environment for prediction. In other words, it enables different platforms to speak the same language, removing the need for custom storage formats.\n\n[caption id=\"attachment_4426\" align=\"aligncenter\" width=\"600\"]<img class=\"wp-image-4426\" src=\"https://databricks.com/wp-content/uploads/2015/07/Screen-Shot-2015-07-02-at-9.53.31-AM-1024x229.png\" alt=\"PMML\" width=\"600\" height=\"134\" /> Image courtesy of Villu Ruusmann[/caption]\n\nWhat's more, adopting a standard encourages best practices (established ways of structuring models) and transparency (PMML documents are fully intelligible and not black boxes).\n<h2>Why is Spark supporting PMML?</h2>\nBuilding a model (producer) and scoring it (consumer) are two tasks very much decoupled as they require different systems and supporting infrastructure.\n\nModel building is a complex task, it is performed on a large amount of historical data and requires a fast and scalable engine to produce correct results: this is where Apache Spark's MLlib shines.\n\nModel scoring is performed by operational applications tuned for high throughput and detached from the analytical platform. Exporting MLlib's models in PMML enables sharing models between Spark and operational apps and is key for the success of predictive analytics.\n<h2>A Code Example</h2>\nIn Spark, Exporting a data mining model to PMML is as simple as calling <code>model.toPMML</code>. Here a complete example, in Scala, of building a KMeansModel and exporting it to a local file:\n\n[scala]\nimport org.apache.spark.mllib.clustering.KMeans\nimport org.apache.spark.mllib.linalg.Vectors\n\n// Load and parse the data\nval data = sc.textFile(&quot;/path/to/file&quot;)\n  .map(s =&gt; Vectors.dense(s.split(',').map(_.toDouble)))\n\n// Cluster the data into three classes using KMeans\nval numIterations = 20\nval numClusters = 3\nval kmeansModel = KMeans.train(data, numClusters, numIterations)\n\n// Export clustering model to PMML\nkmeansModel.toPMML(&quot;/path/to/kmeans.xml&quot;)\n[/scala]\n\nThe PMML document generated is in this file: <code><a href=\"https://databricks.com/wp-content/uploads/2015/07/kmeans.pmml_.txt\" target=\"_blank\">kmeans.pmml</a></code>\n\nFor more examples of models exported and how those may be scored separately from Spark using the JPMML library, see\n\n<a href=\"http://spark-packages.org/package/selvinsource/spark-pmml-exporter-validator\">http://spark-packages.org/package/selvinsource/spark-pmml-exporter-validator</a>.\n<h2>Summary</h2>\nWith Apache Spark 1.4 PMML model export has been introduced, making MLlib interoperable with PMML compliant systems. You can find the supported models and how to export those to PMML from the official documentation page:\n\n<a href=\"http://spark.apache.org/docs/latest/mllib-pmml-model-export.html\">http://spark.apache.org/docs/latest/mllib-pmml-model-export.html</a>. We want to thank everyone who helped review and QA the implementation.\n\nThere is still work to do for MLlib\u2019s PMML support, for example, supporting PMML export for more models and add Python API. For more details, please visit <a href=\"https://issues.apache.org/jira/browse/SPARK-8545\">https://issues.apache.org/jira/browse/SPARK-8545</a>."}
{"status": "publish", "description": "This blog highlights new visualizations introduced specifically for understanding Apache Spark Streaming applications.", "creator": "tdas", "link": "https://databricks.com/blog/2015/07/08/new-visualizations-for-understanding-apache-spark-streaming-applications.html", "authors": null, "id": 4458, "categories": ["Apache Spark", "Engineering Blog", "Streaming"], "dates": {"publishedOn": "2015-07-08", "tz": "UTC", "createdOn": "2015-07-08"}, "title": "New Visualizations for Understanding Apache Spark Streaming Applications", "slug": "new-visualizations-for-understanding-apache-spark-streaming-applications", "content": "Earlier, we presented <a href=\"https://databricks.com/blog/2015/06/22/understanding-your-spark-application-through-visualization.html\">new visualizations</a> introduced in Apache Spark 1.4.0 to understand the behavior of Spark applications. Continuing the theme, this blog highlights new visualizations introduced specifically for understanding Spark Streaming applications. We have updated the Streaming tab of the Spark UI to show the following:\n<ul>\n\t<li>Timelines and statistics of events rates, scheduling delays and processing times of past batches.</li>\n\t<li>Details of all the Spark jobs in each batch.</li>\n</ul>\nAdditionally, the <a href=\"https://databricks.com/blog/2015/06/22/understanding-your-spark-application-through-visualization.html\">execution DAG visualization</a> is augmented with the streaming information for understanding the job execution in the context of the streaming operations.\n\nLet\u2019s take a look at these in more detail with an end-to-end example of analyzing a streaming application.\n<h2>Timelines and Histograms for Processing Trends</h2>\nWhen debugging Spark Streaming applications, users are often interested in the rate at which data is being received and the processing time of each batch. The new UI in the streaming tab makes it easy to see the current metrics as well as the trends over that past 1000 batches. While running a streaming application, you will see something like <i>figure 1</i> below if you visit the streaming tab in the Spark UI (Red letters such as [A] are our annotations, not part of the UI):\n<p style=\"text-align: center;\"><a href=\"https://databricks.com/wp-content/uploads/2015/07/image1.png\"><img class=\"aligncenter wp-image-4459\" src=\"https://databricks.com/wp-content/uploads/2015/07/image1-1024x822.png\" alt=\"Streaming UI figure 1\" width=\"600\" height=\"482\" /></a>Figure 1: Streaming tab in the Spark UI</p>\nThe first line (marked as <b>[A]</b>) shows the current status of the streaming application - in this example, the application has been running for almost 40 minutes at a 1-second batch interval. Below that, the timeline of <b>Input Rate</b> (marked as <b>[B]</b>) shows that the streaming app has been receiving data at a rate of about 49 events/second across all its sources. In this example, the timeline shows a slight dip in the average rate in the middle (marked as<b> [C]</b>), from which the application recovered towards the end of the timeline. If you want get more details, you can click the dropdown beside <b>Input Rate</b> (near<b> [B]</b>) to show timelines organized by each source, as shown in <i>figure 2 </i>below.\n<p style=\"text-align: center;\"><a href=\"https://databricks.com/wp-content/uploads/2015/07/image2.png\"><img class=\"aligncenter wp-image-4461\" src=\"https://databricks.com/wp-content/uploads/2015/07/image2.png\" alt=\"Spark streaming UI figure 2\" width=\"600\" height=\"404\" /></a>Figure 2</p>\n<em>Figure 2</em> shows that the app had two sources (<b><i>SocketReceiver-0</i></b> and <b><i>SocketReceiver-1),</i></b> one of which caused the overall receive rate to dip because it had stopped receiving data for a short duration.\n\nFurther down in the page (marked as <b><i>[D]</i></b> in<i> figure 1</i>), the timeline for <b><i>Processing Time </i></b>shows that these batches have been processed within 20 ms on average. Having a shorter processing time comparing to the batch interval (1s in this example) means that the <b><i>Scheduling Delay</i></b> (defined as the time a batch waits for previous batches to complete, and marked as <b><i>[E] </i></b>in <i>figure 1</i>) is mostly zero because the batches are processed as fast as they are created. This scheduling delay is the key indicator of whether your streaming application is stable or not, and this UI makes it easy to monitor it.\n<h2>Batch Details</h2>\nReferring to <i>figure 1</i> once again, you may be curious regarding why some batches towards the right took longer to complete (note<b> [F]</b> in <i>figure 1</i>). You can easily analyze this through the UI. First of all, you can click on the points in the timeline graph that have higher batch processing times. This will take you to the list of completed batches further down in the page.\n<p style=\"text-align: center;\"><a href=\"https://databricks.com/wp-content/uploads/2015/07/image3.png\"><img class=\"aligncenter wp-image-4464\" src=\"https://databricks.com/wp-content/uploads/2015/07/image3.png\" alt=\"image3\" width=\"600\" height=\"195\" /></a>Figure 3</p>\nIt will show all primary details of the individual batch (highlighted in green in <em>figure 3</em> above). As you can see, this batch has longer processing time than other batches. The next obvious question is what Spark jobs caused the longer processing time of this batch. You can investigate this by clicking on the batch time (the blue links in the first column), which will take you to the detailed information of the corresponding batch to show you the output operations and their Spark jobs (<i>Figure 4</i>).\n<p style=\"text-align: center;\"><a href=\"https://databricks.com/wp-content/uploads/2015/07/image4.png\"><img class=\"aligncenter wp-image-4465\" src=\"https://databricks.com/wp-content/uploads/2015/07/image4.png\" alt=\"image4\" width=\"600\" height=\"174\" /></a>Figure 4</p>\n<i>Figure 4</i> above shows that there was one output operation that generated 3 Spark jobs. You can click on the job IDs to continue digging into the stages and tasks for further analysis.\n<h2>Execution DAGs of Streaming RDDs</h2>\nOnce you have started analyzing the tasks and stages generated by the batch jobs, it is useful to get a deeper understanding of the execution graph. As shown in the <a href=\"https://databricks.com/blog/2015/06/22/understanding-your-spark-application-through-visualization.html\">previous blog post</a>, Spark 1.4.0 has added visualizations of the execution DAG (that is, directed acyclic graph) that shows the chain of RDD dependencies and how the RDDs are processed with a chain of dependent stages. If these RDDs are generated by DStreams in a streaming application, then the visualization shows additional streaming semantics. Let\u2019s start with a simple streaming word count program in which we count the words received in each batch. See the example <a href=\"https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala\">NetworkWordCount</a>. It uses DStream operations <b><i>flatMap, map</i></b> and <b><i>reduceByKey</i></b> compute the word count. The execution DAG of a Spark job in any batch will look like <i>figure 5</i> below.\n<p style=\"text-align: center;\"><a href=\"https://databricks.com/wp-content/uploads/2015/07/image5.png\"><img class=\"aligncenter wp-image-4466\" src=\"https://databricks.com/wp-content/uploads/2015/07/image5.png\" alt=\"image5\" width=\"400\" height=\"436\" /></a>Figure 5</p>\nThe black dots in the visualization represents the RDDs generated by DStream at batch time 16:06:50. The blue shaded boxes refer to the DStream operations that were used to transform the RDDs, and the pink boxes refer to the stages in which these transformations were executed. Overall this shows the following:\n<ul>\n\t<li>The data was received from a single<b><i> socket text stream</i></b> at batch time 16:06:50</li>\n\t<li>The job used two stages to compute word counts from the data using the transformations <b><i>flatMap</i></b>, <b><i>map</i></b>, and <b><i>reduceByKey</i></b>.</li>\n</ul>\n<p style=\"text-align: left;\">While this was a simple graph, it can get more complex with more input streams and advanced DStream transformations like <b><i>window</i></b> operations and <b><i>updateStateByKey</i></b> operation. For example, if we compute counts over a moving window of 3 batches (that is, using <b><i>reduceByKeyAndWindow</i></b>) using data from two socket text streams, the execution DAG of one of the batch jobs would look like <i>figure 6</i> below:</p>\n<p style=\"text-align: center;\"><a href=\"https://databricks.com/wp-content/uploads/2015/07/image6.png\"><img class=\"aligncenter wp-image-4467\" src=\"https://databricks.com/wp-content/uploads/2015/07/image6.png\" alt=\"image6\" width=\"600\" height=\"148\" /></a>Figure 6</p>\n<i>Figure 6 </i>shows a lot of information about a Spark job that counts words across data from 3 batches:\n<ul>\n\t<li>The first three stages essentially count the words within each of the 3 batches in the window. These are roughly similar to the first stage in the simple <em><strong>NetworkWordCount</strong></em> above, with <em><strong>map</strong></em> and <em><strong>flatMap</strong></em> operations. However note the following differences:\n<ol>\n\t<li>There were two input RDDs, one from each of the two <b><i>socket text streams.</i></b> These two RDDs were <strong><i>union</i>ed</strong> together into a single RDD and then further transformed to generate the per-batch intermediate counts.</li>\n\t<li>Two of these stages are grayed out because the intermediate counts of the older two batches are already cached in memory and hence do not require recomputation. Only the latest batch needs to be computed from scratch.</li>\n</ol>\n</li>\n\t<li>The last stage on the right uses <b><i>reduceByKeyAndWindow</i></b> to combine per-batch word counts into the \u201cwindowed\u201d word counts.</li>\n</ul>\n<p style=\"text-align: left;\">These visualizations enable developers to monitor the status and trends of streaming applications as well as understand their relations with the underlying Spark jobs and execution plans.</p>\n\n<h2>Future Directions</h2>\n<p style=\"text-align: left;\">One significant improvement expected in Spark 1.5.0 is more information about input data in every batch (<a href=\"https://issues.apache.org/jira/browse/SPARK-8701\">JIRA</a>, <a href=\"https://github.com/apache/spark/pull/7081\">PR</a>). For example, if you were using Kafka, the batch details page will show the topics, partitions and offsets processed in that batch. Here is a preview:</p>\n<p style=\"text-align: center;\"><a href=\"https://databricks.com/wp-content/uploads/2015/07/image7.png\"><img class=\"aligncenter wp-image-4468\" src=\"https://databricks.com/wp-content/uploads/2015/07/image7.png\" alt=\"image7\" width=\"600\" height=\"210\" /></a>Figure 7</p>"}
{"status": "publish", "description": null, "creator": "denny", "link": "https://databricks.com/blog/2015/07/10/announcing-sparkhub-a-community-site-for-apache-spark.html", "authors": null, "id": 4495, "categories": ["Announcements", "Company Blog"], "dates": {"publishedOn": "2015-07-10", "tz": "UTC", "createdOn": "2015-07-10"}, "title": "Announcing SparkHub: A Community Site for Apache Spark", "slug": "announcing-sparkhub-a-community-site-for-apache-spark", "content": "Today, we are happy to announce <em>SparkHub</em> (<a href=\"http://sparkhub.databricks.com\">http://sparkhub.databricks.com</a>), a service for the Apache Spark community to easily find the most relevant Spark resources on the web.\n\n<a href=\"http://sparkhub.databricks.com\"><img class=\"alignnone wp-image-4517\" src=\"https://www.databricks.com/wp-content/uploads/2015/07/sparkhub-frontpage-1024x554.png\" alt=\"sparkhub-frontpage\" width=\"750\" height=\"406\" /></a>\n\n<em>SparkHub</em> contains the <a href=\"http://sparkhub.databricks.com/news/\">latest news</a> about Spark, <a href=\"http://sparkhub.databricks.com/videos/\">newest videos</a> of Spark talks, most <a href=\"http://sparkhub.databricks.com/resources/\">recent Spark packages</a>, and upcoming <a href=\"http://sparkhub.databricks.com/events/\">Spark events</a> around the world. \u00a0Want to find the next Spark Meetup close to you? <em>SparkHub</em> also has a <a href=\"http://sparkhub.databricks.com/meetups/\">directory</a> to help you to do so easily.\n\nWe will continue to expand the site in the coming months as we add even more content. We hope <em>SparkHub</em>\u00a0will help you find Spark related information faster than ever. Everything is sourced from the Spark community, and we welcome input from you as well.\n\nPlease check out <em>SparkHub</em> now! If you have content suggestions, questions, or comments, we would like to <a href=\"mailto:sparkhub@databricks.com?Subject=Spark%20Summit%20Content%20Suggestion\">hear from you!</a>\n\n&nbsp;"}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2015/08/03/guest-blog-sequoiadb-connector-for-apache-spark.html", "authors": null, "id": 4496, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2015-08-03", "tz": "UTC", "createdOn": "2015-08-03"}, "title": "Guest blog: SequoiaDB Connector for Apache Spark", "slug": "guest-blog-sequoiadb-connector-for-apache-spark", "content": "This is a guest blog from Tao Wang at <a href=\"http://www.sequoiadb.com/\">SequoiaDB</a>. He is the co-founder and CTO of SequoiaDB, leading its long-term technology vision, and is responsible for the leadership of advanced technology incubations. SequoiaDB is a JSON document-oriented transactional database.\n\n<hr />\n\n<h2>Why We Chose Apache Spark</h2>\nSequoiaDB is a NoSQL database that has the capability to replicate data on different physical nodes and allows users to specify which \u201ccopy of data\u201d that the application should access. It is capable of running analytical and operational workloads simultaneously on the same cluster with minimal I/O or CPU contention.\n\nThe joint solution of Apache Spark and SequoiaDB allows users to build a single platform such that a wide variety of workloads (e.g., interactive SQL and streaming) can run together on the same physical cluster.\n<h2>Making SequoiaDB Work with Spark: The Spark-SequoiaDB Connector</h2>\n<a href=\"http://spark-packages.org/package/SequoiaDB/spark-sequoiadb\">Spark-SequoiaDB Connector</a> is a Spark data source that allows users to read and write data against SequoiaDB collections with Spark SQL. It is used to integrate SequoiaDB and Spark, combining the advantages of a schema-less storage model with dynamic indexing and the power of Spark clusters.\n\n<img class=\" wp-image-4497 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/07/Screen-Shot-2015-07-08-at-11.28.23-AM-1024x443.png\" alt=\"Screen Shot 2015-07-08 at 11.28.23 AM\" width=\"600\" height=\"260\" />\n\nSpark and SequoiaDB can be installed in the same physical environment or in different clusters. The Spark-SequoiaDB Connector pushes down the search conditions to SequoiaDB, and only retrieves the records that match the query predicates. \u00a0This optimization enables analytics against operational data sources without the need to perform ETL between SequoiaDB and Spark.\n\nHere is a code example of how to use the Spark-SequoiaDB Connector in SparkSQL:\n\n<pre>sqlContext.sql(\"CREATE temporary table org_department ( deptno string, deptname string, mgrno string, admrdept string, location string ) using com.sequoiadb.spark OPTIONS ( host 'host-60-0-16-2:50000', collectionspace 'org', collection 'department', username 'sdbreader', password 'sdb_reader_pwd')\")\nres2: org.apache.spark.sql.DataFrame = []\nsqlContext.sql(\"CREATE temporary table org_employee ( empno int, firstnme string, midinit string, lastname string, workdept string, phoneno string, hiredate date, job string, edlevel int, sex string, birthdate date, salary int, bonus int, comm int ) using com.sequoiadb.spark OPTIONS ( host 'host-60-0-16-2:50000', collectionspace 'org', collection 'employee', username 'sdb_reader', password 'sdb_reader_pwd')\")\nres3: org.apache.spark.sql.DataFrame = []\nsqlContext.sql(\"select * from org_department a, org_employee b where a.deptno='D11'\").collect().take(3).foreach(println)\n[D11,MANUFACTURING SYSTEMS,000060,D01,null,10,CHRISTINE,I,HAAS,A00,3978,null,PRES,18,F,null,152750,1000,4220]\n[D11,MANUFACTURING SYSTEMS,000060,D01,null,20,MICHAEL,L,THOMPSON,B01,3476,null,MANAGER,18,M,null,94250,800,3300]\n[D11,MANUFACTURING SYSTEMS,000060,D01,null,30,SALLY,A,KWAN,C01,4738,null,MANAGER,20,F,null,98250,800,3060]</pre>\n\n<h2>Financial Services Industry Use Case: Improved Transaction History Archiving System</h2>\nThe joint solution of Spark and SequoiaDB can help organizations to retain more and get more value out of their data. Here we will showcase a financial services industry example, where a bank implemented an improved transaction history archiving system with Spark and SequoiaDB.\n\nFor the past few decades, most banks run their core banking systems on mainframes. The technical limitations of their mainframe systems meant that transaction history older than one year had to be removed from the mainframes and archived on tapes.\n\nHowever, banking customers today have much higher expectations of customer service than the past, driven by the broad adoption of online and mobile banking. To compete for customers more effectively, one of our customers - who is a large bank - wanted to improve its offering by allowing their customers to search for transactions older than one year.\n\n<img class=\"aligncenter wp-image-4720\" src=\"https://databricks.com/wp-content/uploads/2015/07/Screen-Shot-2015-07-29-at-11.52.34-AM-1024x960.png\" alt=\"Screen Shot 2015-07-29 at 11.52.34 AM\" width=\"400\" height=\"375\" />\n\nUsing SequoiaDB, this bank can save 15 years of all customer transaction data in 50 physical nodes (occupying more than 1PB disk space). This new system allows customers to access their full transaction history easily on the mobile device as well as the website.\n\n<img class=\"aligncenter wp-image-4721\" src=\"https://databricks.com/wp-content/uploads/2015/07/Screen-Shot-2015-07-29-at-11.53.23-AM-1024x784.png\" alt=\"Screen Shot 2015-07-29 at 11.53.23 AM\" width=\"400\" height=\"306\" />\n<h3><b>Financial Services Industry Use Case: Product Recommendation with Spark and SequoiaDB Integration</b></h3>\nOnce the full transaction history of all customers is readily available, the bank built a customer profiling system based on the transaction data to find the appropriate investment products for each customer.\n\nOnce the customer profiling system calculates product recommendations when processing all of the transaction data and logs, these properties are written back to a collection with a tag array for each customer.\n\nThese properties are used by the front desk staff and the recommendation engine to identify the potential interests of each customer. After deploying this system, the success rate of financial derivatives recommendation increased by more than ten times.\n\n<img class=\"aligncenter wp-image-4504\" src=\"https://databricks.com/wp-content/uploads/2015/07/Screen-Shot-2015-07-08-at-11.38.57-AM-1024x481.png\" alt=\"Screen Shot 2015-07-08 at 11.38.57 AM\" width=\"500\" height=\"235\" />\n<h3><b>What next with Spark at SequoiaDB</b></h3>\nMost of our financial customers are interested in streaming (for anti-money laundering and high-frequency trading use cases) or interactive SQL processing (for government supervision). We intend to put more efforts to improve the features and stability of these Apache Spark components, such as helping SparkSQL to support standard SQL2003.\n\nFor more information about Spark-SequoiaDB Connector please visit:\n\n<a href=\"https://github.com/SequoiaDB/spark-sequoiadb\">https://github.com/SequoiaDB/spark-sequoiadb</a>"}
{"status": "publish", "description": "Databricks R Notebooks include the SparkR package by default so that data scientists can effortlessly benefit from the power of Apache Spark in their R analyses.", "creator": "dave_wang", "link": "https://databricks.com/blog/2015/07/13/introducing-r-notebooks-in-databricks.html", "authors": null, "id": 4524, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2015-07-13", "tz": "UTC", "createdOn": "2015-07-13"}, "title": "Introducing R Notebooks in Databricks", "slug": "introducing-r-notebooks-in-databricks", "content": "Apache Spark 1.4 was <a href=\"https://databricks.com/blog/2015/06/11/announcing-apache-spark-1-4.html\">released</a> on June 11 and one of the exciting new features was <a href=\"https://databricks.com/blog/2015/06/09/announcing-sparkr-r-on-spark.html\">SparkR</a>. I am happy to announce that we now support R notebooks and SparkR in Databricks, our <a href=\"https://databricks.com/blog/2015/06/15/databricks-is-now-generally-available.html\">hosted Spark service</a>. Databricks lets you easily use SparkR in an interactive notebook environment or standalone jobs.\n\nR and Spark nicely complement each other for several important use cases in statistics and data science. Databricks R Notebooks include the SparkR package by default so that data scientists can effortlessly benefit from the power of Apache Spark in their R analyses. In addition to SparkR, any R package can be easily installed into the notebook. In this blog post, I will highlight a few of the features in our R Notebooks.\n<h2>Getting Started with SparkR</h2>\n<img class=\" wp-image-4542 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/07/Screen-Shot-2015-07-10-at-1.16.56-PM.png\" alt=\"Screen Shot 2015-07-10 at 1.16.56 PM\" width=\"500\" height=\"222\" />\n\nTo get started with R in Databricks, simply choose R as the language when creating a notebook. \u00a0Since SparkR is a recent addition to Spark, remember to attach the R notebook to any cluster running Spark version 1.4 or later. The SparkR package is imported and configured by default. You can run Spark queries in R:\n\nUsing SparkR you can access and manipulate very large data sets (e.g., terabytes of data) from distributed storage (e.g., Amazon S3) or data warehouses (e.g., Hive).\n\n<pre>airlinesDF <- read.df(sqlContext, path=\"dbfs:/databricks-datasets/airlines\", \n   source=\"com.databricks.spark.csv\", header=\"true\")\nregisterTempTable(airlinesDF, \"airlines\")</pre>\n\nSparkR offers distributed DataFrames that are syntax compatible with R data frames. You can also collect a SparkR DataFrame to local data frames.\n\n<pre>delays <- collect(sql(sqlContext, \"select avg(Distance) as distance, \n  avg(ArrDelay) as arrivalDelay, \n  avg(DepDelay) as departureDelay, \n  Origin, \n  Dest, \n  UniqueCarrier as carrier from airlines group by Origin, Dest, UniqueCarrier\"))</pre>\n\nFor an overview of SparkR features see our recent <a href=\"https://databricks.com/blog/2015/06/09/announcing-sparkr-r-on-spark.html\">blog post</a>. Additional details on SparkR API can be found on the <a href=\"http://spark.apache.org/docs/latest/api/R/index.html\">Spark website.</a>\n<h2>Autocomplete and Libraries</h2>\n<img class=\" wp-image-4526 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/07/Screen-Shot-2015-07-09-at-1.40.09-PM.png\" alt=\"Screen Shot 2015-07-09 at 1.40.09 PM\" width=\"500\" height=\"247\" />Databricks R notebooks offer autocomplete similar to the R shell. Pressing TAB will complete the code or present available options if multiple exist.\n\nYou can install any R library in your notebooks using <code>install.packages()</code>. Once you import the new library, autocomplete will also apply to the newly introduced methods and objects.\n<h2>Interactive Visualization</h2>\nAt Databricks we believe visualization is a critical part of data analysis. As a result we embraced R\u2019s powerful visualization and complemented it with many additional visualization features.\n<h3>Inline plots</h3>\nIn R Notebooks you can use any R visualization library, including base plotting, <a href=\"http://ggplot2.org/\">ggplot</a>, Lattice, or any other plotting library. Plots are displayed inline in the notebook and can be conveniently resized with the mouse.\n\n<pre>library(ggplot2)\np <- ggplot(delays, aes(departureDelay, arrivalDelay)) +  \n  geom_point(alpha = 0.2) + facet_wrap(~carrier)\np</pre>\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/07/ggplot1.png\"><img class=\"aligncenter wp-image-4534\" src=\"https://databricks.com/wp-content/uploads/2015/07/ggplot1.png\" alt=\"ggplot1\" width=\"700\" height=\"700\" /></a>\n\nYou can set options to change aspect ratio and resolution of inline plots.\n\n<pre>options(repr.plot.height = 500, repr.plot.res = 120)\np + geom_point(aes(color = Dest)) + geom_smooth() + \n  scale_x_log10() + scale_y_log10() + theme_bw()</pre>\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/07/ggplot2.png\"><img class=\"aligncenter wp-image-4535\" src=\"https://databricks.com/wp-content/uploads/2015/07/ggplot2.png\" alt=\"ggplot2\" width=\"700\" height=\"365\" /></a>\n<h3>One-click visualizations</h3>\nYou can use Databricks\u2019s built-in<code> display()</code> function on any R or SparkR DataFrame. The result will be rendered as a table in the notebook, which you can then plot with one click without writing any custom code.\n\n<img class=\"aligncenter wp-image-4536 size-full\" src=\"https://databricks.com/wp-content/uploads/2015/07/display-animation.gif\" alt=\"display animation\" width=\"700\" height=\"260\" />\n<h3>Advanced interactive visualizations</h3>\nSimilar to other Databricks notebooks, you can use <code>displayHTML()</code> function in R notebooks to render any HTML and Javascript visualization.\n<h1>Running Production Jobs</h1>\nDatabricks is an end-to-end solution to make building a data pipeline easier - from ingest to production. The same concept applies to R Notebooks as well: You can schedule your R notebooks to run as jobs on existing or new Spark clusters. The results of each job run, including visualizations, are available to browse, making it much simpler and faster to turn the work of data scientists into production.\n\n<img class=\" wp-image-4537 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/07/Screen-Shot-2015-07-10-at-1.09.54-PM-1024x488.png\" alt=\"Screen Shot 2015-07-10 at 1.09.54 PM\" width=\"700\" height=\"334\" />\n<h2>Summary</h2>\nR Notebooks in Databricks let anyone familiar with R take advantage of the power of Spark through simple Spark cluster management, rich one-click visualizations, and instant deployment to production jobs. We believe SparkR and R Notebooks will bring even more people to the rapidly growing Spark community.\n\nTo try out the powerful R Notebooks for yourself, <a href=\"https://accounts.cloud.databricks.com/registration.html#signup\">sign-up for a 14-day free trial of Databricks today</a>!"}
{"status": "publish", "description": "In this blog post, we introduce the new window function feature that was added in Spark 1.4.", "creator": "michael", "link": "https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html", "authors": null, "id": 4569, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2015-07-15", "tz": "UTC", "createdOn": "2015-07-15"}, "title": "Introducing Window Functions in Spark SQL", "slug": "introducing-window-functions-in-spark-sql", "content": "In this blog post, we introduce the new window function feature that was added in <a href=\"https://databricks.com/blog/2015/06/11/announcing-apache-spark-1-4.html\">Apache Spark 1.4</a>. Window functions allow users of Spark SQL to calculate results such as the rank of a given row or a moving average over a range of input rows. They significantly improve the expressiveness of Spark\u2019s SQL and DataFrame APIs. This blog will first introduce the concept of window functions and then discuss how to use them with Spark SQL and Spark\u2019s DataFrame API.\n<h2>What are Window Functions?</h2>\nBefore 1.4, there were two kinds of functions supported by Spark SQL that could be used to calculate a single return value. <i>Built-in functions</i> or <i>UDFs</i>, such as\u00a0<code>substr</code> or\u00a0<code>round</code>, take values from a single row as input, and they generate a single return value for every input row. <i>Aggregate functions, </i>such as <code>SUM</code> or <code>MAX</code><i>,</i> operate on a group of rows and calculate a single return value for every group.\n\nWhile these are both very useful in practice, there is still a wide range of operations that cannot be expressed using these types of functions alone. Specifically, there was no way to both operate on a group of rows while still returning a single value for every input row. This limitation makes it hard to conduct various data processing tasks like calculating a moving average, calculating a cumulative sum, or accessing the values of a row appearing before the current row. Fortunately for users of Spark SQL, window functions fill this gap.\n\nAt its core, a window function calculates a return value for every input row of a table based on a group of rows, called the <i>Frame</i>. Every input row can have a unique frame associated with it. This characteristic of window functions makes them more powerful than other functions and allows users to express various data processing tasks that are hard (if not impossible) to be expressed without window functions in a concise way. Now, let\u2019s take a look at two examples.\n\nSuppose that we have a <i>productRevenue</i> table as shown below.\n\n<img class=\"aligncenter wp-image-4572\" src=\"https://databricks.com/wp-content/uploads/2015/07/1-1.png\" alt=\"1-1\" width=\"300\" height=\"353\" />\n\nWe want to answer two questions:\n<ol>\n \t<li>What are the best-selling and the second best-selling products in every category?</li>\n \t<li>What is the difference between the revenue of each product and the revenue of the best-selling product in the same category of that product?</li>\n</ol>\nTo answer the first question \u201c<i>What are the best-selling and the second best-selling products in every category?</i>\u201d, we need to rank products in a category based on their revenue, and to pick the best selling and the second best-selling products based the ranking. Below is the SQL query used to answer this question by using window function <code>dense_rank</code> (we will explain the syntax of using window functions in next section).\n<pre>SELECT\n  product,\n  category,\n  revenue\nFROM (\n  SELECT\n    product,\n    category,\n    revenue,\n    dense_rank() OVER (PARTITION BY category ORDER BY revenue DESC) as rank\n  FROM productRevenue) tmp\nWHERE\n  rank &lt;= 2\n</pre>\nThe result of this query is shown below. Without using window functions, it is very hard to express the query in SQL, and even if a SQL query can be expressed, it is hard for the underlying engine to efficiently evaluate the query.\n\n<img class=\" wp-image-4574 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/07/1-2.png\" alt=\"1-2\" width=\"300\" height=\"184\" />\n\nFor the second question \u201c<i>What is the difference between the revenue of each product and the revenue of the best selling product in the same category as that product?</i>\u201d, to calculate the revenue difference for a product, we need to find the highest revenue value from products in the same category for each product. Below is a Python DataFrame program used to answer this question.\n<pre>import sys\nfrom pyspark.sql.window import Window\nimport pyspark.sql.functions as func\nwindowSpec = \\\n  Window \n    .partitionBy(df['category']) \\\n    .orderBy(df['revenue'].desc()) \\\n    .rangeBetween(-sys.maxsize, sys.maxsize)\ndataFrame = sqlContext.table(\"productRevenue\")\nrevenue_difference = \\\n  (func.max(dataFrame['revenue']).over(windowSpec) - dataFrame['revenue'])\ndataFrame.select(\n  dataFrame['product'],\n  dataFrame['category'],\n  dataFrame['revenue'],\n  revenue_difference.alias(\"revenue_difference\"))\n</pre>\nThe result of this program is shown below. Without using window functions, users have to find all highest revenue values of all categories and then join this derived data set with the original <em>productRevenue</em> table to calculate the revenue differences.\n\n<img class=\"aligncenter wp-image-4578\" src=\"https://databricks.com/wp-content/uploads/2015/07/1-3.png\" alt=\"1-3\" width=\"400\" height=\"287\" />\n<h2>Using Window Functions</h2>\nSpark SQL supports three kinds of window functions: ranking functions, analytic functions, and aggregate functions. The available ranking functions and analytic functions are summarized in the table below. For aggregate functions, users can use any existing aggregate function as a window function.\n<table class=\"table\">\n<tbody>\n<tr>\n<td></td>\n<td><strong>SQL</strong></td>\n<td><strong>DataFrame API</strong></td>\n</tr>\n<tr>\n<td rowspan=\"5\"><strong>Ranking functions</strong></td>\n<td>rank</td>\n<td>rank</td>\n</tr>\n<tr>\n<td>dense_rank</td>\n<td>denseRank</td>\n</tr>\n<tr>\n<td>percent_rank</td>\n<td>percentRank</td>\n</tr>\n<tr>\n<td>ntile</td>\n<td>ntile</td>\n</tr>\n<tr>\n<td>row_number</td>\n<td>rowNumber</td>\n</tr>\n<tr>\n<td rowspan=\"5\"><strong>Analytic functions</strong></td>\n<td>cume_dist</td>\n<td>cumeDist</td>\n</tr>\n<tr>\n<td>first_value</td>\n<td>firstValue</td>\n</tr>\n<tr>\n<td>last_value</td>\n<td>lastValue</td>\n</tr>\n<tr>\n<td>lag</td>\n<td>lag</td>\n</tr>\n<tr>\n<td>lead</td>\n<td>lead</td>\n</tr>\n</tbody>\n</table>\nTo use window functions, users need to mark that a function is used as a window function by either\n<ul>\n \t<li>Adding an <i>OVER</i> clause after a supported function in SQL, e.g. <code>avg(revenue) OVER (...)</code>; or</li>\n \t<li>Calling the <i>over</i> method on a supported function in the DataFrame API, e.g. <code>rank().over(...)</code><i>.</i></li>\n</ul>\nOnce a function is marked as a window function, the next key step is to define the <i>Window Specification </i>associated with this function. A window specification defines which rows are included in the frame associated with a given input row. A window specification includes three parts:\n<ol>\n \t<li>Partitioning Specification: controls which rows will be in the same partition with the given row. Also, the user might want to make sure all rows having the same value for \u00a0the category column are collected to the same machine before ordering and calculating the frame. \u00a0If no partitioning specification is given, then all data must be collected to a single machine.</li>\n \t<li>Ordering Specification: controls the way that rows in a partition are ordered, determining the position of the given row in its partition.</li>\n \t<li>Frame Specification: states which rows will be included in the frame for the current input row, based on their relative position to the current row. \u00a0For example, \"the three rows preceding the current row to the current row\" describes a frame including the current input row and three rows appearing before the current row.</li>\n</ol>\nIn SQL, the <code>PARTITION BY</code> and <code>ORDER BY</code> keywords are used to specify partitioning expressions for the partitioning specification, and ordering expressions for the ordering specification, respectively. The SQL syntax is shown below.\n\n<code>OVER (PARTITION BY ... ORDER BY ...)</code>\n\nIn the DataFrame API, we provide utility functions to define a window specification. Taking Python as an example, users can specify partitioning expressions and ordering expressions as follows.\n<pre>from pyspark.sql.window import Window\nwindowSpec = \\\n  Window \\\n    .partitionBy(...) \\\n    .orderBy(...)\n</pre>\nIn addition to the ordering and partitioning, users need to define the start boundary of the frame, the end boundary of the frame, and the type of the frame, which are three components of a frame specification.\n\nThere are five types of boundaries, which are<code> UNBOUNDED PRECEDING</code>, <code>UNBOUNDED FOLLOWING</code>, <code>CURRENT ROW</code>, <code>&lt;value&gt; PRECEDING</code>, and <code>&lt;value&gt; FOLLOWING</code>. <code>UNBOUNDED PRECEDING</code> and <code>UNBOUNDED FOLLOWING</code> represent the first row of the partition and the last row of the partition, respectively. For the other three types of boundaries, they specify the offset from the position of the current input row and their specific meanings are defined based on the type of the frame. There are two types of frames, <i>ROW</i> frame and <i>RANGE</i> frame.\n\n<b>ROW frame</b>\n\nROW frames are based on physical offsets from the position of the current input row, which means that <code>CURRENT ROW</code>, <code>&lt;value&gt; PRECEDING</code>, or <code>&lt;value&gt; FOLLOWING</code> specifies a physical offset. If <code>CURRENT ROW</code> is used as a boundary, it represents the current input row. <code>&lt;value&gt; PRECEDING</code> and <code>&lt;value&gt; FOLLOWING</code> describes the number of rows appear before and after the current input row, respectively. The following figure illustrates a ROW frame with a<code> 1 PRECEDING</code> as the start boundary and <code>1 FOLLOWING</code> as the end boundary (<code>ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING</code> in the SQL syntax).\n\n<img class=\"aligncenter wp-image-4581\" src=\"https://databricks.com/wp-content/uploads/2015/07/2-1-1024x338.png\" alt=\"2-1\" width=\"700\" height=\"231\" />\n\n<b>RANGE frame</b>\n\nRANGE frames are based on logical offsets from the position of the current input row, and have similar syntax to the ROW frame. A logical offset is the difference between the value of the ordering expression of the current input row and the value of that same expression of the boundary row of the frame. Because of this definition, when a RANGE frame is used, only a single ordering expression is allowed. Also, for a RANGE frame, all rows having the same value of the ordering expression with the current input row are considered as same row as far as the boundary calculation is concerned.\n\nNow, let\u2019s take a look at an example. In this example, the ordering expressions is <code>revenue</code>; the start boundary is <code>2000 PRECEDING</code>; and the end boundary is <code>1000 FOLLOWING</code> (this frame is defined as <code>RANGE BETWEEN 2000 PRECEDING AND 1000 FOLLOWING</code> in the SQL syntax). The following five figures illustrate how the frame is updated with the update of the current input row. Basically, for every current input row, based on the value of revenue, we calculate the revenue range <code>[current revenue value - 2000, current revenue value + 1000]</code>. All rows whose revenue values fall in this range are in the frame of the current input row.\n\n<img class=\"aligncenter wp-image-4582\" src=\"https://databricks.com/wp-content/uploads/2015/07/2-2-1024x369.png\" alt=\"2-2\" width=\"700\" height=\"252\" />\n\n<img class=\" wp-image-4583 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/07/2-3-1024x263.png\" alt=\"2-3\" width=\"700\" height=\"180\" />\n\n<img class=\" wp-image-4584 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/07/2-4-1024x263.png\" alt=\"2-4\" width=\"700\" height=\"180\" />\n\n<img class=\"aligncenter wp-image-4585\" src=\"https://databricks.com/wp-content/uploads/2015/07/2-5-1024x263.png\" alt=\"2-5\" width=\"700\" height=\"180\" />\n\n<img class=\"aligncenter wp-image-4586\" src=\"https://databricks.com/wp-content/uploads/2015/07/2-6-1024x263.png\" alt=\"2-6\" width=\"700\" height=\"180\" />\n\nIn summary, to define a window specification, users can use the following syntax in SQL.\n\n<code>OVER (PARTITION BY ... ORDER BY ... frame_type BETWEEN start AND end)</code>\n\nHere, <code>frame_type</code> can be either ROWS (for ROW frame) or RANGE (for RANGE frame); <code>start</code> can be any of <code>UNBOUNDED PRECEDING</code>, <code>CURRENT ROW</code>, <code>&lt;value&gt; PRECEDING</code>, and <code>&lt;value&gt; FOLLOWING</code>; and <code>end</code> can be any of <code>UNBOUNDED FOLLOWING</code>, <code>CURRENT ROW</code>, <code>&lt;value&gt; PRECEDING</code>, and <code>&lt;value&gt; FOLLOWING.</code>\n\nIn the Python DataFrame API, users can define a window specification as follows.\n<pre>from pyspark.sql.window import Window\n# Defines partitioning specification and ordering specification.\nwindowSpec = \\\n  Window \\\n    .partitionBy(...) \\\n    .orderBy(...)\n# Defines a Window Specification with a ROW frame.\nwindowSpec.rowsBetween(start, end)\n# Defines a Window Specification with a RANGE frame.\nwindowSpec.rangeBetween(start, end)\n</pre>\n<h2>What\u2019s next?</h2>\nSince the release of Spark 1.4, we have been actively working with community members on optimizations that improve the performance and reduce the memory consumption of the operator evaluating window functions. Some of these will be added in Spark 1.5, and others will be added in our future releases. Besides performance improvement work, there are two features that we will add in the near future to make window function support in Spark SQL even more powerful. First, we have been working on adding Interval data type support for Date and Timestamp data types (<a href=\"https://issues.apache.org/jira/browse/SPARK-8943\">SPARK-8943</a>). With the Interval data type, users can use intervals as values specified in <code>&lt;value&gt; PRECEDING</code> and <code>&lt;value&gt; FOLLOWING</code> for RANGE frame, which makes it much easier to do various time series analysis with window functions. Second, we have been working on adding the support for user-defined aggregate functions in Spark SQL (<a href=\"https://issues.apache.org/jira/browse/SPARK-3947\">SPARK-3947</a>). With our window function support, users can immediately use their user-defined aggregate functions as window functions to conduct various advanced data analysis tasks.\n\n<em>To try out these Spark features, <a href=\"https://databricks.com/try-databricks\">get a free trial of Databricks or use the Community Edition</a>.</em>\n<h2>Acknowledgements</h2>\nThe development of the window function support in Spark 1.4 is is a joint work by many members of the Spark community. In particular, we would like to thank Wei Guo for contributing the initial patch."}
{"status": "publish", "description": "We are proud to announce that support for the Apache Optimized Row Columnar (ORC) file format is included in Apache Spark 1.4 as a new data source. This support was added through a collaboration between Hortonworks and Databricks, tracked by SPARK-2883.", "creator": "patrick", "link": "https://databricks.com/blog/2015/07/16/joint-blog-post-bringing-orc-support-into-apache-spark.html", "authors": null, "id": 4600, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2015-07-16", "tz": "UTC", "createdOn": "2015-07-16"}, "title": "Joint Blog Post: Bringing ORC Support into Apache Spark", "slug": "joint-blog-post-bringing-orc-support-into-apache-spark", "content": "This is a joint blog post with our partner Hortonworks.\u00a0Zhan Zhang is a member of technical staff at Hortonworks, where he collaborated with the Databricks team on this new feature.\n\n<hr />\n\nIn version 1.2.0, Apache Spark introduced a <a href=\"https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html\">Data Source API</a> (<a href=\"https://issues.apache.org/jira/browse/SPARK-3247\">SPARK-3247</a>) to enable deep platform integration with a larger number of data sources and sinks. We are proud to announce that support for the Apache <a href=\"https://orc.apache.org/\">Optimized Row Columnar</a> (ORC) file format is included in Spark 1.4 as a new data source. This support was added through a collaboration between Hortonworks and Databricks, tracked by <a href=\"https://issues.apache.org/jira/browse/SPARK-2883\">SPARK-2883</a>.\n\nThe Apache ORC file format and associated libraries recently became <a href=\"https://orc.apache.org/\">a top level project</a> at the Apache Software Foundation. ORC is a self-describing type-aware columnar file format designed for Hadoop ecosystem workloads. The columnar format lets the reader read, decompress, and process only the columns that are required for the current query. In addition, it has support for ACID transactions and snapshot isolation, build-in indexes and complex types. Many large Hadoop deployments rely on ORC, including those at Yahoo! and Facebook.\n\nSpark\u2019s ORC support leverages recent improvements to the data source API included in Spark 1.4 (<a href=\"https://issues.apache.org/jira/browse/SPARK-5180\">SPARK-5180</a>). This API makes it easier to bring more data to Spark by simply providing new data source implementations. The API includes support for optimizations such as data partitioning and filter push-down. Since these concepts are now first class in the data source API, new data source implementations only need to focus on the data format specific logic in the physical plan execution without worrying about higher layer query plan optimization.\n\nAs ORC is one of the primary file formats supported in Apache Hive, users of Spark\u2019s SQL and DataFrame APIs will now have fast access to ORC data contained in Hive tables.\n<h2>Accessing ORC in Spark</h2>\nSpark\u2019s ORC data source supports complex data types (i.e., array, map, and struct), and provides read and write access to ORC files. It leverages Spark SQL's Catalyst engine to do common optimizations, such as column pruning, predicate push-down, and partition pruning, etc.\n\nWe\u2019ll now give several examples of Spark\u2019s ORC integration and show how such optimizations are applied to user programs. To get started, Spark\u2019s ORC support requires only a HiveContext instance:\n\n<pre>import org.apache.spark.sql._\nval sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)</pre>\n\nOur examples will use a few data structures to demonstrate working with complex types. The <b>Person</b> struct has name, age, and a sequence of <b>Contact</b>\u2019s, which are themselves defined by names and phone numbers.\n\n<pre>case class Contact(name: String, phone: String)\ncase class Person(name: String, age: Int, contacts: Seq[Contact])</pre>\n\nWe create 100 records as below to be used in our example. In the physical file, they will be saved in the columnar format, but users still see rows when accessing ORC files via DataFrame API. Each row represents one Person record.\n\n<pre>val records = (1 to 100).map { i =>\n    Person(s\"name_$i\", i, (0 to 1).map { m => Contact(s\"contact_$m\", s\"phone_$m\") })\n}</pre>\n\n<h2>Reading and Writing with ORC</h2>\nSpark\u2019s <b>DataFrameReader</b> and <b>DataFrameWriter</b> are used to access ORC files, in a similar manner to other data sources.\n\nWe can write People objects as ORC files to directory \u201cpeople\u201d using\n\n<pre>sc.parallelize(records).toDF().write.format(\"orc\").save(\"people\")</pre>\n\nFurthermore, we can read it back by\n\n<pre>val people = sqlContext.read.format(\"orc\").load(\"people\")</pre>\n\nFor reuse in future operations, we register it as a temporary table \u201cpeople\u201d as below:\n\n<pre>people.registerTempTable(\"people\")</pre>\n\n<h2>Column Pruning</h2>\nNow the table is registered as a temporary table named \u201cpeople\u201d. The following SQL query references two columns from the underlying table. At runtime, the physical table scan will only load columns <b>name</b> and <b>age</b>, without reading the <b>contacts</b> column from the file system, and thus speeds up read performance:\n\n<pre>sql(\"SELECT name FROM people WHERE age < 15\").count()</pre>\n\nORC saves IO bandwidth by only touching required columns, and requires significantly fewer seek operations because all columns within a single stripe are stored together on disk.\n<h2>Predicate Push-down</h2>\nThe columnar nature of the ORC format helps to avoid reading unnecessary columns. However, we are still reading unnecessary rows even if the query has <i>WHERE</i> clause filter. In our example, we have to read all rows with age between 0 and 100, although only the rows with age less than 15 are required and all others will be discarded. \u00a0Such full table scanning is an expensive operation.\n\nORC is able to avoid this type of overhead by performing predicate push-down with its build-in indexes. \u00a0ORC provides three level of indexes within each file, file level, stripe level, and row level. The file and stripe level statistics are in the file footer so that they are easy to access to determine if the rest of the file needs to be read at all. Row level indexes include both column statistics for each row group and position for seeking to the start of the row group. ORC utilizes these indexes to moves the filter operation to the data loading phase by only reading the data that potentially includes required rows..\n\nThe combination of indexed data and columnar storage reduces disk IO significantly, especially for larger datasets where IO bandwidth becomes the main bottleneck for performance.\n\nBy default, ORC predicate push-down is disabled in the Spark SQL and need to be explicitly enabled:\n\n<pre>sqlContext.setConf(\"spark.sql.orc.filterPushdown\", \"true\")</pre>\n\n<h2>Partition Pruning</h2>\nWhen predicate pushdown is not applicable, for example if all stripes containing records matching the predicate condition, a query with <i>WHERE</i> clause filter may need to read the entire data set, which becomes a bottleneck over a large table. Partition pruning is another optimization method that can avoid reading large amounts of data by exploiting query semantics.\n\nPartition pruning is possible when data within a table is split across multiple logical partitions. Each partition corresponds to a particular value(s) of partition column(s) and is stored as a sub-directory within the table\u2019s root directory on HDFS. When the table is queried, where applicable, only the required partitions (subdirectories) of the table are queried, thereby avoiding unnecessary IO.\n\nSpark supports saving data out in a partitioned layout seamlessly, through the <b>partitionBy </b>method available during data source writes. In this example we partition the people table by the \u201cage\u201d column:\n\n<pre>person.write.format(\"orc\").partitionBy(\"age\").save(\"peoplePartitioned\")</pre>\n\nRecords will be automatically partitioned by the age field and saved into different directories, for example, peoplePartitioned/age=1/, peoplePartitioned/age=2/, etc.\n\nAfter partitioning the data, future queries which access the data will be able to skip large amounts of IO when the partition column is referenced in predicates. For example, following query will automatically locate and load the file under peoplePartitioned/age=20/ only, and skip all others.\n\n<pre>val peoplePartitioned = sqlContext.read.format(\"orc\").load(\"peoplePartitioned\")\npeoplePartitioned.registerTempTable(\"peoplePartitioned\")\nsql(\"SELECT * FROM peoplePartitioned WHERE age = 20\")</pre>\n\n<h2>DataFrame Support</h2>\nSpark 1.3 added a new DataFrame API. DataFrames look similar to Spark\u2019s RDDs, but have higher level semantics built into their operators, allowing optimization to be pushed down to the underlying query engine. ORC data can be conveniently loaded into DataFrames.\n\nHere's the Scala API translation of the SELECT query above using the DataFrame API\n\n<pre>val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)\nsqlContext.setConf(\"spark.sql.orc.filterPushdown\", \"true\") \nval people = sqlContext.read.format(\"orc\").load(\"peoplePartitioned\")\npeople.filter(people(\"age\")<15).select(\"name\").show()</pre>\n\nAnd of course, DataFrames aren\u2019t limited to Scala, there is a Java API, and, for data scientists, a Python API binding:\n\n<pre>sqlContext = HiveContext(sc)\nsqlContext.setConf(\"spark.sql.orc.filterPushdown\", \"true\") \npeople = sqlContext.read.format(\"orc\").load(\"peoplePartitioned\")\npeople.filter(people.age < 15).select(\"name\").show()</pre>\n\nThat's it! Simply save your data in ORC, adopt the DataFrame API for working with datasets, and you can significantly speedup queries over large datasets. And we haven't even looked at the compression and run-length encoding features yet\u2014both of which can reduce the IO bandwidth even further.\n<h2>Putting It All Together</h2>\nWe've just given a quick overview of how Spark 1.4 supports ORC files. The combination of the ORC storage format, optimized for query performance, and the DataFrame API means that Spark applications can work with data stored in ORC files as easily as any other data source, yet gain significant performance advantages compared to unoptimized storage formats. And because it can also be used by other tools and applications in the Hadoop stack, ORC-formatted data generated by other parts of a large system, can be easily consumed by Spark applications and other interactive tools.\n<h2>What\u2019s Next?</h2>\nCurrently, the code for Spark SQL ORC support is under package org.apache.spark.sql.hive and must be used together with Spark SQL's HiveContext. This is because ORC is still tightly coupled with Hive for now. However, it doesn't require existing Hive installation to access ORC files.\n\nNow that ORC has already become an independent Apache top level project. After decoupling ORC from Hive, Hive dependencies will not be necessary to access ORC files.\n\nWe look forward to helping producing a future version of Apache Spark which makes ORC even easier to work with.\n<h2>Further Information</h2>\nIf you want to know more about Spark's ORC Support, <a href=\"http://spark.apache.org/\">download Apache Spark</a> 1.4.0 or later versions, and explore the new features through the <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html\">DataFrame API</a>.\n<h2>References</h2>\n<ul>\n\t<li>Apache ORC website: <a href=\"https://orc.apache.org/\">https://orc.apache.org/</a></li>\n\t<li>ORC performance: <a href=\"http://hortonworks.com/blog/orcfile-in-hdp-2-better-compression-better-performance/\">http://hortonworks.com/blog/orcfile-in-hdp-2-better-compression-better-performance/</a></li>\n</ul>"}
{"status": "publish", "description": null, "creator": "matei", "link": "https://databricks.com/blog/2015/07/21/be-heard-with-the-spark-survey.html", "authors": null, "id": 4625, "categories": ["Announcements", "Company Blog"], "dates": {"publishedOn": "2015-07-21", "tz": "UTC", "createdOn": "2015-07-21"}, "title": "Be Heard with the Spark Survey", "slug": "be-heard-with-the-spark-survey", "content": "<p dir=\"ltr\"><strong>Please note that this survey is now closed.</strong></p>\n<p dir=\"ltr\">At Databricks, we are constantly working to improve Apache Spark. To help us and the Spark community, we would love to hear from you to help set Spark\u2019s future direction. A recent\u00a0example of the community helping to direct Spark would be SparkR. As noted in the Datanami article\u00a0<a href=\"http://www.datanami.com/2015/07/13/python-versus-r-in-apache-spark/\" target=\"_blank\">Python Versus R in Apache Spark</a>,\u00a0we were\u00a0bombarded with requests over the past year to add support for R in Apache Spark. \u00a0Because of the overwhelming response, together with\u00a0the Spark community we released SparkR as part of last month's <a href=\"https://databricks.com/blog/2015/06/11/announcing-apache-spark-1-4.html\" target=\"_blank\">Spark 1.4 release</a>.</p>\n<p dir=\"ltr\">To give you a voice in future releases of Apache Spark, we have designed a short 15-min survey. \u00a0In it we would like to\u00a0better understand what technology integrations you are building, \u00a0your primary scenarios for using Spark, and the type of environments you are working in. \u00a0We hope you will take some\u00a0time to fill it out to\u00a0help us better understand your needs and to improve Spark.</p>\n<p dir=\"ltr\">In appreciation, you will be entered to win one of <strong>THREE</strong> prizes once you complete the survey: $200 Visa card, iPad Mini, or Beats headphones.</p>\nThank you and we hope to hear from you!\n\nTake the <a href=\"https://docs.google.com/forms/d/1gczrU_-oz_GQyeTfDKzc-KkNO3iTi0-mCp-fSRh3Y6c/viewform\" target=\"_blank\">Spark Survey</a>"}
{"status": "publish", "description": "We are happy to announce that Yesware has Databricks to build its production data pipeline, completing the project in record time -- in just under three weeks.", "creator": "kavitha", "link": "https://databricks.com/blog/2015/07/23/yesware-deploys-production-data-pipeline-in-record-time-with-databricks.html", "authors": null, "id": 4633, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2015-07-23", "tz": "UTC", "createdOn": "2015-07-23"}, "title": "Yesware Deploys Production Data Pipeline in Record Time with Databricks", "slug": "yesware-deploys-production-data-pipeline-in-record-time-with-databricks", "content": "We are happy to announce that Yesware chose Databricks to build its production data pipeline, completing the project in record time -- in just under three weeks.\n\nPress release:\u00a0<a href=\"http://www.marketwired.com/press-release/yesware-deploys-production-data-pipeline-in-record-time-with-databricks-2041188.htm\" target=\"_blank\">http://www.marketwired.com/press-release/yesware-deploys-production-data-pipeline-in-record-time-with-databricks-2041188.htm</a>\n\n<a href=\"http://www.yesware.com/\" target=\"_blank\">Yesware</a>,\u00a0the leading sales acceleration software for sales teams at major enterprise companies such as eBay, New Relic, and IBM,\u00a0enables sales professionals to have highly effective and successful engagements by providing analytics on their daily interactions with potential customers using billions of data points,\u00a0including open and reply rate of emails, effectiveness of email templates, engagement rates of e-mails, CTA click-through\u2019s, and more.\n\nYesware encountered many difficulties when it first attempted to build and operate a production data pipeline. It needed to ingest a large volume of data (hundreds of GB per day, and growing rapidly), build highly customizable reports, and must do all of the above with minimum latency. The initial solution was too slow, difficult to maintain, and just not scalable enough. Yesware decided it was time they sourced a better solution.\n\nApache Spark became the big data technology of choice because of its flexible and easy-to-learn API, fast performance, and native support for crucial capabilities such as SQL and machine learning algorithms. Yesware chose Databricks to implement and operationalize a Spark data pipeline. Why? Because the Databricks platform makes building and running production Spark applications much faster and easier with its cluster manager, interactive workspace, and <a href=\"https://databricks.com/blog/2015/03/18/databricks-launches-jobs-feature-for-production-workloads.html\" target=\"_blank\">job scheduler</a>. Databricks also easily integrated with Yesware's existing infrastructure in Amazon Web Services (AWS), further accelerating their adoption and on-ramp processes.\n\nThe benefits Yesware gained with Databricks included:\n<ul>\n\t<li>Deploying a production data pipeline in just under three weeks, compared to over six months with the prior solution.</li>\n\t<li>Processing over 180 days of historical data in two hours. This was a drastic speedup compared to their prior solution that took 12 hours to process 90 days of data.</li>\n\t<li>Improving the efficiency of their infrastructure by reducing the cost to just 10% of the prior solution.</li>\n\t<li>Accelerating the development of new features, enabling their developers and data scientists to collaborate seamlessly, reducing time to prototype new algorithm to hours instead of days.</li>\n</ul>\nDownload this <a title=\"Customer Case Studies\" href=\"https://databricks.com/wp-content/uploads/2015/07/Databricks_Case_Study_Yesware.pdf\" target=\"_blank\">case study</a>\u00a0to learn more about how Yesware is using Databricks.\n\nTo try out Databricks for yourself, <a href=\"http://dbricks.co/1g8cHFA\" target=\"_blank\">sign-up for a 14-day free trial of Databricks today</a>!"}
{"status": "publish", "description": "In an earlier post we described how you can easily integrate your favorite IDE with Databricks to speed up your application development. In this post, we will show you how to import 3rd party libraries, specifically Apache Spark packages, into Databricks by providing Maven coordinates.", "creator": "dave_wang", "link": "https://databricks.com/blog/2015/07/28/using-3rd-party-libraries-in-databricks-apache-spark-packages-and-maven-libraries.html", "authors": null, "id": 4689, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2015-07-28", "tz": "UTC", "createdOn": "2015-07-28"}, "title": "Using 3rd Party Libraries in Databricks: Apache Spark Packages and Maven Libraries", "slug": "using-3rd-party-libraries-in-databricks-apache-spark-packages-and-maven-libraries", "content": "In an earlier post, we described how you can <a href=\"https://databricks.com/blog/2015/06/05/making-databricks-cloud-better-for-developers-ide-integration.html\">easily integrate your favorite IDE with Databricks</a> to speed up your application development. In this post, we will show you how to import 3rd party libraries, specifically Apache Spark packages, into Databricks by providing Maven coordinates.\n<h2>Background on Spark Packages</h2>\n<img class=\" wp-image-4690 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/07/spark-packages-1024x584.png\" alt=\"spark-packages\" width=\"599\" height=\"342\" />\n\nSpark Packages (<a href=\"http://spark-packages.org\">http://spark-packages.org</a>) is a community package index for libraries built on top of Apache Spark. The purpose of Spark Packages is to bridge the gap between Spark developers and users. Without Spark Packages, you need to to go multiple repositories, such as GitHub, PyPl, and Maven Central, to find the libraries you want. This makes the search for a package that fits your needs a pain - the goal of Spark Packages is to simplify this process for you by becoming the one-stop-shop for your search.\n\nAt the time of this writing, there are 95 packages on Spark Packages, with a number of new packages appearing daily. These packages range from pluggable data sources and data formats for DataFrames (such as spark-csv, spark-avro, spark-redshift, spark-cassandra-connector, hbase) to machine learning algorithms, to deployment scripts that enable Spark deployment in cloud environments.\n<h2>Support for Spark Packages and Maven libraries in Databricks</h2>\nDid you know that you could download libraries from any public Maven repository, including all its dependencies, with a few clicks to Databricks? Databricks provides you with a browser that allows you to search both Spark Packages and Maven Central. Here\u2019s how it all works:\n\nSelect where you would like to create the library in the <strong>Workspace</strong>, and open the <strong>Create Library</strong> dialog:\n\n<img class=\" wp-image-4691 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/07/create-lib.png\" alt=\"create-lib\" width=\"300\" height=\"168\" />\n\nFrom the <strong>Source</strong> drop-down menu, select <strong>Maven Coordinate</strong>:\n\n<img class=\" wp-image-4692 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/07/select-maven-1024x711.png\" alt=\"select-maven\" width=\"400\" height=\"278\" />\n\nIf you already know the Maven coordinate, you can enter it directly to create the library in Databricks instantly. Alternatively, you can also browse Maven libraries and Spark Packages to look through your options by clicking the <strong>Search Spark Packages and Maven Central</strong>\u00a0button without entering a coordinate.\n\n<img class=\" wp-image-4693 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/07/maven-page-1024x571.png\" alt=\"maven-page\" width=\"400\" height=\"223\" />\n\nNow, all available Spark Packages are at your fingertips! You can sort packages by name, organization, and rating. You can also filter the results by writing a query in the search bar. The results will automatically refresh.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/07/browser.png\"><img class=\"aligncenter wp-image-4694\" src=\"https://databricks.com/wp-content/uploads/2015/07/browser-1024x548.png\" alt=\"browser\" width=\"800\" height=\"428\" /></a>\n\n&nbsp;\n\nIf you want to find out more details about a\u00a0package, simply click on its name in the browser.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/07/sp-detail.png\"><img class=\" wp-image-4695 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/07/sp-detail-1024x561.png\" alt=\"sp-detail\" width=\"800\" height=\"438\" /></a>\n\n&nbsp;\n\nTo browse <strong>Maven Central</strong>,\u00a0by select the Maven Central option from the drop-down menu on the top right.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/07/maven-central.png\"><img class=\"aligncenter wp-image-4696\" src=\"https://databricks.com/wp-content/uploads/2015/07/maven-central-1024x561.png\" alt=\"maven-central\" width=\"800\" height=\"438\" /></a>\n\n&nbsp;\n\nAfter you identify the package you are interested in, you can choose the release version from the drop-down menu. Once you click the\u00a0<strong>Select</strong>\u00a0button, the <strong>Maven Coordinate</strong>\u00a0field from the previous menu will be automatically filled for you.\n\nYou can also provide more advanced options, such as the URL for a repository, and any dependencies that you would like to exclude (in case of dependency conflicts).\n\n<img class=\"aligncenter wp-image-4698\" src=\"https://databricks.com/wp-content/uploads/2015/07/resolving-1024x757.png\" alt=\"resolving\" width=\"400\" height=\"296\" />\n\n&nbsp;\n\nOnce you click the <strong>Create Library</strong> button, and the library and all its dependencies will be fetched for you automatically!\n\n<img class=\"aligncenter wp-image-4699\" src=\"https://databricks.com/wp-content/uploads/2015/07/resolved-1024x311.png\" alt=\"resolved\" width=\"600\" height=\"182\" />\n\n&nbsp;\n\nNow you can attach it to any cluster that you wish, and start using the library immediately!\n<h2>Summary</h2>\nIn this post, we showed how simple it is to integrate Spark Packages to your applications in Databricks using Maven coordinate support. Spark Packages help you to find the code you need to get the job done, and its tight integration with Databricks makes reusing existing code to speed up your Spark application development even simpler.\n\nTo try out Spark Packages in Databricks for yourself, <a href=\"http://dbricks.co/1g8cHFA\" target=\"_blank\">sign-up for a 14-day free trial today</a>!"}
{"status": "publish", "description": null, "creator": "joseph", "link": "https://databricks.com/blog/2015/07/29/new-features-in-machine-learning-pipelines-in-apache-spark-1-4.html", "authors": null, "id": 4705, "categories": ["Apache Spark", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2015-07-29", "tz": "UTC", "createdOn": "2015-07-29"}, "title": "New Features in Machine Learning Pipelines in Apache Spark 1.4", "slug": "new-features-in-machine-learning-pipelines-in-apache-spark-1-4", "content": "Apache Spark 1.2 introduced Machine Learning (ML) Pipelines to facilitate the creation, tuning, and inspection of practical ML workflows. Spark\u2019s latest release, Spark 1.4, significantly extends the ML library. \u00a0In this post, we highlight \u00a0several new features in the ML Pipelines API, including:\n<ul>\n\t<li>A stable API \u2014 <i>Pipelines have graduated from Alpha!</i></li>\n\t<li>New feature transformers</li>\n\t<li>Additional ML algorithms</li>\n\t<li>A more complete Python API</li>\n\t<li>A pluggable API for customized, third-party Pipeline components</li>\n</ul>\nIf you\u2019re new to using ML Pipelines, you can get familiar with the key concepts like Transformers and Estimators by reading our previous <a href=\"https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html\">blog post</a>.\n<h2>New Features in Spark 1.4</h2>\nWith significant contributions from the Spark community, ML Pipelines are much more featureful in the 1.4 release. \u00a0The API includes many common feature transformers and more algorithms.\n<h2>New Feature Transformers</h2>\nA big part of any ML workflow is massaging the data into the right features for use in downstream processing. \u00a0To simply feature extraction, Spark provides many feature transformers out-of-the-box. \u00a0The table below outlines most of the feature transformers available in Spark 1.4 along with descriptions of each one. Much of the API is inspired by scikit-learn; for reference, we provide names of similar scikit-learn transformers where available.\n<table class=\"table\">\n<tbody>\n<tr>\n<td><b>Transformer</b></td>\n<td><b>Description</b></td>\n<td><b>scikit-learn</b></td>\n</tr>\n<tr>\n<td>Binarizer</td>\n<td>Threshold numerical feature to binary</td>\n<td>Binarizer</td>\n</tr>\n<tr>\n<td>Bucketizer</td>\n<td>Bucket numerical features into ranges</td>\n<td></td>\n</tr>\n<tr>\n<td>ElementwiseProduct</td>\n<td>Scale each feature/column separately</td>\n<td></td>\n</tr>\n<tr>\n<td>HashingTF</td>\n<td>Hash text/data to vector. Scale by term frequency</td>\n<td>FeatureHasher</td>\n</tr>\n<tr>\n<td>IDF</td>\n<td>Scale features by inverse document frequency</td>\n<td>TfidfTransformer</td>\n</tr>\n<tr>\n<td>Normalizer</td>\n<td>Scale each row to unit norm</td>\n<td>Normalizer</td>\n</tr>\n<tr>\n<td>OneHotEncoder</td>\n<td>Encode k-category feature as binary features</td>\n<td>OneHotEncoder</td>\n</tr>\n<tr>\n<td>PolynomialExpansion</td>\n<td>Create higher-order features</td>\n<td>PolynomialFeatures</td>\n</tr>\n<tr>\n<td>RegexTokenizer</td>\n<td>Tokenize text using regular expressions</td>\n<td>(part of <a href=\"http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text\">text methods</a>)</td>\n</tr>\n<tr>\n<td>StandardScaler</td>\n<td>Scale features to 0 mean and/or unit variance</td>\n<td>StandardScaler</td>\n</tr>\n<tr>\n<td>StringIndexer</td>\n<td>Convert String feature to 0-based indices</td>\n<td>LabelEncoder</td>\n</tr>\n<tr>\n<td>Tokenizer</td>\n<td>Tokenize text on whitespace</td>\n<td>(part of <a href=\"http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text\">text methods</a>)</td>\n</tr>\n<tr>\n<td>VectorAssembler</td>\n<td>Concatenate feature vectors</td>\n<td>FeatureUnion</td>\n</tr>\n<tr>\n<td>VectorIndexer</td>\n<td>Identify categorical features, and index</td>\n<td></td>\n</tr>\n<tr>\n<td>Word2Vec</td>\n<td>Learn vector representation of words</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n* Only 3 of the above transformers were available in Spark 1.3 (HashingTF, StandardScaler, and Tokenizer).\n\nThe following code snippet demonstrates how multiple feature encoders can be strung together into a complex workflow. This example begins with two types of features: <em>text</em> (String) and <em>userGroup</em> (categorical). \u00a0For example:\n\n<img class=\"aligncenter wp-image-4712\" src=\"https://databricks.com/wp-content/uploads/2015/07/table-data.png\" alt=\"table data\" width=\"400\" height=\"196\" />\n\nWe generate text features using both hashing and the Word2Vec algorithm, and then apply a one-hot encoding to <em>userGroup</em>. \u00a0Finally, we combine all features into a single feature vector which can be used by ML algorithms such as Logistic Regression.\n\n<pre>from pyspark.ml.feature import *\nfrom pyspark.ml import Pipeline\ntok = Tokenizer(inputCol=\"text\", outputCol=\"words\")\nhtf = HashingTF(inputCol=\"words\", outputCol=\"tf\", numFeatures=200)\nw2v = Word2Vec(inputCol=\"text\", outputCol=\"w2v\")\nohe = OneHotEncoder(inputCol=\"userGroup\", outputCol=\"ug\")\nva = VectorAssembler(inputCols=[\"tf\", \"w2v\", \"ug\"], outputCol=\"features\")\npipeline = Pipeline(stages=[tok,htf,w2v,ohe,va])</pre>\n\nThe following diagram shows the full pipeline. Pipeline stages are shown as blue boxes, and DataFrame columns are shown as bubbles.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/07/simple-pipeline.png\"><img class=\" size-full wp-image-4713 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/07/simple-pipeline.png\" alt=\"simple pipeline\" width=\"436\" height=\"648\" /></a>\n<h2>Better Algorithm Coverage</h2>\nIn Spark 1.4, the Pipelines API now includes trees and ensembles: <a href=\"https://en.wikipedia.org/wiki/Decision_tree_learning\">Decision Trees</a>, <a href=\"https://en.wikipedia.org/wiki/Random_forest\">Random Forests</a>, and <a href=\"https://en.wikipedia.org/wiki/Gradient_boosting\">Gradient-Boosted Trees</a>. \u00a0These are some of the most important algorithms in machine learning. \u00a0They can be used for both regression and classification, are flexible enough to handle many types of applications, and can use both continuous and categorical features.\n\nThe Pipelines API also includes Logistic Regression and Linear Regression using <a href=\"https://en.wikipedia.org/wiki/Elastic_net_regularization\">Elastic Net regularization</a>, an important statistical tool mixing L1 and L2 regularization.\n\nSpark 1.4 also introduces OneVsRest (a.k.a. One-Vs-All), which converts any binary classification \"base\" algorithm into a multiclass algorithm. \u00a0This flexibility to use any base algorithm in OneVsRest highlights the versatility of the Pipelines API. \u00a0By using DataFrames, which support varied data types, OneVsRest can remain oblivious to the specifics of the base algorithm.\n<h2>More Complete Python API</h2>\nML Pipelines have a near-complete Python API in Spark 1.4. \u00a0Python APIs have become much simpler to implement after significant improvements to internal Python APIs, plus the unified DataFrame API. \u00a0See the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.ml.html\">Python API docs for ML Pipelines</a> for a full feature list.\n<h2>Customizing Pipelines</h2>\nWe have opened up APIs for users to write their own Pipeline stages. \u00a0If you need a custom feature transformer, ML algorithm, or evaluation metric in your workflow, you can write your own and plug it into ML Pipelines. \u00a0Stages communicate via DataFrames, which act as a simple, flexible API for passing data through a workflow.\n\nThe key abstractions are:\n<ul>\n\t<li><b>Transformer</b>: This includes feature transformers (e.g., OneHotEncoder) and trained ML models (e.g., LogisticRegressionModel).</li>\n\t<li><b>Estimator</b>: This includes ML algorithms for training models (e.g., LogisticRegression).</li>\n\t<li><b>Evaluator</b>: These evaluate predictions and compute metrics, useful for tuning algorithm parameters (e.g., BinaryClassificationEvaluator).</li>\n</ul>\nTo learn more, start with the overview of ML Pipelines in the <a href=\"http://spark.apache.org/docs/latest/ml-guide.html\">ML Pipelines Programming Guide</a>.\n<h2>Looking Ahead</h2>\nThe roadmap for Spark 1.5 includes:\n<ul>\n\t<li><i>API</i>: More complete algorithmic coverage in Pipelines, and more featureful Python API. \u00a0There is also initial work towards an MLlib API in Spark R.</li>\n\t<li><i>Algorithms</i>: More feature transformers (such as CountVectorizer, DiscreteCosineTransform, MinMaxScaler, and NGram) and algorithms (such as KMeans clustering and Naive Bayes).</li>\n\t<li><i>Developers</i>: Improvements for developers, including to the feature attributes API and abstractions.</li>\n</ul>\nML Pipelines do not yet cover all algorithms in MLlib, but the two APIs can interoperate. \u00a0If your workflow requires components from both APIs, all you need to do is convert between RDDs and DataFrames. \u00a0For more information on conversions, see the <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#interoperating-with-rdds\">DataFrame guide</a>.\n<h2>Acknowledgements</h2>\nThanks very much to the community contributors during this release! \u00a0You can find a complete list of JIRAs for ML Pipelines with contributors on the <a href=\"https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20status%20in%20(Resolved%2C%20Closed)%20AND%20fixVersion%20in%20(1.4.0%2C%201.4.1)%20AND%20component%20%3D%20ML%20ORDER%20BY%20priority%20DESC\">Apache Spark JIRA</a>.\n<h2>Learning More</h2>\nTo get started, <a href=\"http://spark.apache.org/downloads.html\">download Spark 1.4</a> and check out the <a href=\"http://spark.apache.org/docs/latest/ml-guide.html\">ML Pipelines User Guide</a>! \u00a0Also try out the <a href=\"https://github.com/apache/spark/tree/07f778978d80f0af57d3dafda4c566a813ad2d09/examples/src/main/scala/org/apache/spark/examples/ml\">ML package code examples</a>. Experts can get started writing their own Transformers and Estimators by looking at the <a href=\"https://github.com/apache/spark/blob/07f778978d80f0af57d3dafda4c566a813ad2d09/examples/src/main/scala/org/apache/spark/examples/ml/DeveloperApiExample.scala\">DeveloperApiExample code snippet</a>.\n\nTo contribute, follow the <a href=\"https://issues.apache.org/jira/browse/SPARK-8445\">MLlib 1.5 Roadmap JIRA</a>. \u00a0Good luck!"}
{"status": "publish", "description": "In this post, we outline Apache Spark Streaming\u2019s architecture and explain how it provides the above benefits. We also discuss some of the interesting ongoing work in the project that leverages the execution model.", "creator": "tdas", "link": "https://databricks.com/blog/2015/07/30/diving-into-apache-spark-streamings-execution-model.html", "authors": null, "id": 4726, "categories": ["Apache Spark", "Engineering Blog", "Streaming"], "dates": {"publishedOn": "2015-07-30", "tz": "UTC", "createdOn": "2015-07-30"}, "title": "Diving into Apache Spark Streaming's Execution Model", "slug": "diving-into-apache-spark-streamings-execution-model", "content": "With so many distributed stream processing engines available, people often ask us about the unique benefits of Apache Spark Streaming. From early on, Apache Spark has provided an unified engine that natively supports both batch and streaming workloads. This is different from other systems that either have a processing engine designed only for streaming, or have similar batch and streaming APIs but compile internally to different engines. Spark\u2019s single execution engine and unified programming model for batch and streaming lead to some unique benefits over other traditional streaming systems. In particular, four major aspects are:\n<ul>\n\t<li>Fast recovery from failures and stragglers</li>\n\t<li>Better load balancing and resource usage</li>\n\t<li>Combining of streaming data with static datasets and interactive queries</li>\n\t<li>Native integration with advanced processing libraries (SQL, machine learning, graph processing)</li>\n</ul>\nIn this post, we outline Spark Streaming\u2019s architecture and explain how it provides the above benefits. We also discuss some of the interesting ongoing work in the project that leverages the execution model.\n<h2>Stream Processing Architectures - The Old and the New</h2>\nAt a high level, modern distributed stream processing pipelines execute as follows:\n<ol>\n\t<li><b>Receive </b>streaming data from data sources (e.g. live logs, system telemetry data, IoT device data, etc.) into some data ingestion system like Apache Kafka, Amazon Kinesis, etc.</li>\n\t<li><b>Process</b> the data in parallel on a cluster. This is what stream processing engines are designed to do, as we will discuss in detail next.</li>\n\t<li><b>Output</b> the results out to downstream systems like HBase, Cassandra, Kafka, etc.</li>\n</ol>\nTo process the data, most traditional stream processing systems are designed with a <i>continuous operator model</i>, which works as follows:\n<ul>\n\t<li>There is a set of worker nodes, each of which run one or more <b>continuous operators</b>.</li>\n\t<li>Each continuous operator processes the streaming data one record at a time and forwards the records to other operators in the pipeline.</li>\n\t<li>There are \u201csource\u201d operators for receiving data from ingestion systems, and \u201csink\u201d operators that output to downstream systems.</li>\n</ul>\n<p style=\"text-align: center;\"><img class=\" wp-image-4727 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/07/image11-1024x655.png\" alt=\"image1\" width=\"600\" height=\"384\" /><em>Figure 1: Architecture of traditional stream processing systems</em></p>\nContinuous operators are a simple and natural model. However, with today\u2019s trend towards larger scale and more complex real-time analytics, this traditional architecture has also met some challenges. We designed Spark Streaming to satisfy the following requirements:\n<ul>\n\t<li><em>Fast failure and straggler recovery</em> - With greater scale, there is a higher likelihood of a cluster node failing or unpredictably slowing down (i.e. stragglers). The system must be able to automatically recover from failures and stragglers to provide results in real time. Unfortunately, the static allocation of continuous operators to worker nodes makes it challenging for traditional systems to recover quickly from faults and stragglers.</li>\n</ul>\n<ul>\n\t<li><em>Load balancing</em> - Uneven allocation of the processing load between the workers can cause bottlenecks in a continuous operator system. This is more likely to occur in large clusters and dynamically varying workloads. The system needs to be able to dynamically adapt the resource allocation based on the workload.</li>\n</ul>\n<ul>\n\t<li><i>Unification of streaming, batch and interactive workloads</i> - In many use cases, it is also attractive to query the streaming data interactively (after all, the streaming system has it all in memory), or to combine it with static datasets (e.g. pre-computed models). This is hard in continuous operator systems as they are not designed to the dynamically introduce new operators for ad-hoc queries. This requires a single engine that can combine batch, streaming and interactive queries.</li>\n</ul>\n<ul>\n\t<li><i>Advanced analytics like machine learning and SQL</i> <i>queries</i> - More complex workloads require continuously learning and updating data models, or even querying the \u201clatest\u201d view of streaming data with SQL queries. Again, having a common abstraction across these analytic tasks makes the developer\u2019s job much easier.</li>\n</ul>\nTo address these requirements, Spark Streaming uses a new architecture called <a href=\"http://people.csail.mit.edu/matei/papers/2013/sosp_spark_streaming.pdf\" target=\"_blank\">discretized streams</a> that directly leverages the rich libraries and fault tolerance of the Spark engine.\n<h2>Architecture of Spark Streaming: Discretized Streams</h2>\nInstead of processing the streaming data one record at a time, Spark Streaming discretizes the streaming data into tiny, sub-second micro-batches. In other words, Spark Streaming\u2019s Receivers accept data in parallel and buffer it in the memory of Spark\u2019s workers nodes. Then the latency-optimized Spark engine runs short tasks (tens of milliseconds) to process the batches and output the results to other systems. Note that unlike the traditional continuous operator model, where the computation is statically allocated to a node, Spark tasks are assigned dynamically to the workers based on the locality of the data and available resources. This enables both better load balancing and faster fault recovery, as we will illustrate next.\n\nIn addition, each batch of data is a Resilient Distributed Dataset (RDD), which is the basic abstraction of a fault-tolerant dataset in Spark. This allows the streaming data to be processed using any Spark code or library.\n<p style=\"text-align: center;\"><img class=\"aligncenter wp-image-4730\" src=\"https://databricks.com/wp-content/uploads/2015/07/image21-1024x734.png\" alt=\"image2\" width=\"600\" height=\"430\" /><em>Figure 2: Spark Streaming Architecture</em></p>\n&nbsp;\n<h2>Benefits of Discretized Stream Processing</h2>\n<p style=\"text-align: left;\">Let\u2019s see how this architecture allows Spark Streaming to achieve the goals we set earlier.</p>\n\n<h3>Dynamic load balancing</h3>\nDividing the data into small micro-batches allows for fine-grained allocation of computations to resources. For example, consider a simple workload where the input data stream needs to partitioned by a key and processed. In the traditional record-at-a-time approach taken by most other systems, if one of the partitions is more computationally intensive than the others, the node statically assigned to process that partition will become a bottleneck and slow down the pipeline. In Spark Streaming, the job\u2019s tasks will be naturally load balanced across the workers -- some workers will process a few longer tasks, others will process more of the shorter tasks.\n<p style=\"text-align: center;\"><img class=\"aligncenter wp-image-4732\" src=\"https://databricks.com/wp-content/uploads/2015/07/image31-1024x581.png\" alt=\"image3\" width=\"600\" height=\"340\" /><em>Figure 3: Dynamic load balancing</em></p>\n&nbsp;\n<h3>Fast failure and straggler recovery</h3>\n<p style=\"text-align: left;\">In case of node failures, traditional systems have to restart the failed continuous operator on another node and replay some part of the data stream to recompute the lost information. Note that only one node is handling the recomputation, and the pipeline cannot proceed until the new node has caught up after the replay. In Spark, the computation is already discretized into small, deterministic tasks that can run anywhere without affecting correctness. So failed tasks can be relaunched <i>in</i> <i>parallel</i> on all the other nodes in the cluster, thus evenly distributing all the recomputations across many nodes, and recovering from the failure faster than the traditional approach.</p>\n<p style=\"text-align: center;\"><img class=\"aligncenter wp-image-4733\" src=\"https://databricks.com/wp-content/uploads/2015/07/image41-1024x602.png\" alt=\"image4\" width=\"600\" height=\"353\" /><em>Figure 4:\u00a0Faster failure recovery with redistribution of computation</em></p>\n&nbsp;\n<h3>Unification of batch, streaming and interactive analytics</h3>\n<p style=\"text-align: left;\">The key programming abstraction in Spark Streaming is a DStream, or distributed stream. Each batch of streaming data is represented by an RDD, which is Spark\u2019s concept for a distributed dataset. Therefore a DStream is just a series of RDDs. This common representation allows batch and streaming workloads to interoperate seamlessly. Users can apply arbitrary Spark functions on each batch of streaming data: for example, it\u2019s easy to join a DStream with a precomputed static dataset (as an RDD).</p>\n\n<pre>// Create data set from Hadoop file\nval dataset = sparkContext.hadoopFile(\"file\")\n// Join each batch in stream with the dataset\nkafkaDStream.transform { batchRDD =>\n  batchRDD.join(dataset).filter(...)\n}</pre>\n\nSince the batches of streaming data are stored in the Spark\u2019s worker memory, it can be interactively queried on demand. For example, you can expose all the streaming state through the Spark SQL JDBC server, as we will show in the next section. This kind of unification of batch, streaming and interactive workloads is very simple in Spark, but hard to achieve in systems without a common abstraction for these workloads.\n<h3>Advanced analytics like machine learning and interactive SQL</h3>\nSpark interoperability extends to rich libraries like MLlib (machine learning), SQL, DataFrames, and GraphX. Let\u2019s explore a few use cases:\n<h4>Streaming + SQL and DataFrames</h4>\nRDDs generated by DStreams can be converted to DataFrames (the programmatic interface to Spark SQL), and queried with SQL. For example, using Spark SQL\u2019s <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#running-the-thrift-jdbcodbc-server\">JDBC server</a>, you can expose the state of the stream to any external application that talks SQL.\n\n<pre>val hiveContext = new HiveContext(sparkContext)\n// ...\nwordCountsDStream.foreachRDD { rdd =>\n  // Convert RDD to DataFrame and register it as a SQL table\n  val wordCountsDataFrame = rdd.toDF(\"word\", \"count\") \n  wordCountsDataFrame.registerTempTable(\"word_counts\") \n}\n// ...\n// Start the JDBC server\nHiveThriftServer2.startWithContext(hiveContext)</pre>\n\nThen you can interactively query the continuously updated \u201cword_counts\u201d table through the JDBC server, using the <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#running-the-thrift-jdbcodbc-server\">beeline</a> client that ships with Spark, or tools like Tableau.\n\n<pre>> show tables;\n+--------------+--------------+\n| \u00a0tableName \u00a0\u00a0| isTemporary \u00a0|\n+--------------+--------------+\n| word_counts \u00a0| true \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0|\n+--------------+--------------+\n1 row selected (0.102 seconds)\n\n> select * from word_counts;\n+-----------+--------+\n| \u00a0\u00a0word \u00a0\u00a0\u00a0| count \u00a0|\n+-----------+--------+\n| 2015 \u00a0\u00a0\u00a0\u00a0\u00a0| 264 \u00a0\u00a0\u00a0|\n| PDT \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0| 264 \u00a0\u00a0\u00a0|\n| 21:45:41 \u00a0| 27 \u00a0\u00a0\u00a0\u00a0|</pre>\n\n<h4>Streaming + MLlib</h4>\nMachine learning models generated offline with MLlib can applied on streaming data. For example, the following code trains a KMeans clustering model with some static data and then uses the model to classify events in a Kafka data stream.\n\n<pre>// Learn model offline\nval model = KMeans.train(dataset, ...)\n// Apply model online on stream\nval kafkaStream = KafkaUtils.createDStream(...)\nkafkaStream.map { event => model.predict(featurize(event)) }</pre>\n\nWe demonstrated this offline-learning-online-prediction at our Spark Summit 2014 Databricks demo. Since then, we have also added streaming machine learning algorithms in MLLib that can continuously train from a labelled data stream. Other Spark libraries can also easily be called from Spark Streaming.\n<h2>Performance</h2>\nGiven the unique design of Spark Streaming, how fast does it run? In practice, Spark Streaming's ability to batch data and leverage the Spark engine leads to comparable or <a href=\"https://spark-summit.org/2015/events/towards-benchmarking-modern-distributed-streaming-systems/\">higher throughput</a> to other streaming systems. In terms of latency, Spark Streaming can achieve latencies as low as a few hundred milliseconds. Developers sometimes ask whether the micro-batching inherently adds too much latency. In practice,\u00a0batching latency is only a small component of end-to-end pipeline latency. For example, many applications compute results over a sliding window, and even in continuous operator systems, this window is only updated periodically (e.g. a 20 second window that slides every 2 seconds). Many pipelines collect records from multiple sources and wait for a short period to process delayed or out-of-order data. Finally, any automatic triggering algorithm tends to wait for some time period to fire a trigger. Therefore, compared to the end-to-end latency, batching rarely adds significant overheads. In fact, the throughput gains from DStreams often means that you need fewer machines to handle the same workload.\n<h2>Future Directions for Spark Streaming</h2>\nSpark Streaming is one of the most widely used components in Spark, and there is a lot more coming for streaming users down the road. Some of the highest priority items our team is working on are discussed below. You can expect these in the next few releases of Spark:\n<ul>\n\t<li><i>Backpressure</i> - Streaming workloads can often have bursts of data (e.g. sudden spike in tweets during the Oscars) and the processing system must be able to handle them gracefully. In the upcoming Spark 1.5 release (next month), Spark will be adding better backpressure mechanisms that allow Spark Streaming dynamically control the ingestion rate for such bursts. This feature represents joint work between us at Databricks and engineers at Typesafe.</li>\n</ul>\n<ul>\n\t<li><i>Dynamic scaling</i> - Controlling the ingestion rate may not be sufficient to handle longer terms variations in data rates (e.g. sustained higher tweet rate during the day than night). Such variations can be handled by dynamically scaling the cluster resource based on the processing demands. This is very easy to do within the Spark Streaming architecture -- since the computation is already divided into small tasks, they can be dynamically redistributed to a larger cluster if more nodes are acquired from the cluster manager (YARN, Mesos, Amazon EC2, etc). We plan to add support for automatic dynamic scaling.</li>\n</ul>\n<ul>\n\t<li><i>Event time and out-of-order data </i>- In practice, users sometimes have records that are delivered out of order, or with a timestamp that differs from the time of ingestion. Spark streaming will support \u201cevent time\u201d by allowing user-defined time extraction function. This will include a slack duration for late or out-of-order data.</li>\n</ul>\n<ul>\n\t<li><i>UI enhancements</i> - Finally, we want to make it easy for developers to debug their streaming applications. For this purpose, in Spark 1.4, we added <a href=\"https://databricks.com/blog/2015/07/08/new-visualizations-for-understanding-spark-streaming-applications.html\">new visualizations to the streaming Spark UI</a> that let developers closely monitor the performance of their application. In Spark 1.5, we are further improving this by showing more <a href=\"https://issues.apache.org/jira/browse/SPARK-8701\">input information like Kafka offsets</a> processed in each batch.</li>\n</ul>\nTo learn more about Spark Streaming, read the <a href=\"http://spark.apache.org/docs/latest/streaming-programming-guide.html\">official programming guide</a>, or the <a href=\"http://people.csail.mit.edu/matei/papers/2013/sosp_spark_streaming.pdf\" target=\"_blank\">Spark Streaming research paper</a> that introduces its execution and fault tolerance model."}
{"status": "publish", "description": "When we started Databricks, we thought that extracting insights from big data was insanely difficult for no good reason.", "creator": "ali", "link": "https://databricks.com/blog/2015/08/05/helping-the-democratization-of-big-data.html", "authors": null, "id": 4757, "categories": ["Announcements", "Company Blog", "Product"], "dates": {"publishedOn": "2015-08-05", "tz": "UTC", "createdOn": "2015-08-05"}, "title": "Helping the Democratization of Big Data", "slug": "helping-the-democratization-of-big-data", "content": "When we started Databricks, we thought that extracting insights from big data was insanely difficult for no good reason. You almost needed an advanced degree to be able to get any meaningful work done. As a result, only a select few in each organization could ask questions from their big data, the people who set up the clusters and knew how to use advanced tools such as Hive and MapReduce. We therefore set out to build a software as a service product that would drastically simplify big data processing.\n\nSoon after launching we learned something interesting. In many organizations, a democratization effort was taking place with more and more people starting to use Databricks to ask questions from the data. They were no longer bottlenecked by the chosen few that knew how to talk to the data. However, as some organizations had over hundreds of users using Databricks, a new set of challenges had risen. First, users wanted to control access to their data. Second, they wanted version control and management of multiple Apache Spark versions. Third, they wanted R-support. These requirements were all interlinked. I'm proud to announce that after much hard work, we are now releasing Databricks with all these features. Below I explain the story of how each of these came about and the lessons that are behind these features.\n\nAs employees with different functions started asking questions from the data, it very soon became a hard requirement to be able to control who in their organization should see or modify their queries, which could contain very sensitive information or could not be shared due to security compliance reasons. This is natural in a large organization. In our case, this requirement became even more important because we had developed a new way in which hundreds of users could use separate notebooks on the same shared Spark cluster, enabling huge cost savings for their organizations. This was not possible before, as before this feature, each notebook and user would have to have a separate isolated cluster. By enabling such cluster sharing, it was even more important that your coworkers couldn't snoop on your most sensitive notebooks. Databricks now comes with access control features that let you control who can see, who can run and parameterize, and who can edit and manage your notebooks. We are the first vendor to offer this feature for Spark.\n<p style=\"text-align: center;\"><img class=\" wp-image-4759 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/08/Screen-Shot-2015-08-04-at-9.59.01-PM1-1024x540.png\" alt=\"Screen Shot 2015-08-04 at 9.59.01 PM\" width=\"400\" height=\"211\" /><em>Setting permissions in Databricks</em></p>\nWe tried from day one to make it really easy to collaborate on live notebooks, with features such as real-time updates and commenting features. But as collaboration started to seriously happen, users wanted to have auditability. Who modified my notebooks and how can I go back to an earlier version? Furthermore, many users were already using external version control systems, such as GitHub. Finally, many users wanted to sometimes explore some new features of a Spark release on a small experimental cluster, but continue to use old Spark versions on production clusters. As they gained more experience with the new Spark version, they would like to reuse their old notebooks on the new Spark version. Hence, they wanted to manage multiple Spark versions and be able to easily go between these with their jobs and notebooks. Databricks current release now comes with these features for version control, GitHub integration, and management of multiple Spark versions.\n<p style=\"text-align: center;\"><a href=\"https://databricks.com/wp-content/uploads/2015/08/Screen-Shot-2015-08-05-at-7.28.06-AM.png\"><img class=\" wp-image-4761 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/08/Screen-Shot-2015-08-05-at-7.28.06-AM-1024x513.png\" alt=\"Screen Shot 2015-08-05 at 7.28.06 AM\" width=\"800\" height=\"401\" /></a><em>Notebook version control</em></p>\nFinally, as more job functions started asking questions from the data, we heard that more and wanted to use R as their preferred language to talk to the data. SQL and Python were already supported for a while and these were wildly popular. But we did not have R support. This trend seemed to be very prominent as a lot of people without computing degrees were being trained in R at universities, in classes, and other settings. We therefore accelerated the incorporation of SparkR into Spark and also <a href=\"https://databricks.com/blog/2015/07/13/introducing-r-notebooks-in-databricks.html\" target=\"_blank\">added R as a first class language as part of Databricks</a>, making us the first company to commercially support SparkR.\n<p style=\"text-align: center;\"><img class=\"aligncenter wp-image-4525\" src=\"https://databricks.com/wp-content/uploads/2015/07/RNotebooks.png\" alt=\"RNotebooks\" width=\"400\" height=\"183\" /><i>R Notebooks in Databricks</i></p>\nThis release of Databricks is dubbed \"version 2.0\" since it contains many all of the above features that enable the democratization effort inside many organizations. I used quotes around the version number, because as a SaaS product versions don't play the same role as for traditional software. We will continue to maintain a two week release cadence, each containing new exciting features that our users requested.\n\n<a href=\"http://dbricks.co/1IoKfGA\" target=\"_blank\">Try these features yourself</a> and please let us know what you think.\n\n&nbsp;\n\n&nbsp;"}
{"status": "publish", "description": "With the introduction of window operations in Apache Spark 1.4, you can finally port pretty much any relevant piece of Pandas\u2019 DataFrame computation to Apache Spark parallel computation framework using Spark SQL\u2019s DataFrame.", "creator": "dave_wang", "link": "https://databricks.com/blog/2015/08/12/from-pandas-to-apache-sparks-dataframe.html", "authors": null, "id": 4790, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2015-08-12", "tz": "UTC", "createdOn": "2015-08-12"}, "title": "From Pandas to Apache Spark's DataFrame", "slug": "from-pandas-to-apache-sparks-dataframe", "content": "This is a cross-post from the blog of\u00a0<span class=\"gD\">Olivier Girardot. Olivier is a\u00a0software engineer and\u00a0the co-founder of Lateral Thoughts, where he works on Machine Learning, Big Data, and DevOps solutions.</span>\n\n<hr />\n\nWith the introduction of window operations in Apache Spark 1.4, you can finally port pretty much any relevant piece of Pandas\u2019 DataFrame computation to Apache Spark parallel computation framework using Spark SQL\u2019s DataFrame. If you\u2019re not yet familiar with <a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\" target=\"_blank\">Spark\u2019s DataFrame</a>, don\u2019t hesitate to check out\u00a0<a title=\"RDDs are the new bytecode of Apache Spark\" href=\"https://ogirardot.wordpress.com/2015/05/29/rdds-are-the-new-bytecode-of-apache-spark/\">RDDs are the new bytecode of Apache Spark</a>\u00a0and come back here after.\n\nI figured some feedback on how to port existing <em>complex</em> code might be useful, so the goal of this article will be to take a few concepts from Pandas DataFrame and see how we can translate this to PySpark\u2019s DataFrame using Spark 1.4.\n\n<strong>Disclaimer</strong>: \u00a0a few operations that you can do in Pandas don't translate to Spark well. Please remember that DataFrames in Spark are like RDD in the sense that they\u2019re an immutable data structure. Therefore things like:\n\n[python]\n# to create a new column &quot;three&quot;\ndf['three'] = df['one'] * df['two']\n[/python]\n\nCan\u2019t exist, just because this kind of affectation goes against the principles of Spark. Another example would be trying to access by\u00a0index a single element within a DataFrame. Don\u2019t forget that you\u2019re using a distributed data structure, not an in-memory random-access data structure.\n\nTo be clear, this doesn\u2019t mean that you can\u2019t do the same kind of thing (i.e. create a new column) using Spark, it means that you have to think immutable/distributed and re-write parts of your code, mostly the parts that are not purely thought of as transformations on a stream of data.\n\nSo let\u2019s dive in.\n<h2>Column selection</h2>\nThis part is not that much different in Pandas and Spark, but you have to take into account the immutable character of your DataFrame. First let\u2019s create two DataFrames one in Pandas *pdf* and one in Spark *df*:\n\n[python]\n# Pandas =&gt; pdf\nIn [17]: pdf = pd.DataFrame.from_items([('A', [1, 2, 3]), ('B', [4, 5, 6])])\n\nIn [18]: pdf.A\nOut[18]:\n0    1\n1    2\n2    3\nName: A, dtype: int64\n\n# SPARK SQL =&gt; df\nIn [19]: df = sqlCtx.createDataFrame([(1, 4), (2, 5), (3, 6)], [&quot;A&quot;, &quot;B&quot;])\n\nIn [20]: df\nOut[20]: DataFrame[A: bigint, B: bigint]\n\nIn [21]: df.show()\n+-+-+\n|A|B|\n+-+-+\n|1|4|\n|2|5|\n|3|6|\n+-+-+\n[/python]\n\nNow in Spark SQL or Pandas you use the same syntax to refer to a column:\n\n[python]\nIn [27]: df.A\nOut[27]: Column&lt;A&gt;\nOut[27]: Column&lt;A&gt;\n\nIn [28]: df['A']\nOut[28]: Column&lt;A&gt;\n\nIn [29]: pdf.A\nOut[29]:\n0    1\n1    2\n2    3\nName: A, dtype: int64\n\nIn [30]: pdf['A']\nOut[30]:\n0    1\n1    2\n2    3\nName: A, dtype: int64\n[/python]\n\nThe output seems different, but these are still the same ways of referencing a column using Pandas or Spark. The only difference is that in Pandas, it is a mutable data structure that you can change - not in Spark.\n<h2>Column adding</h2>\n\n[python]\nIn [31]: pdf['C'] = 0\n\nIn [32]: pdf\nOut[32]:\n   A  B  C\n0  1  4  0\n1  2  5  0\n2  3  6  0\n\n# In Spark SQL you'll use the withColumn or the select method,\n# but you need to create a &quot;Column&quot;, a simple int won't do :\nIn [33]: df.withColumn('C', 0)\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-33-fd1261f623cf&gt; in &lt;module&gt;()\n----&gt; 1 df.withColumn('C', 0)\n\n/Users/ogirardot/Downloads/spark-1.4.0-bin-hadoop2.4/python/pyspark/sql/dataframe.pyc in withColumn(self, colName, col)\n   1196         &quot;&quot;&quot;\n-&gt; 1197         return self.select('*', col.alias(colName))\n   1198\n   1199     @ignore_unicode_prefix\n\nAttributeError: 'int' object has no attribute 'alias'\n\n# Here's your new best friend &quot;pyspark.sql.functions.*&quot;\n# If you can't create it from composing columns\n# this package contains all the functions you'll need :\nIn [35]: from pyspark.sql import functions as F\nIn [36]: df.withColumn('C', F.lit(0))\nOut[36]: DataFrame[A: bigint, B: bigint, C: int]\n\nIn [37]: df.withColumn('C', F.lit(0)).show()\n+-+-+-+\n|A|B|C|\n+-+-+-+\n|1|4|0|\n|2|5|0|\n|3|6|0|\n+-+-+-+\n[/python]\n\nMost of the time in Spark SQL you can use Strings to reference columns but there are two cases where you\u2019ll want to use the Column objects rather than Strings :\n<ul>\n\t<li>In Spark SQL DataFrame columns are allowed to have the same name, they\u2019ll be given unique names inside of Spark SQL, but this means that you can\u2019t reference them with the column name only as this becomes ambiguous.</li>\n\t<li>When you need to manipulate columns using expressions like <em>Adding two columns to each other</em>, <em>Twice the value of this column</em><strong>\u00a0</strong>or even <em>Is the column value larger than 0 ?</em>, you won\u2019t be able to use simple strings and will need the Column reference.</li>\n\t<li>Finally if you need renaming, cast or any other complex feature, you\u2019ll need the Column reference too.</li>\n</ul>\nHere\u2019s an example:\n\n[python]\nIn [39]: df.withColumn('C', df.A * 2)\nOut[39]: DataFrame[A: bigint, B: bigint, C: bigint]\n\nIn [40]: df.withColumn('C', df.A * 2).show()\n+-+-+-+\n|A|B|C|\n+-+-+-+\n|1|4|2|\n|2|5|4|\n|3|6|6|\n+-+-+-+\n\nIn [41]: df.withColumn('C', df.B &gt; 0).show()\n+-+-+----+\n|A|B|   C|\n+-+-+----+\n|1|4|true|\n|2|5|true|\n|3|6|true|\n+-+-+----+\n[/python]\n\nWhen you\u2019re selecting columns, to create another <em>projected</em> DataFrame, you can also use expressions:\n\n[python]\nIn [42]: df.select(df.B &gt; 0)\nOut[42]: DataFrame[(B &gt; 0): boolean]\n\nIn [43]: df.select(df.B &gt; 0).show()\n+-------+\n|(B &gt; 0)|\n+-------+\n|   true|\n|   true|\n|   true|\n+-------+\n[/python]\n\ns you can see the column name will actually be computed according to the expression you defined, if you want to rename this, you\u2019ll need to use the <strong>alias\u00a0</strong>method on Column:\n\n[python]\nIn [44]: df.select((df.B &gt; 0).alias(&quot;is_positive&quot;)).show()\n+-----------+\n|is_positive|\n+-----------+\n|       true|\n|       true|\n|       true|\n+-----------+\n[/python]\n\nAll of the expressions that we\u2019re building here can be used for Filtering, Adding a new column or even inside Aggregations, so once you get a general idea of how it works, you\u2019ll be fluent throughout all of the DataFrame manipulation framework.\n<h2>Filtering</h2>\nFiltering is pretty much straightforward too, you can use the <em>RDD-like</em>\u00a0<code>filter</code><strong>\u00a0</strong>method and copy any of your existing Pandas expression/predicate for filtering:\n\n[python]\nIn [48]: pdf[(pdf.B &gt; 0) &amp; (pdf.A &lt; 2)] Out[48]:    A  B  C 0  1  4  0 In [49]: df.filter((df.B &gt; 0) &amp; (df.A &lt; 2)).show() +-+-+ |A|B| +-+-+ |1|4| +-+-+ In [55]: df[(df.B &gt; 0) &amp; (df.A &lt; 2)].show()\n+-+-+\n|A|B|\n+-+-+\n|1|4|\n+-+-+\n[/python]\n\n<h2>Aggregations</h2>\nWhat can be confusing at first in using aggregations is that the minute you write\u00a0<code>groupBy</code><strong>\u00a0</strong>you\u2019re not using a DataFrame object, you\u2019re actually using a <code>GroupedData</code><strong>\u00a0</strong>object and you need to precise your aggregations to get back the output DataFrame:\n\n[python]\nIn [77]: df.groupBy(&quot;A&quot;)\nOut[77]: &lt;pyspark.sql.group.GroupedData at 0x10dd11d90&gt;\n\nIn [78]: df.groupBy(&quot;A&quot;).avg(&quot;B&quot;)\nOut[78]: DataFrame[A: bigint, AVG(B): double]\n\nIn [79]: df.groupBy(&quot;A&quot;).avg(&quot;B&quot;).show()\n+-+------+\n|A|AVG(B)|\n+-+------+\n|1|   4.0|\n|2|   5.0|\n|3|   6.0|\n+-+------+\n[/python]\n\nAs a syntactic sugar if you need only one aggregation, you can use the simplest functions like:\u00a0<em>avg, cout, max, min, mean</em> and <em>sum</em><strong>\u00a0</strong>directly on <code>GroupedData</code>, but most of the time, this will be too simple and you\u2019ll want to create a few aggregations during a single groupBy operation. After all (c.f.\u00a0<a title=\"RDDs are the new bytecode of Apache Spark\" href=\"https://ogirardot.wordpress.com/2015/05/29/rdds-are-the-new-bytecode-of-apache-spark/\">RDDs are the new bytecode of Apache Spark</a>\u00a0) this is one of the greatest features of the DataFrames. To do so you\u2019ll be using the\u00a0<code>agg</code><strong>\u00a0</strong>method:\n\n[python]\nIn [83]: df.groupBy(&quot;A&quot;).agg(F.avg(&quot;B&quot;), F.min(&quot;B&quot;), F.max(&quot;B&quot;)).show()\n+-+------+------+------+\n|A|AVG(B)|MIN(B)|MAX(B)|\n+-+------+------+------+\n|1|   4.0|     4|     4|\n|2|   5.0|     5|     5|\n|3|   6.0|     6|     6|\n+-+------+------+------+\n[/python]\n\nOf course, just like before, you can use any expression especially column compositions, alias definitions etc\u2026 and some other non-trivial functions:\n\n[python]\nIn [84]: df.groupBy(&quot;A&quot;).agg(\n   ....: F.first(&quot;B&quot;).alias(&quot;my first&quot;),\n   ....: F.last(&quot;B&quot;).alias(&quot;my last&quot;),\n   ....: F.sum(&quot;B&quot;).alias(&quot;my everything&quot;)\n   ....: ).show()\n+-+--------+-------+-------------+\n|A|my first|my last|my everything|\n+-+--------+-------+-------------+\n|1|       4|      4|            4|\n|2|       5|      5|            5|\n|3|       6|      6|            6|\n+-+--------+-------+-------------+\n[/python]\n\n<h2>Complex operations &amp; Windows</h2>\nNow that\u00a0Spark 1.4 is out, the Dataframe API provides an efficient and easy to use Window-based framework \u2013 this single feature is what makes any\u00a0Pandas to Spark migration\u00a0actually do-able for 99% of the projects \u2013 even considering some of Pandas\u2019 features that seemed <em>hard</em> to reproduce in a distributed environment.\n\nA simple example that we can pick is that\u00a0in Pandas you can compute a\u00a0diff<strong>\u00a0</strong>on a column and Pandas will compare the values of one line to the last one and compute the difference between them. Typically the kind of feature hard to do in a distributed environment because each line is supposed to be treated independently, now with Spark 1.4 window operations you can define a window on which Spark will <em>execute some aggregation functions</em>\u00a0but relatively to a specific line. Here\u2019s how to port some existing Pandas code using diff:\n\n[python]\nIn [86]: df = sqlCtx.createDataFrame([(1, 4), (1, 5), (2, 6), (2, 6), (3, 0)], [&quot;A&quot;, &quot;B&quot;])\n\nIn [95]: pdf = df.toPandas()\n\nIn [96]: pdf\nOut[96]:\n   A  B\n0  1  4\n1  1  5\n2  2  6\n3  2  6\n4  3  0\n\nIn [98]: pdf['diff'] = pdf.B.diff()\n\nIn [102]: pdf\nOut[102]:\n   A  B  diff\n0  1  4   NaN\n1  1  5     1\n2  2  6     1\n3  2  6     0\n4  3  0    -6\n[/python]\n\nIn Pandas you can compute a diff on an arbitrary column, with no regard for keys, no regards for order or anything. It\u2019s cool\u2026 but most of the time not exactly what you want and you might end up cleaning up the mess afterwards by setting the column value back to NaN from one line to another when the keys changed.\n\nHere\u2019s how you can do such a thing in PySpark using Window functions, a Key and, if you want, in a specific order:\n\n[python]\nIn [107]: from pyspark.sql.window import Window\n\nIn [108]: window_over_A = Window.partitionBy(&quot;A&quot;).orderBy(&quot;B&quot;)\n\nIn [109]: df.withColumn(&quot;diff&quot;, F.lead(&quot;B&quot;).over(window_over_A) - df.B).show()\n+---+---+----+\n|  A|  B|diff|\n+---+---+----+\n|  1|  4|   1|\n|  1|  5|null|\n|  2|  6|   0|\n|  2|  6|null|\n|  3|  0|null|\n+---+---+----+\n[/python]\n\nWith that you are\u00a0now able to compute a diff line by line \u2013 ordered or not \u2013 given a specific key. The great point about Window operation is that you\u2019re\u00a0not\u00a0actually breaking the structure of your data. Let me explain myself.\n\nWhen you\u2019re computing some\u00a0kind of aggregation\u00a0(once again according to a key), you\u2019ll usually be executing a\u00a0<code>groupBy</code><strong>\u00a0</strong>operation given this key and compute the multiple metrics that you\u2019ll need (<em>at the same time</em> if you\u2019re lucky, otherwise\u00a0in multiple\u00a0<code>reduceByKey</code><strong>\u00a0</strong>or\u00a0<code>aggregateByKey</code>\u00a0transformations).\n\nBut whether you\u2019re using RDDs or DataFrame, if you\u2019re not using window operations then you\u2019ll actually crush your data in a part of your flow and then you\u2019ll need to join back again the results of your aggregations to the <em>main</em> - dataflow. Window operations allow\u00a0you to execute your computation and copy the results as additional columns without any explicit join.\n\nThis is a quick way to enrich your data adding rolling computations as just another column directly. Two additional\u00a0resources are worth noting regarding these new features, the official <a href=\"https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html\">Databricks blog\u00a0article on Window operations</a>\u00a0and\u00a0<a href=\"http://twitter.com/chris_bour\">Christophe Bourguignat</a>\u2018s\u00a0article evaluating\u00a0<a href=\"https://medium.com/@chris_bour/6-differences-between-pandas-and-spark-dataframes-1380cec394d2\">Pandas and Spark DataFrame differences</a>.\n\nTo sum up,\u00a0you now have all the tools you need in Spark 1.4 to port any Pandas computation in a distributed environment using the <em>very</em> similar\u00a0DataFrame API."}
{"status": "publish", "description": "We are excited to announce that starting today, Apache Spark 1.5.0 is available as a preview in Databricks.", "creator": "rxin", "link": "https://databricks.com/blog/2015/08/18/apache-spark-1-5-preview-now-available-in-databricks.html", "authors": null, "id": 4902, "categories": ["Announcements", "Company Blog", "Product"], "dates": {"publishedOn": "2015-08-18", "tz": "UTC", "createdOn": "2015-08-18"}, "title": "Apache Spark 1.5 Preview Now Available in Databricks", "slug": "apache-spark-1-5-preview-now-available-in-databricks", "content": "We are excited to announce that starting today, Apache Spark 1.5.0 is available as a preview in Databricks. Our users can now choose to provision clusters with Spark 1.5 or previous Spark versions ready-to-go with a few clicks.\n\nOfficially, Spark 1.5 is expected to be released in a few weeks, and the community is doing QA testing for the release. Given the fast-paced development of Spark, we feel it is important to enable our users to leverage the new development and features as soon as possible. With traditional on-premise software deployment, it can take months, and sometimes even years, to receive software updates from vendors. With Databricks\u2019 cloud model, we can push updates in a matter of hours and let users try their Spark version of choice.\n<h2>What\u2019s New?</h2>\nThe last few releases of Spark focus on making data science more accessible, through high-level programming APIs such as <a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\">DataFrames</a>, <a href=\"https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html\">machine learning pipelines</a>, and <a href=\"https://databricks.com/blog/2015/07/13/introducing-r-notebooks-in-databricks.html\">R language support</a>. A large part of Spark 1.5, on the other hand, focuses on <b>under-the-hood changes </b>to improve Spark\u2019s <b>performance, usability, and operational stability</b>.\n\nSpark 1.5 delivers the first phase of <a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\">Project Tungsten</a>, a new execution backend for DataFrames/SQL. Through code generation and cache-aware algorithms, Project Tungsten improves the runtime performance with out-of-the-box configurations. Through explicit memory management and external operations, the new backend also mitigates the inefficiency in JVM garbage collection and improves robustness in large-scale workloads.\n\n<img class=\" wp-image-4904 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/08/image01.png\" alt=\"image01\" width=\"499\" height=\"165\" />\n\nOver the next few weeks, we will be writing about Project Tungsten. To give you a sneak peek, the above chart compares the out-of-the-box (i.e. no configuration changes) performance of an aggregation query (16 million records and 1 million composite keys) using Spark 1.4 and Spark 1.5 on my laptop.\n\nStreaming workloads typically run 24/7 and have stringent stability requirements. In this release, Typesafe has introduced <a href=\"https://issues.apache.org/jira/browse/SPARK-7398\">Backpressure</a> in Spark Streaming. With this feature, Spark Streaming can dynamically control the data ingest rates to adapt to unpredictable variations in processing load. This allows streaming applications to be more robust against bursty workloads and downstream delays.\n\nOf course, Spark 1.5 is the work of more than 220 open source contributors from over 80 organizations, and includes a lot more than the above two. Some examples include:\n<ul>\n\t<li>New machine learning algorithms: multilayer perceptron classifier, PrefixSpan for sequential pattern mining, association rule generation, etc.</li>\n\t<li>Improved R language support and GLMs with R formula.</li>\n\t<li>Better instrumentation and reporting of memory usage in web UI.</li>\n</ul>\nStay tuned for future blog posts covering the release as well as deep dives into specific improvements.\n<h2>How do I use it?</h2>\nLaunching a Spark 1.5 cluster is as easy as selecting Spark 1.5 experimental version in the cluster creation interface in Databricks.\n\n<img class=\"aligncenter wp-image-4903\" src=\"https://databricks.com/wp-content/uploads/2015/08/image00.png\" alt=\"image00\" width=\"500\" height=\"364\" />\n\nOnce you hit confirm, you will get a Spark cluster ready to go with Spark 1.5.0 and start testing the new release. <a href=\"https://databricks.com/blog/2015/08/05/databricks-2-0-leading-the-charge-to-democratize-data.html\" target=\"_blank\">Multiple Spark version support</a> in Databricks also enables users to run Spark 1.5 canary clusters side-by-side with existing production Spark clusters.\n\nYou can find the <a href=\"https://spark.apache.org/docs/1.5.0/programming-guide.html\">work-in-progress documentation for Spark 1.5.0 here</a>. Please be aware that just like any other preview software, Spark 1.5.0 support is experimental. There will be bugs and quirks that we find and fix in the next couple of weeks. The good news is that you don\u2019t have to worry about following the development or upgrading yourself. As we discover and fix bugs in the open source project, the Spark 1.5 option in Databricks will also be updated automatically. If you encounter a bug, please report it by <a href=\"https://issues.apache.org/jira/browse/SPARK\">filing a JIRA ticket</a>.\n\nTo try Databricks, <a href=\"http://dbricks.co/1LgbUwL\">sign up for a free 30-day trial</a>.\n\n&nbsp;"}
{"status": "publish", "description": "We are happy to announce that the full agenda is now finalized, you can find the full list of community talks along with the first set of keynotes on Spark-Summit.org.", "creator": "scott", "link": "https://databricks.com/blog/2015/08/31/spark-summit-europe-full-agenda-available-online.html", "authors": null, "id": 5059, "categories": ["Company Blog", "Events"], "dates": {"publishedOn": "2015-08-31", "tz": "UTC", "createdOn": "2015-08-31"}, "title": "Spark Summit Europe Full Agenda Available Online", "slug": "spark-summit-europe-full-agenda-available-online", "content": "This October, join the Apache Spark community in Amsterdam at the <a href=\"https://spark-summit.org/eu-2015/venue\">Beurs Van Berlage</a> for the very first Spark Summit in Europe! We are happy to announce that the full agenda is now finalized, you can find the full list of 39\u00a0<a href=\"https://spark-summit.org/eu-2015/schedule/\">community talks</a> along with the first set of <a href=\"https://spark-summit.org/eu-2015/speakers/\">keynotes</a> on <a href=\"https://spark-summit.org/eu-2015/\" target=\"_blank\">Spark-Summit.org</a>. Those looking to get hands-on with Spark are encouraged to sign up for one of our <a href=\"https://spark-summit.org/eu-2015/spark-training/\">workshops</a>.\n\nThe phenomenal growth of the global Spark community is reflected in the development of the Spark Summit events. The Spark Summit started as a gathering of 450 early adopters in 2013. After doubling in size in 2014, the Spark Summit grew to host events in multiple cities in 2015 - in San Francisco and New York, with over 3,000 combined attendees. Given the rapid growth of the European Spark Community, we are thrilled to bring the first Spark Summit to Europe as well!\n\nThe first European Spark Summit will have a wide variety of content, catering to different interests. Attendees will hear from production users of Spark, Spark SQL, Spark Streaming, MLlib, and related projects; find out where the project development is going; and learn how to use the Spark stack in a variety of applications.\n\nHere is a brief highlight of what to expect in Amsterdam.\n<h2>Keynotes</h2>\nThought leaders will share their perspective on the growth of Spark in their respective organizations and provide examples of large-scale production use cases:\n<ul>\n\t<li>Matei Zaharia, Creator of Spark and CTO at Databricks</li>\n\t<li>Ion Stoica, CEO at Databricks</li>\n\t<li>Martin Odersky, Founder at Typesafe</li>\n\t<li>Quentin Clark, CTO at SAP</li>\n\t<li>Rob Anderson, VP Worldwide Systems Engineering at MapR</li>\n\t<li>Vincent Saulys, VP at Goldman Sachs</li>\n</ul>\n<h2>Community Talk Highlights</h2>\nWe would like to thank everyone who submitted a presentation, and congratulate the selected community talk presenters. The tracks for Spark Summit EU are organized by four themes: Developer, use cases, data science, and research.\n\nCheck out a sample of the topics (see full list of topics <a href=\"https://spark-summit.org/eu-2015/schedule/\"> here</a><span style=\"font-weight: 400;\">):</span>\n<ul>\n\t<li><a href=\"https://spark-summit.org/eu-2015/events/3-trillion-app-recommendations-with-less-than-100-lines-of-spark-code-in-less-than-25-minutes/\">3 Trillion App recommendations, with less than 100 Lines of Spark Code in less than 25 Minutes</a> - Ayman Farahat (Yahoo)</li>\n\t<li><a href=\"https://spark-summit.org/eu-2015/events/hundreds-of-queries-in-the-time-of-one/\">Hundreds of queries in the time of one</a> - Sam Savage &amp; Gianmario Spacagna (Barclays)</li>\n\t<li><a href=\"https://spark-summit.org/eu-2015/events/spark-in-production-lessons-from-100-production-users/\">Spark in Production: Lessons from 100+ production users</a> - Aaron Davidson (Databricks)</li>\n\t<li><a href=\"https://spark-summit.org/eu-2015/events/making-better-news-with-spark/\">Making better news with Spark</a> - Philip Wills (The Guardian)</li>\n\t<li><a href=\"https://spark-summit.org/eu-2015/events/a-scalable-implementation-of-deep-learning-on-spark/\">A Scalable Implementation of Deep Learning on Spark</a> - Alexander Ulanov (Hewlett-Packard Labs) &amp; Xiangrui Meng (Databricks)</li>\n\t<li><a href=\"https://spark-summit.org/eu-2015/events/data-science-at-scale-using-apache-spark-for-data-science-at-bitly/\">Data Science at Scale: Using Apache Spark for Data Science at Bitly</a> - Sarah Guido (Bitly)</li>\n</ul>\n<h2>Training</h2>\nFor people who are interested in becoming Spark experts, there are <a href=\"https://spark-summit.org/eu-2015/spark-training/\">three workshops </a>that cater to different interests:\n<ul>\n\t<li><a href=\"https://spark-summit.org/eu-2015/spark-training/#intro\">Intro to Apache Spark</a>: Learn the core Spark APIs in-depth.</li>\n\t<li><a href=\"https://spark-summit.org/eu-2015/spark-training/#datasci\">Data Science with Spark</a>: Learn how to combine the scalability of Spark with machine learning and graph processing.</li>\n\t<li><a href=\"https://spark-summit.org/eu-2015/spark-training/#devops\">DevOps with Spark</a>: Understanding the deep architecture of Spark Core.</li>\n</ul>\n<h2>How to get tickets</h2>\nTickets are available online now, <a href=\"https://www.prevalentdesignevents.com/sparksummit2015/europe/registration.aspx?source=AgendaBlog\">register</a> before the tickets sell out! Use promo code \u201cDatabricksEU\u201d to receive 20% off your registration fee.\n\nIf you are an academic affiliated to a university, please fill out <a href=\"http://go.spark-summit.org/eu-2015/academic-pass-2015\">this form</a> to apply for a free 2-day pass to Spark Summit EU. Applications will be accepted through September 11, 2015.\n<h2>Thanks to Our Sponsors</h2>\nOur esteemed <a href=\"http://spark-summit.org/eu-2015/sponsors\">sponsors</a> are instrumental in bringing Spark Summit Europe to life. You\u2019ve heard this before but without our sponsors, the Summits wouldn\u2019t happen.\n\n&nbsp;"}
{"status": "publish", "description": "Many of the major changes in Apache Spark 1.5 are under-the-hood changes to improve performance, usability, and operational stability.", "creator": "rxin", "link": "https://databricks.com/blog/2015/09/09/announcing-apache-spark-1-5.html", "authors": null, "id": 5086, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2015-09-09", "tz": "UTC", "createdOn": "2015-09-09"}, "title": "Announcing Apache Spark 1.5", "slug": "announcing-apache-spark-1-5", "content": "The inaugural Spark Summit Europe will be held in Amsterdam this October.\u00a0<a href=\"https://spark-summit.org/eu-2015/\" target=\"_blank\">Check out the full agenda and get your ticket </a>before it sells out!\n\n<hr />\n\nToday we are happy to announce the availability of Apache Spark\u2019s 1.5 release! In this post, we outline the major development themes in Spark 1.5 and some of the new features we are most excited about. In the coming weeks, our blog will feature more detailed posts on specific components of Spark 1.5. For a comprehensive list of features in Spark 1.5, you can also find the detailed Apache release notes below.\n\nMany of the major changes in Spark 1.5 are under-the-hood changes to improve Spark\u2019s <strong>performance, usability, and operational stability</strong>. Spark 1.5 ships major pieces of <a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\" target=\"_blank\">Project Tungsten</a>, an initiative focused on increasing Spark\u2019s performance through several low-level architectural optimizations. The release also adds operational features for the streaming component, such as backpressure support. Another major theme of this release is <strong>data science</strong>: Spark 1.5 ships several new machine learning algorithms and utilities, and extends Spark\u2019s new R API.\n\nOne interesting tidbit is that in Spark 1.5, we have crossed the 10,000 mark for JIRA number (i.e. more than 10,000 tickets have been filed to request features or report bugs). Hopefully the added digit won\u2019t slow down our development too much!\n<h2 style=\"text-align: left;\"><strong>Performance Improvements and Project Tungsten</strong></h2>\nEarlier this year we announced <a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\" target=\"_blank\">Project Tungsten</a> - a set of major changes to Spark\u2019s internal architecture designed to improve performance and robustness. Spark 1.5 delivers the first major pieces of Project Tungsten. This includes <strong>binary processing</strong>, which circumvents the Java object model using a custom binary memory layout. Binary processing significantly reduces garbage collection pressure for data-intensive workloads. It also includes a new <strong>code generation</strong> framework, where optimized byte code is generated at runtime for evaluating expressions in user code. Spark 1.5 adds a large number of built-in functions that are code generated, for common tasks like date handling and string manipulation.\n\nOver the next few weeks, we will be writing about Project Tungsten. To give you a teaser, the chart below compares the out-of-the-box (i.e. no configuration changes) performance of aggregation queries using Spark 1.4 and Spark 1.5, for both small aggregations and large aggregations.\n<h2><img class=\"wp-image-5095 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/09/image01-1024x409.png\" alt=\"image01\" width=\"600\" height=\"240\" /></h2>\nThe release also includes a variety of other performance enhancements. Support for the Apache Parquet file format sees improved input/output performance, with predicate push down now enabled by default and a faster metadata lookup path. Spark\u2019s joins also receive some attention, with a new broadcast outer join operator and the ability to do sort-merge outer joins.\n<h2 style=\"text-align: left;\"><strong>Usability and Interoperability</strong></h2>\nSpark 1.5 also focuses on usability aspects - such as providing interoperability with a wide variety of environments. After all, you can only use Spark if it connects to your data source or works on your cluster. And Spark programs need to be easy to understand if you want to debug them.\n\nSpark 1.5 adds visualization of SQL and DataFrame query plans in the web UI, with dynamic update of operational metrics such as the selectivity of a filter operator and the runtime memory usage of aggregations and joins. Below is an example of a plan visualization from the web UI (click on the image to see details).\n<h2><a href=\"https://databricks.com/wp-content/uploads/2015/09/image00.png\"><img class=\"aligncenter wp-image-5094\" src=\"https://databricks.com/wp-content/uploads/2015/09/image00.png\" alt=\"image00\" width=\"300\" height=\"398\" /></a></h2>\nIn addition, we have invested a significant amount of work to improve interoperability with other ecosystem projects. As an example, using classloader isolation techniques, a single instance of Spark (SQL and DataFrames) can now <strong>connect to multiple versions of Hive </strong>metastores, from Hive 0.12 all the way to Hive 1.2.1. Aside from being able to connect to different metastores, Spark can now <strong>read several Parquet variants</strong> generated by other systems, including parquet-avro, parquet-thrift, parquet-protobuf, Impala, Hive. To the best of our knowledge, Spark is the only system that is capable of connecting to various versions of Hive and supporting the litany of Parquet formats that exist in the wild.\n<h2 style=\"text-align: left;\"><strong>Operational Utilities in Spark Streaming</strong></h2>\nSpark Streaming adds several new features in this release, with a focus on operational stability for long-lived production streaming workloads. These features are largely based on feedback from existing streaming users. Spark 1.5 adds <strong>backpressure support</strong>, which throttles the rate of receiving when the system is in an unstable state. For example, if there is a large burst in input, or a temporary delay in writing output, the system will adjust dynamically and ensure that the streaming program remains stable. This feature was developed in collaboration with Typesafe.\n\nA second operational addition is the ability to load balance and schedule data receivers across a cluster, and better control over re-launching of receivers for long running jobs. Spark streaming also adds several Python API\u2019s in this release, including Amazon Kinesis, Apache Flume, and the MQTT protocol.\n<h2 style=\"text-align: left;\"><strong>Expansion of Data Science API\u2019s</strong></h2>\nOne of the primary focuses of Spark in 2015 is to empower large-scale data science. We kicked this theme off with three major additions to Spark: <a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\" target=\"_blank\">DataFrames</a>, <a href=\"https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html\" target=\"_blank\">machine learning pipelines</a>, and <a href=\"https://databricks.com/blog/2015/06/09/announcing-sparkr-r-on-spark.html\" target=\"_blank\">R language support</a>. These three additions brought APIs similar to best-in-class single-node tools to Spark. In Spark 1.5, we have greatly expanded their capabilities.\n\nAfter the initial release of <strong>DataFrames</strong> in Spark 1.3, one of the most common user requests was to support more string and date/time functions out-of-the-box. We are happy to announce that Spark 1.5 introduces over 100 built-in functions. Almost all of these built-in functions also implement code generation, so applications using them can take advantage of the changes we made as part of Project Tungsten.\n\nR language support was introduced as an alpha component in Spark 1.4. Spark 1.5 improves R usability as well as introduces support for scalable machine learning via integration with MLlib. R frontend now supports <strong>GLMs with R formula</strong>, binomial/Gaussian families, and elastic-net regularization.\n\nFor machine learning, Spark 1.5 brings better coverage for the <strong>new pipeline API</strong>, with new pipeline modules and algorithms. New pipeline features include feature transformers like CountVectorizer, DCT, MinMaxScaler, NGram, PCA, RFormula, StopWordsRemover, and VectorSlicer, algorithms like multilayer perceptron classifier, enhanced tree models, k-means, and naive Bayes, and tuning tools like train-validation split and multiclass classification evaluator. Other new algorithms include PrefixSpan for sequential pattern mining, association rule generation, 1-sample Kolmogorov-Smirnov test, etc.\n<h2 style=\"text-align: left;\"><strong>Growth of Spark Package Ecosystem</strong></h2>\nThe 1.5 release is also a good time to mention the growth of Spark\u2019s <a href=\"http://spark-packages.org/\" target=\"_blank\">package ecosystem</a>. Today, there are more than 100 packages that can be enabled with a simple flag for any Spark program. Packages include machine learning algorithms, connectors to various data sources, experimental new features, and much more. Several packages have released updates coinciding with the Spark 1.5 release, including the spark-csv, spark-redshift, and spark-avro data source connectors.\n\nSpark 1.5.0 featured contributions from more than 230 developers - thanks to everyone who helped make this release possible! Stay tuned to the Databricks blog to learn more about Spark 1.5\u2019s features and get a peek of upcoming Spark development.\n\nFor your convenience, we have attached the entire release notes here. If you want to try out these new features, you can already use Spark 1.5 in Databricks. <a href=\"http://dbricks.co/1Nhoqhf\" target=\"_blank\">Sign up for a 14-day free trial here</a>.\n<h2 style=\"text-align: left;\"><strong>Apache Spark 1.5 Release Notes</strong></h2>\n<h3 id=\"apis-rdd-dataframe-and-sql\" style=\"text-align: left;\"><strong>APIs: RDD, DataFrame and SQL</strong></h3>\n<ul>\n\t<li>Consistent resolution of column names (see Behavior Changes section)</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-3947\">SPARK-3947</a>: New experimental user-defined aggregate function (UDAF) interface</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-8300\">SPARK-8300</a>: DataFrame hint for broadcast joins</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-8668\">SPARK-8668</a>: expr function for turning a SQL expression into a DataFrame column</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-9076\">SPARK-9076</a>: Improved support for NaN values\n<ul>\n\t<li>NaN functions: isnan, nanvl</li>\n\t<li>dropna/fillna also fill/drop NaN values in addition to NULL values</li>\n\t<li>Equality test on NaN = NaN returns true</li>\n\t<li>NaN is greater than all other values</li>\n\t<li>In aggregation, NaN values go into one group</li>\n</ul>\n</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-8828\">SPARK-8828</a>: Sum function returns null when all input values are nulls</li>\n\t<li>Data types\n<ul>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-8943\">SPARK-8943</a>: CalendarIntervalType for time intervals</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-7937\">SPARK-7937</a>: Support ordering on StructType</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-8866\">SPARK-8866</a>: TimestampType\u2019s precision is reduced to 1 microseconds (1us)</li>\n</ul>\n</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-8159\">SPARK-8159</a>: Added ~100 functions, including <strong>date/time</strong>, <strong>string</strong>, <strong>math</strong>.</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-8947\">SPARK-8947</a>: Improved type coercion and error reporting in plan analysis phase (i.e. most errors should be reported in analysis time, rather than execution time)</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-1855\">SPARK-1855</a>: Memory and local disk only checkpointing support</li>\n</ul>\n<h3 id=\"backend-execution-dataframe-and-sql\" style=\"text-align: left;\"><strong>Backend Execution: DataFrame and SQL</strong></h3>\n<ul>\n\t<li><strong>Code generation on by default</strong> for almost all DataFrame/SQL functions</li>\n\t<li><strong>Improved aggregation</strong> execution in DataFrame/SQL\n<ul>\n\t<li>Cache friendly in-memory hash map layout</li>\n\t<li>Fallback to external-sort-based aggregation when memory is exhausted</li>\n\t<li>Code generation on by default for aggregations</li>\n</ul>\n</li>\n\t<li><strong>Improved join</strong> execution in DataFrame/SQL\n<ul>\n\t<li>Prefer (external) sort-merge join over hash join in shuffle joins (for left/right outer and inner joins), i.e. join data size is now bounded by disk rather than memory</li>\n\t<li>Support using (external) sort-merge join method for left/right outer joins</li>\n\t<li>Support for broadcast outer join</li>\n</ul>\n</li>\n\t<li><strong>Improved sort</strong> execution in DataFrame/SQL\n<ul>\n\t<li>Cache-friendly in-memory layout for sorting</li>\n\t<li>Fallback to external sorting when data exceeds memory size</li>\n\t<li>Code generated comparator for fast comparisons</li>\n</ul>\n</li>\n\t<li><strong>Native memory management &amp; representation</strong>\n<ul>\n\t<li>Compact binary in-memory data representation, leading to lower memory usage</li>\n\t<li>Execution memory is explicitly accounted for, without relying on JVM GC, leading to less GC and more robust memory management</li>\n</ul>\n</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-8638\">SPARK-8638</a>: <strong>Improved performance &amp; memory usage in window functions</strong></li>\n\t<li><strong>Metrics instrumentation, reporting, and visualization</strong>\n<ul>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-8856\">SPARK-8856</a>: Plan visualization for DataFrame/SQL</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-8735\">SPARK-8735</a>: Expose metrics for runtime memory usage in web UI</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-4598\">SPARK-4598</a>: Pagination for jobs with large number of tasks in web UI</li>\n</ul>\n</li>\n</ul>\n<h3></h3>\n<h3 id=\"integrations-data-sources-hive-hadoop-mesos-and-cluster-management\" style=\"text-align: left;\"><span style=\"color: #000000;\"><strong>Integrations: Data Sources, Hive, Hadoop, Mesos and Cluster Management</strong></span></h3>\n<ul>\n\t<li><strong>Mesos</strong>\n<ul>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-6284\">SPARK-6284</a>: Support framework authentication and Mesos roles</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-6287\">SPARK-6287</a>: Dynamic allocation in Mesos coarse-grained mode</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-6707\">SPARK-6707</a>: User specified constraints on Mesos slave attributes</li>\n</ul>\n</li>\n\t<li><strong>YARN</strong>\n<ul>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-4352\">SPARK-4352</a>: Dynamic allocation in YARN works with preferred locations</li>\n</ul>\n</li>\n\t<li><strong>Standalone Cluster Manager</strong>\n<ul>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-4751\">SPARK-4751</a>: Dynamic resource allocation support</li>\n</ul>\n</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-6906\">SPARK-6906</a>: Improved <strong>Hive and metastore support</strong>\n<ul>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-8131\">SPARK-8131</a>: Improved Hive database support</li>\n\t<li>Upgraded Hive dependency Hive 1.2</li>\n\t<li>Support connecting to Hive 0.13, 0.14, 1.0/0.14.1, 1.1, 1.2 metastore</li>\n\t<li>Support partition pruning pushdown into the metastore (off by default; config flag spark.sql.hive.metastorePartitionPruning)</li>\n\t<li>Support persisting data in Hive compatible format in metastore</li>\n</ul>\n</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-9381\">SPARK-9381</a>: Support data partitioning for <strong>JSON</strong> data sources</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-5463\">SPARK-5463</a>: <strong>Parquet</strong> improvements\n<ul>\n\t<li>Upgrade to Parquet 1.7</li>\n\t<li>Speedup metadata discovery and schema merging</li>\n\t<li>Predicate pushdown on by default</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-6774\">SPARK-6774</a>: Support for reading non-standard legacy Parquet files generated by various libraries/systems by fully implementing all backwards-compatibility rules defined in parquet-format spec</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-4176\">SPARK-4176</a>: Support for writing decimal values with precision greater than 18</li>\n</ul>\n</li>\n\t<li><strong>ORC</strong> improvements (various bug fixes)</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-8890\">SPARK-8890</a>: Faster and more robust <strong>dynamic partition insert</strong></li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-9486\">SPARK-9486</a>: DataSourceRegister interface for external data sources to specify short names</li>\n</ul>\n<h3 id=\"r-language\" style=\"text-align: left;\"><strong>R Language</strong></h3>\n<ul>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-6797\">SPARK-6797</a>: Support for <strong>YARN cluster mode in R</strong></li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-6805\">SPARK-6805</a>: <strong>GLMs with R formula</strong>, binomial/Gaussian families, and elastic-net regularization</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-8742\">SPARK-8742</a>: Improved error messages for R</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-9315\">SPARK-9315</a>: Aliases to make DataFrame functions more R-like</li>\n</ul>\n<h3 id=\"machine-learning-and-advanced-analytics\" style=\"text-align: left;\"><strong>Machine Learning and Advanced Analytics</strong></h3>\n<ul>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-8521\">SPARK-8521</a>: <strong>New Feature transformers</strong>: CountVectorizer, Discrete Cosine transformation, MinMaxScaler, NGram, PCA, RFormula, StopWordsRemover, and VectorSlicer.</li>\n\t<li><strong>New Estimators in Pipeline API</strong>: <a href=\"https://issues.apache.org/jira/browse/SPARK-8600\">SPARK-8600</a> naive Bayes, <a href=\"https://issues.apache.org/jira/browse/SPARK-7879\">SPARK-7879</a> k-means, and <a href=\"https://issues.apache.org/jira/browse/SPARK-8671\">SPARK-8671</a> isotonic regression.</li>\n\t<li><strong>New Algorithms</strong>: <a href=\"https://issues.apache.org/jira/browse/SPARK-9471\">SPARK-9471</a> multilayer perceptron classifier, <a href=\"https://issues.apache.org/jira/browse/SPARK-6487\">SPARK-6487</a> PrefixSpan for sequential pattern mining, <a href=\"https://issues.apache.org/jira/browse/SPARK-8559\">SPARK-8559</a> association rule generation, <a href=\"https://issues.apache.org/jira/browse/SPARK-8598\">SPARK-8598</a> 1-sample Kolmogorov-Smirnov test, etc.</li>\n\t<li><strong>Improvements to existing algorithms</strong>\n<ul>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-5572\"><strong>LDA</strong></a>: online LDA performance, asymmetric doc concentration, perplexity, log-likelihood, top topics/documents, save/load, etc.</li>\n\t<li><strong>Trees and ensembles</strong>: class probabilities, feature importance for random forests, thresholds for classification, checkpointing for GBTs, etc.</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-9436\"><strong>Pregel-API</strong></a>: more efficient Pregel API implementation for GraphX.</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-5016\"><strong>GMM</strong></a>: distribute matrix inversions.</li>\n</ul>\n</li>\n\t<li><strong>Model summary</strong> for <a href=\"https://issues.apache.org/jira/browse/SPARK-8539\">linear</a> and <a href=\"https://issues.apache.org/jira/browse/SPARK-9112\">logistic regression</a>.</li>\n\t<li><strong>Python API</strong>: <a href=\"https://issues.apache.org/jira/browse/SPARK-5572\">distributed matrices</a>, <a href=\"https://issues.apache.org/jira/browse/SPARK-5572\">streaming k-means</a> and <a href=\"https://issues.apache.org/jira/browse/SPARK-5572\">linear models</a>, <a href=\"https://issues.apache.org/jira/browse/SPARK-5572\">LDA</a>, <a href=\"https://issues.apache.org/jira/browse/SPARK-5572\">power iteration clustering</a>, etc.</li>\n\t<li><strong>Tuning and evaluation</strong>: <a href=\"https://issues.apache.org/jira/browse/SPARK-8484\">train-validation split</a> and <a href=\"https://issues.apache.org/jira/browse/SPARK-7690\">multiclass classification evaluator</a>.</li>\n\t<li><strong>Documentation</strong>: document the release version of public API methods</li>\n</ul>\n<h3 id=\"spark-streaming\" style=\"text-align: left;\"><strong>Spark Streaming</strong></h3>\n<ul>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-7398\">SPARK-7398</a>: <strong>Backpressure</strong>: Automatic and dynamic rate controlling in Spark Streaming for handling bursty input streams. This allows a streaming pipeline to dynamically adapt to changes in ingestion rates and computation loads. This works with receivers, as well as, the Direct Kafka approach.</li>\n\t<li><strong>Python API for streaming sources</strong>\n<ul>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-8389\">SPARK-8389</a>: Kafka offsets of Direct Kafka streams available through Python API</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-8564\">SPARK-8564</a>: Kinesis Python API</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-8378\">SPARK-8378</a>: Flume Python API</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-5155\">SPARK-5155</a>: MQTT Python API</li>\n</ul>\n</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-3258\">SPARK-3258</a>: <strong>Python API for streaming machine learning algorithms</strong>: K-Means, linear regression, and logistic regression</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-9215\">SPARK-9215</a>: <strong>Improved reliability of Kinesis streams</strong> : No need for enabling write ahead logs for saving and recovering received data across driver failures</li>\n\t<li><strong>Direct Kafka API graduated</strong>: Not experimental any more.</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-8701\">SPARK-8701</a>: <strong>Input metadata in UI</strong>: Kafka offsets, and input files are visible in the batch details UI</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-8882\">SPARK-8882</a>: Better load balancing and scheduling of receivers across cluster</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-4072\">SPARK-4072</a>: Include streaming storage in web UI</li>\n</ul>\n<h3 id=\"deprecations-removals-configs-and-behavior-changes\" style=\"text-align: left;\"><strong>Deprecations, Removals, Configs, and Behavior Changes</strong></h3>\n<h4 id=\"spark-core\" style=\"text-align: left;\">Spark Core</h4>\n<ul>\n\t<li>DAGScheduler\u2019s local task execution mode has been removed</li>\n\t<li>Default driver and executor memory increased from 512m to 1g</li>\n\t<li>Default setting of JVM\u2019s MaxPermSize increased from 128m to 256m</li>\n\t<li>Default logging level of spark-shell changed from INFO to WARN</li>\n\t<li>NIO-based ConnectionManager is deprecated, and will be removed in 1.6</li>\n</ul>\n<h4 id=\"spark-sql--dataframes\" style=\"text-align: left;\">Spark SQL &amp; DataFrames</h4>\n<ul>\n\t<li>Optimized execution using manually managed memory (Tungsten) is now enabled by default, along with code generation for expression evaluation. These features can both be disabled by setting spark.sql.tungsten.enabled to false.</li>\n\t<li>Parquet schema merging is no longer enabled by default. It can be re-enabled by setting spark.sql.parquet.mergeSchema to true.</li>\n\t<li>Resolution of strings to columns in Python now supports using dots (.) to qualify the column or access nested values. For example df[\u2018table.column.nestedField\u2019]. However, this means that if your column name contains any dots you must now escape them using backticks (e.g., <code>table.`column.with.dots`.nested</code>).</li>\n\t<li>In-memory columnar storage partition pruning is on by default. It can be disabled by setting spark.sql.inMemoryColumnarStorage.partitionPruning to false.</li>\n\t<li>Unlimited precision decimal columns are no longer supported, instead Spark SQL enforces a maximum precision of 38. When inferring schema from BigDecimal objects, a precision of (38, 18) is now used. When no precision is specified in DDL then the default remains Decimal(10, 0).</li>\n\t<li>Timestamps are now processed at a precision of 1us, rather than 100ns.</li>\n\t<li>Sum function returns null when all input values are nulls (null before 1.4, 0 in 1.4).</li>\n\t<li>In the sql dialect, floating point numbers are now parsed as decimal. HiveQL parsing remains unchanged.</li>\n\t<li>The canonical name of SQL/DataFrame functions are now lower case (e.g. sum vs SUM).</li>\n\t<li>It has been determined that using the DirectOutputCommitter when speculation is enabled is unsafe and thus this output committer will not be used by parquet when speculation is on, independent of configuration.</li>\n\t<li>JSON data source will not automatically load new files that are created by other applications (i.e. files that are not inserted to the dataset through Spark SQL). For a JSON persistent table (i.e. the metadata of the table is stored in Hive Metastore), users can use REFRESH TABLE SQL command or HiveContext\u2019s refreshTable method to include those new files to the table. For a DataFrame representing a JSON dataset, users need to recreate the DataFrame and the new DataFrame will include new files.</li>\n</ul>\n<h4 id=\"spark-streaming-1\" style=\"text-align: left;\">Spark Streaming</h4>\n<ul>\n\t<li>New experimental backpressure feature can be enabled by setting the configuration spark.streaming.backpressure.enabled to true.</li>\n\t<li>Write Ahead Log does not need to be abled for Kinesis streams. The updated Kinesis receiver keeps track of Kinesis sequence numbers received in each batch, and uses that information re-read the necessary data while recovering from failures.</li>\n\t<li>The number of times the receivers are relaunched on failure are not limited by the max Spark task attempts. The system will always try to relaunch receivers after failures until the StreamingContext is stopped.</li>\n\t<li>Improved load balancing of receivers across the executors, even after relaunching.</li>\n\t<li>Enabling checkpointing when using queueStream throws exception as queueStream cannot be checkpointed. However, we found this to break certain existing apps. So this change will be reverted in Spark 1.5.1.</li>\n</ul>\n<h4 id=\"mllib\" style=\"text-align: left;\">MLlib</h4>\nIn the spark.mllib package, there are no breaking API changes but some behavior changes:\n<ul>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-9005\">SPARK-9005</a>: RegressionMetrics.explainedVariance returns the average regression sum of squares.</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-8600\">SPARK-8600</a>: NaiveBayesModel.labels become sorted.</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-3382\">SPARK-3382</a>: GradientDescent has a default convergence tolerance 1e-3, and hence iterations might end earlier than 1.4.</li>\n</ul>\nIn the experimental spark.ml package, there exists one breaking API change and one behavior change:\n<ul>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-9268\">SPARK-9268</a>: Java\u2019s varargs support is removed from Params.setDefault due to a Scala compiler bug.</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-10097\">SPARK-10097</a>: Evaluator.isLargerBetter is added to indicate metric ordering. Metrics like RMSE no longer flip signs as in 1.4.</li>\n</ul>\n<h3 id=\"known-issues\" style=\"text-align: left;\"><strong>Known Issues</strong></h3>\nThe following issues are known in 1.5.0, and will be fixed in 1.5.1 release.\n<h4 id=\"sqldataframe\" style=\"text-align: left;\">SQL/DataFrame</h4>\n<ul>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-10301\">SPARK-10301</a>: Reading parquet files with different schema (schema merging) for nested structs can return the wrong answer</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-10466\">SPARK-10466</a>: AssertionError when spilling data during sort-based shuffle with data spill</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-10441\">SPARK-10441</a>: Timestamp data type cannot be written out as JSON</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-10495\">SPARK-10495</a>: Date values saved to JSON are stored as strings representing the number of days from epoch (1970-01-01 00:00:00 UTC) instead of strings in the format of \u201cyyyy-mm-dd\u201d.</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-10403\">SPARK-10403</a>: Tungsten mode does not work with tungsten-sort shuffle manager (which is off by default)</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-10422\">SPARK-10422</a>: In-memory cache of string type with dictionary encoding is broken</li>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-10434\">SPARK-10434</a> Parquet files with null elements in arrays written by Spark 1.5.0 cannot be read by earlier versions of Spark</li>\n</ul>\n<h4 id=\"streaming\" style=\"text-align: left;\">Streaming</h4>\n<ul>\n\t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-10224\">SPARK-10224</a> Small chance of data loss when StreamingContext is stopped gracefully</li>\n</ul>\n<h3 id=\"credits\" style=\"text-align: left;\"><strong>Credits</strong></h3>\nWe would like to thank the following organizations for testing the release candidates with their workloads: Tencent, Mesosphere, Typesafe, Palantir, Cloudera, Hortonworks, Huawei, Shopify, Netflix, Intel, Yahoo, Kixer, UC Berkeley and Databricks.\n\nLast but not least, this release would not have been possible without the following contributors: Aaron Davidson, Adam Roberts, Ai He, Akshat Aranya, Alex Shkurenko, Alex Slusarenko, Alexander Ulanov, Alok Singh, Amey Chaugule, Andrew Or, Andrew Ray, Animesh Baranawal, Ankur Chauhan, Ankur Dave, Ben Fradet, Bimal Tandel, Brennan Ashton, Brennon York, Brian Lockwood, Bryan Cutler, Burak Yavuz, Calvin Jia, Carl Anders Duvel, Carson Wang, Chen Xu, Cheng Hao, Cheng Lian, Cheolsoo Park, Chris Freeman, Christian Kadner, Cody Koeninger, Damian Guy, Daniel Darabos, Daniel Emaasit, Daoyuan Wang, Dariusz Kobylarz, David Arroyo Cazorla, Davies Liu, DB Tsai, Dennis Huo, Deron Eriksson, Devaraj K, Dibyendu Bhattacharya, Dong Wang, Emiliano Leporati, Eric Liang, Favio Vazquez, Felix Cheung, Feynman Liang, Forest Fang, Francois Garillot, Gen Tang, George Dittmar, Guo Wei, GuoQiang Li, Han JU, Hao Zhu, Hari Shreedharan, Herman Van Hovell, Holden Karau, Hossein Falaki, Huang Zhaowei, Hyukjin Kwon, Ilya Ganelin, Imran Rashid, Iulian Dragos, Jacek Lewandowski, Jacky Li, Jan Prach, Jean Lyn, Jeff Zhang, Jiajin Zhang, Jie Huang, Jihong MA, Jonathan Alter, Jose Cambronero, Joseph Batchik, Joseph Gonzalez, Joseph K. Bradley, Josh Rosen, Judy Nash, Juhong Park, Kai Sasaki, Kai Zeng, KaiXinXiaoLei, Kan Zhang, Kashif Rasul, Kay Ousterhout, Keiji Yoshida, Kenichi Maehashi, Keuntae Park, Kevin Conor, Konstantin Shaposhnikov, Kousuke Saruta, Kun Xu, Lars Francke, Leah McGuire, lee19, Liang-Chi Hsieh, Lianhui Wang, Luca Martinetti, Luciano Resende, Manoj Kumar, Marcelo Vanzin, Mark Smith, Martin Zapletal, Matei Zaharia, Mateusz Buskiewicz, Matt Massie, Matthew Brandyberry, Meethu Mathew, Meihua Wu, Michael Allman, Michael Armbrust, Michael Davies, Michael Sannella, Michael Vogiatzis, Michel Lemay, Mike Dusenberry, Min Zhou, Mingfei Shi, mosessky, Moussa Taifi, Mridul Muralidharan, NamelessAnalyst, Namit Katariya, Nan Zhu, Nathan Howell, Navis Ryu, Neelesh Srinivas Salian, Nicholas Chammas, Nicholas Hwang, Nilanjan Raychaudhuri, Niranjan Padmanabhan, Nishkam Ravi, Nishkam Ravi, Noel Smith, Oleksiy Dyagilev, Oleksiy Dyagilev, Paavo Parkkinen, Patrick Baier, Patrick Wendell, Pawel Kozikowski, Pedro Rodriguez, Perinkulam I. Ganesh, Piotr Migdal, Prabeesh K, Pradeep Chhetri, Prayag Chandran, Punya Biswal, Qian Huang, Radek Ostrowski, Rahul Palamuttam, Ram Sriharsha, Rekha Joshi, Rekha Joshi, Rene Treffer, Reynold Xin, Roger Menezes, Rohit Agarwal, Rosstin Murphy, Rowan Chattaway, Ryan Williams, Saisai Shao, Sameer Abhyankar, Sandy Ryza, Santiago M. Mola, Scott Taylor, Sean Owen, Sephiroth Lin, Seth Hendrickson, Sheng Li, Shilei Qian, Shivaram Venkataraman, Shixiong Zhu, Shuo Bai, Shuo Xiang, Simon Hafner, Spiro Michaylov, Stan Zhai, Stefano Parmesan, Steve Lindemann, Steve Loughran, Steven She, Su Yan, Sudhakar Thota, Sun Rui, Takeshi YAMAMURO, Takuya Ueshin, Tao Li, Tarek Auel, Tathagata Das, Ted Blackman, Ted Yu, Thomas Omans, Thomas Szymanski, Tien-Dung Le, Tijo Thomas, Tim Ellison, Timothy Chen, Tom Graves, Tom White, Tomohiko K., Vincent D. Warmerdam, Vinod K C, Vinod KC, Vladimir Vladimirov, Vyacheslav Baranov, Wang Tao, Wang Wei, Weizhong Lin, Wenchen Fan, Wisely Chen, Xiangrui Meng, Xu Tingjun, Xusen Yin, Yadong Qi, Yanbo Liang, Yash Datta, Yijie Shen, Yin Huai, Yong Tang, Yu ISHIKAWA, Yuhao Yang, Yuming Wang, Yuri Saito, Yuu ISHIKAWA, Zc He, Zhang, Liye, Zhichao Li, Zhongshuai Pei, Zoltan Zvara, and a few unknown contributors (please indicate your email and name in your git commit to show up here).\n\nYou can download the release at <a href=\"http://spark.apache.org/downloads.html\" target=\"_blank\">http://spark.apache.org/downloads.html</a>"}
{"status": "publish", "description": null, "creator": "michael", "link": "https://databricks.com/blog/2015/09/16/apache-spark-1-5-dataframe-api-highlights.html", "authors": null, "id": 5113, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2015-09-16", "tz": "UTC", "createdOn": "2015-09-16"}, "title": "Apache Spark 1.5 DataFrame API Highlights: Date/Time/String Handling, Time Intervals, and UDAFs", "slug": "apache-spark-1-5-dataframe-api-highlights", "content": "<em>To try new features highlighted in this blog post, <a href=\"http://spark.apache.org/downloads.html\">download Spark 1.5</a> or <a href=\"https://accounts.cloud.databricks.com/registration.html#signup\">sign up Databricks for a 14-day free trial today</a>.</em>\n\n<hr />\n\nA few days ago, <a href=\"https://databricks.com/blog/2015/09/09/announcing-spark-1-5.html\">we announced the release of Apache Spark 1.5</a>. This release contains major under-the-hood changes that improve Spark\u2019s performance, usability, and operational stability. Besides these changes, we have been continuously improving DataFrame API. In this blog post, we\u2019d like to highlight three major improvements to DataFrame API in Spark 1.5, which are:\n<ul>\n\t<li>New built-in functions;</li>\n\t<li>Time intervals; and</li>\n\t<li>Experimental user-defined aggregation function (UDAF) interface.</li>\n</ul>\n<h2>New Built-in Functions in Spark 1.5</h2>\nIn Spark 1.5, we have added a comprehensive list of built-in functions to the DataFrame API, complete with optimized code generation for execution. This code generation allows pipelines that call functions to take full advantage of the efficiency changes made as part of <a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\">Project Tungsten</a>. With these new additions, Spark SQL now supports a wide range of built-in functions for various use cases, including:\n<table class=\"table\">\n<thead>\n<tr>\n<th>Category</th>\n<th>Functions</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Aggregate Functions</td>\n<td><code>approxCountDistinct, avg, count, countDistinct, first, last, max, mean, min, sum, sumDistinct</code></td>\n</tr>\n<tr>\n<td>Collection Functions</td>\n<td><code>array_contains, explode, size, sort_array</code></td>\n</tr>\n<tr>\n<td>Date/time Functions</td>\n<td><b>Date/timestamp conversion:</b>\n\n<code>unix_timestamp, from_unixtime, to_date, quarter, day, dayofyear, weekofyear, from_utc_timestamp, to_utc_timestamp</code>\n\n<b>Extracting fields from a date/timestamp value:</b>\n\n<code>year, month, dayofmonth, hour, minute, second</code>\n\n<b>Date/timestamp calculation:</b>\n\n<code>datediff, date_add, date_sub, add_months, last_day, next_day, months_between</code>\n\n<b>Misc.:</b>\n\n<code>current_date, current_timestamp, trunc, date_format</code></td>\n</tr>\n<tr>\n<td>Math Functions</td>\n<td><code>abs, acros, asin, atan, atan2, bin, cbrt, ceil, conv, cos, sosh, exp, expm1, factorial, floor, hex, hypot, log, log10, log1p, log2, pmod, pow, rint, round, shiftLeft, shiftRight, shiftRightUnsigned, signum, sin, sinh, sqrt, tan, tanh, toDegrees, toRadians, unhex</code></td>\n</tr>\n<tr>\n<td>Misc. Functions</td>\n<td><code>array, bitwiseNOT, callUDF, coalesce, crc32, greatest, if, inputFileName, isNaN, isnotnull, isnull, least, lit, md5, monotonicallyIncreasingId, nanvl, negate, not, rand, randn, sha, sha1, sparkPartitionId, struct, when</code></td>\n</tr>\n<tr>\n<td>String Functions</td>\n<td><code>ascii, base64, concat, concat_ws, decode, encode, format_number, format_string, get_json_object, initcap, instr, length, levenshtein, locate, lower, lpad, ltrim, printf, regexp_extract, regexp_replace, repeat, reverse, rpad, rtrim, soundex, space, split, substring, substring_index, translate, trim, unbase64, upper</code></td>\n</tr>\n<tr>\n<td>Window Functions (in&nbsp;addition to&nbsp;Aggregate Functions)</td>\n<td><code>cumeDist, denseRank, lag, lead, ntile, percentRank, rank, rowNumber</code></td>\n</tr>\n</tbody>\n</table>\nFor all available built-in functions, please refer to our API docs (<a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\">Scala Doc</a>, <a href=\"https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/functions.html\">Java Doc</a>, and <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\">Python Doc</a>).\n\nUnlike normal functions, which execute immediately and return a result, DataFrame functions return a <code>Column</code>, that will be evaluated inside of a parallel job. These columns can be used inside of DataFrame operations, such as <code>select</code>, <code>filter</code>, <code>groupBy</code>, etc. The input to a function can either be another Column (i.e. <code>df['columnName']</code>) or a literal value (i.e. a constant value). To make this more concrete, let\u2019s look at the syntax for calling the <code>round</code> function in Python.\n\n<code>round</code> is a function that rounds a numeric value to the specified precision. When the given precision is a positive number, a given input numeric value is rounded to the decimal position specified by the precision. When the specified precision is a zero or a negative number, a given input numeric value is rounded to the position of the integral part specified by the precision.\n\n[python]\n# Create a simple DataFrame\ndata = [\n  (234.5, &quot;row1&quot;),\n  (23.45, &quot;row2&quot;),\n  (2.345, &quot;row3&quot;),\n  (0.2345, &quot;row4&quot;)]\ndf = sqlContext.createDataFrame(data, [&quot;i&quot;, &quot;j&quot;])\n\n# Import functions provided by Spark\u2019s DataFrame API\nfrom pyspark.sql.functions import *\n\n# Call round function directly\ndf.select(\n  round(df['i'], 1),\n  round(df['i'], 0),\n  round(df['i'], -1)).show()\n\n\n+----------+----------+-----------+\n|round(i,1)|round(i,0)|round(i,-1)|\n+----------+----------+-----------+\n|\u00a0\u00a0\u00a0\u00a0\u00a0234.5|     235.0|      230.0|\n|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a023.5|      23.0|       20.0|\n|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a02.3|       2.0|        0.0|\n|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a00.2|       0.0|        0.0|\n+----------+----------+-----------+\n[/python]\n\nAlternatively, all of the added functions are also available from SQL using standard syntax:\n\n[sql]SELECT round(i, 1) FROM dataFrame[/sql]\n\nFinally, you can even mix and match SQL syntax with DataFrame operations by using the <code>expr</code> function. By using <code>expr</code>, you can construct a DataFrame column expression from a SQL expression String.\n\n[python]\ndf.select(\n  expr(&quot;round(i, 1) AS rounded1&quot;),\n  expr(&quot;round(i, 0) AS rounded2&quot;),\n  expr(&quot;round(i, -1) AS rounded3&quot;)).show()\n[/python]\n\n<h2></h2>\n<h2>Time Interval Literals</h2>\nIn the last section, we introduced several new date and time functions that were added in Spark 1.5 (e.g. <code>datediff</code>, <code>date_add</code>, <code>date_sub</code>), but that is not the only new feature that will help users dealing with date or timestamp values. Another related feature is a new data type, interval, that allows developers to represent fixed periods of time (i.e. 1 day or 2 months) as interval literals. Using interval literals, it is possible to perform subtraction or addition of an arbitrary amount of time from a date or timestamp value. This representation can be useful when you want to add or subtract a time period from a fixed point in time. For example, users can now easily express queries like <em>\u201cFind all transactions that have happened during the past hour\u201d</em>.\n\nAn interval literal is constructed using the following syntax:\n\n[sql]INTERVAL value unit[/sql]\n\nBreaking the above expression down, all time intervals start with the <code>INTERVAL</code> keyword. Next, the value and unit together specify the time difference. Available units are <code>YEAR</code>, <code>MONTH</code>, u<code>DAY</code>, <code>HOUR</code>, <code>MINUTE</code>, <code>SECOND</code>, <code>MILLISECOND</code>, and <code>MICROSECOND</code>. For example, the following interval literal represents 3 years.\n\n[sql]INTERVAL 3 YEAR[/sql]\n\nIn addition to specifying an interval literal with a single unit, users can also combine different units. For example, the following interval literal represents a 3-year and 3-hour time difference.\n\n[sql]INTERVAL 3 YEAR 3 HOUR[/sql]\n\nIn the DataFrame API, the <code>expr</code> function can be used to create a <code>Column</code> representing an interval. The following code in Python is an example of using an interval literal to select records where <code>start_time</code> and <code>end_time</code> are in the same day and they differ by less than an hour.\n\n[python]\n# Import functions.\nfrom pyspark.sql.functions import *\n\n# Create a simple DataFrame.\ndata = [\n  (&quot;2015-01-01 23:59:59&quot;, &quot;2015-01-02 00:01:02&quot;, 1),\n  (&quot;2015-01-02 23:00:00&quot;, &quot;2015-01-02 23:59:59&quot;, 2),\n  (&quot;2015-01-02 22:59:58&quot;, &quot;2015-01-02 23:59:59&quot;, 3)]\ndf = sqlContext.createDataFrame(data, [&quot;start_time&quot;, &quot;end_time&quot;, &quot;id&quot;])\ndf = df.select(\n  df.start_time.cast(&quot;timestamp&quot;).alias(&quot;start_time&quot;),\n  df.end_time.cast(&quot;timestamp&quot;).alias(&quot;end_time&quot;),\n  df.id)\n\n# Get all records that have a start_time and end_time in the\n# same day, and the difference between the end_time and start_time\n# is less or equal to 1 hour.\ncondition = \\\n  (to_date(df.start_time) == to_date(df.end_time)) &amp; \\\n  (df.start_time + expr(&quot;INTERVAL 1 HOUR&quot;) &gt;= df.end_time)\n\ndf.filter(condition).show()\n+---------------------+---------------------+---+\n|start_time           |            end_time |id |\n+---------------------+---------------------+---+\n|2015-01-02 23:00:00.0|2015-01-02 23:59:59.0|2  |\n+---------------------+---------------------+---+\n[/python]\n\n<h2>User-defined Aggregate Function Interface</h2>\nFor power users, Spark 1.5 introduces an experimental API for user-defined aggregate functions (UDAFs). These UDAFs can be used to compute custom calculations over groups of input data (in contrast, UDFs compute a value looking at a single input row), such as calculating geometric mean or calculating the product of values for every group.\n\nA UDAF maintains an aggregation buffer to store intermediate results for every group of input data. It updates this buffer for every input row. Once it has processed all input rows, it generates a result value based on values of the aggregation buffer.\n\nAn UDAF inherits the base class <code>UserDefinedAggregateFunction</code> and implements the following eight methods, which are:\n<ul>\n\t<li><code>inputSchema: inputSchema</code> returns a <code>StructType</code> and every field of this StructType represents an input argument of this UDAF.</li>\n\t<li><code>bufferSchema: bufferSchema</code> returns a <code>StructType</code> and every field of this StructType represents a field of this UDAF\u2019s intermediate results.</li>\n\t<li><code>dataType: dataType</code> returns a <code>DataType</code> representing the data type of this UDAF\u2019s returned value.</li>\n\t<li><code>deterministic: deterministic</code> returns a boolean indicating if this UDAF always generate the same result for a given set of input values.</li>\n\t<li><code>initialize: initialize</code> is used to initialize values of an aggregation buffer, represented by a <code>MutableAggregationBuffer</code>.</li>\n\t<li><code>update: update</code> is used to update an aggregation buffer represented by a <code>MutableAggregationBuffer</code> for an input <code>Row</code>.</li>\n\t<li><code>merge: merge</code> is used to merge two aggregation buffers and store the result to a <code>MutableAggregationBuffer</code>.</li>\n\t<li><code>evaluate: evaluate</code> is used to generate the final result value of this UDAF based on values stored in an aggregation buffer represented by a <code>Row</code>.</li>\n</ul>\nBelow is an example UDAF implemented in Scala that calculates the <a href=\"https://en.wikipedia.org/wiki/Geometric_mean\">geometric mean</a> of the given set of double values. The geometric mean can be used as an indicator of the typical value of an input set of numbers by using the product of their values (as opposed to the standard builtin mean which is based on the sum of the input values). For the purpose of simplicity, null handling logic is not shown in the following code.\n\n[scala]\nimport org.apache.spark.sql.expressions.MutableAggregationBuffer\nimport org.apache.spark.sql.expressions.UserDefinedAggregateFunction\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.types._\n\nclass GeometricMean extends UserDefinedAggregateFunction {\n  def inputSchema: org.apache.spark.sql.types.StructType =\n    StructType(StructField(&quot;value&quot;, DoubleType) :: Nil)\n\n  def bufferSchema: StructType = StructType(\n    StructField(&quot;count&quot;, LongType) ::\n    StructField(&quot;product&quot;, DoubleType) :: Nil\n  )\n\n  def dataType: DataType = DoubleType\n\n  def deterministic: Boolean = true\n\n  def initialize(buffer: MutableAggregationBuffer): Unit = {\n    buffer(0) = 0L\n    buffer(1) = 1.0\n  }\n\n  def update(buffer: MutableAggregationBuffer,input: Row): Unit = {\n    buffer(0) = buffer.getAs[Long](0) + 1\n    buffer(1) = buffer.getAs[Double](1) * input.getAs[Double](0)\n  }\n\n  def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {\n    buffer1(0) = buffer1.getAs[Long](0) + buffer2.getAs[Long](0)\n    buffer1(1) = buffer1.getAs[Double](1) * buffer2.getAs[Double](1)\n  }\n\n  def evaluate(buffer: Row): Any = {\n    math.pow(buffer.getDouble(1), 1.toDouble / buffer.getLong(0))\n  }\n}\n[/scala]\n\nA UDAF can be used in two ways. First, an instance of a UDAF can be used immediately as a function. Second, users can register a UDAF to Spark SQL\u2019s function registry and call this UDAF by the assigned name. The example code is shown below.\n\n[scala]\nimport org.apache.spark.sql.functions._\n// Create a simple DataFrame with a single column called &quot;id&quot;\n// containing number 1 to 10.\nval df = sqlContext.range(1, 11)\n\n// Create an instance of UDAF GeometricMean.\nval gm = new GeometricMean\n\n// Show the geometric mean of values of column &quot;id&quot;.\ndf.groupBy().agg(gm(col(&quot;id&quot;)).as(&quot;GeometricMean&quot;)).show()\n\n// Register the UDAF and call it &quot;gm&quot;.\nsqlContext.udf.register(&quot;gm&quot;, gm)\n// Invoke the UDAF by its assigned name.\ndf.groupBy().agg(expr(&quot;gm(id) as GeometricMean&quot;)).show()\n[/scala]\n\n<h2>Summary</h2>\nIn this blog post, we introduced three major additions to DataFrame APIs, a set of built-in functions, time interval literals, and user-defined aggregation function interface. With new built-in functions, it is easier to manipulate string data and data/timestamp data, and to apply math operations. If your existing programs use any user-defined functions that do the same work with these built-in functions, we strongly recommend you to migrate your code to these new built-in functions to take full advantage of the efficiency changes made as part of <a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\">Project Tungsten</a>. Combining date/time functions and interval literals, it is much easier to work with date/timestamp data and to calculate date/timestamp values for various use cases. With user-defined aggregate function, users can apply custom aggregations over groups of input data in the DataFrame API.\n\nTo try new these new features, <a href=\"http://spark.apache.org/downloads.html\">download Spark 1.5</a> or <a href=\"https://accounts.cloud.databricks.com/registration.html#signup\">sign up Databricks for a 14-day free trial today</a>.\n<h2>Acknowledgements</h2>\nThe development of features highlighted in this blog post has been a community effort. In particular, we would like to thank the following contributors: Adrian Wang, Tarek Auel, Yijie Shen, Liang-Chi Hsieh, Zhichao Li, Pedro Rodriguez, Cheng Hao, Shilei Qian, Nathan Howell, and Wenchen Fan."}
{"status": "publish", "description": "Apache Spark 1.4 and 1.5 introduced an online algorithm for running LDA incrementally, support for more queries on trained LDA models, and performance metrics such as likelihood and perplexity", "creator": "joseph", "link": "https://databricks.com/blog/2015/09/22/large-scale-topic-modeling-improvements-to-lda-on-apache-spark.html", "authors": null, "id": 5142, "categories": ["Apache Spark", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2015-09-22", "tz": "UTC", "createdOn": "2015-09-22"}, "title": "Large Scale Topic Modeling: Improvements to LDA on Apache Spark", "slug": "large-scale-topic-modeling-improvements-to-lda-on-apache-spark", "content": "<em>This blog was written by Feynman Liang and Joseph Bradley from Databricks, and Yuhao Yang from Intel.</em>\n\n<em>To get started using LDA, <a href=\"http://spark.apache.org/downloads.html\">download Apache Spark 1.5</a> or <a href=\"https://accounts.cloud.databricks.com/registration.html#signup\">sign up for a 14-day free trial of Databricks today</a>.</em>\n\n<hr />\n\nWhat are people discussing on Twitter? To catch up on distributed computing, what news articles should I read? These are questions that can be answered by topic models, a technique for analyzing the topics present in collections of documents. This blog post discusses improvements in Apache Spark 1.4 and 1.5 for topic modeling using the powerful Latent Dirichlet Allocation (LDA) algorithm.\n\nSpark 1.4 and 1.5 introduced an online algorithm for running LDA incrementally, support for more queries on trained LDA models, and performance metrics such as likelihood and perplexity. We give an example here of training a topic model over a dataset of 4.5 million Wikipedia articles.\n<h2 style=\"text-align: left;\">Topic models and LDA</h2>\nTopic models take a collection of documents and automatically infer the topics being discussed. For example, when we run Spark\u2019s LDA on a dataset of 4.5 million Wikipedia articles, we can obtain topics like those in the table below.\n<p style=\"text-align: center;\"><img class=\"wp-image-5144 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/09/lda_blog_table01-1024x182.png\" alt=\"lda_blog_table01\" width=\"600\" height=\"107\" /><em>Table 1: Example LDA topics learned from Wikipedia articles dataset</em></p>\n<p style=\"text-align: left;\">In addition, LDA tells us which topics each document is about; document X might be 30% about Topic 1 (\u201cpolitics\u201d) and 70% about Topic 5 (\u201cairlines\u201d). Latent Dirichlet Allocation (LDA) has been one of the most successful topic models in practice. <a href=\"https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html\" target=\"_blank\">See our previous blog post on LDA</a> to learn more.</p>\n\n<h2 style=\"text-align: left;\">A new online variational learning algorithm</h2>\nOnline variational inference is a technique for learning an LDA model by processing the data incrementally in small batches. By processing in small batches, we are able to easily scale to very large datasets. MLlib implements an algorithm for performing online variational inference originally described by <a href=\"https://www.cs.princeton.edu/~blei/papers/HoffmanBleiBach2010b.pdf\" target=\"_blank\">Hoffman et al</a>.\n<h3 style=\"text-align: left;\">Performance comparison</h3>\nThe table of topics shown previously were learned using the newly developed online variational learning algorithm. If we compare timing results, we can see a significant speedup in using the new online algorithm over the old EM algorithm:\n<p style=\"text-align: center;\"><img class=\"wp-image-5146 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/09/lda_blog_figure01.png\" alt=\"lda_blog_figure01\" width=\"500\" height=\"375\" /><em>Figure 1: Online learning algorithm learns faster than earlier EM algorithm</em></p>\n\n<h3 style=\"text-align: left;\">Experiment details</h3>\nWe first preprocessed the data by filtering common English stop words and limiting the vocabulary to the 10,000 most common words. We then trained a 100 topic LDA model for 100 iterations using the online LDA optimizer. We ran our experiments using <a href=\"https://databricks.com/product/databricks\" target=\"_blank\">Databricks</a> on a 16 node AWS r3.2xlarge cluster with data stored in S3. For the actual code, see <a href=\"https://goo.gl/8ylBma\" target=\"_blank\">this Github gist</a>.\n<h2 style=\"text-align: left;\">Improved predictions, metrics, and queries</h2>\n<h3 style=\"text-align: left;\">Predict topics on new documents</h3>\nIn addition to describing topics present in the training set, Spark 1.5 makes the trained LDA models more useful by allowing users to predict topics for a new test document.\n<h3 style=\"text-align: left;\">Evaluate your model with likelihood and perplexity</h3>\nAfter learning an LDA model, we are often interested in how well the model fits the data. We have added two new metrics to evaluate this: <a href=\"https://en.wikipedia.org/wiki/Likelihood_function\" target=\"_blank\">likelihood</a> and <a href=\"https://en.wikipedia.org/wiki/Perplexity\" target=\"_blank\">perplexity</a>.\n<h3 style=\"text-align: left;\">Make more queries</h3>\nThis new release also adds several new queries users can perform on a trained LDA model. For example, we can now obtain the top k topics for each document (\u201cWhat is this document discussing?\u201d) as well as the top documents per topic (\u201cTo learn about topic X, what documents should I read?\u201d).\n<h2 style=\"text-align: left;\">Tips for running LDA</h2>\n<ul>\n\t<li>Make sure to run for enough iterations. Early iterations may return useless (e.g. extremely similar) topics, but running for more iterations dramatically improves the results. We have noticed this is especially true for EM.</li>\n\t<li>To handle stop words specific to your data, a common workflow is to run LDA, look at topics, identify stop words that show up in the topics, filter them out, and run LDA again.</li>\n\t<li>Picking the number of topics is an art; there are algorithms to choose automatically, but domain expertise is critical to getting good results.</li>\n\t<li>The <a href=\"http://spark.apache.org/docs/latest/ml-features.html\" target=\"_blank\">Pipelines API feature transformers</a> are very useful for preprocessing text to prepare it for LDA; see Tokenizer, StopwordsRemover and CountVectorizer in particular.</li>\n</ul>\n<h2 style=\"text-align: left;\">What\u2019s next?</h2>\nSpark contributors are actively working on improving our LDA implementation. Some works in progress include: <a href=\"https://issues.apache.org/jira/browse/SPARK-5556\" target=\"_blank\">Gibbs sampling</a> (a slower but sometimes more accurate algorithm), <a href=\"https://issues.apache.org/jira/browse/SPARK-8696\" target=\"_blank\">streaming LDA algorithms</a>, and <a href=\"https://issues.apache.org/jira/browse/SPARK-8555\" target=\"_blank\">hierarchical Dirichlet processes </a>(for automatically choosing the number of topics).\n<h2 style=\"text-align: left;\">Acknowledgements</h2>\nThe development of LDA has been a collaboration between many Spark contributors.\n\nFeynman Liang, Yuhao Yang, Joseph K. Bradley, and others made recent improvements, and <a href=\"https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html\" target=\"_blank\">many others</a> contributed to the earlier work."}
{"status": "publish", "description": "We are excited to introduce the integration of Apache Spark's web UI in Databricks notebooks, which allows the user to understand and debug their Spark application more efficiently.", "creator": "dave_wang", "link": "https://databricks.com/blog/2015/09/23/easier-spark-code-debugging-real-time-progress-bar-and-apache-spark-web-ui-integration-in-databricks.html", "authors": null, "id": 5169, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2015-09-23", "tz": "UTC", "createdOn": "2015-09-23"}, "title": "Easier Spark Code Debugging", "slug": "easier-spark-code-debugging-real-time-progress-bar-and-apache-spark-web-ui-integration-in-databricks", "content": "<em>To try the features mentioned in this blog,\u00a0<a href=\"https://accounts.cloud.databricks.com/registration.html#signup\">sign up for a 14-day free trial of Databricks today</a>.</em>\n\n<hr />\n\nWe are excited to introduce the integration of <a href=\"https://databricks.com/blog/2015/06/22/understanding-your-spark-application-through-visualization.html\">Apache Spark web UIs</a> in Databricks notebooks, which allows the user to understand and debug their Spark application more efficiently.\n\nAs a component of open source Spark, the web UI is designed to help with monitoring and understanding your Spark application, it contains useful information about memory usage, running executors, scheduler stages, and tasks. This information is extremely helpful for debugging.\n\nThe <a href=\"https://databricks.com/product/databricks#notebooks\">Databricks notebook</a> is a visual collaborative workspace that allows users to explore data and develop applications interactively using Apache Spark. It makes working with data a lot easier, as shown in example workflows such as <a href=\"https://databricks.com/blog/2015/04/21/analyzing-apache-access-logs-with-databricks-cloud.html\">analysis access logs</a> and <a href=\"https://databricks.com/blog/2015/06/04/simplify-machine-learning-on-spark-with-databricks.html\">doing machine learning</a>.\n\nDebugging a distributed application is still challenging in the notebook environment. Even though the web UI has the necessary information, there is a gap between web UIs and the development environment: it\u2019s usually difficult to locate information in the web UI that is relevant to the code you are investigating; and there is no easy way to find historical runtime information.\n<h2>How the Integrated web UI helps with coding</h2>\nTo solve this issue, we created a way to directly access the runtime information within the development environment.\n\nDatabricks notebooks now display real-time updates from the Spark nodes in the form of \u201cprogress bars\u201d. If a command launches a Spark job under the hood, the progress bars will be automatically updated as the job executes, which makes monitoring the status of the command way easier!\n<p style=\"text-align: center;\"><img class=\"size-full wp-image-5171 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/09/progress-bar.gif\" alt=\"progress-bar\" width=\"480\" height=\"270\" /><em>The Progress Bar: Displaying Spark Job execution progress in real-time in Databricks Notebook</em></p>\nThe progress bars also directly link to more detailed information about each Spark Job, allowing users to drill down into the web UI of each job for further investigation. The additional visibility means you can view all the system status and runtime information you needed for debugging, side by side with where you write the code.\n<p style=\"text-align: center;\"><img class=\"size-full wp-image-5172 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/09/spark-ui.gif\" alt=\"spark-ui\" width=\"480\" height=\"269\" /><em>Drill down into the web UI associated with each Spark Job</em><a href=\"https://databricks.com/wp-content/uploads/2015/09/spark-ui.gif\">\n</a></p>\nTo get a more detailed look at this feature in action, watch the video below:\n\n<a href=\"https://www.youtube.com/watch?v=ttDLBRndan4\"><img class=\"aligncenter wp-image-5179\" src=\"https://databricks.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-22-at-5.24.34-PM-300x158.png\" alt=\"Screen Shot 2015-09-22 at 5.24.34 PM\" width=\"400\" height=\"210\" /></a>\n<h2>Summary</h2>\nBy integrating Spark's web UI with Databricks notebooks, we have created a shortcut\u00a0to easily access debugging information within your development environment. Hopefully these enhancements will help you debug Spark application more effectively.\n\nThese enhancements are now available to all Databricks users, <a href=\"https://accounts.cloud.databricks.com/registration.html#signup\">sign up for a 14-day free trial</a> to try them out!"}
{"status": "publish", "description": null, "creator": "matei", "link": "https://databricks.com/blog/2015/09/24/spark-survey-2015-results-are-now-available.html", "authors": null, "id": 5193, "categories": ["Announcements", "Company Blog"], "dates": {"publishedOn": "2015-09-24", "tz": "UTC", "createdOn": "2015-09-24"}, "title": "Spark Survey 2015 Results are now available", "slug": "spark-survey-2015-results-are-now-available", "content": "We ran the Spark Survey 2015 this summer to gain insights on how organizations are using Apache Spark.\n\nPress Release: <a href=\"http://www2.marketwire.com/mw/release_html_b1?release_id=1219120\" target=\"_blank\">Apache Spark Outgrowing Hadoop as Users Increasingly Move to the Cloud</a>\n\nThe results of this year\u2019s Spark Survey - reflecting the answers and opinions of over 1,417 respondents representing 842 organizations - strongly indicate the rapid growth of the Spark community and offers valuable insight into the direction Spark is moving. A key focus of Spark has been to make data processing easy and accessible. \u00a0The results of this year's survey suggest that this is resonating with Spark users across many industries.\n\nDownload the <a href=\"http://cdn2.hubspot.net/hubfs/438089/DataBricks_Surveys_-_Content/Spark-Survey-2015-Infographic.pdf?t=1443057549926\" target=\"_blank\">Spark Survey 2015 Infographic</a>.\n\n<a href=\"http://cdn2.hubspot.net/hubfs/438089/DataBricks_Surveys_-_Content/Spark-Survey-2015-Infographic.pdf?t=1443057549926\" target=\"_blank\"><img class=\"alignnone wp-image-5194\" src=\"https://databricks.com/wp-content/uploads/2015/09/spark-survey-twitter.psd-1024x526.png\" alt=\"spark-survey-twitter.psd\" width=\"640\" height=\"329\" /></a>\n\n&nbsp;\n\nTo learn more, download the <a href=\"http://dbricks.co/1PtWxSu\" target=\"_blank\">Spark Survey 2015 Report</a>.\n\nThe three key takeaways from the Spark Survey are:\n<ul>\n\t<li><strong>Spark Adoption Is Growing Rapidly</strong>: Spark is the most active open source project in Big Data with over 600 contributors in the last 12 months (up from 315 in the previous 12-24 months). Just as important, it is being used to create many types of products inside different organizations (69% of respondents are creating two or more data products with Spark). Spark is being embraced by companies far beyond the IT industry and by a growing variety of functional roles within these companies (e.g. data scientists and analysts).</li>\n\t<li><strong>Spark Use Is Growing Beyond Hadoop</strong>: Spark usage in the public cloud (51%) and within Spark\u2019s own cluster manager (48%) have surged within the last year. While some run Spark in on-premise Hadoop clusters, they are no longer a majority of its users. As well, Spark integrates with many storage systems (e.g. Cassandra, HBase, S3). Spark is also pluggable, with dozens of third party libraries and storage integrations.</li>\n\t<li><strong>Spark Is Increasing Access to Big Data</strong>: Spark is unlocking the value of Big Data by making it easier for a wide range of people (e.g. 41% data engineers, 22% data scientists, etc.) to solve a growing variety of data problems. The ability to allows users to program in the language of their choice is an important reason that is driving Spark use across a growing audience of data scientists writing in Python and R. Spark users are expanding into the areas of advanced analytics and real-time streaming while building foundations on data warehousing and BI.</li>\n</ul>\nIn conclusion, as a result of the insights revealed in the results of Spark Survey 2015, we have a better picture of who is using Spark, how they\u2019re using it, and what they\u2019re using it to build. These insights will guide major updates to the Spark platform as we move into Spark\u2019s next phase of growth. Thank you to everyone who participated in Spark Survey 2015 and for your help in shaping Spark\u2019s future!\n\nDownload the <a href=\"http://cdn2.hubspot.net/hubfs/438089/DataBricks_Surveys_-_Content/Spark-Survey-2015-Infographic.pdf?t=1443057549926\" target=\"_blank\">Spark Survey 2015 Infographic</a>\u00a0or the PNG version below.\n\n<a href=\"http://cdn2.hubspot.net/hubfs/438089/Spark-Survey-2015-Infographic.png?t=1443110781822\" target=\"_blank\"><img class=\"alignnone wp-image-5217 size-medium\" src=\"https://databricks.com/wp-content/uploads/2015/09/spark_survey_2015_infographic_thumbnail1-231x300.png\" alt=\"spark_survey_2015_infographic_thumbnail\" width=\"231\" height=\"300\" /></a>\n\n&nbsp;"}
{"status": "publish", "description": null, "creator": "Xiangrui", "link": "https://databricks.com/blog/2015/09/28/improved-frequent-pattern-mining-in-apache-spark-1-5-association-rules-and-sequential-patterns.html", "authors": null, "id": 5224, "categories": ["Apache Spark", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2015-09-28", "tz": "UTC", "createdOn": "2015-09-28"}, "title": "Improved Frequent Pattern Mining in Apache Spark 1.5: Association Rules and Sequential Patterns", "slug": "improved-frequent-pattern-mining-in-apache-spark-1-5-association-rules-and-sequential-patterns", "content": "<em>We would like to thank Jiajin Zhang and Dandan Tu from Huawei for contributing to this blog.</em>\n\n<em>To get started mining patterns from massive datasets, <a href=\"http://spark.apache.org/downloads.html\">download Apache Spark 1.5</a> or <a href=\"https://accounts.cloud.databricks.com/registration.html#signup\">sign up for a 14-day free trial of Databricks today</a>.</em>\n\n<hr />\n\nDiscovering frequent patterns hiding in a big dataset has application across a broad range of use cases. Retailers may be interested in finding items that are frequently purchased together from a large transaction database. Biologists may be interested in frequent DNA or amino acid sequences. In Apache Spark 1.5, we have significantly improved Spark\u2019s frequent pattern mining capabilities by adding algorithms for association rule generation and sequential pattern mining.\n<h2>Association rules generation</h2>\nAssociation rules are generated from frequent itemsets, subsets of items that appear frequently across transactions. Frequent itemset mining was first added in Spark 1.3 using the Parallel FP-growth algorithm. Spark 1.4 adds a new Python API for FP-growth. We refer readers to <a href=\"https://databricks.com/blog/2015/04/17/new-mllib-algorithms-in-spark-1-3-fp-growth-and-power-iteration-clustering.html\">our previous blog post</a> for more details.\n\nIn addition to identifying frequent itemsets, we are often interested in learning <a href=\"https://en.wikipedia.org/wiki/Association_rule_learning\">association rules</a>.\n\nFor example, in a retailer\u2019s transaction database, a rule {toothbrush, floss} =&gt; {toothpaste} with a <i>confidence value</i> 0.8 would indicate that 80% of customers who buy a toothbrush and floss also purchase a toothpaste in the same transaction. The retailer could then use this information, put both toothbrush and floss on sale, but raise the price of toothpaste to increase overall profit.\n\nSpark 1.5 adds support for distributed generation of association rules. Rule generation is done via a simple method call to <a href=\"http://spark.apache.org/docs/latest/mllib-frequent-pattern-mining.html#fp-growth\" target=\"_blank\">FPGrowthModel</a> with a min confidence value, for example:\n\n[scala]\n\nval transactions: RDD[Array[String]] = ...\nval model = new FPGrowth()\n  .setMinSupport(0.2)\n  .setNumPartitions(10)\n  .run(transactions)\nval minConfidence = 0.8\nmodel.generateAssociationRules(minConfidence).collect().foreach { \n  rule =&gt;\n    println(rule.antecedent.mkString(&quot;,&quot;) + &quot; =&gt; &quot; + \n      rule.consequent.mkString(&quot;,&quot;)\n  )\n}\n\n[/scala]\n\n<h2>Sequential pattern mining</h2>\nUnlike frequent itemsets, where the items in a transaction are unordered, sequential pattern mining takes the order of items into account. In many use cases ranging from text mining to DNA sequence motif discovery, we care about the order in which items appear in a pattern.\n\nSequential pattern mining is widely used in Huawei, a Fortune Global 500 telecommunications company. For example, a mobile network of millions of users could generate several hundred GBs of session signaling data per day. Among the signaling sequences, we want to extract frequent sequential patterns like routing updates, activation failures, and broadcasting timeouts that could potentially lead to customer complaints. By identifying those patterns in real traffic, we can proactively reach out to customers with potential issues and help improve their experience.\n\nThanks to a collaboration between Databricks and Huawei, especially to Huawei for initiating the effort, sharing their use cases, and making significant code contribution, we are proud to announce support for parallel sequential pattern mining in Spark 1.5. This latest version ships with a parallel implementation of the PrefixSpan algorithm originally described by <a href=\"http://doi.org/10.1109/ICDE.2001.914830\">Pei et al</a>.\n<h2>Example: mining frequent sequential sign language patterns</h2>\nTo demonstrate PrefixSpan, we will mine frequent sequential patterns from the American Sign Language database <a href=\"http://cs-people.bu.edu/panagpap/Research/asl_mining.htm\">provided by Boston University</a>. Running PrefixSpan to discover frequent sequential patterns requires only a few lines of code:\n\n[scala]\n\nval sequences: RDD[Array[Array[String]]] = ...\nval prefixSpan = new PrefixSpan()\n  .setMinSupport(0.6)\n  .setMaxPatternLength(10)\nval patterns = prefixSpan.run(sequences)\n\n[/scala]\n\nFrom this, we discover that common sequential patterns in the database include:\n<pre>(head pos: tilt fr/bk - front), (eye aperture - ONSET), (POS - Verb)\n(head pos: turn - ONSET), (eye aperture - ONSET), (POS - Verb)\n(head pos: tilt fr/bk - ONSET), (eye aperture - ONSET), (POS - Verb)\n(eye brows - ONSET), (eye aperture - ONSET), (POS - Verb)\n(head pos: tilt fr/bk - front), (POS - Noun), (POS - Verb)</pre>\nwhere each item indicates a sign or a gesture. For details, please see <a href=\"https://goo.gl/a5Am79\">this gist</a>.\n<h2>Implementation</h2>\nWe followed the PrefixSpan algorithm but made modifications to parallelize the algorithm in a novel way for running on Spark. At a high level, our algorithm iteratively extends the lengths of prefixes until its associated projected database (i.e. the set of all sequences with that given prefix) is small enough to fit on a single machine. We then process each of these projected databases locally and combine the results to yield all of the sequential patterns.\n<h2>What\u2019s next?</h2>\nThe improvements to frequent pattern mining have been a collaboration between many Spark contributors. This work is pushing the limits on distributed pattern mining. Ongoing work includes: model import/export for <a href=\"https://issues.apache.org/jira/browse/SPARK-6724\">FPGrowth</a> and <a href=\"https://issues.apache.org/jira/browse/SPARK-10386\">PrefixSpan</a>, a <a href=\"https://issues.apache.org/jira/browse/SPARK-10028\">Python API for PrefixSpan</a>, <a href=\"https://issues.apache.org/jira/browse/SPARK-10678\">optimizing PrefixSpan for single-item itemsets</a>, etc. To get involved, please check the <a href=\"https://issues.apache.org/jira/browse/SPARK-10324\">MLlib 1.6 roadmap</a>."}
{"status": "publish", "description": null, "creator": "EugeneZhulenev", "link": "https://databricks.com/blog/2015/10/13/interactive-audience-analytics-with-apache-spark-and-hyperloglog.html", "authors": null, "id": 5244, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2015-10-13", "tz": "UTC", "createdOn": "2015-10-13"}, "title": "Interactive Audience Analytics With Apache Spark and HyperLogLog", "slug": "interactive-audience-analytics-with-apache-spark-and-hyperloglog", "content": "This is a guest blog from Eugene Zhulenev\u00a0on his experiences with Engineering Machine Learning and Audience Modeling\u00a0at\u00a0<a href=\"http://collective.com/main.html\">Collective</a>.\n\n<hr />\n\nAt <a href=\"http://collective.com/\">Collective</a>, we are working not only on cool things like <a href=\"http://eugenezhulenev.com/blog/2015/06/10/feature-engineering-at-scale/\">Machine Learning and Predictive Modeling</a> but also on reporting that can be tedious and boring. However at our scale even simple reporting application can become a challenging engineering problem. This post is based on a talk that I gave at <a href=\"http://www.meetup.com/ny-scala/events/223751768/\">NY-Scala Meetup</a>. Slides are available <a href=\"http://eugenezhulenev.com/talks/interactive-audience-analytics/\">here</a>.\n<blockquote>Example application is available on github: <a href=\"https://github.com/collectivemedia/spark-hyperloglog\">https://github.com/collectivemedia/spark-hyperloglog</a></blockquote>\n<h2 id=\"impression-log\">Impression Log</h2>\nWe are building reporting application that is based on an impression log. It\u2019s not exactly the way how we get data from out partners, it\u2019s pre-aggregated by Ad, Site, Cookie. And even in this pre-aggregated format it takes hundreds of gigabytes per day on HDFS.\n\n<pre>Ad            | Site          | Cookie          | Impressions | Clicks | Segments                       \n------------- |-------------- | --------------- | ----------- | ------ | -------------------------------\nbmw_X5        | forbes.com    | 13e835610ff0d95 | 10          | 1      | [a.m, b.rk, c.rh, d.sn, ...]   \nmercedes_2015 | forbes.com    | 13e8360c8e1233d | 5           | 0      | [a.f, b.rk, c.hs, d.mr, ...]   \nnokia         | gizmodo.com   | 13e3c97d526839c | 8           | 0      | [a.m, b.tk, c.hs, d.sn, ...]   \napple_music   | reddit.com    | 1357a253f00c0ac | 3           | 1      | [a.m, b.rk, d.sn, e.gh, ...]   \nnokia         | cnn.com       | 13b23555294aced | 2           | 1      | [a.f, b.tk, c.rh, d.sn, ...]   \napple_music   | facebook.com  | 13e8333d16d723d | 9           | 1      | [a.m, d.sn, g.gh, s.hr, ...]   </pre>\n\nEach cookie id has assigned segments which are just 4-6 letters code, that represents some information about the cookie, that we get from 3rd party data providers such as <a href=\"http://www.bluekai.com/\">Blukai</a>.\n\n<pre>- a.m  : Male\n- a.f  : Female\n- b.tk : $75k-$100k annual income\n- b.rk : $100k-$150k annual income\n- c.hs : High School\n- c.rh : College\n- d.sn : Single\n- d.mr : Married\n</pre>\n\nFor example if a cookie has been assigned <code>a.m</code> segment, it means that we think (actually the data provider thinks) that this cookie belongs to a male. The same thing for annual income level and other demographics information.\n\nWe don\u2019t have precise information, to whom exactly a particular cookie belongs nor what is their real annual income level. \u00a0\u00a0These segments are essentially probabilistic, nevertheless we can get very interesting insights from this data.\n<h3 id=\"what-we-can-do-with-this-data\">What we can do with this data</h3>\nUsing this impression log we can answer some interesting questions\n<ul>\n\t<li>We can calculate a given group\u2019s prevalence in a campaign\u2019s audience, eg. what role do\u00a0<strong>males</strong> play in the optimized audience for a <strong>Goodyear Tires</strong> campaign?</li>\n\t<li>What is <strong>male/female</strong> ratio for people who have seen <strong>bmw_X5</strong> ad on <strong>forbes.com</strong></li>\n\t<li>Income distribution for people who have seen an Apple Music ad</li>\n\t<li>Nokia clicks distribution across different education levels</li>\n</ul>\nUsing these basic questions we can create an \u201cAudience Profile\u201d that describes what type of audience is prevailing in an optimized campaign or partner website.\n\n<img class=\"center\" src=\"http://eugenezhulenev.com/talks/interactive-audience-analytics/affinity.png\" alt=\"\" />\n\nBlue bars mean that this particular segment tends to view ad/visit website more than on average, and red bar mean less. For example for <strong>Goodyear Tires</strong> we expect to see more\u00a0<strong>male</strong> audience than <strong>female</strong>.\n<h2 id=\"solving-problem-with-sql\">Solving problems with SQL</h2>\nSQL looks like an easy choice for this problem, however as I already mentioned we have hundreds of gigabytes of data every day, and we need to get numbers based on 1-year history in seconds. Hive/Impala simply cannot solve this problem.\n\n[scala]\nselect count(distinct cookie_id) from impressions\n    where site = 'forbes.com'\n    and ad = 'bmw_X5'\n    and segment contains 'a.m'\n[/scala]\n\nUnfortunately, we have almost infinite combinations of filters that users can define, so it\u2019s not feasible to pre-generate all possible reports. Users can use any arbitrary ad, site, campaign, order filter combinations, and may want to know audience intersection with any segment.\n<h2 id=\"audience-cardinality-approximation-with-hyperloglog\">Audience cardinality approximation with HyperLogLog</h2>\nWe came up with a different solution; instead of providing precise results for every query, we are providing approximated numbers with very high precision. Usually, the error\u00a0rate is around 2% which for this particular application is really good. We don\u2019t need to know an exact number of male/female cookies in the audience. To be able to say what audience is prevailing, approximated numbers are more than enough.\n\nWe use <a href=\"https://en.wikipedia.org/wiki/HyperLogLog\">HyperLogLog</a>, which is an algorithm for the count-distinct problem, approximating the number of distinct elements (cardinality). It uses finite space and has configurable precision. It able to estimate cardinalities of &gt;10^9 with a typical accuracy of 2%, using 1.5kB of memory.\n\n[scala]\ntrait HyperLogLog {\n    def add(cookieId: String): Unit\n    //   |A|\n    def cardinality(): Long\n    //   |A \u222a B|\n    def merge(other: HyperLogLog): HyperLogLog\n    //   |A \u2229 B| = |A| + |B| - |A \u222a B|,\n    def intersect(other: HyperLogLog): Long\n}\n[/scala]\n\nHere is a rough API that is provided by <code>HyperLogLog</code>. You can add a new cookieId to it, get cardinality estimation of unique cookies that were already added to it, merge it with another\u00a0<code>HyperLogLog</code>, and finally get an intersection. It\u2019s important to notice that after the\u00a0<code>intersect\u00a0</code>operation, the\u00a0<code>HyperLogLog</code> object is lost, and you only have approximated intersection cardinality. Therefore, usually <code>HyperLogLog</code> intersection is the last step in the computation.\n\nI suggest you to watch the awesome talk by <a href=\"https://twitter.com/avibryant\">Avi Bryant</a> where he discusses not only HyperLogLog but lot\u2019s of other approximation data structures that can be useful for big-data analytics: <a href=\"http://www.infoq.com/presentations/abstract-algebra-analytics\">http://www.infoq.com/presentations/abstract-algebra-analytics</a>.\n<h2 id=\"from-cookies-to-hyperloglog\">From cookies to HyperLogLog</h2>\nWe split out original impression log into two tables.\n\nFor the ad impressions table, we remove segment information and aggregate cookies, impressions and clicks by Ad and Site. <code>HyperLogLog</code> can be used in an aggregation function similar to how use the\u00a0<code>sum</code> operation. Zero is an\u00a0empty <code>HyperLogLog</code>\u00a0while the plus operation is <code>merge</code> (btw it\u2019s exactly properties required by <code>Monoid</code>)\n<div class=\"bogus-wrapper\"><figure class=\"code\">\n<div class=\"CodeRay\">\n<div class=\"code\">\n<pre>Ad            | Site          | Cookies HLL        | Impressions | Clicks \n------------- | ------------- | ------------------ | ----------- | ------ \nbmw_X5        | forbes.com    | HyperLogLog@23sdg4 | 5468        | 35     \nbmw_X5        | cnn.com       | HyperLogLog@84jdg4 | 8943        | 29     \n</pre>\n</div>\n</div>\n</figure></div>\nFor the segments table, we remove ad and site information, and aggregate data by segment.\n<div class=\"bogus-wrapper\"><figure class=\"code\">\n<div class=\"CodeRay\">\n<div class=\"code\">\n<pre>Segment       | Cookies HLL        | Impressions | Clicks\n------------- | ------------------ | ----------- | ------\nMale          | HyperLogLog@85sdg4 | 235468      | 335   \n$100k-$150k   | HyperLogLog@35jdg4 | 569473      | 194   \n</pre>\n</div>\n</div>\n</figure></div>\n<h3 id=\"percent-of-college-and-high-school-education-in-bmw-campaign\">Percent of college and high school education in the BMW campaign</h3>\nIf you can imagine that we can load these tables into <code>Seq</code>, then audience intersection becomes a really straightforward task, that can be solved by a couple lines of functional scala operations.\n\n[scala]\ncase class Audience(ad: String, site: String, hll: HyperLogLog, imp: Long, clk: Long)\n\ncase class Segment(name: String, hll: HyperLogLog, imp: Long, clk: Long)\n\nval adImpressions: Seq[Audience] = ...\nval segmentImpressions: Seq[Segment] = ...\n\nval bmwCookies: HyperLogLog = adImpressions\n    .filter(_.ad = &quot;bmw_X5&quot;)\n    .map(_.hll).reduce(_ merge _)\n\nval educatedCookies: HyperLogLog = segmentImpressions\n    .filter(_.segment in Seq(&quot;College&quot;, &quot;High School&quot;))\n    .map(_.hll).reduce( _ merge _)\n\nval p = (bmwCookies intersect educatedCookies) / bmwCookies.count()\n[/scala]\n\n<h2 id=\"spark-dataframes-with-hyperloglog\">Apache Spark DataFrames with HyperLogLog</h2>\nObviously we can\u2019t load all the data into a scala <code>Seq</code> on single machine, because it\u2019s huge. \u00a0Even after removing cookie level data and transforming it into <code>HyperLogLog</code> objects, it\u2019s around 1-2 gigabytes of data for a single day.\n\nSo we have to use some distributed data processing framework to solve this problem, and we chose Spark.\n<h3 id=\"what-is-spark-dataframe\">What are Spark DataFrames</h3>\n<ul>\n\t<li>Inspired by R data.frame and Python/Pandas DataFrame</li>\n\t<li>Distributed collection of rows organized into named columns</li>\n\t<li>Used to be SchemaRDD in Spark &lt; 1.3.0</li>\n</ul>\n<h3 id=\"high-level-dataframe-operations\">High-Level DataFrame Operations</h3>\n<ul>\n\t<li>Selecting required columns</li>\n\t<li>Filtering</li>\n\t<li>Joining different data sets</li>\n\t<li>Aggregation (count, sum, average, etc)</li>\n</ul>\nYou can start by referring to the\u00a0<a href=\"https://spark.apache.org/docs/1.3.0/sql-programming-guide.html\">Spark DataFrame guide</a> or <a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\">DataBricks blog post</a>.\n<h3 id=\"ad-impressions-and-segments-in-dataframes\">Ad impressions and segments in DataFrames</h3>\nWe store all of our data on HDFS using Parquet data format, and that\u2019s how it looks after it\u2019s loaded into Spark DataFrames.\n\n[scala]\nval adImpressions: DataFrame = sqlContext.parquetFile(&quot;/aa/audience&quot;)\n\nadImpressions.printSchema()\n// root\n//   | -- ad: string (nullable = true)\n//   | -- site: string (nullable = true)\n//   | -- hll: binary (nullable = true)\n//   | -- impressions: long (nullable = true)\n//   | -- clicks: long (nullable = true)\n\nval segmentImpressions: DataFrame = sqlContext.parquetFile(&quot;/aa/segments&quot;)\n\nsegmentImpressions.printSchema()\n// root\n//   | -- segment: string (nullable = true)\n//   | -- hll: binary (nullable = true)\n//   | -- impressions: long (nullable = true)\n//   | -- clicks: long (nullable = true)\n[/scala]\n\n<code>HyperLogLog</code> is essentially a huge <code>Array[Byte]</code> with some clever hashing and math, so it\u2019s straightforward to store it on HDFS in serialized form.\n<h2 id=\"working-with-spark-dataframe\">Working with a Spark DataFrame</h2>\nWe wanted to know the answer for the question: \u201cPercent of college and high school education in the BMW campaign\u201d.\n\n[scala]\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.HLLFunctions._\n\nval bmwCookies: HyperLogLog = adImpressions\n    .filter(col(&quot;ad&quot;) === &quot;bmw_X5&quot;)\n    .select(mergeHll(col(&quot;hll&quot;)).first() // -- sum(clicks)\n\nval educatedCookies: HyperLogLog = hllSegments\n    .filter(col(&quot;segment&quot;) in Seq(&quot;College&quot;, &quot;High School&quot;))\n    .select(mergeHll(col(&quot;hll&quot;)).first()\n\nval p = (bmwCookies intersect educatedCookies) / bmwCookies.count()\n[/scala]\n\nIt looks pretty familiar, not too far from example based on scala <code>Seq</code>. Only one unusual operation that you might notice if you have some experience with Spark is <code>mergeHLL</code>. It\u2019s not available in Spark by default, it is a custom <code>PartialAggregate</code> function that can compute aggregates for serialized <code>HyperLogLog</code> objects.\n<h3 id=\"writing-your-own-spark-aggregation-function\">Writing your own Spark aggregation function</h3>\nTo write you own aggregation function you need to define a function that will be applied to each row in <code>RDD</code> partition, in this example it\u2019s called <code>MergeHLLPartition</code>. Then you need to define the function that will take results from different partitions and merge them together, for <code>HyperLogLog</code> it\u2019s called <code>MergeHLLMerge</code>. And finally you need to tell Spark how you want it to split your computation across <code>RDD</code> (DataFrame is backed by <code>RDD[Row]</code>)\n\n[scala]\ncase class MergeHLLPartition(child: Expression)\n  extends AggregateExpression with trees.UnaryNode[Expression] { ... }\n\ncase class MergeHLLMerge(child: Expression)\n  extends AggregateExpression with trees.UnaryNode[Expression] { ... }\n\ncase class MergeHLL(child: Expression)\n  extends PartialAggregate with trees.UnaryNode[Expression] {\n\n  override def asPartial: SplitEvaluation = {\n    val partial = Alias(MergeHLLPartition(child), &quot;PartialMergeHLL&quot;)()\n\n    SplitEvaluation(\n      MergeHLLMerge(partial.toAttribute),\n      partial :: Nil\n    )\n  }\n}\n\ndef mergeHLL(e: Column): Column = MergeHLL(e.expr)\n[/scala]\n\nAfter that, writing aggregations becomes a really easy task, and your expressions will look like \u201cnative\u201d DataFrame code, which is really nice, and super easy to read and reason about.\n\nAlso it works much faster than solving this problem with scala transformations on top of <code>RDD[Row]</code>, as Spark catalyst optimizer can execute\u00a0an optimized plan and reduce the amount of data that needs to be shuffled between spark nodes.\n\nAnd finally, it\u2019s so much easier to manage mutable state. Spark encourage you to use immutable transformations, and it\u2019s really cool until you need extreme performance from your code. For example, if you are using something like <code>reduce</code> or <code>aggregateByKey</code> you don\u2019t really know when and where your function instantiated, when it\u2019s done with <code>RDD\u00a0</code>partition, nor when the results are transferred to another Spark node for a merge operation. With <code>AggregateExpression</code> you have explicit control over mutable state, and it\u2019s totally safe to accumulate mutable state during execution for a single partition. \u00a0At the end when you\u2019ll need to send data to another node\u00a0where you can create immutable copy.\n\nIn this particular case, using a mutable <code>HyperLogLog</code> merge implementation helped to speed up computation times by almost 10x. For each partition <code>HyperLogLog</code> state accumulated in single mutable <code>Array[Byte]</code> and at the end when data needs to be transferred somewhere else for merging with another partition, an immutable copy is created.\n<h3 id=\"some-fancy-aggregates-with-dataframe-api\">Some fancy aggregates with DataFrame API</h3>\nYou can write much more complicated aggregation functions, for example, to compute aggregate based on multiple columns. Here is a code sample from our audience analytics project.\n\n[scala]\ncase class SegmentEstimate(cookieHLL: HyperLogLog, clickHLL: HyperLogLog)\n\ntype SegmentName = String\n\nval dailyEstimates: RDD[(SegmentName, Map[LocalDate, SegmentEstimate])] =\n    segments.groupBy(segment_name).agg(\n      segment_name,\n      mergeDailySegmentEstimates(\n        mkDailySegmentEstimate(      // -- Map[LocalDate, SegmentEstimate]\n          dt,\n          mkSegmentEstimate(         // -- SegmentEstimate(cookieHLL, clickHLL)\n            cookie_hll,\n            click_hll)\n        )\n      )\n    )\n[/scala]\n\nThis code calculates daily audience aggregated by segment. Using Spark <code>PartialAggregate\u00a0</code>function saves a lot of network traffic and minimizes the distributed shuffle size.\n\nThis aggregation is possible because of nice properties of <code>Monoid</code>\n<ul>\n\t<li><code>HyperLogLog</code> is a <code>Monoid</code> (has <code>zero</code> and <code>plus</code> operations)</li>\n\t<li><code>SegmentEstimate</code> is a <code>Monoid</code> (tuple of two monoids)</li>\n\t<li><code>Map[K, SegmentEstimate]</code> is a <code>Monoid</code> (map with value monoid value type is monoid itself)</li>\n</ul>\n<h3 id=\"problems-with-custom-aggregation-functions\">Problems with custom aggregation functions</h3>\n<ul>\n\t<li>Right now, it is a closed API so you need to place all of your code under the\u00a0<code>org.apache.spark.sql\u00a0</code>package.</li>\n\t<li>It is not guaranteed that it will work in next Spark release.</li>\n\t<li>If you want to try, I suggest you to start with <code>org.apache.spark.sql.catalyst.expressions.Sum</code>as example.</li>\n</ul>\n<h2 id=\"spark-as-in-memory-sql-database\">Spark as an in-memory SQL database</h2>\nWe use Spark as an in-memory database that serves SQL (composed with DataFrame API) queries.\n\nPeople tend to think about Spark with a very batch oriented mindset. Start a Spark cluster in YARN, do the computation, kill the cluster. Submit you application to standalone Spark cluster (Mesos), kill it. The biggest problem with this approach is that after your application is done, the JVM is killed, <code>SparkContext</code> is lost, and even if you are running Spark in standalone mode, all data cached by your application is lost.\n\nWe use Spark in a totally different way. We start Spark cluster in YARN, load data to it from HDFS, cache it in memory, and <strong>do not shut it down</strong>. We keep JVM running, it holds a reference to <code>SparkContext</code> and keeps all the data in memory on worker nodes.\n\nOur backend application is essentially very simple REST/JSON server built with Spray, that holds the\u00a0<code>SparkContext</code> reference, receive requests via URL parameters, runs queries in Spark, and return responses in JSON.\n\nRight now (July 2015) we have data starting from April, and it\u2019s around 100g cached in 40 nodes. We need to keep 1-year history, so we don\u2019t expect more than 500g. And we are very confident that we can scale horizontally without seriously affecting performance. Right now average request response time is 1-2 seconds which is really good for our use case.\n<h2 id=\"spark-best-practices\">Spark Best practices</h2>\nHere are configuration options that I found really useful for our specific task. You can find more details about each of them in Spark guide.\n\n[scala]\n- spark.scheduler.mode=FAIR\n- spark.yarn.executor.memoryOverhead=4000\n- spark.sql.autoBroadcastJoinThreshold=300000000 // ~300mb\n- spark.serializer=org.apache.spark.serializer.KryoSerializer\n- spark.speculation=true\n[/scala]\n\nAlso, I found that it\u2019s really important to repartition your dataset if you are going to cache it and use for queries. The optimalmal number of partitions is around 4-6 for each executor core, with 40 nodes and 6 executor cores we use 1000 partitions for best performance.\n\nIf you have too many partitions Spark will spend too much time for coordination, and receiving results from all partitions. If too small, you might have problems with too big block during shuffle that can kill not only performance but all your cluster: <a href=\"https://issues.apache.org/jira/browse/SPARK-1476\">SPARK-1476</a>\n<h2 id=\"other-options\">Other Options</h2>\nBefore starting this project, we were evaluating some other options\n<h3 id=\"hive\">Hive</h3>\nObviously it\u2019s too slow for interactive UI backend, but we found it really useful for batch data processing. We use it to process raw logs and build aggregated tables with <code>HyperLogLog\u00a0</code>inside.\n<h3 id=\"impala\">Impala</h3>\nTo get good performance out of Impala, you are required to write C++ user defined functions, and it\u2019s was not the task that I wanted to do. Also, I\u2019m not confident that even with custom C++ function Impala can show performance that we need.\n<h4 id=\"druid\">Druid</h4>\n<a href=\"http://druid.io/\">Druid</a> is a really interesting project, and it\u2019s used in another project at Collective for a slightly different problem, but it\u2019s not in production yet.\n<ul>\n\t<li>Managing separate Druid cluster - it\u2019s not the task that I want to do</li>\n\t<li>We have batch-oriented process - and druid data ingestion is stream based</li>\n\t<li>Bad support for some of type of queries that we need - if I need to know intersection of some particular ad with all segments, in case of druid it will be 10k (number of segments) queries, and it will obviously fail to complete in 1-2 seconds</li>\n\t<li>It was not clear how to get data back from Druid - it\u2019s hard to get data back from Druid later, if it will turn out that it doesn\u2019t solve out problems well</li>\n</ul>\n<h2 id=\"conclusion\">Conclusion</h2>\nSpark is Awesome! I didn\u2019t have any major issues with it, and it just works! The new DataFrame API is amazing, and we are going to build lots of new cool projects at Collective with Spark MLLib and GraphX, and I\u2019m pretty sure they will all be successful."}
{"status": "publish", "description": null, "creator": "EugeneZhulenev", "link": "https://databricks.com/blog/2015/10/20/audience-modeling-with-apache-spark-ml-pipelines.html", "authors": null, "id": 5247, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2015-10-20", "tz": "UTC", "createdOn": "2015-10-20"}, "title": "Audience Modeling With Apache Spark ML Pipelines", "slug": "audience-modeling-with-apache-spark-ml-pipelines", "content": "<div class=\"entry-content\">\n\nThis is a guest blog from Eugene Zhulenev\u00a0on his experiences with Engineering Machine Learning and Audience Modeling\u00a0at \u00a0<a href=\"http://collective.com/main.html\">Collective</a>.\n\n<hr />\n\nAt <a href=\"http://collective.com/\">Collective</a>, we heavily rely on machine learning and predictive modeling to run our digital advertising business. All decisions about what ad to show at this particular time to this particular user are made by machine learning models (some of them are in real-time while some of them are offline).\n\nWe have a lot of projects that uses machine learning, the common name for all of them can be\u00a0<strong>Audience Modeling</strong>, as they all are trying to predict audience conversion (<em>CTR, Viewability Rate, etc\u2026</em>) based on browsing history, behavioral segments, and other type of predictors.\n\nFor most of our new development, we use <a href=\"https://spark.apache.org/\">Apache Spark</a> and <a href=\"https://spark.apache.org/mllib/\">MLLib</a>. However, while it is an awesome project, we found that there are some widely used tools and libraries that are missing in Spark. To add those missing features that we would really like to have in Spark, we created <a href=\"https://github.com/collectivemedia/spark-ext\">Spark Ext</a>\u00a0(Spark Extensions Library).\n<blockquote>Spark Ext on Github: <a href=\"https://github.com/collectivemedia/spark-ext\">https://github.com/collectivemedia/spark-ext</a></blockquote>\nI\u2019m going to show a\u00a0simple example of combining <a href=\"https://github.com/collectivemedia/spark-ext\">Spark Ext</a> with Spark ML pipelines for predicting user conversions based on geo and browsing history data.\n<blockquote>Spark ML pipeline example: <a href=\"https://github.com/collectivemedia/spark-ext/blob/master/sparkext-example/src/main/scala/com/collective/sparkext/example/SparkMlExtExample.scala\">SparkMlExtExample.scala</a></blockquote>\n<h2 id=\"predictors-data\">Predictors Data</h2>\nI\u2019m using a\u00a0dataset with 2 classes, that will be used for solving classification problem (user converted or not). It\u2019s created with <a href=\"https://github.com/collectivemedia/spark-ext/blob/master/sparkext-example/src/main/scala/com/collective/sparkext/example/DataGenerator.scala\">dummy data generator</a>\u00a0so that these 2 classes can be easily separated. It\u2019s pretty similar to real data that usually is available in digital advertising.\n<h3 id=\"browsing-history-log\">Browsing History Log</h3>\nHistory of websites that were visited by a user.\n<div class=\"bogus-wrapper\"><figure class=\"code\">\n<div class=\"CodeRay\">\n<div class=\"code\">\n<pre>Cookie          | Site          | Impressions  \n--------------- |-------------- | -------------\nwKgQaV0lHZanDrp | live.com      | 24\nwKgQaV0lHZanDrp | pinterest.com | 21\nrfTZLbQDwbu5mXV | wikipedia.org | 14\nrfTZLbQDwbu5mXV | live.com      | 1\nrfTZLbQDwbu5mXV | amazon.com    | 1\nr1CSY234HTYdvE3 | youtube.com   | 10\n</pre>\n</div>\n</div>\n</figure></div>\n<h3 id=\"geo-location-log\">Geo Location Log</h3>\nLatitude/Longitude impression history.\n<div class=\"bogus-wrapper\"><figure class=\"code\">\n<div class=\"CodeRay\">\n<div class=\"code\">\n<pre>Cookie          | Lat     | Lng       | Impressions\n--------------- |---------| --------- | ------------\nwKgQaV0lHZanDrp | 34.8454 | 77.009742 | 13\nwKgQaV0lHZanDrp | 31.8657 | 114.66142 | 1\nrfTZLbQDwbu5mXV | 41.1428 | 74.039600 | 20\nrfTZLbQDwbu5mXV | 36.6151 | 119.22396 | 4\nr1CSY234HTYdvE3 | 42.6732 | 73.454185 | 4\nr1CSY234HTYdvE3 | 35.6317 | 120.55839 | 5\n20ep6ddsVckCmFy | 42.3448 | 70.730607 | 21\n20ep6ddsVckCmFy | 29.8979 | 117.51683 | 1\n</pre>\n</div>\n</div>\n</figure></div>\n<h2 id=\"transforming-predictors-data\">Transforming Predictors Data</h2>\nAs you can see the predictors data (sites and geo) is in <em>long</em> format. \u00a0 Each <code>cookie</code> has multiple rows associated with it; in general, it is not a good fit for machine learning. We\u2019d like <code>cookie</code> to be a primary key while all other data should form the\u00a0<code>feature vector</code>.\n<h3 id=\"gather-transformer\">Gather Transformer</h3>\nInspired by R <code>tidyr</code> and <code>reshape2</code> packages, we convert a\u00a0<em>long</em> <code>DataFrame</code> with values for each key into a\u00a0<em>wide</em> <code>DataFrame</code>\u00a0and apply an aggregation function if the single key has multiple values.\n\n[scala]\nval gather = new Gather()\n      .setPrimaryKeyCols(&quot;cookie&quot;)\n      .setKeyCol(&quot;site&quot;)\n      .setValueCol(&quot;impressions&quot;)\n      .setValueAgg(&quot;sum&quot;)         // sum impression by key\n      .setOutputCol(&quot;sites&quot;)\nval gatheredSites = gather.transform(siteLog)      \n[/scala]\n\n<div class=\"bogus-wrapper\"><figure class=\"code\">\n<div class=\"CodeRay\">\n<div class=\"code\">\n<pre>Cookie           | Sites\n-----------------|----------------------------------------------\nwKgQaV0lHZanDrp  | [\n                 |  { site: live.com, impressions: 24.0 }, \n                 |  { site: pinterest.com, impressions: 21.0 }\n                 | ]\nrfTZLbQDwbu5mXV  | [\n                 |  { site: wikipedia.org, impressions: 14.0 }, \n                 |  { site: live.com, impressions: 1.0 },\n                 |  { site: amazon.com, impressions: 1.0 }\n                 | ]\n</pre>\n</div>\n</div>\n</figure></div>\n<h3 id=\"google-s2-geometry-cell-id-transformer\">Google S2 Geometry Cell Id Transformer</h3>\nThe S2 Geometry Library is a spherical geometry library, very useful for manipulating regions on the sphere (commonly on Earth) and indexing geographic data. Basically, it assigns a unique cell id for each region on the earth.\n<blockquote>Good article about S2 library: <a href=\"http://blog.christianperone.com/2015/08/googles-s2-geometry-on-the-sphere-cells-and-hilbert-curve/\">Google\u2019s S2, geometry on the sphere, cells, and Hilbert curve</a></blockquote>\nFor example, you can combine S2 transformer with <code>Gather</code> to convert\u00a0<code>lat</code>/<code>lon</code> to <code>K-V</code>pairs, where the key will be <code>S2</code> cell id. Depending on a level you can assign all people in Greater New York area (level = 4) into one cell, or you can index them block by block (level = 12).\n\n[scala]\n// Transform lat/lon into S2 Cell Id\nval s2Transformer = new S2CellTransformer()\n  .setLevel(5)\n  .setCellCol(&quot;s2_cell&quot;)\n\n// Gather S2 CellId log\nval gatherS2Cells = new Gather()\n  .setPrimaryKeyCols(&quot;cookie&quot;)\n  .setKeyCol(&quot;s2_cell&quot;)\n  .setValueCol(&quot;impressions&quot;)\n  .setOutputCol(&quot;s2_cells&quot;)\n  \nval gatheredCells = gatherS2Cells.transform(s2Transformer.transform(geoDf))\n[/scala]\n\n<div class=\"bogus-wrapper\"><figure class=\"code\">\n<div class=\"CodeRay\">\n<div class=\"code\">\n<pre>Cookie           | S2 Cells\n-----------------|----------------------------------------------\nwKgQaV0lHZanDrp  | [\n                 |  { s2_cell: d5dgds, impressions: 5.0 }, \n                 |  { s2_cell: b8dsgd, impressions: 1.0 }\n                 | ]\nrfTZLbQDwbu5mXV  | [\n                 |  { s2_cell: d5dgds, impressions: 12.0 }, \n                 |  { s2_cell: b8dsgd, impressions: 3.0 },\n                 |  { s2_cell: g7aeg3, impressions: 5.0 }\n                 | ]\n</pre>\n</div>\n</div>\n</figure></div>\n<h2 id=\"assembling-feature-vector\">Assembling Feature Vector</h2>\n<code>K-V</code> pairs from the result of <code>Gather</code> are cool, and groups all the information about the cookie into a single row. \u00a0However, they cannot be used as input for machine learning. To be able to train a model, the predictors data need to be represented as a vector of doubles. This is easy to do if all the features are continuous and numeric. \u00a0But if some of them are categorical or in <code>gathered</code> shape, this is not a trivial task.\n<h3 id=\"gather-encoder\">Gather Encoder</h3>\nEncodes categorical key-value pairs using dummy variables.\n\n[scala]\n// Encode S2 Cell data\nval encodeS2Cells = new GatherEncoder()\n  .setInputCol(&quot;s2_cells&quot;)\n  .setOutputCol(&quot;s2_cells_f&quot;)\n  .setKeyCol(&quot;s2_cell&quot;)\n  .setValueCol(&quot;impressions&quot;)\n  .setCover(0.95) // dimensionality reduction\n[/scala]\n\n<div class=\"bogus-wrapper\"><figure class=\"code\">\n<div class=\"CodeRay\">\n<div class=\"code\">\n<pre>Cookie           | S2 Cells\n-----------------|----------------------------------------------\nwKgQaV0lHZanDrp  | [\n                 |  { s2_cell: d5dgds, impressions: 5.0 }, \n                 |  { s2_cell: b8dsgd, impressions: 1.0 }\n                 | ]\nrfTZLbQDwbu5mXV  | [\n                 |  { s2_cell: d5dgds, impressions: 12.0 }, \n                 |  { s2_cell: g7aeg3, impressions: 5.0 }\n                 | ]\n</pre>\n</div>\n</div>\n</figure></div>\nTransformed into\n<div class=\"bogus-wrapper\"><figure class=\"code\">\n<div class=\"CodeRay\">\n<div class=\"code\">\n<pre>Cookie           | S2 Cells Features\n-----------------|------------------------\nwKgQaV0lHZanDrp  | [ 5.0  ,  1.0 , 0   ]\nrfTZLbQDwbu5mXV  | [ 12.0 ,  0   , 5.0 ]\n</pre>\n</div>\n</div>\n</figure></div>\nNote that it\u2019s 3 unique cell id values, that gives 3 columns in the final feature vector.\n\nOptionally apply dimensionality reduction using <code>top</code> transformation:\n<ul>\n\t<li>Top coverage, is selecting categorical values by computing the count of distinct users for each value, sorting the values in descending order by the count of users, and choosing the top values from the resulting list such that the sum of the distinct user counts over these values covers c percent of all users (e.g. selecting top sites covering 99% of users).</li>\n</ul>\n<h2 id=\"spark-ml-pipelines\">Spark ML Pipelines</h2>\nSpark ML Pipeline - is new high-level API for Spark MLLib.\n<blockquote>A practical ML pipeline often involves a sequence of data pre-processing, feature extraction, model fitting, and validation stages. For example, classifying text documents might involve text segmentation and cleaning, extracting features, and training a classification model with cross-validation. <a href=\"https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html\">Read More.</a></blockquote>\nIn Spark ML it\u2019s possible to split an ML pipeline into multiple independent stages, group them together in a single pipeline and run it with Cross-Validation and Parameter Grid to find the best set of parameters.\n<h3 id=\"put-it-all-together-with-spark-ml-pipelines\">Put It All together with Spark ML Pipelines</h3>\nGather encoder is a natural fit into Spark ML Pipeline API.\n\n[scala]\n// Encode site data\nval encodeSites = new GatherEncoder()\n  .setInputCol(&quot;sites&quot;)\n  .setOutputCol(&quot;sites_f&quot;)\n  .setKeyCol(&quot;site&quot;)\n  .setValueCol(&quot;impressions&quot;)\n\n// Encode S2 Cell data\nval encodeS2Cells = new GatherEncoder()\n  .setInputCol(&quot;s2_cells&quot;)\n  .setOutputCol(&quot;s2_cells_f&quot;)\n  .setKeyCol(&quot;s2_cell&quot;)\n  .setValueCol(&quot;impressions&quot;)\n  .setCover(0.95)\n\n// Assemble feature vectors together\nval assemble = new VectorAssembler()\n  .setInputCols(Array(&quot;sites_f&quot;, &quot;s2_cells_f&quot;))\n  .setOutputCol(&quot;features&quot;)\n\n// Build logistic regression\nval lr = new LogisticRegression()\n  .setFeaturesCol(&quot;features&quot;)\n  .setLabelCol(&quot;response&quot;)\n  .setProbabilityCol(&quot;probability&quot;)\n\n// Define pipeline with 4 stages\nval pipeline = new Pipeline()\n  .setStages(Array(encodeSites, encodeS2Cells, assemble, lr))\n\nval evaluator = new BinaryClassificationEvaluator()\n  .setLabelCol(Response.response)\n\nval crossValidator = new CrossValidator()\n  .setEstimator(pipeline)\n  .setEvaluator(evaluator)\n\nval paramGrid = new ParamGridBuilder()\n  .addGrid(lr.elasticNetParam, Array(0.1, 0.5))\n  .build()\n\ncrossValidator.setEstimatorParamMaps(paramGrid)\ncrossValidator.setNumFolds(2)\n\nprintln(s&quot;Train model on train set&quot;)\nval cvModel = crossValidator.fit(trainSet)\n[/scala]\n\n<h2 id=\"conclusion\">Conclusion</h2>\nThe Spark ML API makes machine learning much easier. <a href=\"https://github.com/collectivemedia/spark-ext\">Spark Ext</a> is a good example of how it is possible to create custom transformers/estimators that later can be used as a part of a bigger pipeline, and can be easily shared/reused by multiple projects.\n<blockquote>Full code for example application is available on <a href=\"https://github.com/collectivemedia/spark-ext/blob/master/sparkext-example/src/main/scala/com/collective/sparkext/example/SparkMlExtExample.scala\">Github</a>.</blockquote>\n</div>"}
{"status": "publish", "description": null, "creator": "Xiangrui", "link": "https://databricks.com/blog/2015/10/05/generalized-linear-models-in-sparkr-and-r-formula-support-in-mllib.html", "authors": null, "id": 5256, "categories": ["Apache Spark", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2015-10-05", "tz": "UTC", "createdOn": "2015-10-05"}, "title": "Generalized Linear Models in SparkR and R Formula Support in MLlib", "slug": "generalized-linear-models-in-sparkr-and-r-formula-support-in-mllib", "content": "<em>To get started with SparkR, <a href=\"http://spark.apache.org/downloads.html\">download Apache Spark 1.5</a> or <a href=\"https://accounts.cloud.databricks.com/registration.html#signup\">sign up for a 14-day free trial of Databricks today</a>.</em>\n\n<hr />\n\nApache Spark 1.5 adds initial support for distributed machine learning over SparkR DataFrames. To provide an intuitive interface for R users, SparkR extends R's native methods for fitting and evaluating models to use MLlib for large-scale machine learning. In this blog post, we cover how to work with generalized linear models in SparkR, and how to use the new R formula support in MLlib to simplify machine learning pipelines. This work was contributed by Databricks in Spark 1.5. We\u2019d also like to thank Alteryx for providing input on early designs.\n<h2>Generalized Linear Models</h2>\nGeneralized linear models unify various statistical models such as linear and logistic regression through the specification of a model family and link function. In R, such models can be fitted by passing an R model formula, family, and training dataset to the <code><a href=\"https://stat.ethz.ch/R-manual/R-devel/library/stats/html/glm.html\">glm()</a></code> function. Spark 1.5 extends <code>glm()</code> to operate over Spark <a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\">DataFrames</a>, which are distributed data collections managed by Spark. We also support elastic-net regularization for these models, the same as in R's <code><a href=\"https://cran.r-project.org/web/packages/glmnet/index.html\">glmnet</a></code> package.\n<h2>Fitting Models</h2>\nSince we extend R's native methods for model fitting, the interface is very similar. R lets you specify the modeling of a response variable in a compact symbolic form. For example, the formula <code>y ~ f0 + f1</code> indicates the response <code>y</code> is modeled linearly by variables <code>f0</code> and <code>f1</code>. In 1.5 we support a subset of the <a href=\"https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html\">R formula operators</a> available. This includes the <code>+</code> (inclusion), <code>-</code> (exclusion), <code>.</code> (include all), and intercept operators. To demonstrate glm in SparkR, we will walk through fitting a model over a 12 GB dataset (with over 120 million records) in the example below. Datasets of this size are hard to train on a single machine due to their size.\n<h3>Preprocessing</h3>\nThe dataset we will operate on is the publicly available <a href=\"http://www.transtats.bts.gov/OT_Delay/OT_DelayCause1.asp\">airlines dataset</a>, which contains twenty years of flight records (from 1987 to 2008). We are interested in predicting airline arrival delay based on the flight departure delay, aircraft type, and distance traveled.\n\nFirst, we read the data from the CSV format using the <a href=\"http://spark-packages.org/package/databricks/spark-csv\">spark-csv</a> package and join it with an auxiliary <a href=\"http://stat-computing.org/dataexpo/2009/supplemental-data.html\">planes table</a> with details on individual aircraft.\n<pre>&gt; airlines &lt;- read.df(sqlContext, path=\"/home/ekl/airlines\",\n    source=\"com.databricks.spark.csv\", header=\"true\", inferSchema=\"true\")\n&gt; planes &lt;- read.df(sqlContext, \"/home/ekl/plane_info\",\n    source=\"com.databricks.spark.csv\", header=\"true\", inferSchema=\"true\")\n&gt; joined &lt;- join(airlines, planes, airlines$TailNum == planes$tailnum)\n</pre>\nWe use functionality from the DataFrame API to apply some preprocessing to the input. As part of the preprocessing, we decide to drop rows containing null values by applying <code>dropna()</code> to the DataFrame.\n<pre>&gt; training &lt;- dropna(joined)\n&gt; showDF(select(training,\n    \u201caircraft_type\u201d, \u201cDistance\u201d, \u201cArrDelay\u201d, \u201cDepDelay\u201d))\n\n aircraft_type              | Distance | DepDelay  | ArrDelay\n----------------------------|----------|-----------|----------\n \"Balloon\"                  | 23       | 18        | 20\n \"Fixed Wing Multi-Engine\"  | 815      | 2         | -2\n \"Fixed Wing Single-Engine\" | 174      | 0         | 1\n</pre>\n<h3>Training</h3>\nThe next step is to use MLlib by calling <code><a href=\"http://spark.apache.org/docs/latest/api/R/glm.html\">glm()</a></code> with a formula specifying the model variables. We specify the Gaussian family here to indicate that we want to perform linear regression. MLlib caches the input DataFrame and launches a series of Spark jobs to fit our model over the distributed dataset.\n<pre>&gt; model &lt;- glm(ArrDelay ~ DepDelay + Distance + aircraft_type,\n    family = \"gaussian\", data = training)</pre>\nNote that parameter \u201clambda\u201d can be used with glm to add regularization and \u201calpha\u201d to adjust elastic-net constant.\n<h3>Evaluation</h3>\nAs with R's native models, coefficients can be retrieved using the <code><a href=\"http://spark.apache.org/docs/latest/api/R/summary.html\">summary()</a></code> function.\n<pre>&gt; summary(model)\n$coefficients\n                                             Estimate\n(Intercept)                             -0.5155037863\nDepDelay                                 0.9776640253\nDistance                                -0.0009826032\naircraft_type__Fixed Wing Multi-Engine   0.3348238914\naircraft_type__Fixed Wing Single-Engine  0.2296622061\naircraft_type__Balloon                   0.5374569269</pre>\nNote that the <code>aircraft_type</code> feature is categorical. Under the hood, SparkR automatically performs one-hot encoding of such features so that it does not need to be done manually. Beyond String and Double type features, it is also possible to fit over <a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.Vector\">MLlib Vector</a> features, for compatibility with other MLlib components.\n\nTo evaluate our model we can also use <code><a href=\"http://spark.apache.org/docs/latest/api/R/predict.html\">predict()</a> </code>just like in R. We can pass in the training data or another DataFrame that contains test data.\n<pre>&gt; preds &lt;- predict(model, training)\n&gt; errors &lt;- select(\n    preds, preds$label, preds$prediction, preds$aircraft_type,\n    alias(preds$label - preds$prediction, \"error\"))</pre>\nSince the returned DataFrame contains the original columns in addition to the label, features, and predicted value, it is easy to inspect the result. Here we take advantage of the built-in visualizations from Databricks to examine the error distribution with respect to the aircraft type.\n<pre>&gt; display(sample(errors, F, .0001))</pre>\n<img class=\"alignnone wp-image-5262\" src=\"https://databricks.com/wp-content/uploads/2015/10/Screen-Shot-2015-10-01-at-4.40.42-PM-1024x307.png\" alt=\"Screen Shot 2015-10-01 at 4.40.42 PM\" width=\"800\" height=\"240\" />\n\nIn summary, SparkR now provides seamless integration of DataFrames with common R modeling functions, making it simple for R users to take advantage of MLlib's distributed machine learning algorithms.\n\nTo learn more about SparkR and its integration with MLlib, see the latest <a href=\"http://spark.apache.org/docs/latest/sparkr.html\">SparkR documentation</a>.\n<h2>R formula support in other languages</h2>\nSparkR implements the interpretation of R model formulas as an <a href=\"http://spark.apache.org/docs/latest/ml-guide.html#transformers\">MLlib feature transformer</a>, for integration with the <a href=\"https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html\">ML Pipelines API</a>. The <a href=\"http://spark.apache.org/docs/latest/ml-features.html#rformula\">RFormula transformer</a> provides a convenient way to specify feature transformations like in R.\n\nTo see how the RFormula transformer can be used, let's start with the same airlines dataset from before. In Python, we create an RFormula transformer with the same formula used in the previous section.\n<pre>&gt;&gt;&gt; import pyspark.ml.feature.RFormula\n&gt;&gt;&gt; formula = RFormula(\n        formula=\"ArrDelay ~ DepDelay + Distance + aircraft_type\")</pre>\nAfter the transformation, a DataFrame with features and label column appended is returned. Note\nthat we have to call <code>fit()</code> on a dataset before we can call <code>transform()</code>. The <code>fit()</code> step determines the mapping of categorical feature values to vector indices in the output, so that the fitted RFormula can be used across different datasets.\n<pre>&gt;&gt;&gt; formula.fit(training).transform(training).show()\n+--------------+---------+---------+---------+--------------------+------+\n| aircraft_type| Distance| DepDelay| ArrDelay|            features| label|\n+--------------+---------+---------+---------+--------------------+------+\n|       Balloon|       23|       18|       20| [0.0,0.0,23.0,18.0]|  20.0|\n|  Multi-Engine|      815|        2|       -2| [0.0,1.0,815.0,2.0]|  -2.0|\n| Single-Engine|      174|        0|        1| [1.0,0.0,174.0,0.0]|   1.0|\n+--------------+---------+---------+---------+--------------------+------+\n</pre>\nAny ML pipeline can include the RFormula transformer as a pipeline stage, which is in fact how SparkR implements <code>glm()</code>. After we have created an appropriate RFormula transformer and an estimator for the desired model family, fitting a GLM model takes only one step:\n<pre>&gt;&gt;&gt; import pyspark.ml.Pipeline\n&gt;&gt;&gt; import pyspark.ml.regression.LinearRegression\n\n&gt;&gt;&gt; estimator = LinearRegression()\n&gt;&gt;&gt; model = Pipeline(stages=[formula, estimator]).fit(training)</pre>\nWhen the pipeline executes, the features referenced by the formula will be encoded into an output feature vector for use by the linear regression stage.\n\nWe hope that RFormula will simplify the creation of ML pipelines by providing a concise way of expressing complex feature transformations. Starting in Spark 1.5 the RFormula transformer is available for use in Python, Java, and Scala.\n<h2>What\u2019s next?</h2>\nIn Spark 1.6 we are adding support for more advanced features of R model formulas, including <a href=\"https://issues.apache.org/jira/browse/SPARK-9681\">feature interactions</a>, <a href=\"https://issues.apache.org/jira/browse/SPARK-9838\">more model families</a>, <a href=\"https://issues.apache.org/jira/browse/SPARK-9840\">link functions</a>, and <a href=\"https://issues.apache.org/jira/browse/SPARK-9836\">better summary support</a>.\n\nAs part of this blog post, we would like to thank Dan Putler and Chris Freeman from Alteryx for useful discussions during the implementation of this functionality in SparkR, and Hossein Falaki for input on content."}
{"status": "publish", "description": null, "creator": "rxin", "link": "https://databricks.com/blog/2015/10/01/apache-spark-1-5-1-and-what-do-version-numbers-mean.html", "authors": null, "id": 5286, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2015-10-02", "tz": "UTC", "createdOn": "2015-10-02"}, "title": "Apache Spark 1.5.1 and What do Version Numbers Mean?", "slug": "apache-spark-1-5-1-and-what-do-version-numbers-mean", "content": "The inaugural Spark Summit Europe will be held in Amsterdam on October 27 - 29. Check out the <a href=\"https://spark-summit.org/eu-2015/schedule/\">full agenda</a> and get your ticket before it sells out!\n\n<hr />\n\nWe are excited to announce the availability of Apache Spark\u2019s 1.5.1 release. Spark 1.5.1 is a maintenance release containing 80 bug fixes in various components, including core, DataFrames, SQL, YARN, Parquet support, ORC support. You can find the detailed list of bug fixes on <a href=\"http://spark.apache.org/releases/spark-release-1-5-1.html\">Spark JIRA</a>.<b> We strongly recommend all 1.5.0 users to upgrade to this release.</b>\n\nIf you are a Databricks customer, we have been upgrading the Spark 1.5 package as we fix bugs, and as a result you are already benefiting from all the work in 1.5.1. Simply choose \u201cSpark 1.5\u201d in the cluster creation interface.\n\nI would also like to take this opportunity to address a very common question we get: what does Spark\u2019s version number (e.g. 1.4.0 vs 1.5.0 vs 1.5.1) mean?\n\n<h2>What do Spark version numbers mean?</h2>\nSince Spark 1.0.0, each Spark release is identified by a 3 part version number: [major].[minor].[maintenance].\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/10/Spark-branch.png\"><img class=\"alignnone wp-image-5285\" src=\"https://databricks.com/wp-content/uploads/2015/10/Spark-branch.png\" alt=\"Spark-branch\" width=\"400\" height=\"459\" /></a>\n\nThe first part, major version number, is used to indicate API compatibility. Unless explicitly marked as experimental, user-facing APIs should be backward compatible throughout a major version. That is to say, from an API point of view, if your application is programmed against Spark 1.0.0, this application should be able to run in all Spark 1.x versions without any modification.\n\nThe second part, minor version number, indicates releases that bring new features and improvements without changes to existing user-facing APIs. Currently, Spark releases a new minor version every 3 months. For example, Spark 1.5.0 was released in early September, and you can expect Spark 1.6.0 to be released in early December.\n\nThe third part, maintenance version number, indicates releases that focus on bug fixes for the same minor version. A patch can only be merged into a maintenance release if the risk for regression is deemed extremely low. Maintenance releases don\u2019t follow a specific schedule. They are made as the community find critical bugs. In general, we encourage users to always upgrade to the latest maintenance release available. That is to say, if you are currently running Spark 1.5.0, you should upgrade to Spark 1.5.1 today.\n\n<h2>What about Spark in Databricks?</h2>\nThe engineering process and the cloud delivery model at Databricks enable us to push updates in a short amount of time. Spark in Databricks follows the official Apache Spark releases, with one improvement: we leverage the fast delivery to provide our customers with new features and bug fixes as soon as possible.\n\nAs we have done with Spark 1.5, we started offering Spark 1.5 preview before the official Apache 1.5.0 was available. The preview version is built based on upstream patches and provided for prototyping and experimentation. Similarly, we patch critical bug fixes in Databricks so our customers can receive the upstream bug fixes in the shortest amount of time possible.\n\nThis applies to all the versions we currently support, i.e. Spark 1.3, 1.4, and 1.5. That is to say, if you choose Spark 1.3 as the version in Databricks, you will automatically receive bug fixes as they are fixed in the branch for Apache Spark 1.3.x (branch-1.3).\n\nTo try Databricks, sign up for a <a href=\"http://dbricks.co/1LgbUwL\">free 14-day trial</a>."}
{"status": "publish", "description": null, "creator": "scott", "link": "https://databricks.com/blog/2015/10/14/call-for-presentation-for-the-2016-spark-summit-east-is-now-open.html", "authors": null, "id": 5329, "categories": ["Company Blog", "Events"], "dates": {"publishedOn": "2015-10-14", "tz": "UTC", "createdOn": "2015-10-14"}, "title": "Call for Presentations for the 2016 Spark Summit East is Now Open", "slug": "call-for-presentation-for-the-2016-spark-summit-east-is-now-open", "content": "We are excited to announce that the call for presentations for the second<a href=\"http://spark-summit.org/east-2016\"> Spark Summit East </a>is now open. Please join us in New York City on February 16 -18, 2016 to share your experience with Apache Spark and celebrate its growth.\n\nSpark Summit East 2016 is looking for presenters who would like to showcase how Spark and its related technologies are used in applications, development, data science, enterprise, and research. Please visit our <a href=\"http://spark-summit.org/east-2016\">submission page</a> for additional details. The Deadline for submissions is <b>November 22, 2015 at 11:59pm PST</b>.\n<p class=\"p1\" style=\"text-align: center;\">[btn href=\"https://spark-summit.org/east-2016/\"]Submit Your Presentation[/btn]</p>\n<p class=\"p1\">Spark Summit is the largest data engineering and data sciences event focused on Apache Spark\u2014assembling the very best engineers, scientists, analysts, and executives from around the globe to share their knowledge and receive training on this open-source powerhouse.</p>\nIf you missed <a href=\"https://spark-summit.org/2015/\">Spark Summit 2015</a> <a href=\"https://spark-summit.org/2015/\">o</a>r <a href=\"https://spark-summit.org/east-2015/\">Spark Summit East 2015</a>, all the content is available online for free."}
{"status": "publish", "description": "In the most recent release of Databricks, we have added the ability to export Databricks notebooks to HTML, allowing users to publish their code in a public setting.", "creator": "dave_wang", "link": "https://databricks.com/blog/2015/10/21/introducing-more-databricks-notebooks-sharing-options.html", "authors": null, "id": 5336, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2015-10-21", "tz": "UTC", "createdOn": "2015-10-21"}, "title": "Introducing More Databricks Notebooks Sharing Options", "slug": "introducing-more-databricks-notebooks-sharing-options", "content": "To try the new export features mentioned in this blog, <a href=\"https://accounts.cloud.databricks.com/registration.html#signup\">sign up for a 14-day free trial of Databricks today.</a>\n\n<hr />\n\nDatabricks notebooks function as a development environment for developers, data engineers, and data scientists to build Apache Spark applications and explore data. Databricks users on the same team can also share notebooks with each other easily in their integrated workspace. In the most recent release of Databricks, we have added the ability to export Databricks notebooks to HTML, allowing users to publish their code in a public setting.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/10/notebook-example.png\"><img class=\"aligncenter wp-image-5337\" src=\"https://databricks.com/wp-content/uploads/2015/10/notebook-example-1024x545.png\" alt=\"notebook example\" width=\"600\" height=\"319\" /></a>\n\nDatabricks notebooks are powerful because one can run the Spark commands interactively, and immediately visualize the result with built-in charts and graphs. However, sometimes getting the right answer is not enough, you also have to be able to share the results and your thought process widely to convince people to take action.\n\nFor many, sharing work that includes charts and graphs require copy / pasting text, taking screenshots, or even creating PDFs. While Databricks users can already export their notebooks as source files or iPython notebooks, we want to provide even more options to share. With the new HTML export functionality, Databricks users can share their notebooks as an HTML page with a single click. <a href=\"https://databricks.com/wp-content/uploads/2015/10/Example_notebook_export.html\" target=\"_blank\">Check out an exported notebook here</a>.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/10/Example_notebook_export.html\"><img class=\"aligncenter wp-image-5354\" src=\"https://databricks.com/wp-content/uploads/2015/10/Export-screenshot-1024x548.png\" alt=\"Export screenshot\" width=\"400\" height=\"214\" /></a>\n\nWhen you export an entire notebook, you get one small HTML file (a few hundred KB). Having the entire notebook in a single, compact file makes it very easy to share - especially via e-mail - because there are no extraneous files to worry about. The exported notebooks even retain\u00a0the dynamic qualities of D3 visualizations. All the charts and graphs remain as SVG elements in exported notebooks, meaning that they support all the same interactions (hover, text highlight, etc.) you would expect. (<a href=\"https://databricks.com/wp-content/uploads/2015/10/D3_Visualization_Notebook_Export.html\" target=\"_blank\">To see an exported notebook with D3 visualizations, click here</a>).\n\nWith the new export feature of Databricks notebooks, sharing your work becomes even easier. Instead of slow and clunky manual export with copy / paste or screenshots, you can simply export the notebook in HTML format with a single click. The ability to easily and accurately share your notebooks with anyone means you can spend more time working with data and telling stories with the insights you\u2019ve developed instead of wrangling with files.\n\nTo try this new feature in Databricks, <a href=\"https://accounts.cloud.databricks.com/registration.html#signup\">sign-up for a 14-day free trial today</a>."}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2015/10/19/introducing-redshift-data-source-for-spark.html", "authors": null, "id": 5356, "categories": ["Ecosystem", "Engineering Blog"], "dates": {"publishedOn": "2015-10-19", "tz": "UTC", "createdOn": "2015-10-19"}, "title": "Introducing Redshift Data Source for Spark", "slug": "introducing-redshift-data-source-for-spark", "content": "This is a guest blog from Sameer Wadkar,\u00a0Big Data Architect/Data Scientist at Axiomine.\n\n<hr />\n\nThe\u00a0<a href=\"https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html\">Spark SQL Data Sources API</a>\u00a0was introduced in Apache Spark 1.2 to provide a pluggable mechanism for integration with structured data sources of all kinds. Spark users can read data from a variety of sources such as Hive tables, JSON files, columnar Parquet tables, and many others. Third party data sources are also available via <a href=\"http://spark-packages.org/\" target=\"_blank\">spark-package.org</a>. This post discusses a new Spark data source for accessing the <a href=\"https://aws.amazon.com/redshift/\">Amazon Redshift Service</a>.\u00a0Redshift Data Source for Spark is a package maintained by Databricks, with community contributions from<b>\u00a0</b>SwiftKey and other companies.\n\nPrior to the introduction of Redshift Data Source for Spark, Spark\u2019s JDBC data source was the only way for Spark users to read data from Redshift. While this method is adequate when running queries returning a small number of rows (order of 100\u2019s), it is too slow when handling large-scale data. The reason being that JDBC provides a <code>ResultSet</code> based approach, where rows are retrieved in a single thread in small batches. Furthermore, the use of JDBC to store large datasets in Redshift is only practical when data needs to be moved between tables inside a Redshift database. The JDBC-based INSERT/UPDATE queries are only practical for small updates to Redshift tables. For users hoping to load or store large volumes of data from/to Redshift, JDBC leaves much to be desired in terms of performance and throughput.\n\nUsing this package simplifies the integration with the Redshift service by automating the set of manual steps that would otherwise be required to move large amounts of data in and out of Redshift. To understand how it does so, let us look at how you would integrate large datasets from a Redshift database with datasets from other data sources.\n\nWe will also explore how this package expands the range of possibilities for Redshift as well as Spark users. Traditionally, data had to be moved from HDFS to Redshift for analytics. However, this package will allow Redshift to interoperate seamlessly (via the Unified Data Sources API) with data stored in S3, Hive tables, CSV or Parquet files on HDFS. This will simplify ETL pipelines and allow users to operate on a logical and unified view of the system.\n<h2>Reading from Redshift</h2>\nSay you want to process an entire table (or a query which returns a large number of rows) in Spark and combine it with a dataset from another large data source such as Hive. The set of commands to load the Redshift table (query) data into a schema compliant DataFrame instance is:\n\n<pre>val jdbcURL = \"jdbc:redshift://test-redshift.czac2vcs84ci.us-east-.redshift.amazonaws.com:5439/testredshift?user=redshift&password=W9P3GC42GJYFpGxBitxPszAc8iZFW\"\nval tempS3Dir = \"s3n://spark-redshift-testing/temp/\"\nval salesDF = sqlContext.read\n    .format(\"com.databricks.spark.redshift\")\n    .option(\"url\", jdbcURL) //Provide the JDBC URL\n    .option(\"tempdir\", tempS3Dir) //User provides a temporary S3 folder\n    .option(\"dbtable\", \"sales\") //or use .option(\"query\",\"select * from sales\") \n    .load()</pre>\n\nThe above command provides a DataFrame instance for the Redshift table (query). The user only needs to provide the JDBC URL, temporary S3 folder to which this package unloads Redshift data, and the name of the table or query.\n\nThe DataFrame instance can be registered as a temporary table in Spark and queries can be executed directly against it.\n\n<pre>salesDF.registerTempTable(\"sales_from_redshift\")\nval newSalesDF = sqlContext.sql(\"SELECT count(*) FROM sales_from_redshift\")</pre>\n\nThe same results can be achieved using the SQL Command Line Interface (CLI) as follows:\n\n<pre>> CREATE TEMPORARY TABLE sales_from_redshift\n  USING com.databricks.spark.redshift\n  OPTIONS (\n    dbtable 'sales',\n    tempdir 's3n://spark-redshift-testing/temp/',\n    url 'jdbc:redshift://test-redshift.czac2vcs84ci.us-east-.redshift.amazonaws.com:5439/testredshift?user=redshift&password=W9P3GC42GJYFpGxQtaCBitxPszAc8iZFW'\n  );\n\n> SELECT count(*) FROM sales_from_redshift;</pre>\n\nNote how we registered the retrieved Redshift table as a temporary table <code>sales_from_redshift</code> in Spark and executed a query directly on it with:\n\n<pre>SELECT count(*) FROM sales_from_redshift;</pre>\n\nUnder the hood, this package executes a Redshift <a href=\"http://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD.html\">UNLOAD</a> command (using JDBC) which copies the Redshift table in parallel to a temporary S3 bucket provided by the user. Next it reads these S3 files in parallel using the Hadoop InputFormat API and maps it to an RDD instance. Finally, it applies the schema of the table (or query) retrieved using JDBC metadata retrieval capabilities to the RDD generated in the prior step to create a DataFrame instance.\n\n<img class=\"wp-image-5366 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/10/image01.gif\" alt=\"image01\" width=\"500\" height=\"201\" />\n\nRedshift Data Source for Spark cannot automatically clean up the temporary files that it creates in S3. As a result, we recommend that you use a dedicated temporary S3 bucket with an <a href=\"http://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html\">object lifecycle configuration</a> to ensure that temporary files are automatically deleted after a specified expiration period.\n<h2>Writing to Redshift</h2>\nSpark Data Sources API is a powerful ETL tool. A common use case in Big Data systems is to source large scale data from one system, apply transformations on it in a distributed manner, and store it back in another system. For example, it is typical to source data from Hive tables in HDFS and copy the tables into Redshift to allow for interactive processing. This package is perfectly suited for this use case.\n\nAssume that a <code>transaction</code> table sourced from Hive is available in the Spark environment and needs to be copied to a corresponding Redshift table <code>redshift_transaction</code>. The following command achieves this goal:\n\n<pre>sqlContext.sql(\"SELECT * FROM transaction\")\n          .write.format(\"com.databricks.spark.redshift\")\n          .option(\"url\", jdbcURL)\n          .option(\"tempdir\", tempS3Dir) \n          .option(\"dbtable\", \"redshift_transaction\") \n          .mode(SaveMode.Overwrite)\n          .save()</pre>\n\nUsing SQL CLI the same results can be obtained as follows:\n\n<pre>CREATE TABLE redshift_transaction\n  USING com.databricks.spark.redshift\n  OPTIONS (\n    dbtable 'redshift_transaction',\n    tempdir 's3n://spark-redshift-testing/temp/',\n    url 'jdbc:redshift://test-redshift.czac2vcs84ci.us-east-.redshift.amazonaws.com:5439/testredshift?user=redshift&password=W9P3GC42GJYFpGxQtaCBitxPszAc8iZFW')\n  AS SELECT * FROM transaction;</pre>\n\n&nbsp;\n\nNote the <code>mode(SaveMode.Overwrite)</code> in the Scala code above. This indicates to Redshift Data Source for Spark to overwrite the table if it exists. By default (only mode available in SQL CLI mode) this package will throw an error if the table already exists.<code>(SaveMode.ErrorIfExists)</code> There is also a <code>SaveMode.Append</code> mode that creates the table if it does not exist and appends to the table if it does exist. The last mode is <code>SaveMode.Ignore</code> which creates the table if it does not exist and quietly ignores the entire command if the table already exists.\n\nUnder the hood, Redshift Data Source for Spark will first create the table in Redshift using JDBC. It then copies the partitioned RDD encapsulated by the source DataFrame (a Hive table in our example) instance to the temporary S3 folder. Finally, it executes the Redshift <a href=\"http://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html\">COPY</a> command that performs a high performance distributed copy of S3 folder contents to the newly created Redshift table.\n\n<img class=\"wp-image-5365 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/10/image00.gif\" alt=\"image00\" width=\"600\" height=\"71\" />\n<h2>Integration with other Data Sources</h2>\nData read via this package is automatically converted to DataFrame objects, Spark\u2019s primary abstraction for large datasets. This promotes interoperability between data sources since types are automatically converted to Spark\u2019s standard representations (for example <code>StringType</code>, <code>DecimalType</code>). A Redshift user can, for instance, join Redshift tables with data stored in S3, Hive tables, CSV or Parquet files stored on HDFS. This flexibility is important to users with complex data pipelines involving multiple sources.\n<h2>Using\u00a0Redshift Data Source for Spark</h2>\nOur goal in this blog entry was to introduce this package and provide an overview on how it integrates Redshift into Spark\u2019s Unified Data Processing platform. To try these new features, <a href=\"http://spark.apache.org/downloads.html\">download Spark 1.5</a> or <a href=\"https://accounts.cloud.databricks.com/registration.html#signup\">sign up for a 14-day free trial with Databricks today</a>. We also provide a very detailed <a href=\"https://github.com/databricks/spark-redshift/blob/master/tutorial\">tutorial</a>. The tutorial will walk you through the process of creating a sample Redshift database. It will then demonstrate how to interact with Redshift via this package from your local development environment."}
{"status": "publish", "description": null, "creator": "joseph", "link": "https://databricks.com/blog/2015/10/27/visualizing-machine-learning-models.html", "authors": null, "id": 5392, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2015-10-27", "tz": "UTC", "createdOn": "2015-10-27"}, "title": "Visualizing Machine Learning Models", "slug": "visualizing-machine-learning-models", "content": "To try the new visualization features mentioned in this blog, <a href=\"https://accounts.cloud.databricks.com/registration.html#signup\">sign up for a 14-day free trial of Databricks today.</a>\n\n<hr />\n\nYou've built your machine learning models and evaluated them with error metrics, but do the numbers make sense? Being able to visualize models is often a vital step in advanced analytics as it is usually easier to understand a diagram than numbers in a table.\n\nDatabricks has a built-in <code>display()</code> command that can display DataFrames as a table and create convenient one-click plots. Recently, we have extended the <code>display()</code> command to visualize machine learning models as well.\n\nIn this post, we will look at how easy visualization can be with Databricks -- a quick <code>display()</code> command can give you immediate feedback about complex models!\n<h2>Linear Models: Fitted vs Residuals</h2>\n<p style=\"text-align: center;\"><i>Scala-only</i></p>\nThe Fitted vs Residuals plot is available for Linear Regression and Logistic Regression models. The Databricks\u2019 Fitted vs Residuals plot is analogous to <a href=\"http://stat.ethz.ch/R-manual/R-patched/library/stats/html/plot.lm.html\">R's \u201cResiduals vs Fitted\u201d plots for linear models</a>.\n\nHere, we will look at how these plots are used with Linear Regression.\n\nLinear Regression computes a prediction as a weighted sum of the input variables. The Fitted vs Residuals plot can be used to assess a linear regression model's goodness of fit.\n<pre>display(linearModel, data, plotType=\"fittedVsResiduals\")</pre>\n<img class=\"aligncenter wp-image-5393\" src=\"https://databricks.com/wp-content/uploads/2015/10/blog-figure1.png\" alt=\"blog-figure1\" width=\"500\" height=\"488\" />\n\nThe above is an example of a fitted vs residuals plot for a linear regression model that is returning good predictions. A good linear model will usually have residuals distributed randomly around the residuals=0 line with no distinct outliers and no clear trends. The residuals should also be small for the whole range of fitted values.\n\n<img class=\"aligncenter wp-image-5394\" src=\"https://databricks.com/wp-content/uploads/2015/10/blog-figure2.png\" alt=\"blog-figure2\" width=\"500\" height=\"487\" />\n\nIn comparison, this visualization is a warning sign for this linear regression model: the range of residuals increases as the fitted values increase. This could mean we should evaluate using relative error instead of absolute error.\n<h2>K-means Clustering: Visualizing Clusters</h2>\nK-means tries to separate data points into clusters by minimizing the sum of squared errors between data points and their nearest cluster centers.\n\nWe can now visualize clusters and plot feature grids to identify trends and correlations. Each plot in the grid corresponds to 2 features, and data points are colored by their respective cluster labels. The plots can be used to visually assess how well your data have been clustered.\n<pre>display(kMeansModel, data)</pre>\n<img class=\"aligncenter wp-image-5396\" src=\"https://databricks.com/wp-content/uploads/2015/10/blog-figure3-1024x908.png\" alt=\"blog-figure3\" width=\"500\" height=\"443\" />\n\nFrom these plots, we notice that clusters 0 and 2 are sometimes overlapping with each other for some features, whereas cluster 1 is always cleanly separated from the rest. \u00a0Features 2 and 3 are particularly useful for distinguishing cluster 1.\n<h2>Logistic Regression: ROC Curves</h2>\n<p style=\"text-align: center;\"><i>Scala-only, with clusters running Apache Spark 1.5 or higher</i></p>\nLogistic Regression is widely used for binary classification, where a logistic function is used to model the class probabilities of your data.\n\nLogistic Regression converts a numerical class probability into a binary (0/1) label using a threshold, and adjusting the threshold allows you to adjust the probability cutoff for predicting 0 vs. 1. To review how your model performs over various thresholds, you can easily plot your model\u2019s ROC Curve with the <code>display()</code> command. The plot will also interactively display threshold values on mouseover.\n<pre>display(logisticModel, data, plotType=\"ROC\")</pre>\n<a href=\"https://databricks.com/wp-content/uploads/2015/10/DB-ML-visualization.gif\"><img class=\"size-full wp-image-5397 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/10/DB-ML-visualization.gif\" alt=\"DB ML visualization\" width=\"498\" height=\"462\" /></a>\n\nThe dotted diagonal line represents how a model will perform if it randomly guesses every prediction, and the (0.00,1.00) point in the top left corner represents a perfect classification. From the above curve, it is clear that our model is doing much better than random guessing, and we can adjust the threshold based on how much we value true positive vs. false positive predictions.\n\nTo see an example of these visualizations and try out the interactive display, check out the exported notebook <a href=\"https://databricks.com/wp-content/uploads/2015/10/Model_Visuals.html\" target=\"_blank\">here</a>.\n<h2>What's Next?</h2>\nThe plots listed above as Scala-only will soon be available in Python notebooks as well. There are also other machine learning model visualizations on the way. Stay tuned for Decision Tree and Machine Learning Pipeline visualizations!"}
{"status": "publish", "description": null, "creator": "scott", "link": "https://databricks.com/blog/2015/10/26/see-you-at-spark-summit-eu.html", "authors": null, "id": 5408, "categories": ["Company Blog", "Events"], "dates": {"publishedOn": "2015-10-26", "tz": "UTC", "createdOn": "2015-10-26"}, "title": "See You at Spark Summit EU 2015!", "slug": "see-you-at-spark-summit-eu", "content": "With the first <a href=\"https://spark-summit.org/eu-2015/\" target=\"_blank\">Spark Summit</a> Europe only days away, we are excited to announce that all 900 tickets have been sold out. Spark Summit is the premier event that brings the Apache Spark community together, where they can hear from leading production users of Spark, Spark SQL, Spark Streaming and related projects; find out where the project development is going, and learn how to use the Spark stack in a variety of applications. The enthusiasm shown by the Spark community turned the Summit from a small gathering of early adopters in San Francisco in 2013 into a global phenomenon of over three thousand people across continents in two year's time.\n\n<a href=\"https://spark-summit.org/eu-2015/schedule/\" target=\"_blank\">A great deal of exciting content</a> has been planned at Amsterdam. To kick things off, the creator of Spark - Matei Zaharia - will give a keynote. Matei will be followed by industry and community leaders from organizations such as Databricks, Goldman Sachs, and many others.\n\nThe diverse set of talks is divided by topics that include: developer, research, data science, and applications. Organization such as Barcelona Supercomputing Center, Barclay, Netflix, The Guardian, and SK Telecom will take the stage to share their valuable experiences in working with Apache Spark.\n\nAs with all Spark Summits, there will be multiple training sessions for people who want to learn Spark. At Amsterdam, the training includes introduction topic such as Spark Essentials as well as advanced topics such as data science and hands-on large-scale data exploration with the Wikipedia dataset.\n\nDon\u2019t worry if you cannot make it to Summit! There are many ways to stay up-to-date with the happenings in Amsterdam: <a href=\"https://twitter.com/databricks?lang=en\">Follow us on twitter</a> to get real-time updates throughout the event; <a href=\"http://go.spark-summit.org/eu-2015-live-stream-registration\">register for the live stream</a> to watch the talks at home; or simply <a href=\"http://go.databricks.com/newsletter-registration\">subscribe to our newsletter</a> to get all the content delivered to your inbox.\u00a0We\u2019ll also post the slides and recordings of the talks on the Summit website - stayed tuned.\n\nFinally, remember that there is always another Spark Summit just around the corner - the call for presentations for Spark Summit in New York City (Feb 16-18, 2016) is already open. <a href=\"https://spark-summit.org/east-2016/\">Submit your ideas now</a>!"}
{"status": "publish", "description": null, "creator": "scott", "link": "https://databricks.com/blog/2015/11/06/its-a-wrap-a-lookback-at-spark-summit-in-amsterdam.html", "authors": null, "id": 5478, "categories": ["Company Blog", "Events"], "dates": {"publishedOn": "2015-11-06", "tz": "UTC", "createdOn": "2015-11-06"}, "title": "It's a Wrap! A Lookback at Spark Summit in Amsterdam", "slug": "its-a-wrap-a-lookback-at-spark-summit-in-amsterdam", "content": "<img class=\"wp-image-5479 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/10/spark-summit-eu-keynote-photo.jpg\" alt=\"spark summit eu keynote photo\" width=\"500\" height=\"375\" />\n\nThe Databricks team would like to thank the Apache Spark community and our partners for making the inaugural Spark Summit EU 2015 a great success! The Summit was a sold out event with close to 930 attendees from over 40 countries and 330 companies. Amongst the attendees, over 400 individuals took part in three different training courses in topics ranging from Spark fundamentals to data science.\n\nThe excitement, momentum, and energy was unmistakable. It goes without saying that we are fortunate to have the best developers, engineers, and partners taking this journey with us.\n<h2>Day 1 Highlights</h2>\nWhat a way to kick off Spark Summit EU 2015 - Matei Zaharia, the creator of Spark gave an inspiring keynote on the evolution of Spark over the past year. Picking up on the theme of evolution, the CEO of Databricks - Ion Stoica - shared a vision of Spark becoming the unifying force behind data sources and data processing in the Enterprise. The other keynotes echoed Ion\u2019s sentiment, a notable example is from Goldman Sachs on how Spark is playing an increasingly important role within their organization.\n\nSlides from the keynotes and a sample of the talks from Day 1:\n<ul>\n\t<li>Matei Zaharia, Co-Founder &amp; CTO, Databricks: <a href=\"http://www.slideshare.net/databricks/spark-summit-eu-2015-matei-zaharia-keynote\">How Spark Usage is Evolving in 2015</a></li>\n\t<li>Ion Stoica, Co-Founder &amp; CEO, Databricks: <a href=\"http://www.slideshare.net/databricks/spark-summit-eu-2015-revolutionizing-big-data-in-the-enterprise-with-spark\">Revolutionizing Big Data in the Enterprise with Spark</a></li>\n\t<li>Hossein Falaki, Software Engineer, Databricks: <a href=\"https://vimeo.com/143883002\">Sentiment Analysis Demo</a> (and <a href=\"https://databricks.com/resources/selected-notebooks\">Notebooks</a>)</li>\n\t<li>Anjul Bhambhri, Vice President of Big Data, IBM: <a href=\"http://www.slideshare.net/SparkSummit/apache-spark-the-analytics-operating-system-by-anjul-bhambhri\">Apache Spark: The Analytics Operating System</a></li>\n\t<li>Vincent Saulys, Vice President, Goldman Sachs: <a href=\"http://www.slideshare.net/SparkSummit/how-spark-is-making-an-impact-at-goldman-sachs-by-vincent-saulys\">How Spark is Making an Impact at Goldman Sachs</a></li>\n\t<li>Michael Armbrust, Software Engineer, Databricks: <a href=\"http://www.slideshare.net/databricks/spark-summit-eu-2015-spark-dataframes-simple-and-fast-analysis-of-structured-data\">Spark DataFrames: Simple and Fast Analysis of Structured Data</a> (code referred to in his slides are hosted <a href=\"https://gist.github.com/marmbrus/50da3d78eaa1acbd7bee\">here</a>)</li>\n\t<li>Aaron Davidson, Software Engineer, Databricks: <a href=\"http://www.slideshare.net/databricks/spark-summit-eu-2015-lessons-from-300-production-users\">Spark in Production: Lessons From 100+ Production Users</a></li>\n</ul>\n<h2>Day 2 Highlights</h2>\nOn the second day, keynote speakers dove into the different facets of the diverse Spark ecosystem. Day 2\u2019s sessions further reinforced the momentum and commitment of the growing community.\n\nSlides from the keynotes and a sample of the talks from Day 2:\n<ul>\n\t<li>Martin Odersky, Chief Architect, Typesafe: <a href=\"http://www.slideshare.net/SparkSummit/spark-the-ultimate-scala-collections-by-martin-odersky\">Spark\u2014The Ultimate Scala Collections</a></li>\n\t<li>Alan Saldich, Vice President of Marketing, Cloudera: <a href=\"http://www.slideshare.net/SparkSummit/spark-in-the-enterprise-2-years-later-by-alan-saldich\">Spark in the Enterprise: Two Years Later</a></li>\n\t<li>Reynold Xin, Co-Founder, Databricks: <a href=\"http://www.slideshare.net/databricks/spark-summit-eu-2015-reynold-xin-keynote\">A Look Ahead at Spark's Development</a></li>\n\t<li>Patrick McFadin, Chief Evangelist, DataStax: <a href=\"http://www.slideshare.net/SparkSummit/spark-and-cassandra-an-amazing-apache-love-story-by-patrick-mcfadin\">Spark and Cassandra: An Amazing Apache Love Story</a></li>\n\t<li>Arun Murthy, Founder, Hortonworks: <a href=\"http://www.slideshare.net/SparkSummit/spark-and-hadoop-perfect-togeher-by-arun-murthy\">Spark and Hadoop Perfect Together</a></li>\n\t<li>Andrew Or, Software Engineer, Databricks: <a href=\"http://www.slideshare.net/databricks/spark-summit-eu-2015-sparkui-visualization-a-lens-into-your-application\">SparkUI Visualization: A Lens into Your Application</a> (<a href=\"https://databricks.com/wp-content/uploads/2015/10/1_Movie_recommendation.html\">Notebook #1</a>, <a href=\"https://databricks.com/wp-content/uploads/2015/10/2_Wikipedia_analysis.html\">Notebook #2</a>, <a href=\"https://databricks.com/wp-content/uploads/2015/10/3_Streaming_word_count.html\">Notebook #3</a>)</li>\n\t<li>Hossein Falaki, Software Engineer, Databricks: <a href=\"http://www.slideshare.net/databricks/enabling-exploratory-data-science-with-spark-and-r\">Enabling Exploratory Data Science with Spark and R</a> (<a href=\"https://databricks.com/wp-content/uploads/2015/10/Wikipedia-notebook.html\">Notebook</a>)</li>\n\t<li>Joseph Bradley, Software Engineer, Databricks: <a href=\"http://www.slideshare.net/databricks/spark-summit-europe-2015-combining-the-strengths-of-mllib-scikitlearn-and-r\">Combining the Strengths of MLlib, scikit-learn, and R</a> (<a href=\"https://databricks.com/wp-content/uploads/2015/10/spark-summit-eu-joseph-demo_-_1.html\">Notebook #1</a>, <a href=\"https://databricks.com/wp-content/uploads/2015/10/spark-summit-eu-joseph-demo_-_2.html\">Notebook #2</a>, <a href=\"https://databricks.com/wp-content/uploads/2015/10/spark-summit-eu-joseph-demo_-_3.html\">Notebook #3</a>, <a href=\"https://databricks.com/wp-content/uploads/2015/10/spark-summit-eu-joseph-demo_-_4.html\">Notebook #4</a>)</li>\n</ul>\nThe video and slides from the event has been posted on the <a href=\"https://spark-summit.org/eu-2015/\">Spark Summit website</a>. Presentations from Databricks speakers are also available <a href=\"https://databricks.com/resources/slides\">here</a>.\n\nThe next Summit is already rapidly approaching \u2013 the call for presentations for Spark Summit in New York City (Feb 16-18, 2016) is already open. <a href=\"https://spark-summit.org/east-2016/\">Submit your ideas now</a>!"}
{"status": "publish", "description": "We are excited to introduce TFOCS for Spark, an optimization package based on the state of the art TFOCS (Templates for First-Order Conic Solvers) package for Matlab.", "creator": "dave_wang", "link": "https://databricks.com/blog/2015/11/02/announcing-the-spark-tfocs-optimization-package.html", "authors": null, "id": 5495, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2015-11-02", "tz": "UTC", "createdOn": "2015-11-02"}, "title": "Announcing the TFOCS for Spark Optimization Package", "slug": "announcing-the-spark-tfocs-optimization-package", "content": "<i>Aaron is the developer of this Apache Spark package, with support from Databricks. Aaron is a freelance software developer with experience in data infrastructure and analytics.</i>\n\n<hr />\n\nClick-through prediction, shipping logistics, and housing market analysis are just a few of the many applications of <i>mathematical optimization</i>. While the mathematical models for these applications come in a variety of flavors, a common optimization algorithm can often be leveraged under the hood.\n\nWith this in mind we are excited to introduce <a href=\"https://github.com/databricks/spark-tfocs\" target=\"_blank\">TFOCS for Spark</a>, a community port of\u00a0the state of the art <a href=\"http://cvxr.com/tfocs/\">TFOCS</a> (Templates for First-Order Conic Solvers) package for Matlab. Whereas Matlab TFOCS operates on data within a single machine\u2019s memory, our Apache Spark port leverages the Apache Spark cluster computing framework to scale to larger problems exceeding the memory capacity of a single machine. This package is a general purpose optimization package for constructing and solving mathematical objective functions. It can be used to train many types of machine learning models and to solve other optimization problems including linear programs, often with little or no configuration required.\n\nThe name \u201cTFOCS\u201d is being used with permission from the original TFOCS developers, who are not involved in the development of this package and hence not responsible for the support. To report issues or request features about TFOCS for Spark, please use our <a href=\"https://github.com/databricks/spark-tfocs/issues\" target=\"_blank\">GitHub issues\u00a0</a>page.\n\nThis blog post presents a technical overview of TFOCS for Spark along with usage examples. While some of the details may not be for the faint of heart, we describe them because TFOCS for Spark will eventually underpin higher level applications accessible to all Spark users. We will also discuss further details of TFOCS for Spark at an upcoming <a href=\"http://www.meetup.com/spark-users/events/226016430/\">Bay Area Spark Meetup</a>. RSVP now!\n<h2>TFOCS</h2>\nTFOCS is a state of the art numeric solver; formally, a first order <a href=\"https://en.wikipedia.org/wiki/Convex_optimization\">convex solver</a>. This means that it optimizes functions that have a global minimum without additional local minima, and that it operates by evaluating an objective function, and the gradient of that objective function, at a series of probe points. The key optimization algorithm implemented in TFOCS is <a href=\"http://dx.doi.org/10.1007/s10107-004-0552-5\">Nesterov\u2019s accelerated gradient descent method</a>, an extension of the familiar gradient descent algorithm. In traditional gradient descent, optimization is performed by moving \u201cdownhill\u201d along a function gradient from one probe point to the next, iteration after iteration. The accelerated gradient descent algorithm tracks a linear combination of prior probe points, rather than only the most recent point, using a clever technique that greatly improves asymptotic performance.\n\nTFOCS fine tunes the accelerated gradient descent algorithm in several ways to ensure good performance in practice, often with minimal configuration. For example TFOCS supports <a href=\"https://en.wikipedia.org/wiki/Backtracking_line_search\">backtracking line search</a>. Using this technique, the optimizer analyzes the rate of change of an objective function and dynamically adjusts the step size when descending along its gradient. As a result, no explicit step size needs to be provided by the user when running TFOCS.\n\nMatlab TFOCS contains an extensive feature set. While the initial version of TFOCS for Spark implements only a subset of the many possible features, it contains <a href=\"https://github.com/databricks/spark-tfocs#spark-tfocs\">sufficient functionality</a> to solve several interesting problems.\n<h2>Example: LASSO Regression</h2>\nA <a href=\"https://en.wikipedia.org/wiki/Least_squares#Lasso_method\">LASSO linear regression</a> problem (otherwise known as L1 regularized least squares regression) can be described and solved easily using TFOCS. Objective functions are provided to TFOCS in three separate parts, which are together referred to as a composite objective function. The complete LASSO objective function can be represented as:\n\n<img class=\"aligncenter wp-image-5496 size-full\" src=\"https://databricks.com/wp-content/uploads/2015/11/tfocs-img1.png\" alt=\"tfocs-img1\" width=\"222\" height=\"51\" />\n\nThis function is provided to TFOCS in three parts. The first part, the linear component, implements matrix multiplication:\n\n<img class=\"aligncenter wp-image-5497 size-full\" src=\"https://databricks.com/wp-content/uploads/2015/11/tfocs-img2.png\" alt=\"tfocs-img2\" width=\"31\" height=\"18\" />\n\nThe next part, the smooth component, implements quadratic loss:\n\n<img class=\"aligncenter wp-image-5498 size-full\" src=\"https://databricks.com/wp-content/uploads/2015/11/tfocs-img3.png\" alt=\"tfocs-img3\" width=\"105\" height=\"51\" />\n\nAnd the final part, the nonsmooth component, implements L1 regularization:\n\n<img class=\"aligncenter wp-image-5499 size-full\" src=\"https://databricks.com/wp-content/uploads/2015/11/tfocs-img4.png\" alt=\"tfocs-img4\" width=\"64\" height=\"27\" />\n\nThe TFOCS optimizer is specifically implemented to leverage this separation of a composite objective function into component parts. For example, the optimizer may evaluate the (expensive) linear component and cache the result for later use.\n\nConcretely, in TFOCS for Spark the above LASSO regression problem can be solved as follows:\n<pre>TFOCS.optimize(new SmoothQuad(b), new LinopMatrix(A), new ProxL1(lambda), x0)</pre>\nHere, `<code>SmoothQuad</code>` is the quadratic loss smooth component, `<code>LinopMatrix</code>` is the matrix multiplication linear component, and `<code>ProxL1</code>` is the L1 norm nonsmooth component. The `<code>x0</code>` variable is an initial starting point for gradient descent. TFOCS for Spark also provides a helper function for solving LASSO problems, which can be called as follows:\n<pre>SolverL1RLS.run(A, b, lambda)</pre>\nA complete LASSO example is presented <a href=\"https://github.com/databricks/spark-tfocs#lasso-example\">here</a>.\n<h2>Example: Linear Programming</h2>\nSolving a <a href=\"https://en.wikipedia.org/wiki/Linear_programming\">linear programming</a> problem requires minimizing a linear objective function subject to a set of linear constraints. TFOCS supports solving smoothed linear programs, which include an approximation term that simplifies finding a solution. Smoothed linear programs can be represented as:\n\n<img class=\"aligncenter wp-image-5500 size-full\" src=\"https://databricks.com/wp-content/uploads/2015/11/tfocs-img5.png\" alt=\"tfocs-img5\" width=\"561\" height=\"51\" />\n\nA smoothed linear program can be solved in TFOCS for Spark using a helper function as follows:\n<pre>SolverSLP.run(c, A, b, mu)</pre>\nA complete Linear Programming example is presented <a href=\"https://github.com/databricks/spark-tfocs#linear-program-example\">here</a>.\n<h2>Implementation</h2>\nThe <a href=\"https://github.com/databricks/spark-tfocs#software-architecture-overview\">implementation</a> of TFOCS for Spark closely follows that of the Matlab TFOCS package. We\u2019ve ensured that all of the functionality in TFOCS for Spark can be easily traced back to the corresponding code in the original Matlab TFOCS implementation.\n\nTFOCS for Spark includes a general purpose optimizer with an implementation that is independent of whatever distributed data storage format is employed during optimization. This means that the same optimizer code runs whether operating on a row matrix or a block matrix, for example. Once some basic operations are implemented for a new storage format, the optimizer \u201cjust works\u201d for data in that format.\n<h2>What\u2019s next?</h2>\nWe\u2019ve implemented some key features in the initial version of TFOCS for Spark, but there\u2019s <a href=\"https://github.com/databricks/spark-tfocs#todos\">more work</a> to be done. We would love to see support for additional data storage formats (for example block matrix storage), sparse vectors, and new objective functions. Contributions from the open source community are both welcome and encouraged.\n<h2>Acknowledgements</h2>\nAs the developer of TFOCS for Spark, I would first like to thank Databricks for supporting this project. I am also immensely grateful to Xiangrui Meng and Reza Zadeh for their guidance during the design and development of TFOCS for Spark. I began working with Spark by submitting small patches as a rookie open source contributor, and since then my involvement with Spark has led to both intellectually rewarding projects and new professional opportunities. I encourage any readers with even a passing interest in Spark hacking to pull out their favorite editor and give it a try!"}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2015/11/05/using-databricks-to-transition-from-concept-to-product.html", "authors": null, "id": 5525, "categories": ["Announcements", "Company Blog", "Customers"], "dates": {"publishedOn": "2015-11-05", "tz": "UTC", "createdOn": "2015-11-05"}, "title": "How Yesware is Using Databricks to Transition from Concept to Product", "slug": "using-databricks-to-transition-from-concept-to-product", "content": "This is a guest blog from Justin Mills, Data Team Lead at <a href=\"http://www.yesware.com/\" target=\"_blank\">Yesware</a>. To try out Databricks for your next Apache Spark application,\u00a0<em><a href=\"https://accounts.cloud.databricks.com/registration.html#signup\">sign up for a 14-day free trial today</a>.</em>\n\n<hr />\n\nYesware provides salespeople with data-driven insights to help them understand whether their outreach efforts are working or not. We accomplish this by logging events such as email opens / clicks from our application (which integrates with a sales person\u2019s e-mail application), and associating these events with the activity of the sales person in the form of a report. For example, by using Yesware a sales team can track the open and reply rates on emails sent using different templates. They can then prioritize the use of templates that have a higher success rate and share best practices with the rest of the team. We track a high volume of email data and event data - over 20 million email activities per month and approximately 200 million events associated with those emails. We also integrate with Salesforce and incorporate its rich dataset into our reports. Apache Spark and Databricks give us the development and production environments for connecting all this data together to build new features.\n\nIn this post, I will walk through how we at Yesware used Databricks to take an important new feature in our Premier Tier from idea to production - <i>The Activity versus Engagement Report</i>. We were able to build this new feature report more quickly than previous reports and at a reduced cost using Spark and Databricks. Moreover, we are confident that Spark on Databricks will provide us with the platform to run more complex jobs and process more data as we scale up.\n<h2 style=\"text-align: left;\">Background</h2>\n<img class=\"aligncenter wp-image-5526\" src=\"https://databricks.com/wp-content/uploads/2015/11/yesware-blog-img1-1024x170.png\" alt=\"yesware blog img1\" width=\"700\" height=\"117\" />\n\nOur data pipeline is fairly simple. Databases are archived into S3; Databricks runs Spark jobs to process those files and produce Resilient Distributed Datasets (RDDs) as output data. These RDDs are then written to timestamped tables in a PostgreSQL table. An API layer sits atop this database and provides clients access to the underlying data by running SQL queries to aggregate the data.\n\nMany of our reports support arbitrary date selection, so we typically store data in \"day\" buckets, which means most of these tables have a key that includes user id and day. Each job writes to a new table name. When the job is done writing the data and building indexes, it updates a metadata table to indicate that the most recent version of a data source is ready for querying. It also includes some other attributes, such as the time range represented in the data and when the job was run to indicate how current the data is. This allows us to quickly roll back to a previous version of the data if new data turns out to be incorrect. The rendering of this data in a report is done in a separate application that consumes the service.\n<h2 style=\"text-align: left;\">Building the Activity vs. Engagement Report with Databricks</h2>\n<h3 style=\"text-align: left;\">Concept</h3>\nWe wanted to give salespeople an easy way to see which accounts they should focus their efforts on. Our idea is to show a correlation of sales activity and prospect engagement in a report called \u201cActivity versus Engagement\u201d. Activity would represent things such as emails sent and engagement would be action taken in response to those emails such as clicking on links, replying, and data from Salesforce (see a mock-up of the idea from our designer below).\n<p style=\"text-align: center;\"><img class=\"aligncenter wp-image-5527\" src=\"https://databricks.com/wp-content/uploads/2015/11/yesware-blog-img2-1024x655.png\" alt=\"yesware blog img2\" width=\"500\" height=\"320\" /><i>Activity vs. Engagement report concept mock-up</i></p>\n\n<h3 style=\"text-align: left;\">Prototyping</h3>\nWe began prototyping using Databricks notebooks in conjunction with our custom-built Spark library (the Yesware JAR - where our production code lives). The library includes production-tested methods to build RDD\u2019s out of our data in S3, and the code to build derived datasets that do things like aggregate event data at the email level.\n\nDuring the prototyping phase, Databricks provided a one-stop shop to build prototype code and the tools to visualize the results in a way that we could start to see what it would look like in the application. We built intermediate Spark RDDs and cached them to make re-computations faster. A typical development cycle might look like:\n<ul>\n\t<li>Use Databricks to start a cluster with the Yesware JAR loaded into it.</li>\n\t<li>Run the first N cells of a notebook, some of them caching intermediate RDDs</li>\n\t<li>Tweak values in a cell</li>\n\t<li>Run that cell and any following cell again</li>\n\t<li>Repeat until data visualizations and table data showed the results we were looking for.</li>\n</ul>\n<p style=\"text-align: left;\">Through this process, we were also able to decide what insights from the initial idea we could keep and what we needed to continue to refine. It was an incredibly useful method for one or two developers to move quickly before an entire team jumped in to build out the production-ready feature.</p>\n<p style=\"text-align: left;\">The early prototypes of this concept were centered around binning data by prospect. We took one of our aggregated datasets (an email with summary statistics) and flattened that out into a record per email address. We then aggregated that data based on email address to compute an activity and engagement score. Databricks notebooks allowed us to incrementally build up the data we ultimately want in the Activity versus Engagement Report in its visual environment:</p>\n<img class=\"aligncenter wp-image-5528\" src=\"https://databricks.com/wp-content/uploads/2015/11/yesware-blog-img3.png\" alt=\"yesware blog img3\" width=\"500\" height=\"414\" />\n<p style=\"text-align: center;\"><i>Prototype of the report in Databricks notebook</i></p>\n\n<h3></h3>\n<h3 style=\"text-align: left;\">Testing</h3>\nAfter we had a working prototype in the form of a Databricks notebook, the next step was to properly test the prototype code. Our ultimate goal was to merge the tested code into the Yesware JAR, where the code becomes part of our production pipeline. This was done by the following process:\n<ol>\n\t<li>Creating objects and classes in Scala to contain the logic in the prototype notebook cells.</li>\n\t<li>Copying this logic over to these classes. This process can sometimes be a straight copy of a block of Spark code, and other times means some refactoring of that code.</li>\n\t<li>Writing tests of the new code in the Yesware JAR</li>\n\t<li>Submitting a pull request for code review</li>\n\t<li>Merging and releasing a new version of the Yesware JAR.</li>\n</ol>\nWe try to write our Spark code to take parameters to the best of our ability. These parameters are things such as how many days of input data to use or the target database to write to. This makes step 2 above much simpler as not all parameter values available in a Databricks Spark environment are available locally for test. The final piece of copying the logic into the Yesware JAR is wrapping all of the individual pieces of code into a single entry point. This is done as a scala object that we can call into, passing all the parameters necessary. So where our prototype notebook may have dozens of cells each with one piece of Spark transformations, the production notebook often has one cell that looks something like this:\n\n[scala]\nval options = Map(\n// Options to the job here\n)\n// sparkContext is defined by Databricks and available\n// to all cells in all notebooks.\nval reportRDD = BuildActivityEngagementReport.run(sparkContext, options)\n[/scala]\n\nPart of the process of copying code from the notebook into source code was reworking the entry point to the job so that the Databricks notebook could provide inputs to control the job such as how many days of data to process, the target database to load into, etc. The result is that our prototyping notebook(s) are comprised of many cells each with Spark transformations and business logic, our production notebook looks something like this:\n\n[scala]\n\nval reportRDD = BuildActivityEngagementReport.run(sparkContext, options)\n\n[/scala]\n\nwhere options is a Map of the parameters mentioned above such as database credentials, etc.\n\nAnother benefit of having code live in our shared library is easier automated testing and code reviews. This is a big part of the Yesware culture and we wanted to add our Spark and Databricks code to the mix. To support this, we have a small sample of production data that we use locally to test our Spark code using ScalaTest. Having live production data is useful for new developers to have enough to play with locally to be able to do interesting things and provides more real-world data for automated tests to run against. Being in a Git repository integrates perfectly with our existing development flow of using Github Pull Requests to review any proposed changes. This process also gives us a chance to look over our implementation again to see if there are any other optimizations we could do. Code reviews, for example, led us to realize we were filtering data after an expensive join, and that with some rearranging, we could reduce the volume of data leading into the join and make it faster. Having this formal process in place and tests to back up the validity of such optimizations has helped us immensely.\n<h3 style=\"text-align: left;\">Production</h3>\nOnce the JAR source code was updated and we cut a release, we used the Databricks jobs feature to schedule this job. In the notebook that calls into our custom Spark library (Yesware JAR), we also include some basic visualizations of the data (see figure below). This is extremely useful to ensure that the job is continuing to produce data that looks correct. It also is a unique way to provide higher level visualizations, such as the overall Activity versus Engagement Report for all of our users, or for our \u201ctop performers\u201d across all customers. The job is setup to prefer spot instances to save on cost but fall back to on-demand if needed. Finally, the email notifications are connected to some mailing lists so we can track progress/alerting via existing mechanisms.\n\n<img class=\"aligncenter wp-image-5531\" src=\"https://databricks.com/wp-content/uploads/2015/11/yesware-blog-img4.png\" alt=\"yesware blog img4\" width=\"500\" height=\"490\" />\n<p style=\"text-align: center;\"><i>Output from the Activity vs. Engagement Report job</i></p>\n\n<h3 style=\"text-align: left;\">Final product</h3>\nThe final product is a chart of activity versus engagement that individual salespeople can use to see how their prospecting efforts are compared to the baseline for their team. It is also a tool for sales managers to direct effort and provide guidance as to what is working and what is not working.\n\n<img class=\"aligncenter wp-image-5532\" src=\"https://databricks.com/wp-content/uploads/2015/11/yesware-blog-img5-1024x671.png\" alt=\"yesware blog img5\" width=\"500\" height=\"327\" />\n<p style=\"text-align: center;\"><i>Final product in the Yesware application</i></p>\nThis report joins several other team-based reports that are available to Yesware users and help enable Sales teams to gain more insights into what is working. We will be looking more to Spark and Databricks in the future for building such features as it has become the cornerstone for our reporting features.\n\n<em>To try Databricks, <a href=\"https://accounts.cloud.databricks.com/registration.html#signup\">sign up for a free trial</a>!</em>"}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2015/11/10/succinct-spark-from-amplab-queries-on-compressed-rdds.html", "authors": null, "id": 5552, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2015-11-10", "tz": "UTC", "createdOn": "2015-11-10"}, "title": "Succinct Spark from AMPLab: Queries on Compressed RDDs", "slug": "succinct-spark-from-amplab-queries-on-compressed-rdds", "content": "<i>This is a guest post from Rachit Agarwal and Anurag Khandelwal of the UC Berkeley AMPLab, leads of an ongoing research project called Succinct.</i>\n\n<hr />\n\n<a href=\"http://succinct.cs.berkeley.edu\">Succinct</a> is a distributed data store that supports a wide range of point queries directly on a compressed representation of the input data. The UC Berkeley AMPLab is very excited to release Succinct Spark, as an Apache Spark package, that enables search, count, range and random access queries on compressed RDDs. This release allows users to use Apache Spark as a document store (with search on documents) similar to ElasticSearch, a key-value interface (with search on values) similar to HyperDex, and an experimental DataFrame interface (with search along columns in a table). When used as a document store, Succinct Spark is over 75x faster than native Spark.\n<h2>Succinct Spark Overview</h2>\n<i>Search</i> is becoming an increasingly powerful primitive in big data analytics and web services. Indeed, many web services support some form of search --- <a href=\"http://www.linkedin.com/search\">LinkedIn search</a>, <a href=\"https://twitter.com/search-home\">Twitter search</a>, <a href=\"http://search.fb.com\">Facebook search</a>, Netflix search, airlines, hotels, and services specifically built around search --- Google, Bing, Yelp, to name a few. Apache Spark supports search via full RDD scans. While fast enough for small datasets, data scans become inefficient as dataset become even moderately large. One way to avoid data scans is to implement indexes, but this approach can significantly increase the memory overhead.\n\nThe AMPLab is very excited to announce the release of Succinct Spark, as a Spark package, that achieves a new and unique tradeoff --- storage overhead no worse (and often lower) than data-scan based techniques and query latency comparable to index-based techniques. Succinct <i>enables search (and a wide range of other queries) directly on compressed representation of the RDDs</i>. What differentiates Succinct is that queries are supported without storing any secondary indexes, without data scans and without data decompression \u2014 all the required information is embedded within the compressed representation and queries are executed directly on the compressed representation.\n\nIn addition, Succinct Spark supports random access of records without scanning the entire RDD, a functionality that we believe will significantly speed up a large number of applications.\n<h2>An example:</h2>\nConsider a collection of Wikipedia articles stored on HDFS as a flat unstructured file. Let us see how Succinct Spark supports the above functionalities:\n\n[scala]\n// Import SuccinctRDD\nimport edu.berkeley.cs.succinct._\n\n// Create a Spark RDD as a collection of articles; sc is the SparkContext\nval articlesRDD = ctx.textFile(&quot;/path/to/data&quot;).map(_.getBytes)\n\n// Compress the Spark RDD into a Succinct Spark RDD, and persist it in memory\n// Note that this is a time consuming step (usually at 8GB/hour/core) since data needs to be compressed. (We are actively working on making this step faster.)\nval succinctRDD = articlesRDD.succinct.persist()\n\n// SuccinctRDD supports a set of powerful primitives directly on compressed data. Let us start by counting the number of occurrences of \u201cBerkeley\u201d across all Wikipedia articles\nval count = succinctRDD.count(&quot;Berkeley&quot;)\n\n// Now suppose we want to find all offsets in the collection at which \u201cBerkeley\u201d occurs and create an RDD containing all resulting offsets \nval offsetsRDD = succinctRDD.search(&quot;Berkeley&quot;)\n\n// Let us look at the first ten results in the above RDD\nval offsets = offsetsRDD.take(10)\n\n// Finally, let us extract 20 bytes before and after one of the occurrences of \u201cBerkeley\u201d\nval offset = offsets(0)\nval data = succinctRDD.extract(offset - 20, 40)\n[/scala]\n\nMany more examples on using Succinct Spark are outlined <a href=\"http://succinct.cs.berkeley.edu/wp/wordpress/?page_id=8\">here</a>.\n<h2>Performance</h2>\nThe figure compares the search performance of Succinct Spark against native Spark. We use a 40GB collection of Wikipedia documents over a 4-server Amazon EC2 cluster with 120GB RAM (so that all systems fit in memory). The search queries use words with varying number of occurrences (1--10,000) with uniform random distribution across 10 bins (1--1000, 1000-2000, etc). Note that the y-axis is on log scale. We note that Succinct Spark is over two orders of magnitude faster than Spark\u2019s native RDDs due to avoiding data scans. Random access on documents has similar performance gains (with some caveats).\n\nBelow, we describe a few interesting use cases for Succinct Spark, including a number of interfaces exposed in the release. For more details on the Succinct Spark release (and Succinct in general), usage and benchmark results, please see the <a href=\"https://amplab.cs.berkeley.edu/succinct-spark-queries-on-compressed-rdds/\">AMPLab blog post</a>, <a href=\"http://succinct.cs.berkeley.edu\">Succinct webpage</a>, <a href=\"https://www.usenix.org/system/files/conference/nsdi15/nsdi15-paper-agarwal.pdf\">the NSDI paper</a>, or a more detailed <a href=\"http://www.cs.berkeley.edu/~rachit/succinct-techreport.pdf\">technical report</a>.\n<h2>Succinct Spark abstractions and use cases:</h2>\nSuccinct Spark exposes three interfaces, each of which may have several interesting use cases. We outline some of them below:\n<h4 style=\"text-align: left;\">SuccinctRDD</h4>\n<ul>\n\t<li><i>Interface: </i>Flat (unstructured) files</li>\n\t<li><i>Example application</i>: log analytics</li>\n\t<li><i>Example:</i> one can search across logs (e.g., errors for debugging), or perform random access (e.g., extract logs at certain timestamps).</li>\n\t<li><i>System with similar functionality:</i> Lucene.</li>\n</ul>\n<h4 style=\"text-align: left;\">SuccinctKVRDD</h4>\n<ul>\n\t<li><i>Interface:</i> Semi-structured data</li>\n\t<li><i>Example application: </i>document stores, key-value stores</li>\n\t<li><i>Example: </i>\n<ul>\n\t<li>(document stores) search across a collection of Wikipedia documents and return all documents that contain, say, string \u201cUniversity of California at Berkeley\u201d. Extract all (or a part of) documents.</li>\n\t<li>(key-value stores) search across a set of tweets stored in a key-value store for tweets that contain \u201cSuccinct\u201d. Extract all tweets from the user \u201c_ragarwal_\u201d.</li>\n</ul>\n</li>\n\t<li><i>System with similar functionality:</i> ElasticSearch</li>\n</ul>\n<h4 style=\"text-align: left;\">(An experimental) DataFrame interface</h4>\n<ul>\n\t<li><i>Interface:</i> Search and random access on structured data like tables</li>\n\t<li><i>Example applications: </i>point queries on columnar stores</li>\n\t<li><i>Example:</i> given a table with schema {userID, location, date-of-birth, salary, ..}, find all users who were born between 1980 and 1985.</li>\n\t<li><i>Caveat:</i> We are currently working on some very exciting projects to support a number of additional SQL operators efficiently directly on compressed RDDs.</li>\n</ul>\n<h2>When not to use Succinct Spark</h2>\nThere are a few applications that are not suitable for Succinct Spark --- long sequential reads, and search for strings that occur very frequently (you may not want to search for \u201ca\u201d or \u201cthe\u201d). We outline the associated tradeoffs on Succinct webpage as well.\n<h2>Looking ahead</h2>\nWe at AMPLab are working on several interesting projects to make Succinct Spark more memory efficient, faster and more expressive. To give you an idea about what is next, we are going to close this post with a hint on our next post: executing Regular Expression queries directly on compressed RDDs. Stay tuned!"}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2015/11/11/elsevier-spark-use-cases-and-contribution-to-apache-spark-packages.html", "authors": null, "id": 5572, "categories": ["Company Blog", "Customers"], "dates": {"publishedOn": "2015-11-11", "tz": "UTC", "createdOn": "2015-11-11"}, "title": "Elsevier Spark Use Cases with Databricks and Contribution to Apache Spark Packages", "slug": "elsevier-spark-use-cases-and-contribution-to-apache-spark-packages", "content": "This is a guest blog from Darin McBeath, Disruptive Technology Director at Elsevier.\u00a0To try out Databricks for your next Apache Spark application,\u00a0<em><a href=\"https://accounts.cloud.databricks.com/registration.html#signup\">sign up for a 14-day free trial today</a>.</em>\n\n<hr />\n\nElsevier is a provider of scientific, technical, and medical information products and services. Elsevier Labs is an advanced technology R&amp;D group within Elsevier. Members of Labs research and create new technologies, help implement proofs of concept, educate Elsevier staff and management, and represent the company in technical discussions.\n\nIn this blog, we will talk about how we utilize Databricks to build Apache Spark applications, and introduce our first publicly released Spark package - spark-xml-utils.\n<h2>Elsevier Labs and Apache Spark on Databricks</h2>\nWithin Elsevier Labs, we have been investigating Apache Spark since early 2014. We accelerated our pace of adoption since we implemented Databricks at the start of 2015.\n\nSome of our use cases with Apache Spark on Databricks:\n<ul>\n\t<li>Usage analysis for one of our web properties using the Databricks REST API.</li>\n\t<li>Text mining with popular tools such as GENIA and Stanford Core NLP against a large portion of our content.</li>\n\t<li>Feature extraction using MLlib.</li>\n\t<li>Extrapolation of author graphs, affiliation graphs, and cited by graphs from our content with GraphX.</li>\n\t<li>Ad hoc analysis using Spark SQL notebooks.</li>\n</ul>\nWe use Databricks to develop standalone Java applications as well as notebooks in Scala and Python. We also use the Databricks job scheduler to automatically schedule daily and weekly jobs.\n<h2>Introducing spark-xml-utils</h2>\nWe have a lot of content in fairly complex XML (highly structured, nested, and with numerous namespaces). While the mere mention of XML can cause much angst and gnashing of teeth, the fact of the matter is there still exists a lot of XML in big data sets. Case in point, Elsevier has tens of millions of complex XML files for journal articles, book chapters, and other information. This surfaced the need for easy to use, powerful, and flexible tools for processing many XML files in parallel.\n\nThe spark-xml-utils library was developed to provide some helpful XML utilities:\n<ul>\n\t<li>Filter documents based on an XPath expression</li>\n\t<li>Return specific nodes for an XPath/XQuery expression</li>\n\t<li>Transform documents using a XSLT stylesheet</li>\n</ul>\nBy providing some basic wrappers to <a href=\"http://www.saxonica.com\">Saxon</a>, the spark-xml-utils library exposes basic XPath, XQuery, and XSLT functionality that can readily be leveraged by a Spark application.\n<h2>Using spark-xml-utils</h2>\nSpark-xml-utils provides access to three common XML tools: \u00a0XPath, XQuery, and XSLT. \u00a0This allows a developer to select the most appropriate or most familiar tool for the task at hand.\n\nA recurring pattern in the example below is the use of mapPartitions. \u00a0This allows us to initialize the processors for XPath, XQuery, and XSLT once per partition for optimal performance. \u00a0We then use an iterator to process each record in the partition.\n\nThe sequence file used in the examples below (a representative and small example for one of these) is publicly available in s3://spark-xml-utils/xml. \u00a0In this sequence file, the key is a unique identifier for the record and the value is the XML (as a string). A sample of a complete XML record is available from (<a href=\"http://tinyurl.com/pxqprfj\">http://tinyurl.com/pxqprfj</a>).\n<h3>XPath</h3>\nThe XPathProcessor class defined in spark-xml-utils provides methods that enable processing of XPath expressions (filter/evaluate) against an XML string.\n\nThe result of a <b>filter operation</b> will be a boolean TRUE/FALSE. As an example, assume we want to subset our content to only \u2018journal\u2019 articles with a published date between 2012 and 2015. \u00a0The code below will filter the records, keeping the journal articles (content-type=\u2019JL\u2019) published after 2012 but before 2015.\n\n<pre>val xmlKeyPair = sc.sequenceFile[String, String](\"s3n://spark-xml-utils/xml/part*\")\nval filtered = xmlKeyPair.mapPartitions(recsIter => {\n                     val xpath = \"/xocs:doc[xocs:meta[xocs:content-type='JL'\n                                     \tand xocs:cover-date-year > xs:int(2012)\n                                     \tand xocs:cover-date-year < xs:int(2015)]]\" val namespaces = new HashMap[String,String](Map( \"xocs\" -> \"http://www.elsevier.com/xml/xocs/dtd\"\n                                          ).asJava)\n                    val proc = XPathProcessor.getInstance(xpath,namespaces)\n                    recsIter.filter(rec => proc.filterString(rec._2))\n                  })</pre>\n\nOutput:\n<pre>Unfiltered Count 110\nFiltered Count 87</pre>\nLike the filter operation, the <b>evaluation operation</b> applies an XPath expression against an XML string. \u00a0But instead of a boolean, its result will be the string result of the XPath expression. For example, assume we want to extract the journal name for every record. \u00a0The following example returns the srctitle (i.e. journal name) for each record.\n\n<pre>val xmlKeyPair = sc.sequenceFile[String, String](\"s3n://spark-xml-utils/xml/part*\")\nval srctitles = xmlKeyPair.mapPartitions(recsIter => {\n                  \tval xpath = \"/xocs:doc/xocs:meta/xocs:srctitle/text()\"\n                \tval namespaces = new HashMap[String,String](Map(\n                                               \"xocs\" -> \"http://www.elsevier.com/xml/xocs/dtd\"\n                                           ).asJava)\n                  \tval proc = XPathProcessor.getInstance(xpath,namespaces)\n                  \trecsIter.map(rec => proc.evaluateString(rec._2))\n                \t})</pre>\n\nOutput:\n<pre>Advances in Mathematics\nAdvances in Mathematics\nAdvances in Mathematics\nBiological Psychiatry\nEarth and Planetary Science Letters\nEarth and Planetary Science Letters\nEarth and Planetary Science Letters\nFuel\nIcarus\nIcarus\n...\n</pre>\nWhile the examples above all included namespaces (because our XML is fairly rich and complex) if the content did not have namespaces, the code could be simplified to the following. \u00a0\u00a0Namespaces are also not needed if wildcards are used (instead of namespace prefixes) in an expression. \u00a0For example: \"<code>/*:doc/*:meta/*:</code>srctitle<code>/text()</code>\".\n\n<pre>val xmlKeyPair = sc.sequenceFile[String, String](\"s3n://spark-xml-utils/xml/part*\")\nval srctitles = xmlKeyPair.mapPartitions(recsIter => {\n                  \tval xpath = \"/doc/meta/srctitle/text()\"\n\t                val proc = XPathProcessor.getInstance(xpath)\n                  \trecsIter.map(rec => proc.evaluateString(rec._2))\n                    })</pre>\n\n<h3>XQuery</h3>\nThe XQueryProcessor class defined in spark-xml-utils provides a method that enables processing of XQuery evaluate expressions against an XML string (similar to the XPath evaluation).\n\nAn evaluation operation can do more than just return raw results; we can also return multiple fields and do some basic formatting. \u00a0Assume we want to return a JSON record for the journal name and the publication year. \u00a0The following example returns a JSON record containing the journal name and the publication year for each record.\n\n<pre>val xmlKeyPair = sc.sequenceFile[String, String](\"s3n://spark-xml-utils/xml/part*\")\nval srcyearJson = xmlKeyPair.mapPartitions(recsIter => {\n                    \tval xquery = \"for $x in /xocs:doc/xocs:meta return \" +\n                      \t\"string-join(('{ \\\"srctitle\\\" :\\\"',$x/xocs:srctitle, '\\\",\\\"year\\\":',$x/xocs:cover-date-year,'}'),'')\"\n                    \tval namespaces = new HashMap[String,String](Map(\n                         \t                    \"xocs\" -> \"http://www.elsevier.com/xml/xocs/dtd\"\n                                             ).asJava)\n                    \tval proc = XQueryProcessor.getInstance(xquery,namespaces)\n                    \trecsIter.map(rec => proc.evaluateString(rec._2))\n                  \t})</pre>\n\nOutput\n<pre>{ \"srctitle\" :\"Advances in Mathematics\",\"year\":2012}\n{ \"srctitle\" :\"Advances in Mathematics\",\"year\":2013}\n{ \"srctitle\" :\"Advances in Mathematics\",\"year\":2014}\n{ \"srctitle\" :\"Biological Psychiatry\",\"year\":2012}\n{ \"srctitle\" :\"Earth and Planetary Science Letters\",\"year\":2012}\n{ \"srctitle\" :\"Earth and Planetary Science Letters\",\"year\":2013}\n{ \"srctitle\" :\"Earth and Planetary Science Letters\",\"year\":2014}\n{ \"srctitle\" :\"Fuel\",\"year\":2014}\n{ \"srctitle\" :\"Icarus\",\"year\":2012}\n{ \"srctitle\" :\"Icarus\",\"year\":2012}\n...</pre>\n<h3>XSLT</h3>\nThe XSLTProcessor class defined in spark-xml-utils provides a method that transforms an XML record by applying an XSLT stylesheet obtained from an S3 bucket (or passed as a string). \u00a0\u00a0While some basic transformation could be done with XPath (or XQuery), XSLT is the tool of choice for complex transformations.\n\nAssume we want to return a JSON record for the journal title. \u00a0The following example returns a JSON record containing the journal name for each record.\n\n<pre>val xmlKeyPair = sc.sequenceFile[String, String](\"s3n://spark-xml-utils/xml/part*\")\nval stylesheet = sc.textFile(\"s3n://spark-xml-utils/stylesheets/srctitle.xsl\").collect.head\n \nval srctitles = xmlKeyPair.mapPartitions(recsIter => {\n  val proc = XSLTProcessor.getInstance(stylesheet)\n  recsIter.map(rec => proc.transform(rec._2))\n})</pre>\n\nThe stylesheet used in the above transformation is listed below.\n\n<pre><?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<xsl:stylesheet version=\"2.0\"\n                xmlns:xocs=\"http://www.elsevier.com/xml/xocs/dtd\"\n                xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\">\n  <xsl:output method=\"text\" encoding=\"utf-8\" indent=\"yes\"/>\n  <xsl:template match=\"/xocs:doc/xocs:meta\">\n    <xsl:text>{</xsl:text>\n    <xsl:text>'srctitle':'</xsl:text>\n    <xsl:value-of select=\"./xocs:srctitle/text()\"/>\n    <xsl:text>'</xsl:text>\n    <xsl:text>}</xsl:text>\n  </xsl:template>\n  <xsl:template match=\"text()\"/>\n</xsl:stylesheet></pre>\n\nOutput:\n<pre>{ 'srctitle':'Advances in Mathematics' }\n{ 'srctitle':'Advances in Mathematics' }\n{ 'srctitle':'Advances in Mathematics' }\n{ 'srctitle':'Biological Psychiatry' }\n{ 'srctitle':'Earth and Planetary Science Letters' }\n{ 'srctitle':'Earth and Planetary Science Letters' }\n{ 'srctitle':'Earth and Planetary Science Letters' }\n{ 'srctitle':'Fuel' }\n{ 'srctitle':'Icarus' }\n{ 'srctitle':'Icarus' }\n...</pre>\nMuch more complex scenarios are also possible (e.g., filter documents where the record is of type \u2018journal\u2019, the stage is \u2018S300\u2019, the publication year is &gt; 2010 and &lt; 2014, the abstract contains \u2018heart\u2019 or \u2018brain\u2019 or \u2018body\u2019 or \u2018number\u2019 and the section contains \u2018red\u2019 or \u2018black\u2019). To see more complex examples of XPath expression at work, check out the <a href=\"https://github.com/elsevierlabs-os/spark-xml-utils\">spark-xml-utils Github site</a>.\n<h2>Conclusion</h2>\nWe are just scratching the surface for what we would like to provide with spark-xml-utils and what is possible. \u00a0Within Labs, we have been using it for over 9 months and have had great success. \u00a0If you are interested in processing XML, I encourage you to install the spark-xml-utils package, play around with the examples (the data and stylesheet is publicly available), or better yet, use your own data. We are certainly receptive to feedback and additions by the community.\n\nThe spark-xml-utils package can be found on the <a href=\"http://spark-packages.org/package/elsevierlabs-os/spark-xml-utils\">Spark Packages site</a> and <a href=\"https://github.com/elsevierlabs-os/spark-xml-utils\">Github</a>.\n\n&nbsp;"}
{"status": "publish", "description": null, "creator": "kavitha", "link": "https://databricks.com/blog/2015/11/12/elsevier-labs-deploys-databricks-for-unified-content-analysis.html", "authors": null, "id": 5613, "categories": ["Announcements", "Company Blog", "Customers"], "dates": {"publishedOn": "2015-11-12", "tz": "UTC", "createdOn": "2015-11-12"}, "title": "Elsevier Labs Deploys Databricks for Unified Content Analysis", "slug": "elsevier-labs-deploys-databricks-for-unified-content-analysis", "content": "We are happy to announce that Elsevier Labs has deployed Databricks as its unified content analysis platform, providing significant productivity gains for the entire team and reducing typical project lengths from weeks to just days.\n\nElsevier Labs is the advanced R&amp;D group within Elsevier - a global provider of scientific information, publishing over 2,500 journals and 33,000 book titles while building web-based information solutions for professionals in science, technology, and medicine.\n\nThey needed a fast and scalable analytics platform to develop new methods to extract insights from the published content. Their development process frequently required the application of complex natural language processing (NLP) algorithms to millions of articles and interpretation of the results. Prior to Databricks, Elsevier Labs\u2019 productivity was severely hampered because:\n<ul>\n\t<li>There was substantial manual data movement during the analytics workflow</li>\n\t<li>The steep learning curve of their legacy analytics platform prevented code reuse</li>\n\t<li>Presenting findings required significant additional time to build reports and UIs</li>\n</ul>\nDatabricks enabled Elsevier Labs to effortlessly manage Apache Spark clusters, access their data, collaboratively develop cutting-edge algorithms, and present their findings in a single platform. With the Databricks integrated workspace, the Elsevier Labs team was able to:\n<ul>\n\t<li>Create, scale, and terminate Spark clusters without specialists with big data DevOps expertise.</li>\n\t<li>Directly access data in S3 buckets and collaboratively perform analysis in a notebook environment, using Python, Scala, SQL, or even R.</li>\n\t<li>Present findings to senior management and share results across the entire organization with account-based access control of notebooks.</li>\n</ul>\nAs a result of deploying Databricks, Elsevier Labs enabled five times more people to contribute to content analysis algorithm development, growing the number of contributors from 3 to 15. Moreover, the people who use Databricks are significantly more productive, reducing typical project lengths from weeks to just days.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/03/Case-Study-Elsevier.pdf\" target=\"_blank\">Download this case study</a> to learn more about how Elsevier is using Databricks.\n\nTo try out Databricks for yourself, <a href=\"https://accounts.cloud.databricks.com/registration.html#signup\">sign-up for a 14-day free trial</a> today!"}
{"status": "publish", "description": null, "creator": "denny", "link": "https://databricks.com/blog/2015/11/19/databricks-launches-meetup-in-a-box-for-apache-spark-meetup-organizers.html", "authors": null, "id": 5511, "categories": ["Company Blog", "Events"], "dates": {"publishedOn": "2015-11-19", "tz": "UTC", "createdOn": "2015-11-19"}, "title": "Databricks launches Meetup-in-a-box for Apache Spark Meetup Organizers", "slug": "databricks-launches-meetup-in-a-box-for-apache-spark-meetup-organizers", "content": "One of the most important reasons for the growth of Apache Spark is the amazing grassroots community interest and support to share, teach, and learn with each other. \u00a0At the heart of the Spark community movement are the Spark Meetup organizers who continue to dedicate their time, resources, and effort. This core group of individuals help evangelize and nurture Spark knowledge sharing, often helping members get projects off the ground. As of November 17th, 2015, the Spark meetup community has rapidly grown to more than 117 groups globally with 46,680 members.\n\n<img class=\"alignnone wp-image-5650 size-large\" src=\"https://databricks.com/wp-content/uploads/2015/11/spark-meetups-20151117-1024x580.png\" alt=\"\" width=\"1024\" height=\"580\" />\n\nRunning a meetup can be an exhausting endeavor from finding the right venue, getting sponsors for those venues, securing speakers to present relevant and up to date information, not to mention promoting the event and receiving sponsorships to cover incidental costs associated with the meetup. \u00a0While a Spark meetup is a local event, Spark Meetup organizers can all help each other out globally and leverage each other\u2019s expertise and experiences.\n\nIf you are running (or looking to run) a Spark meetup, we would like to provide these resources - which we are calling a <b>Meetup-in-a-box</b>. The virtual components of this box include a Spark Meetup Organizers Google Group, Apache Spark Meetup manual, Spark Jeopardy game, Databricks Trial Accounts, and (coming soon) Spark Remote Presentation Session and Remote Q&amp;A sessions. The physical components of this box include books, t-shirts, and prizes; more information on this will be available shortly. \u00a0\u00a0As we add more components to Meetup-in-a-box, you can keep up-to-date at <a href=\"https://sparkhub.databricks.com/spark-meetup-organizer-resources/\" target=\"_blank\">Meetup-in-a-box for Spark Meetup Organizers</a>.\n\n&nbsp;\n<h3><b>Spark Meetup Organizers Google Group</b></h3>\nAs Spark Meetup Organizers and part of the Spark community, we encourage you - the Spark Meetup Organizer and/or coordinator - to join <a href=\"https://groups.google.com/forum/#!forum/spark-meetup-organizers\" target=\"_blank\">Spark Meetup Organizers Google Group</a> as a central way for us to communicate and help each other.\n\n&nbsp;\n<h3><b>Apache Spark Meetup Manual</b></h3>\nThe <a href=\"https://docs.google.com/document/d/1QLw5cRtSjONS3PAMtei_t8851qdWKw5WZsFmHDCafxA/edit#heading=h.v5g3meb9mgzb\" target=\"_blank\">Apache Spark Meetup Manual</a> is a living document to provide guidance and a helping hand to help run Spark Meetups. \u00a0The manual was the brainchild of Mehrdad Pazooki who leads the <a href=\"http://www.meetup.com/Toronto-Apache-Spark\" target=\"_blank\">Toronto Apache Spark Meetup</a> group. \u00a0We invite all of you to review and add comments so we can continue to share how to run meetups more effectively. \u00a0To <em>view</em> the\u00a0<a href=\"https://docs.google.com/document/d/1QLw5cRtSjONS3PAMtei_t8851qdWKw5WZsFmHDCafxA/edit#heading=h.v5g3meb9mgzb\" target=\"_blank\">Apache Spark Meetup Manual</a>, please click on the link or the image below, to <em>edit </em>or<em> comment</em> the document, please join the\u00a0<a href=\"https://groups.google.com/forum/#!forum/spark-meetup-organizers\" target=\"_blank\">Spark Meetup Organizers Google Group</a>\u00a0and email\u00a0<a href=\"mailto:sparkhub@databricks.com\">sparkhub@databricks.com</a>\n\n&nbsp;\n\n<a href=\"https://docs.google.com/document/d/1QLw5cRtSjONS3PAMtei_t8851qdWKw5WZsFmHDCafxA/edit#heading=h.v5g3meb9mgzb\" target=\"_blank\"><img class=\"aligncenter wp-image-5515\" style=\"border: solid 1px black;\" src=\"https://databricks.com/wp-content/uploads/2015/11/apache_spark_meetup_manual.png\" alt=\"apache_spark_meetup_manual\" width=\"501\" height=\"200\" /></a>\n\n&nbsp;\n<h3><b>Spark Jeopardy Game</b></h3>\nCreated by Donna Fernandez of the <a href=\"http://www.meetup.com/Washington-DC-Area-Spark-Interactive/\" target=\"_blank\">Washington DC Area Apache Spark Interactive</a>, this is a fun game based on the game Jeopardy built on top of Apache Spark questions. \u00a0As a Spark meetup organizer, you can run the game for your own event and we can potentially provide some prizes for the event! \u00a0Please email <a href=\"mailto:sparkhub@databricks.com\">sparkhub@databricks.com</a>\u00a0for edit access to this game. \u00a0To play this game yourself, please click on\u00a0<a href=\"http://flipquiz.me/review/32224\" target=\"_blank\">Spark Jeopardy Game</a> or the image below.\n\n<a href=\"http://flipquiz.me/review/32224\" target=\"_blank\"><img class=\"aligncenter wp-image-5518\" src=\"https://databricks.com/wp-content/uploads/2015/11/spark-jeopardy-1024x439.png\" alt=\"spark jeopardy\" width=\"800\" height=\"343\" /></a>\n\nSome quick notes when playing this game:\n<ul>\n\t<li>If the game asks for a password, leave it blank and you will be able to get access to the game without any problems.</li>\n\t<li>For instructions on how to play this game, please click on the <strong>Quick Tour</strong> button at the bottom of the Spark Jeopardy board.</li>\n\t<li>To provide feedback or new questions for this game, please email <a href=\"mailto:sparkhub@databricks.com\" target=\"_blank\">sparkhub@databricks.com</a>.</li>\n</ul>\n<h3></h3>\n&nbsp;\n<h3><b>Databricks Trial Accounts</b></h3>\nIf you are running a Spark workshop or want to showcase something with Apache Spark, Databricks can provide you an account up to three weeks prior to your meetup event. \u00a0This way, instead of worrying about spinning up the infrastructure for your event, you can quickly spin up a cluster and showcase your data engineering and data sciences work through Databricks notebooks in your language of choice. \u00a0\u00a0Please email <a href=\"mailto:sparkhub@databricks.com\">sparkhub@databricks.com</a> to get access to Databricks Trial Accounts for your meetup event.\n\n<img class=\"alignnone wp-image-5665\" src=\"https://databricks.com/wp-content/uploads/2015/11/trial-databricks-1024x651.png\" alt=\"trial-databricks\" width=\"850\" height=\"540\" />\n\n&nbsp;\n<h3><b>Coming Soon: Spark Remote Session and/or Q&amp;A</b></h3>\n<b></b>If you would like a deep dive Apache Spark remote session and/or a Question &amp; Answer session, email <a href=\"mailto:sparkhub@databricks.com\" target=\"_blank\">sparkhub@databricks.com</a> for more information. \u00a0We will provide more information for this as soon as we complete our test sessions to ensure that we have resolved any issues.\n\n<img class=\"aligncenter wp-image-5653\" src=\"https://databricks.com/wp-content/uploads/2015/11/remote-qa.png?noresize\" alt=\"\" width=\"650\" height=\"329\" />\n\n&nbsp;\n<h3><b>Discussion</b></h3>\nAs we continue to update and expand these Meetup-in-a-box resources, we will continue to update the <a href=\"https://sparkhub.databricks.com/spark-meetup-organizer-resources/\" target=\"_blank\">Meetup-in-a-box for Spark Meetup Organizers</a>\u00a0page as well. \u00a0To all Spark Meetup Organizers - please join the <a href=\"https://groups.google.com/forum/#!forum/spark-meetup-organizers\" target=\"_blank\">Spark Meetup Organizers Google Group</a> and email <a href=\"mailto:sparkhub@databricks.com\" target=\"_blank\">sparkhub@databricks.com</a> to get access to these resources, to get more information, or to provide feedback about <a href=\"http://sparkhub.databricks.com\">SparkHub</a>. \u00a0This way, we can work together to help the Apache Spark community be the best community experience!\n\n&nbsp;"}
{"status": "publish", "description": "Databricks - a fast, simple, and scalable way to build a just-in-time data warehouse that will revolutionize the way data teams analyze data.", "creator": "Wayne Chan", "link": "https://databricks.com/blog/2015/11/30/building-a-just-in-time-data-warehouse-platform-with-databricks.html", "authors": null, "id": 5605, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2015-11-30", "tz": "UTC", "createdOn": "2015-11-30"}, "title": "Building a Just-In-Time Data Warehouse Platform with Databricks", "slug": "building-a-just-in-time-data-warehouse-platform-with-databricks", "content": "Data warehouses have been designed to deliver value out of data and it has long served enterprises as the de-facto solution to do just that. But in today\u2019s big data landscape where enterprises are dealing with a new level of volume, variety, and velocity of data, it is too challenging and costly to move that data into your data warehouse and make sense of the entire dataset in a timely manner. \u00a0Common pain points that data engineers and data scientists experience when working with traditional data warehousing solutions include:\n<ul>\n \t<li><b><b>Inelasticity of compute and storage resources</b>:\u00a0</b>Traditional data warehouses require you to plan for the maximum load at any given time. \u00a0This inelasticity makes it very difficult to optimize compute and storage to meet ever-changing demands which can be a very costly way to manage your resources.</li>\n \t<li><strong>Rigid architecture that's difficult to change</strong>: Traditional data warehouses are inherently rigid due to its schema-on-write architecture. \u00a0This results in having to build costly and time-consuming ETL pipeline to access and manipulate the data. \u00a0And as new data types and sources are introduced, the need to augment your ETL pipelines exacerbates the problem.</li>\n \t<li><strong>Limited advanced analytics capabilities</strong>: Traditional data warehouses are limited to SQL queries which can hamper the depth of analysis, impeding your ability to solve more complex problems with machine learning, streaming, and graph computations.</li>\n</ul>\n<b>The Databricks Solution: Just-In-Time Data Warehousing Made Simple</b>\n\nPowered by Apache Spark, Databricks\u00a0provides a fast, simple, and scalable way to augment your existing data warehousing strategy by combining pluggable support for common data sources and the ability to dynamically scale nodes and clusters on-demand. \u00a0Additionally, Databricks\u2019 Spark clusters have built-in SSD caching to complement Spark\u2019s native in-memory caching to provide optimal flexibility and performance. This enables organizations to read data on-the-fly from the original data source and perform \u201cjust-in-time\u201d queries on data wherever it resides rather than investing in complicated and costly ETL pipelines.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/11/JIT-OVERVIEW.png\"><img class=\"alignleft wp-image-7205\" src=\"https://databricks.com/wp-content/uploads/2015/11/JIT-OVERVIEW.png\" alt=\"JIT-OVERVIEW\" width=\"949\" height=\"510\" /></a>\n\nHow exactly does Databricks deliver on this promise?\n<ul>\n \t<li><b><b>Unified platform for a variety of data sources:</b> </b>Rationalize\u00a0the ETL process with APIs in your language of choice and JDBC/ODBC connectors, allowing enterprises to more easily and cost-effectively process data on-demand from any source directly. You also have the flexibility to combine Databricks with an existing data warehouse to achieve a unified view of your data.</li>\n \t<li><b><b>On-schema reads for direct data access</b>: </b>Define the schema at the point in time of reading, avoiding the need to declare, load, partition, and index the data before querying.</li>\n \t<li><b><b><b>Scale on-demand for maximum elasticity:</b></b></b> Independently scale resources according to your data processing and querying needs with a few clicks of a button, allowing data teams to optimize resources.</li>\n \t<li><b>SSD caching for distributed performance</b>: Databricks\u2019 Spark in-memory clusters have built-in SSD caching that minimizes pre-processing and speeds up your queries by caching your files upon extraction.</li>\n \t<li><b>Support for advanced data analytics</b>: Empower your team to easily take data science to the next level with built-in advanced capabilities like machine learning, graph processing, real-time streaming analytics, and more.</li>\n</ul>\nTake a look at Databricks to implement or extend your current data warehousing strategy to utilize more of the data you already have and deliver insights faster.\n\nReady to take your data warehousing to the next level? \u00a0Check out our\u00a0<a href=\"http://go.databricks.com/data-warehousing-solution-brief\"><b>Just-in-Time Data Warehouse Solution Brief\u00a0</b></a>to learn more."}
{"status": "publish", "description": null, "creator": "patrick", "link": "https://databricks.com/blog/2015/11/20/announcing-an-apache-spark-1-6-preview-in-databricks.html", "authors": null, "id": 5730, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2015-11-20", "tz": "UTC", "createdOn": "2015-11-20"}, "title": "Announcing an Apache Spark 1.6 Preview in Databricks", "slug": "announcing-an-apache-spark-1-6-preview-in-databricks", "content": "Today we are happy to announce the availability of an Apache Spark 1.6 preview package in Databricks. The Apache Spark 1.6.0 release is still a few weeks away - this package is intended to provide early access to the features in the upcoming Spark 1.6 release, based on the upstream source code. Using the preview package is as simple as selecting the \u201c1.6.0 (Preview)\u201d version when launching a cluster:\n\n<img class=\"wp-image-5739 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/11/cluster-create-2.png\" alt=\"cluster create 2\" width=\"400\" height=\"269\" />\n\nOnce you have created a cluster, you will have access to new features from the upcoming Spark release. You can also test existing jobs or notebooks with Spark\u00a0to see performance improvements from earlier versions of Spark. Of course, until the upstream Apache Spark 1.6 release is finalized, we do not recommend fully migrating any production workload onto this version. This feature will be rolling out to Databricks customers over the next week as part Databricks\u00a02.8.\n\nSo - what is coming in Spark 1.6? We have curated<a href=\"https://docs.cloud.databricks.com/docs/spark/1.6/index.html\"> a list of major changes</a> along with code examples. We\u2019ll also be running a <a href=\"http://go.databricks.com/apache-spark-1.6-with-patrick-wendell\">webinar on December 1st</a> to outline the features in this release. A few of the major features are:\n\n<b>A new Dataset API</b>. This API is an extension of Spark\u2019s DataFrame API that supports static typing and user functions that run directly on existing JVM types (such as user classes, Avro schemas, etc). Dataset\u2019s use the same runtime optimizer as DataFrames, but support compile-time type checking and offer better performance. More details on Datasets can be found on the <a href=\"https://issues.apache.org/jira/browse/SPARK-9999\">SPARK-9999 JIRA</a>.\n\n<b>Automatic memory configuration</b>. Users no longer need to tune the size of different memory regions in Spark. Instead, Spark at runtime will automatically grow and shrink regions according to the needs of the executing application. For many applications, this will mean a significant increase in available memory that can be used for operators such as joins and aggregations.\n\n<b>Optimized state storage in Spark Streaming.</b> Spark Streaming\u2019s state tracking API has been revamped with a new \u201cdelta-tracking\u201d approach to significantly optimize programs that store large amounts of state. The state tracking feature in Spark Streaming is used for problems like sessionization, where the information for a particular session is updated over time as events stream in. In Spark 1.6, the cost of maintaining this state scales with the number of <i>new updates</i> at any particular time, rather than the total size of state being tracked. This can be a 10X performance gain in many workloads.\n\n<b>Pipeline persistence in Spark ML. </b>Today most Spark machine learning jobs leverage the ML Pipelines API. Starting in Spark 1.6, this API supports persisting pipelines so that they can be re-loaded at runtime from a previous state. This can be useful when certain pipeline components require significant computation time, such as training large models.\n\n<a href=\"https://docs.cloud.databricks.com/docs/spark/1.6/index.html\" target=\"_blank\"><img class=\"wp-image-5737 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/11/spark-1.6-relese-notes-1024x535.png\" alt=\"spark 1.6 relese notes\" width=\"500\" height=\"261\" /></a>\n<p style=\"text-align: center;\"><em>An overview of Spark 1.6 changes and some code examples to demonstrate new features\n(click on image to see full list)</em></p>\nWe are looking forward to finalizing the formal Apache Spark 1.6.0 release. In the meantime, we invite you to take the Spark 1.6 preview package for a spin in Databricks!\n\nTo try Databricks today, <a href=\"https://accounts.cloud.databricks.com/registration.html#signup\">sign up for a free 14-day trial</a>."}
{"status": "publish", "description": "Databricks announces the availability of a major release this week. In addition to several new usability and portability features, this release also provides a preview to the not-yet-released Apache Spark 1.6.", "creator": "dave_wang", "link": "https://databricks.com/blog/2015/11/24/new-databricks-release-preview-of-apache-spark-1-6-easier-search-and-more.html", "authors": null, "id": 5778, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2015-11-24", "tz": "UTC", "createdOn": "2015-11-24"}, "title": "New Databricks release: Preview of Apache Spark 1.6, easier search, and more", "slug": "new-databricks-release-preview-of-apache-spark-1-6-easier-search-and-more", "content": "Databricks announces the availability of a major release this week. In addition to several new usability and portability features, this release also provides a preview to the not-yet-released Apache Spark 1.6. Databricks users have an opportunity to experiment with Spark 1.6 features before the official release, easily search through Spark documentation within the Databricks integrated workspace, and share Databricks notebooks between different Databricks instances with a few clicks. In this blog, I\u2019ll provide a brief tour of these exciting additions.\n<h2>Apache Spark 1.6 Preview</h2>\n<a href=\"https://databricks.com/blog/2015/11/20/announcing-spark-1-6-preview-in-databricks.html\">The preview of Spark 1.6 was announced by Patrick Wendell last week</a>. We are excited to make this preview available to allow our users to take advantage of the rapid evolution of the open source project. This preview access to Spark 1.6 is currently\u00a0available on Databricks.\n\nHaving the preview in Databricks means that our users can launch the preview\u00a0package\u00a0by simply choosing <i>Version 1.6.0 (Preview)</i> from the Databricks Cluster Manager UI. They can create 1.6 clusters alongside their clusters with older versions, or test existing code on Spark 1.6 to see performance improvements from earlier versions of Spark - currently Databricks also offers versions 1.3, 1.4.1, and 1.5. Being able to run multiple Spark versions allows our users to experiment with cutting edge features while still maintaining the stability of their existing production environments.\n\n<img class=\"wp-image-5739 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/11/cluster-create-2.png\" alt=\"cluster create 2\" width=\"400\" height=\"269\" />\n\nPatrick will also give a webinar on Spark 1.6 on December 1st, <a href=\"http://go.databricks.com/apache-spark-1.6-with-patrick-wendell\">sign-up today to get the details</a>.\n<h2>Improved Portability of Databricks Notebooks</h2>\nA few weeks ago, we <a href=\"https://databricks.com/blog/2015/10/21/introducing-more-databricks-notebooks-sharing-options.html\">announced a new feature</a> that allows users to export notebooks into HTML format. We have extended this functionality to allow the import of previously exported HTML notebooks into any Databricks instance. This gives our users more options to share notebooks and collaborate - including collaboration across different Databricks instances.\n\n<img class=\"aligncenter wp-image-5780\" src=\"https://databricks.com/wp-content/uploads/2015/11/import-item1.png\" alt=\"import item\" width=\"300\" height=\"165\" />\n<h2>Easier Spark Documentation Search</h2>\nThe Databricks <a href=\"https://databricks.com/blog/2015/04/02/learning-how-to-write-spark-applications-in-databricks-cloud-with-the-integrated-search-feature.html\">integrated search feature</a> has been an easy way for our users to find relevant information. With this release, a user can also easily search through the official Spark documentation to locate the most up-to-date information.\n\n<img class=\"aligncenter wp-image-5781\" src=\"https://databricks.com/wp-content/uploads/2015/11/doc-search.png\" alt=\"doc search\" width=\"300\" height=\"279\" />\n<h2>Looking Forward</h2>\nBeing a SaaS platform, Databricks gets updated in rapid iterations to improve the user experience continuously. If you already have a Databricks account, we welcome you to try out these new features and provide feedback. If you are interested in taking Databricks for a spin, <a href=\"mailto:sales@databricks.com\">contact one of our solution architects</a> or sign up for a trial today."}
{"status": "publish", "description": "In this blog, we will demonstrate an integration between the Databricks platform and H2O.ai\u2019s Sparking Water that provides Databricks users with an additional set of advanced and scalable machine learning libraries.", "creator": "dave_wang", "link": "https://databricks.com/blog/2015/12/02/databricks-and-h2o-make-it-rain-with-sparkling-water.html", "authors": null, "id": 5793, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2015-12-02", "tz": "UTC", "createdOn": "2015-12-02"}, "title": "Databricks and H2O Make it Rain with Sparkling Water", "slug": "databricks-and-h2o-make-it-rain-with-sparkling-water", "content": "This is a guest blog from\u00a0Michal Malohlava, a\u00a0Software Engineer at H2O.ai\n\n<hr />\n\nDatabricks provides a cloud-based integrated workspace on top of Apache Spark for developers and data scientists. H2O.ai has been an early adopter of Apache Spark and has developed Sparkling Water to seamlessly integrate H2O.ai\u2019s machine learning library on top of Spark.\n\nIn this blog, we will demonstrate an integration between the Databricks platform and H2O.ai\u2019s Sparking Water that provides Databricks users with an additional set of machine learning libraries. The integration allows data scientists to utilize Sparkling Water with Spark in a notebook environment more easily, allowing them to seamlessly combine Spark with H2O and get the best of both worlds.\n\nLet\u2019s begin by preparing a Databricks environment to develop our spam predictor:\n\nThe first step is to log into your Databricks account and create a new library containing Sparkling Water. You can use the Maven coordinates of the Sparkling Water package, for example: <code>ai.h2o:sparkling-water-examples_2.10:1.5.6</code> (this version works with Spark 1.5)\n\n<img class=\"wp-image-5794 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/11/h2o-blog-figure-1.png\" alt=\"h2o blog - figure 1\" width=\"400\" height=\"237\" />\n\nThe next step is to create a new cluster to run the example:\n\n<img class=\"aligncenter wp-image-5795\" src=\"https://databricks.com/wp-content/uploads/2015/11/h2o-blog-figure-2.png\" alt=\"h2o blog - figure 2\" width=\"400\" height=\"281\" />For this version of the Sparkling Water library, we will use Spark 1.5. The name of the created cluster is \u201cHamOrSpamCluster\u201d - keep it handy as we will need it later.\n\nThe next step is to upload data, you can use table import and upload the <a href=\"https://github.com/h2oai/sparkling-water/blob/master/examples/smalldata/smsData.txt\">smsData.txt</a> <a href=\"https://github.com/h2oai/sparkling-water/blob/master/examples/smalldata/smsData.txt\">file</a>\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/11/h2o-blog-figure-3.png\"><img class=\"aligncenter wp-image-5796\" src=\"https://databricks.com/wp-content/uploads/2015/11/h2o-blog-figure-3.png\" alt=\"h2o blog - figure 3\" width=\"400\" height=\"278\" /></a>\n\nNow the environment is ready and you can create a Databricks notebook; connect it to \u201cHamOrSpamCluster\u201d and start building a predictive model!\n\nThe goal of the application is to write a spam detector using a trained model to categorize incoming messages\n\nFirst look at the data. It contains raw text messages that are labeled as either spam or ham.\n\nFor example:\n<table class=\"table\">\n<tbody>\n<tr>\n<td><b>spam</b></td>\n<td>+123 Congratulations - in this week's competition draw u have won the ?1450 prize to claim just call 09050002311 b4280703. T&amp;Cs/stop SMS 08718727868. Over 18 only 150</td>\n</tr>\n<tr>\n<td><b>ham</b></td>\n<td>Yun ah.the ubi one say if ? wan call by tomorrow.call 67441233 look for irene.ere only got bus8,22,65,6</td>\n</tr>\n</tbody>\n</table>\nWe need to transform these messages into vectors of numbers and then train a binomial model to predict whether the text message is either SPAM or HAM. For the transformation of a message into a vector of numbers we will use Spark MLlib string tokenization and word to vector transformers. We are going to split messages into tokens and use the TF (term frequency\u2013inverse document frequency) technique to represent words of importance inside the training data set:\n\n[scala]\n// Representation of a training message\nimport org.apache.spark.mllib.linalg.Vector\ncase class SMS(target: String, fv: Vector)\ndef tokenize(data: RDD[String]): RDD[Seq[String]] = {\n  val ignoredWords = Seq(&quot;the&quot;, &quot;a&quot;, &quot;&quot;, &quot;in&quot;, &quot;on&quot;, &quot;at&quot;, &quot;as&quot;, &quot;not&quot;, &quot;for&quot;)\n  val ignoredChars = Seq(',', ':', ';', '/', '&amp;lt;', '&amp;gt;', '&quot;', '.', '(', ')', '?', '-', '\\'','!','0', '1')\n\n  val texts = data.map( r=&amp;gt; {\n    var smsText = r.toLowerCase\n    for( c &amp;lt;- ignoredChars) { smsText = smsText.replace(c, ' ') } val words =smsText.split(&quot; &quot;).filter(w =&amp;gt; !ignoredWords.contains(w) &amp;amp;&amp;amp; w.length&amp;gt;2).distinct\n\n    words.toSeq\n  })\n  texts\n}\nimport org.apache.spark.mllib.feature._\n\ndef buildIDFModel(tokens: RDD[Seq[String]],\n                  minDocFreq:Int = 4,\n                  hashSpaceSize:Int = 1 &amp;lt;&amp;lt; 10): (HashingTF, IDFModel, RDD[Vector]) = {\n  // Hash strings into the given space\n  val hashingTF = new HashingTF(hashSpaceSize)\n  val tf = hashingTF.transform(tokens)\n  // Build term frequency-inverse document frequency\n  val idfModel = new IDF(minDocFreq = minDocFreq).fit(tf)\n  val expandedText = idfModel.transform(tf)\n  (hashingTF, idfModel, expandedText)\n}\n[/scala]\n\nThe resulting table will contain the following lines:\n<table>\n<tbody>\n<tr>\n<td><b>spam</b></td>\n<td>0, 0, 0.31, 0.12, \u2026.</td>\n</tr>\n<tr>\n<td><b>ham</b></td>\n<td>0.67, 0, 0, 0, 0, 0.003, 0, 0.1</td>\n</tr>\n</tbody>\n</table>\nAfter this we are free to experiment with different binary classification algorithms in H2O.\n\nTo start using H2O, we need to initialize the H2O service by creating an H2OContext:\n\n[scala]\n// Create SQL support\nimport org.apache.spark.sql._\nimplicit val sqlContext = SQLContext.getOrCreate(sc)\nimport sqlContext.implicits._\n\n// Start H2O services\nimport org.apache.spark.h2o._\n@transient val h2oContext = new H2OContext(sc).start()\n[/scala]\n\nH2OContext represents H2O running on top of a Spark cluster. You should see the following output:\n<p style=\"text-align: center;\"><a href=\"https://databricks.com/wp-content/uploads/2015/11/h2o-blog-figure-4.png\"><img class=\"aligncenter wp-image-5798\" src=\"https://databricks.com/wp-content/uploads/2015/11/h2o-blog-figure-4-1024x557.png\" alt=\"h2o blog - figure 4\" width=\"400\" height=\"218\" /></a><em>C</em><em>lick on figure to enlarge</em></p>\nFor this demonstration, we will leverage the H2O Deep Learning method:\n\n[scala]\n// Define function which builds a DL model\nimport org.apache.spark.h2o._\nimport water.Key\nimport _root_.hex.deeplearning.DeepLearning\nimport _root_.hex.deeplearning.DeepLearningParameters\nimport _root_.hex.deeplearning.DeepLearningModel\n\ndef buildDLModel(train: Frame, valid: Frame,\n               epochs: Int = 10, l1: Double = 0.001, l2: Double = 0.0,\n               hidden: Array[Int] = Array[Int](200, 200))\n              (implicit h2oContext: H2OContext): DeepLearningModel = {\nimport h2oContext._\n// Build a model\n\nval dlParams = new DeepLearningParameters()\ndlParams._model_id = Key.make(&quot;dlModel.hex&quot;)\ndlParams._train = train\ndlParams._valid = valid\ndlParams._response_column = 'target\ndlParams._epochs = epochs\ndlParams._l1 = l1\ndlParams._hidden = hidden\n\n// Create a job\nval dl = new DeepLearning(dlParams)\nval dlModel = dl.trainModel.get\n\n// Compute metrics on both datasets\ndlModel.score(train).delete()\ndlModel.score(valid).delete()\n\ndlModel\n}\n[/scala]\n\nHere is the final application:\n\n[scala]\n// Build the application\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.examples.h2o.DemoUtils._\nimport scala.io.Source\n\n// load both columns from the table\nval data = sqlContext.sql(&quot;SELECT * FROM smsData&quot;)\n// Extract response spam or ham\nval hamSpam = data.map( r =&amp;gt; r(0).toString)\nval message = data.map( r =&amp;gt; r(1).toString)\n// Tokenize message content\nval tokens = tokenize(message)\n// Build IDF model\nvar (hashingTF, idfModel, tfidf) = buildIDFModel(tokens)\n\n// Merge response with extracted vectors\nval resultRDD: DataFrame = hamSpam.zip(tfidf).map(v =&amp;gt; SMS(v._1, v._2)).toDF\n\n// Publish Spark DataFrame as H2OFrame\n// This H2OFrame has to be transient because we do not want it to be serialized. When calling for example sc.parallelize(..) the object which we are trying to parallelize takes with itself all variables in its surroundings scope - apart from those marked as serialized.\n// \n@transient val table = h2oContext.asH2OFrame(resultRDD)\nprintln(sc.parallelize(Array(1,2)))\n// Transform target column into categorical\ntable.replace(table.find(&quot;target&quot;), table.vec(&quot;target&quot;).toCategoricalVec()).remove()\ntable.update(null)\n\n// Split table\nval keys = Array[String](&quot;train.hex&quot;, &quot;valid.hex&quot;)\nval ratios = Array[Double](0.8)\n@transient val frs = split(table, keys, ratios)\n@transient val train = frs(0)\n@transient val valid = frs(1)\ntable.delete()\n\n// Build a model\n@transient val dlModel = buildDLModel(train, valid)(h2oContext)\n[/scala]\n\nAnd voila we have a Deep Learning Model ready to detect spam\n\nAt this point you can explore quality of the model:\n\n[scala]\n// Collect model metrics and evaluate model quality\nimport water.app.ModelMetricsSupport\n\nval validMetrics = ModelMetricsSupport.binomialMM(dlModel, valid)\nprintln(validMetrics.auc._auc)\n[/scala]\n\nYou can also use the H2O Flow UI by clicking on the URL provided when you instantiated the H2O Context.\n\n<img class=\"wp-image-5800 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/11/h2o-blog-figure-5-1024x788.png\" alt=\"h2o blog - figure 5\" width=\"600\" height=\"462\" />\n\nAt this point we have everything ready to create a spam detector:\n\n[scala]\n\n// Create a spam detector - a method which will return SPAM or HAM for given text message\nimport water.DKV._\n// Spam detector\ndef isSpam(msg: String,\n       modelId: String,\n       hashingTF: HashingTF,\n       idfModel: IDFModel,\n       h2oContext: H2OContext,\n       hamThreshold: Double = 0.5):String = {\nval dlModel: DeepLearningModel = water.DKV.getGet(modelId)\nval msgRdd = sc.parallelize(Seq(msg))\nval msgVector: DataFrame = idfModel.transform(\n                            hashingTF.transform (\n                              tokenize (msgRdd))).map(v =&amp;gt; SMS(&quot;?&quot;, v)).toDF\nval msgTable: H2OFrame = h2oContext.asH2OFrame(msgVector)\nmsgTable.remove(0) // remove first column\nval prediction = dlModel.score(msgTable)\n//println(prediction)\nif (prediction.vecs()(1).at(0) &amp;lt; hamThreshold) &quot;SPAM DETECTED!&quot; else &quot;HAM&quot;\n}   \n[/scala]\n\nThe method uses built-in models to transform incoming text message and provide a prediction - SPAM or HAM. For example:\n\n<img class=\"aligncenter wp-image-5801\" src=\"https://databricks.com/wp-content/uploads/2015/11/h2o-blog-figure-6-1024x304.png\" alt=\"h2o blog - figure 6\" width=\"600\" height=\"178\" />\n\nWe\u2019ve shown a fast and easy way to build a spam detector with Databricks and Sparkling Water. To try this out for yourself, register for a <a href=\"https://accounts.cloud.databricks.com/registration.html#signup\" target=\"_blank\">free 14-day trial of Databricks</a> and check out the Sparkling Water example in the Databricks Guide.\n\n&nbsp;"}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2015/12/18/guest-blog-streamliner-an-open-source-apache-spark-streaming-application.html", "authors": null, "id": 5889, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2015-12-18", "tz": "UTC", "createdOn": "2015-12-18"}, "title": "Guest Blog: Streamliner - An Open Source Apache Spark Streaming Application", "slug": "guest-blog-streamliner-an-open-source-apache-spark-streaming-application", "content": "This is a guest blog from Ankur Goyal, VP of Engineering at MemSQL\n\n<hr />\n\nOur always-on interconnected world constantly shuttles data between devices and data centers. The need for real-time insights is driving large and small companies alike towards building real-time data pipelines. Apache Spark Streaming tackles several challenges associated with real-time data streams including the need for real-time Extract, Transform, and Load (ETL). This has led to the rapid adoption of Spark Streaming in the industry. In the <a href=\"http://go.databricks.com/2015-spark-survey\">2015 Spark survey</a>, 48% of Spark users indicated they were using streaming. For production deployments, there was a 56% rise in usage of Spark Streaming from 2014 to 2015.\n\nMore importantly, it is becoming a flexible, robust, scalable platform for building end-to-end real-time pipeline solutions. At MemSQL, we observed these trends and built Streamliner - an open source, one-click solution for building real-time data pipelines using Spark Streaming and MemSQL. With Streamliner, you can stream data from real-time data sources (e.g. Apache Kafka), perform data transformations with Spark Streaming, and ultimately load data into MemSQL for persistence and application serving.\n<h3>Developing a Spark Streaming Quickstart</h3>\nEarlier this year, we started digging into customer use cases and noticed a common pattern: using Spark Streaming, connecting to Kafka, and persisting data in a high-performance store such as MemSQL.\n\nWith the goal of simplifying the out-of-box experience for Kafka to Spark Streaming to MemSQL, we looked at what would it take to build these properties into a complete solution. For the common case, there should be no need to write code. But if needed, that code should be pluggable and easy to contribute back to the community.\n\nThe time-to-value needs to be fast, so debugging and error handling should be simple and visible directly in the UI. Pipelines need to be cheap and easy to work with, such that you can add, remove, pause, stop, and restart them dynamically with minimal system impact.\n<h3>Introducing Streamliner</h3>\nThe result was Streamliner, an intuitive, UI-driven solution to build and manage multiple real-time data pipelines on Spark Streaming. Streamliner accomplishes this by exposing three abstractions - Extractors, Transformers, and Loaders - which have pre-built options and are extensible. For example, you can use a pre-built Kafka extractor, plug it into a pre-built JSON transformer, and load it into MemSQL, all without writing any code. You can also add/remove and start/stop pipelines directly from the UI without interrupting other pipelines. Another popular use-case is leveraging the Thrift transformer to convert your custom Thrift-formatted data into database schema + data, without having to write custom code per Thrift schema. Streamliner, along with example applications built with it, are available as open source on GitHub.\n\n<a href=\"https://github.com/memsql/streamliner-starter\">https://github.com/memsql/streamliner-starter</a>\n\nThe premise of Streamliner was to create a simplified experience built on the foundation of Spark Streaming. With that, we needed to meet the following requirements:\n<ul>\n\t<li>Ingest into Spark Streaming. Our choice for the data source was Kafka, a popular distributed message queue.</li>\n\t<li>Scalability, particularly the ability to support multiple data pipelines</li>\n\t<li>Spark API support and compatibility</li>\n</ul>\nWe also identified a few nice to have features:\n<ul>\n\t<li>Easy data persistence</li>\n\t<li>High speed ingest, row-by-row</li>\n\t<li>Exactly-once semantics</li>\n\t<li>Rich, full SQL directly on the data with complex data structures like indexes and column stores.</li>\n\t<li>In-place data transactions that allow users to build an application on top of real-time data</li>\n</ul>\n<img class=\"aligncenter wp-image-5894\" src=\"https://databricks.com/wp-content/uploads/2015/12/spark-streamer-blog-1-1024x370.png\" alt=\"spark streamer blog 1\" width=\"600\" height=\"217\" />\n\nAs we were building Streamliner, we also identified places where our simplifications enabled us to get some big performance wins.\n\nFirst, we are able to run multiple streaming pipelines in a single Spark job by routing work through a single, special MemSQL Spark application (memsql-spark-interface), allowing for resource sharing across pipelines, which ultimately translates to significantly less hardware consumption and shorter times for pipeline deployment.\n\nSecond, we are able to achieve exactly-once semantics in this highly dynamic setting. Streamliner only executes a narrow set of real-time transformations on Spark Streaming, which simplifies the checkpointing mechanisms needed to ensure fault-tolerance guarantees. In addition, the checkpointing is offloaded to MemSQL instead of HDFS, which makes it faster and consistent with the data saved in MemSQL. Finally, Streamliner leverages primary keys and ON DUPLICATE IGNORE semantics in MemSQL to ensure true exactly-once end-to-end semantics, that is, every record is guaranteed to be stored in MemSQL exactly once despite any failures in the system.\n\n<img class=\"wp-image-5895 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/12/spark-streamer-blog-2-1024x511.png\" alt=\"spark streamer blog 2\" width=\"600\" height=\"300\" />\n<h3>Streamliner available as open source on GitHub</h3>\nWe were able to advance a new solution for Spark Streaming with Streamliner. It is now available as open source on <a href=\"http://memsql.github.io/spark-streamliner\">GitHub</a>, along with several <a href=\"https://github.com/memsql/streamliner-starter\">examples</a> to help put it to use.\n\n&nbsp;"}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2015/12/22/the-best-of-databricks-blog-most-read-posts-of-2015.html", "authors": null, "id": 5906, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2015-12-22", "tz": "UTC", "createdOn": "2015-12-22"}, "title": "The Best of The Databricks Blog: Most Read Posts of 2015", "slug": "the-best-of-databricks-blog-most-read-posts-of-2015", "content": "Databricks developers are prolific blog authors when they are not writing code for the Databricks platform or Apache Spark. As 2015 draws to a close, we did a quick tally of page views across all the blog posts published during this year to understand what topics attracted the most interest amongst our readers.\n\nThe result indicates that people are most interested in announcements of new Spark features and practical guides on tuning Spark. While blog posts published earlier in the year have an advantage, there is a clear winner by a wide margin. So here we give you a countdown to the most popular Databricks blog posts of 2015.\n<h2>The countdown</h2>\n<a href=\"https://databricks.com/blog/2015/01/28/introducing-streaming-k-means-in-spark-1-2.html\">#10: Introducing Streaming K-Means in Spark 1.2</a>\n\n<a href=\"https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html\">#9: ML Pipelines: A New High-Level API for MLlib</a>\n\n<a href=\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\">#8: Deep Dive into Spark SQL\u2019s Catalyst Optimizer</a>\n\n<a href=\"https://databricks.com/blog/2015/05/28/tuning-java-garbage-collection-for-spark-applications.html\">#7: Tuning Java Garbage Collection for Spark Applications</a>\n\n<a href=\"https://databricks.com/blog/2015/02/02/an-introduction-to-json-support-in-spark-sql.html\">#6: An Introduction to JSON Support in Spark SQL</a>\n\n<a href=\"https://databricks.com/blog/2015/03/20/using-mongodb-with-spark.html\">#5: Using MongoDB with Spark</a>\n\n<a href=\"https://databricks.com/blog/2015/06/11/announcing-apache-spark-1-4.html\">#4: Announcing Apache Spark 1.4</a>\n\n<a href=\"https://databricks.com/blog/2015/06/09/announcing-sparkr-r-on-spark.html\">#3: Announcing SparkR: R on Spark</a>\n\n<a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\">#2: Project Tungsten: Bringing Spark Closer to Bare Metal</a>\n<h3>And the winner is\u2026</h3>\n<a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\">#1: Introducing DataFrames in Spark for Large Scale Data Science</a>!\n<h2>But wait, what about the classics?</h2>\nA few blog posts written in 2014 remain extremely popular more than a year later, Spark SQL is the hands down winner in this category:\n\n<a href=\"https://databricks.com/blog/2014/07/01/shark-spark-sql-hive-on-spark-and-the-future-of-sql-on-spark.html\">Shark, Spark SQL, Hive on Spark, and the future of SQL on Spark</a>\n\n<a href=\"https://databricks.com/blog/2014/03/26/spark-sql-manipulating-structured-data-using-spark-2.html\">Spark SQL: Manipulating Structured Data Using Spark</a>\n\n<a href=\"https://databricks.com/blog/2014/01/21/spark-and-hadoop.html\">Spark and Hadoop: Working Together</a>\n\n<a href=\"https://databricks.com/blog/2014/10/10/spark-petabyte-sort.html\">Spark the fastest open source engine for sorting a petabyte</a>\n\nWe promise to keep creating great content for the community in the upcoming year. Keep an eye out for tutorials and deep-dives on the upcoming release of Apache Spark here. Follow us on <a href=\"https://twitter.com/databricks?lang=en\">Twitter</a> or <a href=\"https://www.linkedin.com/company/databricks\">LinkedIn</a> to stay up to date!"}
{"status": "publish", "description": "Lendup, a company that builds technology to expand access to credit & lower cost of borrowing deploys Databricks to improve productivity of the data science team in building business-critical credit models.", "creator": "dave_wang", "link": "https://databricks.com/blog/2015/12/28/lendup-expands-access-to-credit-with-databricks.html", "authors": null, "id": 5935, "categories": ["Announcements", "Company Blog", "Customers"], "dates": {"publishedOn": "2015-12-28", "tz": "UTC", "createdOn": "2015-12-28"}, "title": "LendUp Expands Access to Credit with Databricks", "slug": "lendup-expands-access-to-credit-with-databricks", "content": "We are happy to announce a new deployment of Databricks in the financial technology sector with <a href=\"https://www.lendup.com/\">LendUp</a>, a company that builds technology to expand access to credit. LendUp uses Databricks to develop innovative machine learning models that touch all aspects of its lending business. Specifically, it uses Databricks to perform feature engineering at scale and quickly iterate through the model building process. Faster iterations lead to more accurate models, the ability to offer credit to more of the tens of millions of Americans who need it and the ability to establish new products more easily.\n\nBefore Databricks, LendUp performed feature extraction on a single machine in AWS. Processing millions of semi-structured documents took multiple days, which delayed updates to critical machine learning models. The productivity of the LendUp team was further impeded by the poor integration of data storage, feature extraction, modeling, and analytics tools, requiring the team to build custom solutions to bring together disparate data sources and analytics tools.\n\nDatabricks enables LendUp to speed up feature extraction by replacing the single instance machine with highly tuned Apache Spark clusters. Databricks also provided the critical capabilities to build sophisticated machine learning models in an integrated workspace, replacing a host of disjointed tools and eliminating the need to maintain expensive custom solutions.\n\nAs a result of deploying Databricks, LendUp can more readily advance their core mission. LendUp\u2019s credit models now include more diverse and larger volumes of semi-structured data, and could be generated within a shorter amount of time.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/12/Databricks-Case-Study-LendUp.pdf\">Download this case study</a> to learn more about how LendUp is using Databricks.\n\nTo try out Databricks for yourself, <a href=\"https://accounts.cloud.databricks.com/registration.html#signup\">sign-up for a 14-day free trial</a> today!\n\nFor a personalized demo, <a href=\"http://go.databricks.com/contact-databricks\">drop us a note</a>."}
{"status": "publish", "description": "Apache Spark 1.6 is generally available only through Databricks.  Spark 1.6 features a new parquet reader, improved automatic memory streaming state management, new Dataset API and expansion of data science functionality.", "creator": "michael", "link": "https://databricks.com/blog/2016/01/04/announcing-apache-spark-1-6.html", "authors": null, "id": 5943, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2016-01-04", "tz": "UTC", "createdOn": "2016-01-04"}, "title": "Announcing Apache Spark 1.6", "slug": "announcing-apache-spark-1-6", "content": "To learn more about Apache Spark, attend <a href=\"https://spark-summit.org/east-2016/\" target=\"_blank\">Spark Summit East in New York in Feb 2016</a>.\n\n<hr />\n\nToday we are happy to announce the availability of Apache Spark 1.6! With this release, Spark hit a major milestone in terms of community development: the number of people that have contributed code to Spark has crossed 1000, doubling the 500 number we saw at the end of 2014.\n\n<img class=\"aligncenter wp-image-5990\" src=\"https://databricks.com/wp-content/uploads/2016/01/spark-contributors-1024x322.png?noresize\" alt=\"The number of Apache Spark contributors have doubled since 2014.\" width=\"601\" height=\"189\" />\n\nSo \u2013 what\u2019s new in Spark 1.6? Spark 1.6 includes over 1000 patches. In this blog post, we highlight three major development themes: performance improvements, the new Dataset API, and expansion of data science functionality.\n<h2>Performance Improvements</h2>\nAccording to our 2015 <a href=\"https://databricks.com/blog/2015/09/24/spark-survey-results-2015-are-now-available.html\" target=\"_blank\">Spark Survey</a>, 91% of users consider performance as the most important aspect of Spark. As a result, performance optimizations have always been a focus in our Spark development.\n\n<b>Parquet performance</b>: Parquet has been one of the most commonly used data formats with Spark, and Parquet scan performance has pretty big impact on many large applications. In the past, Spark\u2019s Parquet reader relies on parquet-mr to read and decode Parquet files. When we profile Spark applications, often many cycles are spent in \u201crecord assembly\u201d, a process that reconstructs records from Parquet columns. In Spark 1.6. we introduce a <a href=\"https://issues.apache.org/jira/browse/SPARK-11787\" target=\"_blank\">new Parquet reader</a> that bypasses parquert-mr\u2019s record assembly and uses a more optimized code path for flat schemas. In our benchmarks, this new reader increases the scan throughput for 5 columns from 2.9 million rows per second to 4.5 million rows per second, an almost 50% improvement!\n\n<b>Automatic memory management</b>: Another area of performance gains in Spark 1.6 comes from better memory management. Before Spark 1.6, Spark statically divided the available memory into two regions: execution memory and cache memory. Execution memory is the region that is used in sorting, hashing, and shuffling, while cache memory is used to cache hot data. Spark 1.6 introduces a <a href=\"https://issues.apache.org/jira/browse/SPARK-10000\" target=\"_blank\">new memory manager</a> that automatically tunes the size of different memory regions. The runtime automatically grows and shrinks regions according to the needs of the executing application. For many applications, this will mean a significant increase in available memory that can be used for operators such as joins and aggregations, without any user tuning.\n\nWhile the above two improvements apply transparently without any application code change, the following improvement is an example of a new API that enables better performance.\n\n<b>10X speedup for streaming state management</b>: State management is an important function in streaming applications, often used to maintain aggregations or session information. Having worked with many users, we have redesigned the state management API in Spark Streaming and introduced a new <a href=\"https://issues.apache.org/jira/browse/SPARK-2629\" target=\"_blank\">mapWithState API</a> that scales linearly to the number of updates rather than the total number of records. That is to say, it has an efficient implementation that tracks \u201cdeltas\u201d, rather than always requiring full scans over data. This has resulted in an order of magnitude performance improvements in many workloads. We have created a <a href=\"https://docs.cloud.databricks.com/docs/spark/1.6/index.html#examples/Streaming%20mapWithState.html\" target=\"_blank\">notebook to illustrate how you can use this new feature</a>, and we will also publish a separate blog post in the near future on this topic.\n<h2>Dataset API</h2>\nWe introduced DataFrames earlier this year, which provide high-level functions that allow Spark to better understand the structure of data as well as the computation being performed. This additional information enables the <a href=\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\" target=\"_blank\">Catalyst optimizer</a> and the <a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\" target=\"_blank\">Tungsten execution engine</a> to automatically speed up real-world Big Data analyses.\n\nSince we released DataFrames, we have gotten a lot of feedback and one of the main ones is the lack of support for compile-time type safety. To address this, we are introducing a typed extension of the DataFrame API called <a href=\"https://databricks.com/blog/2016/01/04/introducing-spark-datasets.html\" target=\"_blank\">Datasets</a>.\n\nThe Dataset API extends the DataFrame API to supports static typing and user functions that run directly on existing Scala or Java types. When compared with the traditional RDD API, Datasets provide better memory management as well as in the long run better performance.\n\nPlease refer to the blog post <a href=\"https://databricks.com/blog/2016/01/04/introducing-spark-datasets.html\" target=\"_blank\">Introducing Spark Datasets</a>.\n<h2>New Data Science Functionality</h2>\n<b>Machine learning pipeline persistence</b>: A lot of machine learning applications leverage Spark's ML pipeline feature to construct learning pipelines. In the past, if the application wanted to store the pipeline externally, it needed to implement custom persistence code. In Spark 1.6, the pipeline API offers functionality to save and reload pipelines from a previous state and apply models built previously to new data later. For example, users can train a pipeline in a nightly job, and then apply it to production data in a production job.\n\n<b>New algorithms and capabilities</b>: This release also increases algorithm coverage in machine learning, including:\n<ul>\n\t<li>univariate and bivariate statistics</li>\n\t<li>survival analysis</li>\n\t<li>normal equation for least squares</li>\n\t<li>bisecting K-Means clustering</li>\n\t<li>online hypothesis testing</li>\n\t<li>Latent Dirichlet Allocation (LDA) in ML Pipelines</li>\n\t<li>R-like statistics for GLMs</li>\n\t<li>feature interactions in R formula</li>\n\t<li>instance weights for GLMs</li>\n\t<li>univariate and bivariate statistics in DataFrames</li>\n\t<li>LIBSVM data source</li>\n\t<li>non-standard JSON data</li>\n</ul>\nThis blog post only covered some of the major features in this release. We have also compiled a <a href=\"https://docs.cloud.databricks.com/docs/spark/1.6/index.html#Apache%20Spark%201.6%20Release%20Notes.html\" target=\"_blank\">more detailed set of release notes along with working examples here</a>.\n\nWe will be posting more details about some of these new features in the coming weeks. Stay tuned to the Databricks blog to learn more about Spark 1.6. If you want to try out these new features, you can already use Spark 1.6 in Databricks, alongside older versions of Spark. \u00a0<a href=\"https://accounts.cloud.databricks.com/registration.html#signup\" target=\"_blank\">Sign up for a free trial account here</a>.\n\nSpark\u2019s success wouldn\u2019t have been possible without the 1000 contributors to date, and we would like to take this opportunity to thank all of them as well."}
{"status": "publish", "description": null, "creator": "michael", "link": "https://databricks.com/blog/2016/01/04/introducing-apache-spark-datasets.html", "authors": null, "id": 5955, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2016-01-04", "tz": "UTC", "createdOn": "2016-01-04"}, "title": "Introducing Apache Spark Datasets", "slug": "introducing-apache-spark-datasets", "content": "To learn more about Apache Spark, attend <a href=\"https://spark-summit.org/east-2016/\" target=\"_blank\">Spark Summit East in New York in Feb 2016</a>.\n\n<hr />\n\nDevelopers have always loved Apache Spark for providing APIs that are simple yet powerful, a combination of traits that makes complex analysis possible with minimal programmer effort. \u00a0At Databricks, we have continued to push Spark\u2019s usability and performance envelope through the introduction of <a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\" target=\"_blank\">DataFrames</a> and <a href=\"https://databricks.com/blog/2014/03/26/spark-sql-manipulating-structured-data-using-spark-2.html\" target=\"_blank\">Spark SQL</a>. These are high-level APIs for working with structured data (e.g. database tables, JSON files), which let Spark automatically optimize both storage and computation. Behind these APIs, <a href=\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\" target=\"_blank\">the Catalyst optimizer</a> and <a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\" target=\"_blank\">Tungsten execution engine</a> optimize applications in ways that were not possible with Spark\u2019s object-oriented (RDD) API, such as operating on data in a raw binary form.\n\nToday we\u2019re excited to announce Spark Datasets, an extension of the DataFrame API that provides a <i>type-safe, object-oriented programming interface</i>. <a href=\"https://databricks.com/blog/2016/01/04/announcing-spark-1-6.html\" target=\"_blank\">Spark 1.6</a> includes an API preview of Datasets, and they will be a development focus for the next several versions of Spark.\u00a0Like DataFrames, Datasets take advantage of Spark's Catalyst optimizer by exposing expressions and data fields to a query planner. \u00a0Datasets also leverage Tungsten's fast in-memory encoding. \u00a0Datasets extend these benefits with compile-time type safety - meaning production applications can be checked for errors before they are run. They also allow direct operations over user-defined classes.\n\nIn the long run, we expect Datasets to become a powerful way to write more efficient Spark applications. We have designed them to work alongside the existing RDD API, but improve efficiency when data can be represented in a structured form. \u00a0Spark 1.6 offers the first glimpse at Datasets, and we expect to improve them in future releases.\n<h2>Working with Datasets</h2>\nA Dataset is a strongly-typed, immutable collection of objects that are mapped to a relational schema. \u00a0At the core of the Dataset API is a new concept called an encoder, which is responsible for converting between JVM objects and tabular representation. The tabular representation is stored using Spark\u2019s internal Tungsten binary format, allowing for operations on serialized data and improved memory utilization. \u00a0Spark 1.6 comes with support for automatically generating encoders for a wide variety of types, including primitive types (e.g. String, Integer, Long), Scala case classes, and Java Beans.\n\nUsers of RDDs will find the Dataset API quite familiar, as it provides many of the same functional transformations (e.g. map, flatMap, filter). \u00a0Consider the following code, which reads lines of a text file and splits them into words:\n<table class=\"table\">\n<tbody>\n<tr>\n<td><b>RDDs</b>\n\n[scala]\nval lines = sc.textFile(&quot;/wikipedia&quot;)\nval words = lines\n  .flatMap(_.split(&quot; &quot;))\n  .filter(_ != &quot;&quot;)\n[/scala]\n\n</td>\n</tr>\n<tr>\n<td><b>Datasets</b>\n\n[scala]\nval lines = sqlContext.read.text(&quot;/wikipedia&quot;).as[String]\nval words = lines\n  .flatMap(_.split(&quot; &quot;))\n  .filter(_ != &quot;&quot;)\n[/scala]\n\n</td>\n</tr>\n</tbody>\n</table>\nBoth APIs make it easy to express the transformation using lambda functions. The compiler and your IDE understand the types being used, and can provide helpful tips and error messages while you construct your data pipeline.\n\nWhile this high-level code may look similar syntactically, with Datasets you also have access to all the power of a full relational execution engine. For example, if you now want to perform an aggregation (such as counting the number of occurrences of each word), that operation can be expressed simply and efficiently as follows:\n<table class=\"table\">\n<tbody>\n<tr>\n<td><b>RDDs</b>\n\n[scala]\nval counts = words\n    .groupBy(_.toLowerCase)\n    .map(w =&gt; (w._1, w._2.size))\n[/scala]\n\n</td>\n</tr>\n<tr>\n<td><b>Datasets</b>\n\n[scala]\nval counts = words \n    .groupBy(_.toLowerCase)\n    .count()\n[/scala]\n\n</td>\n</tr>\n</tbody>\n</table>\nSince the Dataset version of word count can take advantage of the built-in aggregate <code>count</code>, this computation can not only be expressed with less code, but it will also execute significantly faster. \u00a0As you can see in the graph below, the Dataset implementation runs much faster than the naive RDD implementation. \u00a0In contrast, getting the same performance using RDDs would require users to manually consider how to express the computation in a way that parallelizes optimally.\n\n<img class=\"wp-image-5987 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2016/01/Distributed-Wordcount-Chart-1024x371.png?noresize\" alt=\"Distributed-Wordcount-Chart\" width=\"600\" height=\"217\" />\n\nAnother benefit of this new Dataset API is the reduction in memory usage. Since Spark understands the structure of data in Datasets, it can create a more optimal layout in memory when caching Datasets. In the following example, we compare caching several million strings in memory using Datasets as opposed to RDDs. In both cases, caching data can lead to significant performance improvements for subsequent queries. \u00a0However, since Dataset encoders provide more information to Spark about the data being stored, the cached representation can be optimized to use 4.5x less space.\n\n<img class=\"aligncenter wp-image-5988\" src=\"https://databricks.com/wp-content/uploads/2016/01/Memory-Usage-when-Caching-Chart-1024x359.png?noresize\" alt=\"Memory-Usage-when-Caching-Chart\" width=\"600\" height=\"210\" />\n\nTo help you get started, we've put together some example notebooks: <a href=\"https://docs.cloud.databricks.com/docs/spark/1.6/index.html#examples/Dataset%20Classes.html\" target=\"_blank\">Working with Classes</a>, <a href=\"https://docs.cloud.databricks.com/docs/spark/1.6/index.html#examples/Dataset%20Wordcount.html\" target=\"_blank\">Word Count</a>.\n<h2>Lightning-fast Serialization with Encoders</h2>\nEncoders are highly optimized and use runtime code generation to build custom bytecode for serialization and deserialization. \u00a0As a result, they can operate significantly faster than Java or Kryo serialization.\n\n<img class=\"aligncenter wp-image-5989\" src=\"https://databricks.com/wp-content/uploads/2016/01/Serialization-Deserialization-Performance-Chart-1024x364.png?noresize\" alt=\"Serialization-Deserialization-Performance-Chart\" width=\"601\" height=\"214\" />\n\nIn addition to speed, the resulting serialized size of encoded data can also be significantly smaller (up to 2x), reducing the cost of network transfers. \u00a0Furthermore, the serialized data is already in the Tungsten binary format, which means that many operations can be done in-place, without needing to materialize an object at all. \u00a0Spark has built-in support for automatically generating encoders for primitive types (e.g. String, Integer, Long), Scala case classes, and Java Beans. \u00a0We plan to open up this functionality and allow efficient serialization of custom types in a future release.\n<h2>Seamless Support for Semi-Structured Data</h2>\nThe power of encoders goes beyond performance. \u00a0They also serve as a powerful bridge between semi-structured formats (e.g. JSON) and type-safe languages like Java and Scala.\n\nFor example, consider the following dataset about universities:\n\n[scala]\n{&quot;name&quot;: &quot;UC Berkeley&quot;, &quot;yearFounded&quot;: 1868, numStudents: 37581}\n{&quot;name&quot;: &quot;MIT&quot;, &quot;yearFounded&quot;: 1860, numStudents: 11318}\n...\n[/scala]\n\nInstead of manually extracting fields and casting them to the desired type, you can simply define a class with the expected structure and map the input data to it. \u00a0Columns are automatically lined up by name, and the types are preserved.\n\n[scala]\ncase class University(name: String, numStudents: Long, yearFounded: Long)\nval schools = sqlContext.read.json(&quot;/schools.json&quot;).as[University]\nschools.map(s =&gt; s&quot;${s.name} is ${2015 - s.yearFounded} years old&quot;)\n[/scala]\n\nEncoders eagerly check that your data matches the expected schema, providing helpful error messages before you attempt to incorrectly process TBs of data. For example, if we try to use a datatype that is too small, such that conversion to an object would result in truncation (i.e. numStudents is larger than a byte, which holds a maximum value of 255) the Analyzer will emit an AnalysisException.\n\n[scala]\ncase class University(numStudents: Byte)\nval schools = sqlContext.read.json(&quot;/schools.json&quot;).as[University]\n\norg.apache.spark.sql.AnalysisException: Cannot upcast `yearFounded` from bigint to smallint as it may truncate\n[/scala]\n\nWhen performing the mapping, encoders will automatically handle complex types, including nested classes, arrays, and maps.\n<h2>A Single API for Java and Scala</h2>\nAnother goal to the Dataset API is to provide a single interface that is usable in both Scala and Java. This unification is great news for Java users as it ensure that their APIs won't lag behind the Scala interfaces, code examples can easily be used from either language, and libraries no longer have to deal with two slightly different types of input. \u00a0The only difference for Java users is they need to specify the encoder to use\u00a0since the compiler does not provide type information. \u00a0For example, if wanted to process json data using Java you could do it as follows:\n\n<pre>public class University implements Serializable {\n    private String name;\n    private long numStudents;\n    private long yearFounded;\n\n \u00a0\u00a0\u00a0public void setName(String name) {...}\n \u00a0\u00a0\u00a0public String getName() {...}\n \u00a0\u00a0\u00a0public void setNumStudents(long numStudents) {...}\n \u00a0\u00a0\u00a0public long getNumStudents() {...}\n \u00a0\u00a0\u00a0public void setYearFounded(long yearFounded) {...}\n \u00a0\u00a0\u00a0public long getYearFounded() {...}\n}\n\nclass BuildString implements MapFunction<University, String> {\n    public String call(University u) throws Exception {\n        return u.getName() + \" is \" + (2015 - u.getYearFounded()) + \" years old.\";\n    }\n}\n\nDataset<University> schools = context.read().json(\"/schools.json\").as(Encoders.bean(University.class));\nDataset<String> strings = schools.map(new BuildString(), Encoders.STRING());</pre>\n\n<h2>Looking Forward</h2>\nWhile Datasets are a new API, we have made them interoperate easily with RDDs and existing Spark programs. Simply calling the rdd<code>()</code> method on a Dataset will give an RDD. In the long run, we hope that Datasets can become a common way to work with structured data, and we may converge the APIs even further.\n\nAs we look forward to Spark 2.0, we plan\u00a0some exciting improvements to Datasets, specifically:\n<ul>\n\t<li>Performance optimizations - In many cases, the current implementation of the Dataset API does not yet leverage the additional information it has and can be slower than RDDs. Over the next several releases, we will be working on improving the performance of this new API.</li>\n\t<li>Custom encoders - while we currently autogenerate encoders for a wide variety of types, we'd like to open up an API for custom objects.</li>\n\t<li>Python Support.</li>\n\t<li>Unification of DataFrames with Datasets - due to compatibility guarantees, DataFrames and Datasets currently cannot share a common parent class. With Spark 2.0, we will be able to unify these abstractions with minor changes to the API, making it easy to build libraries that work with both.</li>\n</ul>\nIf you'd like to try out Datasets yourself, they are already available in Databricks. \u00a0We've put together a few example notebooks for you to try out: <a href=\"https://docs.cloud.databricks.com/docs/spark/1.6/index.html#examples/Dataset%20Classes.html\" target=\"_blank\">Working with Classes</a>, <a href=\"https://docs.cloud.databricks.com/docs/spark/1.6/index.html#examples/Dataset%20Wordcount.html\" target=\"_blank\">Word Count</a>.\n\nSpark 1.6 is available on Databricks today, <a href=\"https://accounts.cloud.databricks.com/registration.html#signup\" target=\"_blank\">sign up for a free 14-day trial</a>."}
{"status": "publish", "description": null, "creator": "matei", "link": "https://databricks.com/blog/2016/01/05/databricks-2015-year-in-review-democratizing-access-to-data.html", "authors": null, "id": 6008, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2016-01-05", "tz": "UTC", "createdOn": "2016-01-05"}, "title": "Databricks 2015 Year In Review: Democratizing Access to Data", "slug": "databricks-2015-year-in-review-democratizing-access-to-data", "content": "To learn more about Apache Spark, attend <a href=\"https://spark-summit.org/east-2016/\" target=\"_blank\">Spark Summit East in New York in Feb 2016</a>.\n\n<hr />\n\n2015 has been a phenomenal year of growth for both Databricks and the Apache Spark project. In June, we launched <a href=\"https://databricks.com/blog/2015/06/15/databricks-is-now-generally-available.html\" target=\"_blank\">general availability (GA)</a>\u00a0of our cloud platform, the first end-to-end enterprise data platform based on Spark. At the same time, we have continued our efforts in training Spark developers and of course in developing Spark itself. In this post, we wanted to share some updates about each of these efforts, and let you know what we\u2019ve been up to in 2015:\n<ol>\n\t<li>In the time since GA, Databricks has been adopted by over 200 paying customers, making it the Spark platform with the largest number of customers among any enterprise vendor.</li>\n\t<li>Databricks has trained over 20,000 Spark developers in 2015, again more than any other company.</li>\n\t<li>In 2015, we continued to be the largest contributor to the Apache Spark project, with 10x more code contributions than any other company.</li>\n</ol>\n&nbsp;\n<h2 style=\"text-align: center;\">Simplifying Enterprise Data Applications with Databricks</h2>\nWhen our team first designed Spark at UC Berkeley, we wanted to make writing big data applications easier. However, we realized that much more was needed to make big data simple for an organization: big data projects spend most of their effort managing infrastructure, loading data, and keeping production jobs running. This is why we developed Databricks, an <a href=\"https://databricks.com/product/databricks\" target=\"_blank\">end-to-end, managed platform</a> based on Spark. With Databricks, organizations can immediately start working on their data problems, in an environment accessible to data scientists, engineers, and business users alike.\n\nSince the <a href=\"https://databricks.com/blog/2015/06/15/databricks-is-now-generally-available.html\" target=\"_blank\">GA</a> of this platform in June, Databricks has been adopted by over 200 paying customers, with applications ranging from data warehousing and reporting to real-time machine learning. We have also learned a lot from our customers\u2019 use of the platform. In particular, we saw three interesting trends:\n<ul>\n\t<li><b>Ease of adoption.</b> Although big data has traditionally required specialized expertise, 40% of our customers indicate that they had never used Spark or Hadoop before deploying Databricks, and were still able to become proficient with the platform. Spark\u2019s approachability combined with automatic operations made Databricks easy to adopt for users only familiar with \u201csmall data\u201d.</li>\n\t<li><b>Democratization of data.</b> While most deployments begin with a small data science team, many have quickly grown to 50+ users, because the platform makes data access easy for non-experts. With Databricks, data scientists can publish <a href=\"https://databricks.com/product/databricks\" target=\"_blank\">notebooks or dashboards</a> that business users can use to explore the data curated by the team. This saves time for both parties and lets more users work with the company\u2019s data.</li>\n\t<li><b>Enterprise security enhancements</b>. We learned from our customers that with the ease of sharing and collaboration in notebooks, enterprise security concerns come to the forefront. With full support for <a href=\"https://databricks.com/blog/2015/08/05/databricks-2-0-leading-the-charge-to-democratize-data.html\" target=\"_blank\">role-based access control</a>, auditing, and encryption on wire and disk, sharing data in an organization is secure and easy.</li>\n</ul>\nTo give a sense of what organizations have been able to do with Databricks, some of our customer highlights in 2015 included:\n<table class=\"table\">\n<tbody>\n<tr>\n<td width=\"151\">\u00a0<img class=\"alignnone wp-image-6009\" src=\"https://databricks.com/wp-content/uploads/2016/01/customer_logo_elsevier.png\" alt=\"customer_logo_elsevier\" width=\"151\" height=\"73\" /></td>\n<td>With Databricks, Elsevier Labs \u2013 the advanced R&amp;D group within Elsevier, a global provider of scientific information \u2013 <a href=\"https://databricks.com/blog/2015/11/12/elsevier-labs-deploys-databricks-for-unified-content-analysis.html\" target=\"_blank\">completed advanced analytics projects faster (weeks to days) and broadened access to data (15 people contributing instead of limiting to two or three specialists)</a>.</td>\n</tr>\n<tr>\n<td width=\"151\"><img class=\"alignnone wp-image-6013\" src=\"https://databricks.com/wp-content/uploads/2016/01/customer_logo_myfitnesspal1.png\" alt=\"customer_logo_myfitnesspal1\" width=\"151\" height=\"73\" /></td>\n<td>MyFitnessPal\u2019s legacy data pipeline was slow, did not scale, and lacked flexibility. \u00a0<a href=\"https://databricks.com/wp-content/uploads/2015/07/Databricks_Case_Study_MyFitnessPal.pdf\" target=\"_blank\">Databricks helped them solve all of these challenges</a> with our automatically managed Spark clusters, interactive workspace, and a production job scheduler to easily transition from development to production.</td>\n</tr>\n<tr>\n<td width=\"151\"><img class=\"alignnone wp-image-6012\" src=\"https://databricks.com/wp-content/uploads/2016/01/customer_logo_celtra.png\" alt=\"customer_logo_celtra\" width=\"151\" height=\"73\" /></td>\n<td>Celtra expanded the number of people able to work with their data by a factor of four allowing them to <a href=\"https://databricks.com/blog/2015/04/15/celtra-scales-big-data-analysis-projects-six-fold-with-databricks-cloud.html\" target=\"_blank\">increase the amount of ad-hoc analysis done six-fold</a>. View the webinar <a href=\"http://go.databricks.com/how-celtra-optimizes-its-advertising-platform-with-databricks\" target=\"_blank\">How Celtra Optimizes its Advertising Platform with Databricks</a> to see how users across the organization use this data.</td>\n</tr>\n</tbody>\n</table>\n&nbsp;\n<h2 style=\"text-align: center;\">Training Data Scientists and Engineers</h2>\nAs developers at heart, a key part of our mission has also been to empower other professionals to tackle big data problems. We are happy to note that in 2015 we trained over 20,000 developers on Spark, more than any other company.\n\n<img class=\"aligncenter wp-image-6007\" src=\"https://databricks.com/wp-content/uploads/2016/01/Spark-Developers-Trained-Over-20K.png?noresize\" alt=\"Spark-Developers-Trained-Over-20K\" width=\"300\" height=\"263\" />\n\nSpark education was top of mind for us in 2015 with the launch of several key programs:\n<ol>\n\t<li><a href=\"https://databricks.com/spark/training\" target=\"_blank\"><b>Databricks private training program</b></a> providing on-site instructor-led training. Our trainers are certified by our core Spark development team, which also contributes directly to our courseware.</li>\n\t<li>We partnered with UC Berkeley and UCLA to launch two <a href=\"https://databricks.com/blog/2015/06/01/databricks-launches-mooc-data-science-on-spark.html\" target=\"_blank\"><b>massive online open courses (MOOCs)</b></a> on Spark. The first course, <a href=\"https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x\" target=\"_blank\">Introduction to Big Data with Apache Spark</a>, teaches students about Spark and data analysis. The second course, <a href=\"https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x\" target=\"_blank\">Scalable Machine Learning</a>, introduces students to machine learning with Spark. Both courses are freely available on the edX platform. Over 125,000 students registered for the first delivery of the two classes with 24% engaged and 12% passing <a href=\"https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x\" target=\"_blank\">Introduction to Big Data with Apache Spark</a> (this is 2.5 times greater than the <a href=\"https://www.insidehighered.com/news/2013/05/10/new-study-low-mooc-completion-rates\" target=\"_blank\">average MOOC completion rate</a>).</li>\n\t<li>The <a href=\"https://databricks.com/academic\" target=\"_blank\"><b>academic partners program</b></a> gives educators free access to Databricks for research and classroom use. We worked with universities across the world including Stanford, UC Berkeley, and UIUC, supporting Spark education for over 1200 students. In addition to supporting academic research, we have been publishing our own work at top research venues such as <a href=\"http://www.sigmod.org/\" target=\"_blank\">SIGMOD</a> and <a href=\"http://vldb.org/\" target=\"_blank\">VLDB</a>.</li>\n</ol>\n&nbsp;\n<h2 style=\"text-align: center;\">Spark Community Leadership</h2>\nOur 2015 <a href=\"https://databricks.com/blog/2015/09/24/spark-survey-results-2015-are-now-available.html\" target=\"_blank\">Spark Survey</a> results validate that our work to make Spark data processing easy and accessible is resonating with Spark users across many industries. Key findings from the survey included:\n<ul>\n\t<li><b>Spark is growing beyond Hadoop: </b>Only 40% of Spark users deploy it inside Hadoop\u00a0while 48% deploy it standalone and 11% on Apache Mesos. Whereas most Spark deployments had traditionally been in Hadoop, we now see cloud deployments (51%) and data sources other than Hadoop (e.g. Cassandra) become increasingly popular.</li>\n\t<li><b>Streaming and advanced analytics uses rising:</b> Spark is being used for an increasingly diverse set of applications, particularly machine learning, streaming, and graph analytics.</li>\n\t<li><b>Increasing access to big data: </b>Spark is breaking down technology barriers between data scientists, analysts, and engineers, who are working collaboratively to solve problems. In particular, we see the rapid growth of Spark use in languages like SQL and Python and through BI tools.</li>\n\t<li><b>Spark's most popular use cases came to light: </b>the most common use cases were business intelligence (68%), data warehousing (52% of organizations), recommendation engines (48%), log processing (40%), and fraud detection and security (29%).</li>\n</ul>\nAs the Spark community expands at an amazing pace (with 650 contributors in 2015 alone), Databricks has continued to be the largest contributor to the Apache Spark project, providing 10x more code than any other company. We consider the success of Spark one of our key missions, and to this end we have contributed to all areas of Spark in 2015. Some of our major contributions this year were:\n<ul>\n\t<li><a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\" target=\"_blank\">DataFrames</a>, an easy-to-use and efficient API for working with structured data similar to \u201csmall data\u201d tools like R.</li>\n\t<li><a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\" target=\"_blank\">Project Tungsten</a>, the largest update to date of Spark\u2019s internals to provide more efficient execution on modern hardware.</li>\n\t<li><a href=\"https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html\" target=\"_blank\">Machine learning pipelines</a>, an easy-to-use API for complete machine learning workflows.</li>\n\t<li>Multiple features in <a href=\"https://databricks.com/blog/2015/06/09/announcing-sparkr-r-on-spark.html\" target=\"_blank\">SparkR</a>, the new R language interface to Spark.</li>\n\t<li>New <a href=\"https://databricks.com/blog/2015/01/21/random-forests-and-boosting-in-mllib.html\" target=\"_blank\">advanced</a> <a href=\"https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html\" target=\"_blank\">analytics</a> <a href=\"https://databricks.com/blog/2015/09/22/large-scale-topic-modeling-improvements-to-lda-on-spark.html\" target=\"_blank\">algorithms</a>, <a href=\"https://databricks.com/blog/2015/03/30/improvements-to-kafka-integration-of-spark-streaming.html\" target=\"_blank\">data</a> <a href=\"https://databricks.com/blog/2015/02/02/an-introduction-to-json-support-in-spark-sql.html\" target=\"_blank\">sources</a>, and <a href=\"https://databricks.com/blog/2015/06/22/understanding-your-spark-application-through-visualization.html\" target=\"_blank\">monitoring</a> <a href=\"https://databricks.com/blog/2015/07/08/new-visualizations-for-understanding-spark-streaming-applications.html\" target=\"_blank\">tools</a>.</li>\n</ul>\nFor a deep dive on the major additions in 2015, please read Reynold Xin\u2019s blog post here: <a href=\"https://databricks.com/blog/2016/01/05/spark-2015-year-in-review.html\" target=\"_blank\">Spark 2015 Year \u00a0In Review</a>.\n\nBut it\u2019s not all about the code: nurturing the Spark community is also about bringing together users. To this end, we have brought together 4000 attendees through three <a href=\"https://spark-summit.org/\" target=\"_blank\">Spark Summits</a>, bringing the conference to Europe and New York for the first time. We have also contributed to dozens of local meetup groups with our <a href=\"https://databricks.com/blog/2015/11/19/meetup-in-a-box.html\" target=\"_blank\">Meetup-in-the-box initiative</a>. \u00a0We plan to expand both these initiatives in 2016.\n\nFinally, 2015 was also a significant year for our partners. We are happy to see IBM, Hortonworks, Intel, Cloudera, and MapR, just to name a few, investing significantly in Spark. We look forward to continuing our collaboration with them in 2016, to build a stronger Spark community.\n\n&nbsp;\n<h2 style=\"text-align: center;\">Conclusion</h2>\nWhile 2015 was exciting, we believe it is still only the beginning for both Databricks and Apache Spark. Our overall mission is to make big data simple, allowing every enterprise to gain value from its data. Our experience with Databricks customers so far shows that this is indeed possible: with a fully managed end-to-end platform, customers are completing projects in a fraction of the time it had taken with previous tools, and simultaneously making their data accessible to more users in their organization well beyond the \u201cbig data experts\u201d. \u00a0In 2016, we will continue to work with our customers, partners, and the Spark community to make extracting value from data even easier."}
{"status": "publish", "description": "A snapshot of our 2015 - launch of General Availability of enterprise data platform based on Spark, training over 20,000 spark developers and the largest contributor to Apache Spark.", "creator": "rxin", "link": "https://databricks.com/blog/2016/01/05/apache-spark-2015-year-in-review.html", "authors": null, "id": 6020, "categories": ["Apache Spark", "Ecosystem", "Engineering Blog"], "dates": {"publishedOn": "2016-01-05", "tz": "UTC", "createdOn": "2016-01-05"}, "title": "Apache Spark 2015 Year In Review", "slug": "apache-spark-2015-year-in-review", "content": "To learn more about Apache Spark, attend <a href=\"https://spark-summit.org/east-2016/\" target=\"_blank\">Spark Summit East in New York in Feb 2016</a>.\n\n<hr />\n\n2015 has been a year of tremendous growth for Apache Spark. The pace of development is the fastest ever. We went through 4 releases (Spark 1.3 to 1.6) in a single year, and each of them added hundreds of improvements.\n\nThe number of developers that have contributed code to Spark has also grown to over 1000, doubling from 500 in 2014. To the best of our knowledge, Spark is now the most actively developed open source project among data tools, big or small. We\u2019ve been humbled by Spark\u2019s growth and the community that has made Spark the project it is today.\n\nAt Databricks, we are still working hard to drive Spark forward -- indeed, we contributed about 10x more code to Spark than any organization in 2015. In this blog post, we wanted to highlight some of the major developments that went into the project in 2015:\n<ol>\n\t<li>APIs for data science, including DataFrames, Machine Learning Pipelines, and R support.</li>\n\t<li>Platform APIs</li>\n\t<li>Project Tungsten and Performance Optimizations</li>\n\t<li>Spark Streaming</li>\n</ol>\nWith this rapid pace of development, we are also happy to see how quickly users adopt new versions. For example, the graph below shows the Spark versions run by over 200 customers at Databricks (note that a single customer can also run multiple Spark versions):\n\n<img class=\"aligncenter wp-image-6003\" src=\"https://databricks.com/wp-content/uploads/2016/01/spark-version-trend-final-1024x512.png?noresize\" alt=\"spark-version-trend final\" width=\"600\" height=\"300\" />\n\nAs you can see from the graph, Spark users track the latest versions pretty well. Merely 3 months after Spark 1.5 was released, the majority of our customers were running it. And a small fraction have been using <a href=\"https://databricks.com/blog/2016/01/04/announcing-spark-1-6.html\" target=\"_blank\">Spark 1.6</a>, as it was available as a <a href=\"https://databricks.com/blog/2015/11/20/announcing-spark-1-6-preview-in-databricks.html\" target=\"_blank\">preview package</a> since late November.\n\nNow, let\u2019s dive into the major changes in 2015.\n<h2 style=\"text-align: center;\">Data Science APIs: DataFrames, ML Pipelines, and R</h2>\nBefore Spark, the primer to Big Data included a daunting list of concepts, from distributed computing to MapReduce functional programming. As a result, Big Data tooling was used primarily by data infrastructure teams with an advanced level of technical sophistication.\n\nThe first major theme of development for Spark in 2015 was to build simplified APIs for big data, similar to those for data science. Rather than forcing data scientists to learn a whole new development paradigm, we wanted to substantially lower the learning curve and provide something that resembles the tools they are already familiar with.\n\nTo that end, we introduced three major API additions to Spark.\n<ul>\n\t<li><a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\" target=\"_blank\"><b>DataFrames</b></a>: an easy-to-use and efficient API for working with structured data similar to \u201csmall data\u201d tools like R and Pandas in Python.</li>\n</ul>\n<ul>\n\t<li><a href=\"https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html\" target=\"_blank\"><b>Machine Learning Pipelines</b></a>: an easy-to-use API for complete machine learning workflows.</li>\n</ul>\n<ul>\n\t<li><a href=\"https://databricks.com/blog/2015/06/09/announcing-sparkr-r-on-spark.html\" target=\"_blank\"><b>SparkR</b></a>: Together with Python, R is the most popular programming language among data scientists. With a little learning, data scientists can now use R and Spark to process data that is larger than what their single machine can handle.</li>\n</ul>\nAlthough they have only been released for a few months, already 62% of Spark users reported via our\u00a0<a href=\"https://databricks.com/blog/2015/09/24/spark-survey-results-2015-are-now-available.html\" target=\"_blank\">2015 Spark Survey</a>\u00a0that they are using the DataFrame API.\u00a0 Just as revealing, \u00a0survey respondents had predominantly identified themselves as data engineers (41%) or data scientists (22%).\u00a0 This uptick in interest by data scientists is even more apparent when evaluating the languages used in Spark with 58% (49% increase over 2014) of respondents using Python and 18% already using the R API.\n\n<img class=\"aligncenter wp-image-6002\" src=\"https://databricks.com/wp-content/uploads/2016/01/Languages-Used-2015-1024x332.png?noresize\" alt=\"Languages-Used-2015\" width=\"599\" height=\"194\" />\n\nSince we released DataFrames, we have been collecting feedback from the community. One of the most important pieces of feedback is that for users building larger, more complicated data engineering projects, type safety provided by the traditional RDD API is a useful feature. In response to this, we are developing a <a href=\"https://databricks.com/blog/2016/01/04/introducing-spark-datasets.html\" target=\"_blank\">new typed Dataset API in Spark 1.6</a> for working with these kinds of data.\n<h2 style=\"text-align: center;\">Platform APIs</h2>\nTo application developers, Spark is becoming the universal runtime. Applications only need to program against a single set of APIs, and can then run in a variety of environments (on-prem, cloud, Hadoop, \u2026) and connect to a variety of data sources. Earlier this year, we introduced a standard <a href=\"https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html\" target=\"_blank\">pluggable data source API</a> for 3rd-party developers that enables intelligent pushdown of predicates into these sources. \u00a0Some of the data sources that are now available include:\n<ul>\n\t<li>CSV, JSON, XML</li>\n\t<li>Avro, Parquet</li>\n\t<li>MySQL, PostgreSQL, Oracle, Redshift</li>\n\t<li>Cassandra, MongoDB, ElasticSearch</li>\n\t<li>Salesforce, Google Spreadsheets</li>\n</ul>\n<img class=\"aligncenter wp-image-6019\" src=\"https://databricks.com/wp-content/uploads/2016/01/open-source-ecosystem-1024x576.png?noresize\" alt=\"open-source-ecosystem\" width=\"600\" height=\"337\" />\n\nTo make it easier to find libraries for data sources and algorithms, we have also introduced <a href=\"http://spark-packages.org\" target=\"_blank\">spark-packages.org</a>, a central repository for Spark libraries.\n\nAnother interesting trend is that while the early adopters of Spark were predominantly using Spark alongside Hadoop, as Spark grows, Hadoop no longer represents the majority of Spark users. According to our\u00a0<a href=\"https://databricks.com/blog/2015/09/24/spark-survey-results-2015-are-now-available.html\" target=\"_blank\">2015 Spark Survey</a>, 48% of Spark deployments now run on Spark\u2019s standalone cluster manager, while the usage of Hadoop YARN is only around 40%.\n<h2 style=\"text-align: center;\">Project Tungsten and Performance Optimizations</h2>\nAccording to our <a href=\"https://databricks.com/blog/2015/09/24/spark-survey-results-2015-are-now-available.html\" target=\"_blank\">2015 Spark Survey</a>, 91% of users consider performance the most important aspect of Spark. As a result, performance optimizations have always been a focus in our Spark development.\n\nEarlier this year we announced <a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\" target=\"_blank\">Project Tungsten</a> \u2013 a set of major changes to Spark\u2019s internal architecture designed to improve performance and robustness. \u00a0Spark 1.5 delivered the first pieces of Tungsten. This includes <b>binary processing</b>, which circumvents the Java object model using a custom binary memory layout. \u00a0Binary processing significantly reduces garbage collection pressure for data-intensive workloads. \u00a0It also includes a new <b>code generation</b> framework where optimized byte code is generated at runtime for evaluating expressions in user code. Throughout the 4 releases in 2015, we also added a large number of built-in functions that are code generated, for common tasks like date handling and string manipulation.\n\nIn addition, data ingest performance is as critical as query execution. Parquet has been one of the most commonly used data formats with Spark, and Parquet scan performance has a pretty big impact on many large applications. In <a href=\"https://databricks.com/blog/2016/01/04/announcing-spark-1-6.html\" target=\"_blank\">Spark 1.6</a>, we introduced a new Parquet reader that uses a more optimized code path for flat schemas. In our benchmarks, this new reader increases the scan throughput by almost 50%.\n<h2 style=\"text-align: center;\">Spark Streaming</h2>\nWith the rise of Internet of Things (IoT), an increasing number of organizations are looking into deploying streaming applications. Integrating these streaming applications with traditional pipelines is crucial, and Spark Streaming simplifies this with a <a href=\"https://databricks.com/blog/2015/07/30/diving-into-spark-streamings-execution-model.html\" target=\"_blank\">unified engine for both batch and streaming data processing</a>. \u00a0Some of the main additions to Spark Streaming in 2015 included:\n<ul>\n\t<li><a href=\"https://databricks.com/blog/2015/03/30/improvements-to-kafka-integration-of-spark-streaming.html\" target=\"_blank\"><b>Direct Kafka connector</b></a>: Spark 1.3 improves the integration with Kafka so streaming applications can provide exactly-once data processing semantics and simplify operations. <a href=\"https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html\" target=\"_blank\">Additional work</a> also improves fault-tolerance and ensures zero data loss.</li>\n\t<li><a href=\"https://databricks.com/blog/2015/07/08/new-visualizations-for-understanding-spark-streaming-applications.html\" target=\"_blank\"><b>Web UI for monitoring and easier debugging</b></a>: To help monitoring and debugging streaming applications that are often run 24/7, Spark 1.4 introduces a new web UI that displays timelines and histograms for processing trends, as well as details about each discretized streams.</li>\n\t<li><b>10X speedup for state management</b>: In Spark 1.6, we redesigned the state management API in Spark Streaming and introduced a new mapWithState API that scales linearly to the number of updates rather than the total number of records. This has resulted in an order of magnitude performance improvements in many workloads.</li>\n</ul>\n<h2 style=\"text-align: center;\">In Closing</h2>\nAnother aspect where Databricks invested heavily in is training and education for Spark users. In 2015, we partnered with UC Berkeley and UCLA and offered two MOOCs. The first course, <a href=\"https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x\" target=\"_blank\">Introduction to Big Data with Apache Spark</a>, teaches students about Spark and data analysis. The second course, <a href=\"https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x\" target=\"_blank\">Scalable Machine Learning</a>, introduces students to machine learning with Spark. Both courses are freely available on the edX platform. Over 125,000 students registered for the first delivery of the two classes, and we plan to offer them again this year.\n\nWe are proud of the progress we have made together with the community in one year\u00a0and are thrilled to continue working to bring even more great features to Spark. Stay tuned on our blog for new developments in 2016."}
{"status": "publish", "description": null, "creator": "pat.mcdonough", "link": "https://databricks.com/blog/2016/01/12/announcing-spark-essentials-for-the-public-sector-workshop.html", "authors": null, "id": 6057, "categories": ["Company Blog", "Events"], "dates": {"publishedOn": "2016-01-12", "tz": "UTC", "createdOn": "2016-01-12"}, "title": "Announcing Apache Spark Essentials for the Public Sector workshop", "slug": "announcing-spark-essentials-for-the-public-sector-workshop", "content": "We are proud to announce Apache Spark Essentials - the first in a series of free technical workshops tailored for the public sector. \u00a0Data scientists, engineers, and analysts who attend Session 1: Spark Essentials, will be introduced to the Spark platform and ecosystem. It provides a hands-on introduction of how to effectively use Spark\u2019s various processing engines and higher-level libraries to tackle a unified use case.\n\nSign up today at <a href=\"http://go.databricks.com/public-sector-workshop\"><b>Apache Spark for Public Sector workshop</b></a> as space is limited:\n<ul>\n\t<li>Date: Friday, January 29th</li>\n\t<li>Time: 9:00 a.m. to 4:00 p.m.</li>\n\t<li>Location: CACI Inc, 14360 Newbrook Dr, Chantilly, VA</li>\n</ul>\n<img class=\"aligncenter wp-image-6058\" src=\"https://databricks.com/wp-content/uploads/2016/01/Spark-Essentials-public-sector-1024x209.png\" alt=\"Spark-Essentials-public-sector\" width=\"652\" height=\"133\" />\n\nSubsequent sessions will follow, covering in-depth exercises on machine learning, real-time stream analysis, and graph processing.\n\n<b>Agenda</b>\n<ul>\n\t<li>Learn how to mix usage of different Spark engines for sophisticated analysis:\n<ul>\n\t<li>DataFrames + Spark SQL</li>\n\t<li>RDDs</li>\n\t<li>Spark Streaming</li>\n\t<li>MLlib (Machine Learning)</li>\n\t<li>GraphX</li>\n</ul>\n</li>\n\t<li>ETL to/from various data sources: S3, HDFS, Parquet, Hive, Cassandra, MySQL, MongoDB, neo4j, etc.</li>\n\t<li>Launch and monitor Spark clusters in the Amazon Cloud</li>\n\t<li>Understand Spark Architecture fundamentals</li>\n\t<li>Use the Spark UI to analyze the performance of a job</li>\n\t<li>Use various visualization tools (Databricks native, matplotlib, Google Charts, D3.js, etc) to surface insights</li>\n</ul>\nFor more information, check out <a href=\"http://cdn2.hubspot.net/hubfs/438089/Spark_Public_Sector_Workshop_150108.pdf?t=1452551948615\">Spark for the Public Sector: A Workshop Series from Databricks</a>."}
{"status": "publish", "description": null, "creator": "scott", "link": "https://databricks.com/blog/2016/01/13/spark-summit-east-2016-agenda-is-now-available.html", "authors": null, "id": 6064, "categories": ["Company Blog", "Events"], "dates": {"publishedOn": "2016-01-13", "tz": "UTC", "createdOn": "2016-01-13"}, "title": "Spark Summit East 2016 Agenda is now available", "slug": "spark-summit-east-2016-agenda-is-now-available", "content": "This February, join the Apache Spark community in New York City at the <a href=\"https://spark-summit.org/east-2016/venue/\" target=\"_blank\">New York Midtown Hilton</a> for the second annual <a href=\"https://spark-summit.org/east-2016/\" target=\"_blank\">Spark Summit East</a> on February 16th-18th! \u00a0\u00a0We are happy to announce that the community talks agenda has been finalized and you can find the full list of 60 community talks available at the <a href=\"https://spark-summit.org/east-2016/schedule/\" target=\"_blank\">Spark Summit East 2016 Schedule</a>. \u00a0Those looking to get hands-on with Spark are encouraged to sign up for one of our <a href=\"https://spark-summit.org/east-2016/spark-training/\" target=\"_blank\">Databricks Spark Training workshops</a>.\n\nThe phenomenal growth of the global Spark community is reflected in the development of the Spark Summit events. \u00a0In 2015, we had over 4000 combined attendees with Spark Summits in San Francisco, New York, and Amsterdam. \u00a0With the continued rapid growth, we are expecting 1800 attendees for the first Spark Summit of 2016!\n\nFor Spark Summit East 2016, we will continue to have a wide variety of content, catering to different interests. \u00a0We will continue to have our popular developer, data science, application and research tracks. \u00a0But this year, we will be introducing our new enterprise tracks featuring enterprise customer case studies from Bloomberg, Viacom, Comcast, Huawei, IBM, Microsoft, EMC, and Netflix. \u00a0We will also have sessions featuring analysts and thought leaders, Mike Gualtieri (Forrester), Tony Baer (Ovum), Nik Rouda (ESG), and Thomas Dinsmore (Big Analytics Blog).\n<h2 style=\"text-align: center;\">Session Highlights</h2>\nWe would like to thank everyone who submitted a presentation, and congratulate the selected community talk presenters. For both days, the conference sessions (February 17th and 18th) will kick off with keynotes from leaders of the Spark Community. \u00a0Leading off on Wednesday will be Matei Zaharia and on Thursday will be Reynold Xin.\n\nCheck out a sample of topics (see the full schedule <a href=\"https://spark-summit.org/east-2016/schedule/\" target=\"_blank\">here</a>):\n<ul>\n\t<li><a href=\"https://spark-summit.org/east-2016/events/distributed-time-travel-for-feature-generation/\" target=\"_blank\">Distributed Time Travel for Feature Generation</a>: \u00a0DB Tsai, Prasanna Padmanabhan, and Mohammed H. Taghavi (Netflix)</li>\n\t<li><a href=\"https://spark-summit.org/east-2016/events/using-spark-to-power-the-office-365-delve-organization-analytics/\" target=\"_blank\">Using Spark to Power the Office 365 Delve Organization Analytics</a>: Yi Wang and Paavany Jayanty (Microsoft)</li>\n\t<li><a href=\"https://spark-summit.org/east-2016/events/structuring-spark-dataframes-datasets-and-streaming/\" target=\"_blank\">Structuring Spark: DataFrames, Datasets, and Streaming</a>: Michael Armbrust (Databricks)</li>\n\t<li><a href=\"https://spark-summit.org/east-2016/events/petabyte-scale-anomaly-detection-using-r-spark/\" target=\"_blank\">Petabyte Scale Anomaly Detection Using R &amp; Spark</a>: Sridhar Alla and Kiran Muglurmath (Comcast)</li>\n\t<li><a href=\"https://spark-summit.org/east-2016/events/generalized-linear-models-in-spark-mllib-and-sparkr/\" target=\"_blank\">Generalized Linear Models in Spark MLlib and SparkR</a>: Xiangrui Meng (Databricks)</li>\n\t<li><a href=\"https://spark-summit.org/east-2016/events/implementing-near-realtime-datacenter-health-analytics-using-model-driven-vertex-centric-programming-on-spark-streaming-and-graphx/\" target=\"_blank\">Implementing Near-Realtime Datacenter Health Analytics using Model-driven Vertex-centric Programming on Spark Streaming and GraphX</a>: David Ohsie and Cheuk Lam (EMC)</li>\n\t<li><a href=\"https://spark-summit.org/east-2016/events/lambda-at-weather-scale/\" target=\"_blank\">Lambda at Weather Scale</a>: Robbie Strickland (The Weather Company)</li>\n\t<li><a href=\"https://spark-summit.org/east-2016/events/spark-at-bloomberg/\" target=\"_blank\">Spark at Bloomberg</a>: Sudarshan Kadambi and Partha Nageswaran (Bloomberg)</li>\n</ul>\n&nbsp;\n<h2 style=\"text-align: center;\">Training</h2>\nFor people who are interested in becoming Spark experts, there are three workshops that cater to different interests:\n<ul>\n\t<li><a href=\"https://spark-summit.org/east-2016/spark-training/#apache-spark-essentials\" target=\"_blank\">Apache Spark Essentials</a> will help you get productive with the core capabilities of Spark, as well as provide an overview and examples for some of Spark\u2019s more advanced features.</li>\n\t<li><a href=\"https://spark-summit.org/east-2016/spark-training/#data-science\" target=\"_blank\">Data Science with Apache Spark</a> will show how to use Apache Spark to perform exploratory data analysis (EDA), develop machine learning pipelines, and use the APIs and algorithms available in Spark ML and Spark MLlib. It is designed for software developers, data analysts, data engineers, and data scientists.</li>\n\t<li><a href=\"https://spark-summit.org/east-2016/spark-training/#explore-wikipedia-with-spark\" target=\"_blank\">Advanced: Exploring Wikipedia with Spark (Tackling a unified use case)</a>: The real power and value proposition of Apache Spark is in building a unified use case that combines ETL, batch analytics, real-time stream analysis, machine learning, graph processing, and visualizations. In class, we will explore various Wikipedia datasets while applying the ideal programming paradigm for each analysis.</li>\n</ul>\n&nbsp;\n<h2 style=\"text-align: center;\">How to get tickets</h2>\nTickets are <a href=\"http://www.prevalentdesignevents.com/sparksummit2016/east/registration.aspx?source=DBblog\" target=\"_blank\">available online now</a>, register before the tickets sell out! Use promo code \u201cDatabricks20\u201d to receive 20% off your registration fee.\n\n&nbsp;\n<h2 style=\"text-align: center;\">Thanks to Our Sponsors</h2>\nOur esteemed <a href=\"https://spark-summit.org/east-2016/sponsors/\" target=\"_blank\">sponsors</a> are instrumental in bringing Spark Summit East 2016 to life. You\u2019ve heard this before but without our sponsors, the Summits wouldn\u2019t happen."}
{"status": "publish", "description": null, "creator": "joseph", "link": "https://databricks.com/blog/2016/01/21/mllib-highlights-in-apache-spark-1-6.html", "authors": null, "id": 6078, "categories": ["Apache Spark", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2016-01-21", "tz": "UTC", "createdOn": "2016-01-21"}, "title": "MLlib Highlights in Apache Spark 1.6", "slug": "mllib-highlights-in-apache-spark-1-6", "content": "To learn more about Apache Spark, attend <a href=\"https://spark-summit.org/east-2016/\" target=\"_blank\">Spark Summit East in New York in Feb 2016</a>.\n\n<hr />\n\nWith the latest release, Apache Spark\u2019s Machine Learning library (MLlib) includes many improvements and new features. Users can now save and load ML Pipelines, use extended R and Python APIs, and run new ML algorithms. \u00a0This blog post highlights major developments in MLlib 1.6 and mentions the roadmap ahead.\n\nMany thanks to the 90+ contributors during this release, as well as the community for valuable feedback. \u00a0MLlib's continued success is due to your hard work!\n<h2 style=\"text-align: center;\">Pipeline persistence</h2>\nAfter training an ML model, users often need to deploy it in production on another cluster or system. \u00a0Model persistence allows users to save a model trained on one system and load that model on a separate production system. \u00a0With MLlib 1.6, models and even entire pipelines can be saved and loaded.\n\nThis new persistence functionality is part of the Pipelines API, which integrates with DataFrames and provides tools for constructing ML workflows. \u00a0To refresh on Pipelines, see our <a href=\"https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html\" target=\"_blank\">original blog post</a>. \u00a0With this latest release, a user can train a pipeline, save it, and reload exactly the same pipeline at a later time or on another Spark cluster. \u00a0Users can also persist untrained pipelines and individual models.\n\nSaving and loading a full pipeline may be done with single lines of code:\n\n[scala]\nval model = pipeline.fit(data)\nmodel.save(\u201cs3n://my-location/myModel\u201d)\nval sameModel = PipelineModel.load(\u201cs3n://my-location/myModel\u201d)\n[/scala]\n\nTo learn more, check out a <a href=\"http://go.databricks.com/hubfs/notebooks/ML/1.6/ML_Pipeline_Persistence.html\" target=\"_blank\">simple example in this notebook</a>. \u00a0Persistence is available in Scala and Java, and Python support will be added in the next release.\n\n&nbsp;\n<h2 style=\"text-align: center;\">New ML algorithms</h2>\nMLlib 1.6 adds several new ML algorithms for important application areas.\n<ul>\n\t<li><b>Survival analysis</b> has many applications, such as modeling and predicting customer churn. \u00a0(<i>How long will a customer stay with our product, and what can be done to increase that lifetime?</i>) \u00a0MLlib now has <a href=\"http://spark.apache.org/docs/latest/ml-classification-regression.html#survival-regression\" target=\"_blank\">log-linear models for survival analysis</a>.</li>\n\t<li><b></b></li>\n\t<li><b><b>Streaming hypothesis testing</b> </b>can be used for A/B testing to choose between models or to do canary testing of a new model. \u00a0We now provide <a href=\"http://spark.apache.org/docs/latest/mllib-statistics.html#streaming-significance-testing\" target=\"_blank\">testing using the Spark Streaming framework</a>.</li>\n\t<li></li>\n\t<li><b>Summary statistics</b> in DataFrames help users to quickly understand their data. \u00a0Spark 1.6 adds new statistics such as variance, standard deviation, correlations, skewness, and kurtosis.</li>\n\t<li><b>Bisecting k-means clustering</b> is an accelerated clustering algorithm, useful for identifying patterns and groups within unlabeled data. \u00a0<a href=\"http://spark.apache.org/docs/latest/mllib-clustering.html#bisecting-k-means\" target=\"_blank\">See an example here</a>.</li>\n\t<li><b>New feature transformers</b> in MLlib 1.6 include <a href=\"http://spark.apache.org/docs/latest/ml-features.html#chisqselector\" target=\"_blank\">ChiSqSelector</a> (feature selection), <a href=\"http://spark.apache.org/docs/latest/ml-features.html#quantilediscretizer\" target=\"_blank\">QuantileDiscretizer</a> (adaptive discretization of features), and <a href=\"http://spark.apache.org/docs/latest/ml-features.html#sqltransformer\" target=\"_blank\">SQLTransformer</a> (SQL operations within ML Pipelines).</li>\n</ul>\nSee the corresponding sections in the <a href=\"http://spark.apache.org/docs/latest/ml-guide.html\" target=\"_blank\">ML User Guide</a> for examples.\n\n&nbsp;\n<h2 style=\"text-align: center;\">ML in SparkR</h2>\nSpark 1.5 introduced MLlib in SparkR with Generalized Linear Models (GLMs) as described in the blog post <a href=\"https://databricks.com/blog/2015/10/05/generalized-linear-models-in-sparkr-and-r-formula-support-in-mllib.html\" target=\"_blank\">Generalized Linear Models in SparkR and R Formula Support in MLlib</a>. These efforts give R users access to distributed Machine Learning algorithms, using familiar APIs. \u00a0Spark 1.6 expands this functionality with two key features.\n\n<b>Model summary</b>: R users who inspect Spark Linear Regression models using <code>summary(model)</code> will see more statistics, including deviance residuals and coefficient standard errors, t-values, and p-values.\n\n<b>Feature interactions</b>: R users can build more expressive GLMs using feature interactions (using the R \u201c:\u201d operator).\n\nThe code snippet below demonstrates these new features; see the full example in <a href=\"http://go.databricks.com/hubfs/notebooks/ML/1.6/ML_Feature_Interactions.html\" target=\"_blank\">this notebook</a>.\n<pre># Create a Spark Dataframe from the \"iris\" dataset\ndf &lt;- createDataFrame(sqlContext, iris)\n\n# Fit a model using a new interaction feature created from base features.\nmodel2 &lt;- glm(Sepal_Length ~ Sepal_Width : Species, data = df, family = \"gaussian\")\n\n# Summarize the model, just as with native R models\nsummary(model2)\n\n$devianceResiduals\nMin \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Max\n-1.167923 1.523675\n\n$coefficients\n                               Estimate  Std. Error t value  Pr(&gt;|t|)    \n(Intercept)                    3.357888  0.3300034  10.17531 0           \nSepal_Width:Species_versicolor 0.9299109 0.1197601  7.764782 1.317835e-12\nSepal_Width:Species_virginica  1.084014  0.1116593  9.70823  0           \nSepal_Width:Species_setosa     0.4832626 0.09682807 4.990935 1.68765e-06 \n</pre>\nModel summary statistics and feature interactions in R formulae are also available from the Scala, Java, and Python APIs.\n\n&nbsp;\n<h2 style=\"text-align: center;\">Testable example code (for developers)</h2>\nFor developers, one of the most useful additions to MLlib 1.6 is testable example code. \u00a0The code snippets in the user guide can now be tested more easily, which helps to ensure examples do not break across Spark versions.\n\nSpecifically, pieces of code in the <a href=\"https://github.com/apache/spark/tree/master/examples/src/main\" target=\"_blank\">\u201cexamples\u201d folder</a> can be inserted into the user guide automatically. \u00a0This means that scripts can check to ensure that examples compile and run, rather than requiring developers to check the user guide examples manually. \u00a0Thanks to the <a href=\"https://issues.apache.org/jira/browse/SPARK-11337\" target=\"_blank\">many contributors</a> to this effort!\n\n&nbsp;\n<h2 style=\"text-align: center;\">Looking ahead</h2>\nWe hope to have more MLlib contributors than ever during the upcoming release cycle.\n\nGiven very positive feedback about ML Pipelines, we will continue to expand and improve upon this API. \u00a0Python support for Pipeline persistence is a top priority.\n\nGeneralized Linear Models (GLMs) and the R API are key features for data scientists. \u00a0In future releases, we plan to extend functionality with more models families and link functions, better model inspection, and more MLlib algorithms available in R.\n\nFor more details on the roadmap, please see the <a href=\"https://issues.apache.org/jira/browse/SPARK-12626\" target=\"_blank\">MLlib 2.0 Roadmap JIRA</a>. \u00a0We also recommend that users follow <a href=\"http://spark-packages.org/\" target=\"_blank\">Spark Packages</a>, where many new ML algorithms are hosted.\n\nWe always welcome new contributors! \u00a0To get started, check out the <a href=\"https://issues.apache.org/jira/browse/SPARK-12626\" target=\"_blank\">MLlib 2.0 Roadmap JIRA</a> and the <a href=\"https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark\" target=\"_blank\">Wiki on Contributing to Spark</a>.\n\n&nbsp;\n<h2 style=\"text-align: center;\">Learning more</h2>\nFor details on the MLlib 1.6 release, see the <a href=\"https://docs.cloud.databricks.com/docs/spark/1.6/index.html#Apache%20Spark%201.6%20Release%20Notes.html\" target=\"_blank\">release notes</a>. \u00a0For guides, examples, and API docs, see the <a href=\"http://spark.apache.org/docs/latest/mllib-guide.html\" target=\"_blank\">MLlib User Guide</a> and the <a href=\"http://spark.apache.org/docs/latest/ml-guide.html\" target=\"_blank\">Pipelines guide</a>.\n\nGood luck, and thanks for your continued support of MLlib!"}
{"status": "publish", "description": null, "creator": "denny", "link": "https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html", "authors": null, "id": 6117, "categories": ["Apache Spark", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2016-01-25", "tz": "UTC", "createdOn": "2016-01-25"}, "title": "Deep Learning with Apache Spark and TensorFlow", "slug": "deep-learning-with-apache-spark-and-tensorflow", "content": "To learn more about Apache Spark, attend <a href=\"https://spark-summit.org/east-2016/\" target=\"_blank\">Spark Summit East in New York in Feb 2016</a>.\n\n<hr />\n\nNeural networks have seen spectacular progress during the last few years and they are now the state of the art in image recognition and automated translation. \u00a0<a href=\"https://www.tensorflow.org/\" target=\"_blank\">TensorFlow</a> is a new framework released by Google for numerical computations and neural networks. In this blog post, we are going to demonstrate how to use TensorFlow and Spark together to train and apply deep learning models.\n\nYou might be wondering: what\u2019s Apache Spark\u2019s use here when most high-performance deep learning implementations are single-node only? To answer this question, we walk through two use cases and explain how you can use Spark and a cluster of machines to improve deep learning pipelines with TensorFlow:\n<ol>\n\t<li><b>Hyperparameter Tuning: </b>use Spark to find the best set of hyperparameters for neural network training, leading to 10X reduction in training time and 34% lower error rate.</li>\n\t<li><b>Deploying models at scale: </b>use Spark to apply a trained neural network model on a large amount of data.</li>\n</ol>\n<h2 style=\"text-align: center;\">Hyperparameter Tuning</h2>\nAn example of a deep learning machine learning (ML) technique is artificial neural networks. They take a complex input, such as an image or an audio recording, and then apply complex mathematical transforms on these signals. The output of this transform is a vector of numbers that is easier to manipulate by other ML algorithms. Artificial neural networks perform this transformation by mimicking the neurons in the visual cortex of the human brain (in a much-simplified form).\n\nJust as humans learn to interpret what they see, artificial neural networks need to be trained to recognize specific patterns that are \u2018interesting\u2019. For example, these can be simple patterns such as edges, circles, but they can be <a href=\"http://research.google.com/archive/unsupervised_icml2012.html\" target=\"_blank\">much more complicated</a>. Here, we are going to use a classical dataset put together by NIST and train a neural network to recognize these digits:\n\n<img class=\"aligncenter wp-image-6120 size-full\" src=\"https://databricks.com/wp-content/uploads/2016/01/image02.png\" alt=\"image02\" width=\"280\" height=\"280\" />\n\nThe TensorFlow library automates the creation of training algorithms for neural networks of various shapes and sizes. The actual process of building a neural network, however, is more complicated than just running some function on a dataset. There are typically a number of very important hyperparameters (configuration parameters in layman\u2019s terms) to set, which affects how the model is trained. Picking the right parameters leads to high performance, while bad parameters can lead to prolonged training and bad performance. In practice, machine learning practitioners rerun the same model multiple times with different hyperparameters in order to find the best set. This is a classical technique called hyperparameter tuning.\n\nWhen building a neural network, there are many important hyperparameters to choose carefully. For example:\n<ul>\n\t<li>Number of neurons in each layer: Too few neurons will reduce the expression power of the network, but too many will substantially increase the running time and return noisy estimates.</li>\n\t<li>Learning rate: If it is too high, the neural network will only focus on the last few samples seen and disregard all the experience accumulated before. If it is too low, it will take too long to reach a good state.</li>\n</ul>\nThe interesting thing here is that even though TensorFlow itself is not distributed, the hyperparameter tuning process is \u201cembarrassingly parallel\u201d and can be distributed using Spark. In this case, we can use Spark to broadcast the common elements such as data and model description, and then schedule the individual repetitive computations across a cluster of machines in a fault-tolerant manner.\n\n<img class=\"aligncenter wp-image-6122 size-full\" src=\"https://databricks.com/wp-content/uploads/2016/01/image04.png\" alt=\"image04\" width=\"680\" height=\"315\" />\n\nHow does using Spark improve the accuracy? The accuracy with the <a href=\"https://www.tensorflow.org/versions/master/tutorials/mnist/pros/index.html#train-and-evaluate-the-model\" target=\"_blank\">default set of hyperparameters</a> is 99.2%. Our best result with hyperparameter tuning has a 99.47% accuracy on the test set, which is a <b>34% reduction of the test error</b>. Distributing the computations scaled linearly with the number of nodes added to the cluster: using a 13-node cluster, we were able to train 13 models in parallel, which translates into a <b>7x speedup</b> compared to training the models one at a time on one machine. Here is a graph of the computation times (in seconds) with respect to the number of machines on the cluster:\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/01/Computation-Time-in-Seconds-by-Machines.png\"><img class=\"alignnone wp-image-6136 size-large\" src=\"https://databricks.com/wp-content/uploads/2016/01/Computation-Time-in-Seconds-by-Machines-1024x555.png\" alt=\"\" width=\"1024\" height=\"555\" /></a>\n\nMore important though, we get insights into the sensibility of the training procedure to various hyperparameters of training. For example, we plot the final test performance with respect to the learning rate, for different numbers of neurons:\n\n<img class=\"aligncenter wp-image-6121\" src=\"https://databricks.com/wp-content/uploads/2016/01/image03.png\" alt=\"image03\" width=\"600\" height=\"413\" />\n\nThis shows a typical tradeoff curve for neural networks:\n<ul>\n\t<li>The learning rate is critical: if it is too low, the neural network does not learn anything (high test error). If it is too high, the training process may oscillate randomly and even diverge in some configurations.</li>\n\t<li>The number of neurons is not as important for getting a good performance, and networks with many neurons are much more sensitive to the learning rate. This is Occam\u2019s Razor principle: simpler model tend to be \u201cgood enough\u201d for most purposes. If you have the time and resource to go after the missing 1% test error, you must be willing to invest a lot of resources in training, and to find the proper hyperparameters that will make the difference.</li>\n</ul>\nBy using a sparse sample of parameters, we can zero in on the most promising sets of parameters.\n<h2 style=\"text-align: center;\">How do I use it?</h2>\nSince TensorFlow can use all the cores on each worker, we only run one task at one time on each worker and we batch them together to limit contention. The TensorFlow library can be installed on Spark clusters as a regular Python library, following the <a href=\"https://www.tensorflow.org/get_started/os_setup.html\" target=\"_blank\">instructions on the TensorFlow website</a>. The following notebooks below show how to install TensorFlow and let users rerun the experiments of this blog post:\n<ul>\n\t<li><a href=\"http://go.databricks.com/hubfs/notebooks/TensorFlow/Distributed_processing_of_images_using_TensorFlow.html\" target=\"_blank\">Distributed processing of images using TensorFlow</a></li>\n\t<li><a href=\"http://go.databricks.com/hubfs/notebooks/TensorFlow/Test_distributed_processing_of_images_using_TensorFlow.html\" target=\"_blank\">Testing the distribution\u00a0processing of images using TensorFlow</a></li>\n</ul>\n<h2 style=\"text-align: center;\">Deploying Models at Scale</h2>\nTensorFlow models can directly be embedded within pipelines to perform complex recognition tasks on datasets. As an example, we show how we can label a set of images from a <a href=\"http://arxiv.org/abs/1512.00567\" target=\"_blank\">stock neural network model that was already trained</a>.\n\nThe model is first distributed to the workers of the clusters, using Spark\u2019s built-in broadcasting mechanism:\n\n[scala]\nwith gfile.FastGFile( 'classify_image_graph_def.pb', 'rb') as f:\n  model_data = f.read()\nmodel_data_bc = sc.broadcast(model_data)\n[/scala]\n\nThen this model is loaded on each node and applied to images. This is a sketch of the code being run on each node:\n\n[scala]\ndef apply_batch(image_url):\n  # Creates a new TensorFlow graph of computation and imports the model\n  with tf.Graph().as_default() as g:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(model_data_bc.value)\n    tf.import_graph_def(graph_def, name='')\n    \n    # Loads the image data from the URL:\n    image_data = urllib.request.urlopen(img_url, timeout=1.0).read()\n    \n    # Runs a tensor flow session that loads the \n    with tf.Session() as sess:\n      softmax_tensor = sess.graph.get_tensor_by_name('softmax:0')\n      predictions = sess.run(softmax_tensor, {'DecodeJpeg/contents:0': image_data})\n      return predictions\n[/scala]\n\nThis code can be made more efficient by batching the images together.\n\nHere is an example of image:\n\n<img class=\"aligncenter wp-image-6119 size-full\" src=\"https://databricks.com/wp-content/uploads/2016/01/image01.jpg\" alt=\"image01\" width=\"405\" height=\"540\" />\n\nAnd here is the interpretation of this image according to the neural network, which\u00a0is pretty accurate:\n\n[scala]\n('coral reef', 0.88503921),\n   ('scuba diver', 0.025853464),\n   ('brain coral', 0.0090828091),\n   ('snorkel', 0.0036010914),\n   ('promontory, headland, head, foreland', 0.0022605944)])\n[/scala]\n\n<h2 style=\"text-align: center;\">Looking forward</h2>\nWe have shown how to combine Spark and TensorFlow to train and deploy neural networks on handwritten digit recognition and image labeling. Even though the neural network framework we used itself only works in a single-node, we can use Spark to distribute the hyperparameter tuning process and model deployment. This not only cuts down the training time\u00a0but also improves accuracy and gives us a better understanding of various hyperparameters\u2019 sensibility.\n\nWhile this support is only available on Python, we look forward to providing deeper integration between TensorFlow and the rest of the Spark framework.\n\n&nbsp;"}
{"status": "publish", "description": null, "creator": "tdas", "link": "https://databricks.com/blog/2016/02/01/faster-stateful-stream-processing-in-apache-spark-streaming.html", "authors": null, "id": 6143, "categories": ["Apache Spark", "Engineering Blog", "Streaming"], "dates": {"publishedOn": "2016-02-01", "tz": "UTC", "createdOn": "2016-02-01"}, "title": "Faster Stateful Stream Processing in Apache Spark Streaming", "slug": "faster-stateful-stream-processing-in-apache-spark-streaming", "content": "To learn the latest developments in Apache Spark, <a href=\"https://spark-summit.org/east-2016/\" target=\"_blank\">register today</a> to join the Spark community at Spark Summit in New York City!\n\n<hr />\n\nMany complex stream processing pipelines must maintain state across a period of time. For example, if you are interested in understanding user behavior on your website in real-time, you will have to maintain information about each \u201cuser session\u201d on the website as a persistent state and continuously update this state based on the user's actions. Such stateful streaming computations could be implemented in Spark Streaming using its <code>updateStateByKey</code> operation.\n\nIn Apache Spark 1.6, we have dramatically improved our support for stateful stream processing with a new API, <code>mapWithState</code>. The new API has built-in support for the common patterns that previously required hand-coding and optimization when using <code>updateStateByKey</code> (e.g. sessions timeouts). As a result, <code>mapWithState</code> can provide up to 10x higher performance when compared to <code>updateStateByKey</code>\u00a0. In this blog post, we are going to explain <code>mapWithState</code> in more detail as well as give a sneak peek of what is coming in the next few releases.\n<h2>Stateful Stream Processing with mapWithState</h2>\nOne of the most powerful features of Spark Streaming is the simple API for stateful stream processing and the associated native, fault-tolerant, state management. Developers only have to specify the structure of the state and the logic to update it, and Spark Streaming takes care of distributing the state in the cluster, managing it, transparently recovering from failures, and giving end-to-end fault-tolerance guarantees. While the existing DStream operation <code>updateStateByKey</code> allows users to perform such stateful computations, with the new <code>mapWithState</code> operation we have made it easier for users to express their logic and get up to 10x higher performance. Let\u2019s illustrate these advantages using an example.\n\nLet\u2019s say we want to learn about user behavior on a website in real time by monitoring the history of their actions. For each user, we need to maintain a history of user actions. Furthermore, based on this history, we want to output the user\u2019s behavior model to a downstream data store.\n\nTo build this application with Spark Streaming, we have to get a stream of user actions as input (say, from Kafka or Kinesis), \u00a0transform it using <code>mapWithState</code> to generate the stream of user models, and then push them to the data store.\n\n<img class=\"aligncenter wp-image-6145\" src=\"https://databricks.com/wp-content/uploads/2016/01/blog-faster-stateful-streaming-figure-1-1024x562.png\" alt=\"blog faster stateful streaming figure 1\" width=\"600\" height=\"329\" />\n<p style=\"text-align: center;\"><i>Maintain user sessions with stateful stream processing in Spark Streaming</i></p>\nThe <code>mapWithState</code> operation has the following abstraction. Imagine it to be an operator that takes a user action and the current user session as the input. Based on an input action, the operator can choose to update the user session and then output the updated user model for downstream operations. The developer specifies this updating function when defining the <code>mapWithState</code> operation.\n\nTurning this into code, we start by defining the state data structure and the function to update the state.\n\n[scala]\ndef stateUpdateFunction(\nuserId: UserId,\nnewData: UserAction, \nstateData: State[UserSession]): UserModel = {\n\n    val currentSession = stateData.get()\t// Get current session data\n    val updatedSession = ... \t\t\t// Compute updated session using newData\n    stateData.update(updatedSession)   \t        // Update session data     \n\n    val userModel = ...         \t\t// Compute model using updatedSession\n    return userModel\t\t\t\t// Send model downstream\n}\n[/scala]\n\nThen, we define the <code>mapWithState</code> operation on a DStream of user actions. This is done by creating a <code>StateSpec</code> object which contains all the specification of the operation.\n\n[scala]\n// Stream of user actions, keyed by the user ID\nval userActions = ...  // stream of key-value tuples of (UserId, UserAction)\n\n// Stream of data to commit\nval userModels = userActions.mapWithState(StateSpec.function(stateUpdateFunction))\n[/scala]\n\n<h2>New Features and Performance Improvements with mapWithState</h2>\nNow that we have seen at an example of its use, let\u2019s dive into the specific advantages of using this new API.\n<h3>Native support for session timeouts</h3>\nMany session-based applications\u00a0require timeouts, where a session should be closed if it has not received new data for a while (e.g., the user left the session without explicitly logging out). Instead of hand-coding it in <code>updateStateByKey</code>, developers can directly set timeouts in <code>mapWithState</code>.\n\n[scala]\nuserActions.mapWithState(StateSpec.function(stateUpdateFunction).timeout(Minutes(10)))\n[/scala]\n\nBesides timeouts, developers can also set partitioning schemes and initial state information for bootstrapping.\n<h3>Arbitrary data can be sent downstream</h3>\nUnlike <code>updateStateByKey</code>, arbitrary data can be sent downstream from the state update function, as already illustrated in the example above (i.e. the user model returned from the user session state). Furthermore, snapshots of the up-to-date state (i.e. user sessions) can also be accessed.\n\n[scala]\nval userSessionSnapshots = userActions.mapWithState(statSpec).snapshotStream()\n[/scala]\n\nThe <code>userSessionSnapshots</code> is a DStream where each RDD is a snapshot of updated sessions after each batch of data is processed. This DStream is equivalent to the DStream returned by <code>updateStateByKey</code>.\n<h3>Higher performance</h3>\nFinally, <code>mapWithState</code> can provide 6X lower latency and maintain state for 10X more keys than when using <code>updateStateByKey</code>. This increase in performance and scalability is demonstrated by \u00a0the following benchmark results. All these results were generated with 1-second batches and the same cluster size. The following graph compares the average time taken to process each 1-second batch when using <code>mapWithState</code> and <code>updateStateByKey</code>. In each case, we maintained the state for the same number of keys (from 0.25 to 1 million\u00a0keys), and updated them at the same rate (30k updates / sec). As shown below, <code>mapWithState</code> can provide up to 8X lower processing times than <code>updateStateByKey</code>, therefore allowing lower end-to-end latencies.\n\n<img class=\"wp-image-6148 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2016/01/blog-faster-stateful-streaming-figure-2-1024x489.png\" alt=\"blog faster stateful streaming figure 2\" width=\"600\" height=\"287\" />\n<p style=\"text-align: center;\"><i>Up to 8X lower batch processing times (i.e.latency) with mapWithState than updateStateByKey</i></p>\nFurthermore, faster processing allows <code>mapWithState</code> to manage 10X more keys compared with <code>updateStateByKey</code> (with the same batch interval, cluster size, update rate in both cases).\n\n<img class=\"aligncenter wp-image-6150\" src=\"https://databricks.com/wp-content/uploads/2016/01/blog-faster-stateful-streaming-figure-3-1024x479.png\" alt=\"blog faster stateful streaming figure 3\" width=\"600\" height=\"281\" />\n<p style=\"text-align: center;\"><i>Up to 10X more keys in state with mapWithState than updateStateByKey</i></p>\nThis dramatic improvement is achieved by avoiding unnecessary processing for keys where no new data has arrived. Limiting the computation to keys with new data reduces the amount of processing time for each batch, allowing lower latencies and higher number of keys to be maintained.\n<h2>Other Improvements in Spark Streaming</h2>\nBesides <code>mapWithState</code>, there are a number of additional improvements in Spark Streaming in Spark 1.6. Some of them are as follows:\n<ul>\n\t<li>Streaming UI improvements [<a href=\"https://issues.apache.org/jira/browse/SPARK-10885\">SPARK-10885</a>, <a href=\"https://issues.apache.org/jira/browse/SPARK-11742\">SPARK-11742</a>]: Job failures and other details have been exposed in the streaming UI for easier debugging.</li>\n</ul>\n<ul>\n\t<li>API improvements in Kinesis integration [<a href=\"https://issues.apache.org/jira/browse/SPARK-11198\">SPARK-11198</a>, <a href=\"https://issues.apache.org/jira/browse/SPARK-10891\">SPARK-10891</a>]: Kinesis streams have been upgraded to use KCL 1.4.0 and support\u00a0transparent de-aggregation of KPL-aggregated records. In addition, arbitrary function can now be applied to a Kinesis record in the Kinesis receiver before to customize what data is to be stored in memory</li>\n</ul>\n<ul>\n\t<li>Python Streaming Listener API [<a href=\"https://issues.apache.org/jira/browse/SPARK-6328\">SPARK-6328</a>] - Get streaming statistics (scheduling delays, batch processing times, etc.) in streaming.</li>\n</ul>\n<ul>\n\t<li>Support for S3 for writing Write Ahead Logs (WALs) [<a href=\"https://issues.apache.org/jira/browse/SPARK-11324\">SPARK-11324</a>, <a href=\"https://issues.apache.org/jira/browse/SPARK-11141\">SPARK-11141</a>]: Write Ahead Logs are used by Spark Streaming to ensure fault-tolerance of received data. Spark 1.6 allows WALs to be used on S3 and other file systems that do not support file flushes. See the <a href=\"http://spark.apache.org/docs/latest/streaming-programming-guide.html#deploying-applications\">programming guide</a> for more details.</li>\n</ul>\n<p style=\"text-align: left;\">If you want to try out these new features, you can already use Spark 1.6 in Databricks, alongside older versions of Spark. \u00a0<a href=\"https://accounts.cloud.databricks.com/registration.html#signup\">Sign up for a free trial account here</a>.</p>"}
{"status": "publish", "description": "Just-in-time Data Warehousing and Databricks for Advertising Analytics", "creator": "grega", "link": "https://databricks.com/blog/2016/02/02/an-illustrated-guide-to-advertising-analytics.html", "authors": null, "id": 6171, "categories": ["Company Blog", "Customers", "Product"], "dates": {"publishedOn": "2016-02-03", "tz": "UTC", "createdOn": "2016-02-03"}, "title": "An Illustrated Guide to Advertising Analytics", "slug": "an-illustrated-guide-to-advertising-analytics", "content": "To learn the latest developments in Apache Spark, <a href=\"https://spark-summit.org/east-2016/\" target=\"_blank\">register today</a> to join the Spark community at Spark Summit in New York City!\n\n<hr />\n\n<em>\nThis is a joint blog with our friend at Celtra. Grega Ke\u0161pret is the Director of Engineering. He leads a team of engineers and data scientists to build analytics pipeline and optimization systems for Celtra.</em>\n\n---\n\nAdvertising technology companies that want to analyze their immense stores and varieties of data require a scalable, extensible, and elastic platform. \u00a0With Databricks, Celtra was able to scale their Big Data analysis projects six-fold, leading to better-informed product design and quicker issue detection and resolution.\n\n<a href=\"http://www.celtra.com/\" target=\"_blank\">Celtra</a> provides agencies, media suppliers, and brand leaders alike with an integrated, scalable HTML5 technology for brand advertising on smartphones, tablets, and desktop. The platform, AdCreator 4, gives clients such as MEC, Kargo, Pepsi, and Macy\u2019s the ability to easily create, manage, and traffic sophisticated data-driven dynamic ads, optimize them on the go, and track their performance with insightful analytics.\n\n&nbsp;\n<h2 style=\"text-align: center;\">Advertising Analytics Challenges</h2>\nLike most advertising platforms, Celtra needed the ability to go far beyond calculations provided by precomputed aggregations (e.g. canned reports). \u00a0They also needed:\n<ul>\n\t<li>the flexibility to perform uniques, order statistics, and other metrics outside the boundaries of existing pre-designed data models;</li>\n\t<li>to combine their metric calculations with visualizations to more quickly and better understand their data;</li>\n\t<li>and to have short development cycles so they could experiment with different analysis much more quickly.</li>\n</ul>\nTo complicate matters further, Celtra's\u00a0data sources are diverse involving a wide variety of creative capabilities within a complex ecosystem (e.g. high cardinality). \u00a0With analysis focused on consumer engagement with their clients\u2019 ads, Celtra was constantly exploring new ways to leverage this information to improve their data products.\n\nIn the past, Celtra's\u00a0original environments had the following issues:\n<ul>\n\t<li>they were dealing with complex setup and configurations that required their data scientists to focus on infrastructure work instead of focusing their data problems;</li>\n\t<li>there\u00a0was\u00a0a limited number of people working on their solution resulting in all of their big data analysis being bottlenecked with their analytics engineering team;</li>\n\t<li>and the lack of a collaborative environment resulted in analyses that were not reproducible nor repeatable.</li>\n</ul>\n&nbsp;\n<h2 style=\"text-align: center;\">Data Sciences and Simplified Operations with Databricks</h2>\nWith Databricks, Celtra was able to address the challenges above and:\n<ul>\n\t<li>They reduced the load on their analytics engineering team by expanding access to the number of people able to work with the data directly by a factor of four.</li>\n\t<li>Allowed their teams to effortlessly manage their Apache Spark clusters and managed issues ranging from high availability to the optimized setups and configurations within AWS.</li>\n\t<li>This increased the amount of ad-hoc analysis done six-fold, leading to better-informed product design and quicker issue detection and resolution.</li>\n\t<li>With Databricks' integrated workspace, they were able to quickly build notebooks with visualizations that increased collaboration and improved reproducibility and repeatability of analysis.</li>\n</ul>\nWhile Spark allows you to solve a wide variety of data problems with multiple languages in a scalable, elastic, distributed environment; Databricks simplified operations and reduced the need for dedicated personnel to maintain the infrastructure.\n\n&nbsp;\n<h2 style=\"text-align: center;\">Why Spark for Event Analytics</h2>\nApache Spark\u2122 is a powerful open-source processing engine built around speed, ease of use, and sophisticated analytics. Spark comes packaged with support for ETL, interactive queries (SQL), advanced analytics (e.g. machine learning), and streaming over large datasets.\n\n<img class=\"aligncenter wp-image-6174\" src=\"https://databricks.com/wp-content/uploads/2016/02/Spark-Stack-1024x474.png\" alt=\"Spark-Stack\" width=\"600\" height=\"278\" />\n\nIn addition to being scalable and fault tolerant, Spark allows you to program in your language of choice including Python, Java, Scala, R, and SQL. \u00a0For Celtra and other advertising customers, Spark provides distinct benefits for AdTech and event analytics including:\n<ul>\n\t<li>the ability to solve\u00a0multiple data problems (e.g. streaming, machine learning, and analytics) using the same data platform;</li>\n\t<li>access to an expressive computation layer;</li>\n\t<li>a fast pace of innovation (with 1000 total code contributors in 2015);</li>\n\t<li>and seamless integration with S3.</li>\n</ul>\nParticularly convenient is the ability to do event sessionization with a simple yet powerful API such as the code below:\n\n[scala]\ndef analyze(events: RDD[Event]): RDD[Session] = {\n   events\n      .keyBy(_.sessionId)\n      .groupByKey(groupTasks)\n      .values()\n      .flatMap(computeSession)\n}\n[/scala]\n\n&nbsp;\n\n<hr />\n\nFor more information, please check out the webinar\u00a0<a href=\"http://go.databricks.com/how-celtra-optimizes-its-advertising-platform-with-databricks\" target=\"_blank\">How Celtra Optimizes its Advertising Platform with Databricks</a>\n\n<hr />\n\n&nbsp;\n<h2 style=\"text-align: center;\">Making Sense of your Advertising Data</h2>\nTo make sense of your advertising weblog data, log into Databricks and you will immediately be able to begin working with a Databricks notebook. \u00a0Our notebooks provide much more than just data visualization, they also support multiple languages (R, Python, Scala, SQL, and Markdown), mixing languages within the same notebook, versioning with GitHub, real-time collaboration, one-click to production (the ability to execute a notebook as a separate scheduled job), and the ability to export notebooks in multiple archive formats including HTML.\n\nIn this example, we will perform the following tasks:\n<ol>\n\t<li>Create an external table against a large amount of web access logs including the use of a regular expression to parse a series of logs.</li>\n\t<li>Identify each visitor's\u00a0country (ISO-3166-1 three-letter\u00a0ISO country\u00a0code) based on IP address by calling a REST Web service API.</li>\n\t<li>Identify each visitor's\u00a0browser and OS information based on their User-Agent string using the user-agents PyPI package.</li>\n\t<li>Convert the Apache web logs date information, create a userid, and join back to the browser and OS information.</li>\n</ol>\n&nbsp;\n<h3>Step 1: Making sense of the\u00a0access logs</h3>\nThe primary data source for advertising is an Apache web access log. Below is a sample row from one of those logs.\n\n[scala]\n10.109.100.123 - - [04/Dec/2015:08:15:00 +0000]  &quot;GET /company/info HTTP/1.1&quot; 200 8572 &quot;https://databricks.com/&quot; &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.80 Safari/537.36&quot; 0.304 &quot;11.111.111.111, 22.222.222.2222, 33.111.111.111, 44.111.111.111&quot;\n[/scala]\n\nTraditionally, to make sense of this data, developers would need to build custom ETL processes to provide structure against this data (i.e. convert it into a table). \u00a0With Spark, instead of spending a lot of resources to make sense of the Apache access log\u00a0format, you can define an external table using regular expressions to parse your weblogs stored within S3.\n\n[scala]\nCREATE EXTERNAL TABLE accesslog ( \n  ipaddress STRING,\n  ...\n)\nROW FORMAT\n  SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'\nWITH SERDEPROPERTIES ( \n  &quot;input.regex&quot; = '^(\\\\S+) (\\\\S+) (\\\\S+) \\\\[([\\\\w:/]+\\\\s[+\\\\-]\\\\d{4})\\\\]  \\\\&quot;(\\\\S+) (\\\\S+) (\\\\S+)\\\\&quot; (\\\\d{3}) (\\\\d+) \\\\&quot;(.*)\\\\&quot; \\\\&quot;(.*)\\\\&quot; (\\\\S+) \\\\&quot;(\\\\S+), (\\\\S+), (\\\\S+), (\\\\S+)\\\\&quot;'\n)\nLOCATION\n  &quot;/mnt/mdl/accesslogs/&quot;\n[/scala]\n\nWith the creation of this table, you can execute a Spark SQL query against this data similar to how you would query any other structured data source. \u00a0Note, the underlying source of this external table is still your log files that you had stored in S3.\n<img class=\"aligncenter wp-image-6176\" src=\"https://databricks.com/wp-content/uploads/2016/02/Adtech-AccessLog-Cell-1024x416.png\" alt=\"Adtech-AccessLog-Cell\" width=\"650\" height=\"264\" />\n\nIn\u00a0addition to Spark\u2019s in-memory computing, Databricks makes use of the blazingly fast SSD-backed EC2 R3 instances to provide both in-memory and file caching for faster processing and querying. \u00a0Prior to creating your table, you can create a Databricks File System mount (as denoted by /mnt in the above Spark SQL code) by following the <a href=\"https://databricks.com/wp-content/uploads/2015/08/Databricks-how-to-data-import.pdf\" target=\"_blank\">Databricks Data Import How-To Guide</a> to leverage both SSDs and Tachyon in-memory file system.\n\n&nbsp;\n<h3>Step 2: Identify Geo-location information based on IP address</h3>\nOften included within weblogs are the client IP addresses that can potentially provide you the approximate location of the site visitor. While multiple methods exist to convert IP address to geolocation information, a quick (non-production) way to do this is to make use of an external web service such as <a href=\"http://freegeoip.net/\" target=\"_blank\">http://freegeoip.net/</a>. \u00a0In the sample code below, we are making a web service call directly from Spark:\n\n[python]\n# Obtain the unique agents from the accesslog table\nipaddresses = sqlContext.sql(&quot;select distinct ip1 from \\\n accesslog where ip1 is not null&quot;).rdd\n\n# getCCA2: Obtains two letter country code based on IP address\ndef getCCA2(ip): \n  url = 'http://freegeoip.net/csv/' + ip\n  str = urllib2.urlopen(url).read() \n  return str.split(&quot;,&quot;)[1]\n\n# Loop through distinct IP addresses and obtain two-letter country codes\nmappedIPs = ipaddresses.map(lambda x: (x[0], getCCA2(x[0])))\n[/python]\n\nUsing Python and Spark, the first line of code creates the\u00a0<code>ipaddresses</code>\u00a0RDD which uses a <code>sqlContext</code> to store the distinct IP addresses from the\u00a0<code>accesslog</code>\u00a0 external table (based on the access log\u00a0data stored on S3). \u00a0The subsequent <code>getCCA2</code> function is the call to the <a href=\"http://freegeoip.net\" target=\"_blank\">http://freegeoip.net</a> web service which receives an IP address and returns (in this case) a comma-delimited message containing the geo-location information. \u00a0The final call of this code snippet is a map function which allows Spark to loop through all of the unique IP addresses stored within\u00a0<code>ipaddresses</code>\u00a0RDD and make the web service call defined by\u00a0<code>getCCA2</code>. \u00a0This is similar to a\u00a0<code>for</code> loop, except this workload is partitioned and distributed to many nodes in your Spark cluster to be completed in parallel.\n\n&nbsp;\n<h3>Step 3: Making sense of Browser and OS information</h3>\nThe user agent string (or agent) is part of a browser (or client) header when visiting a web page. This header often contains interesting information such as what browser, operating system, or device that users\u00a0are using to view your ads. For example, the on-demand webinar <a href=\"http://go.databricks.com/how-celtra-optimizes-its-advertising-platform-with-databricks\" target=\"_blank\">How Celtra Optimizes its Advertising Platform with Databricks</a> discussed how Celtra was able to troubleshoot different devices viewing their ads by analyzing their web and troubleshooting logs.\n\nBelow is an example user agent string which reveals an\u00a0operating system (Mac OSX 10.11 El Capitan) and browser (Google Chrome 46.0.2490.80).\n\n[scala]\nMozilla/5.0 (Macintosh; Intel Mac OS X 10_11_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.80 Safari/537.36\n[/scala]\n\nInstead of spending time writing custom\u00a0code to parse this string, Databricks is extensible and allows you to include external packages such as the\u00a0<a href=\"https://pypi.python.org/pypi/user-agents\" target=\"_blank\">user-agents</a>\u00a0via PyPI. \u00a0Together with PySpark\u00a0UDFs, you can add columns to an existing Spark DataFrame combining a python function and Spark SQL.\n\n[scala]\nfrom user_agents import parse\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql.functions import udf\n\n# Create UDFs to extract out Browser Family information\ndef browserFamily(ua_string) : return parse(ua_string).browser.family\nudfBrowserFamily = udf(browserFamily, StringType())\n\n# Obtain the unique agents from the accesslog table\nuserAgentTbl = sqlContext.sql(&quot;select distinct agent from accesslog&quot;)\n\n# Add new columns to the UserAgentInfo DataFrame containing browser information\nuserAgentInfo = userAgentTbl.withColumn('browserFamily', \\\n   udfBrowserFamily(userAgentTbl.agent))\n\n# Register the DataFrame as a table\nuserAgentInfo.registerTempTable(&quot;UserAgentInfo&quot;)\n[/scala]\n\nThe <code>browserFamily</code> function utilizes the user-agents PyPI package parse function which takes an user agent string input and returns the browser Family information (e.g. Firefox, Safari, Chrome, etc.). \u00a0The subsequent <code>udfBrowserFamily</code> UDF defines the output of the <code>browserFamily</code> function as <code>StringType()</code> so it can be properly internalized within a subsequent DataFrame.\n\nThe <code>userAgentTbl</code> is a Spark DataFrame that contains the unique agents from the <code>accesslog</code> table from Step 1. \u00a0To add the browser family information as defined by the agent string, the new <code>UserAgentInfo</code> DataFrame is created by using <code>.withColumn</code> defining the column name (<code>browserFamily</code>) and the string datatype output from\u00a0<code>udfBrowserFamily</code>.\n\nOnce the DataFrame has been created, you can execute a Spark SQL query within the same Python notebook. For example, to see the breakdown by <code>browserFamily</code> within the <code>UserAgentInfo</code> table, execute the following query in your notebook:\n\n[scala]\n%sql \nSELECT browserFamily, count(1) \nFROM UserAgentInfo \nGROUP BY browserFamily\n[/scala]\n\nWithin the same notebook, you will see the following donut chart:\n\n<img class=\"aligncenter wp-image-6177\" src=\"https://databricks.com/wp-content/uploads/2016/02/Adtech-BrowserFamily-Cell-1024x653.png\" alt=\"Donut chart outlining distribution of visitors' browsers\" width=\"599\" height=\"382\" />\n<h3>Step 4: Complete our basic preparation</h3>\nTo put this all together, we will do the following tasks to complete our basic preparation of these web\u00a0access logs\u00a0for analysis:\n\n[scala]\n# Define function (converts Apache web log time)\ndef weblog2Time(weblog_timestr): \u2026\n\n# Define and Register UDF\nudfWeblog2Time = udf(weblog2Time, DateType())\nsqlContext.registerFunction(&quot;udfWeblog2Time&quot;, lambda x: weblog2Time(x))\n\n# Create DataFrame\naccessLogsPrime = sqlContext.sql(&quot;select hash(a.ip1, a.agent) as UserId, m.cca3, udfWeblog2Time(a.datetime) as LogDateTime,... from accesslog join UserAgentInfo u on u.agent = a.agent join mappedIP3 m on m.ip = a.ip1&quot;)\n[/scala]\n\nThe <code>weblog2Time</code> function performs the task of converting the Apache web log time to an ISO-8601 date format. \u00a0Within the <code>sqlContext</code>, to unique-ify the site visitors (in this case we lack a cookieId that anonymously identifies users), we can combine IP address and the user agent string as the <code>UserId</code>. \u00a0To combine back to the browser and OS information as well as country (based on IP address) information, the same <code>sqlContext</code> includes a join statement to the <code>UserAgentInfo</code> and <code>mappedIP3</code> tables.\n\n&nbsp;\n<h3>Visualize This!</h3>\nWith your preparation completed, you can quickly analyze and visualize your data, all from within the same notebook. \u00a0For example, with the browser information included within the\u00a0<code>accesslogprime</code>\u00a0\u00a0DataFrame, we can quickly identify the top 5 browsers by users and events.\n\n<img class=\"aligncenter wp-image-6179\" src=\"https://databricks.com/wp-content/uploads/2016/02/Adtech-BrowserFamily-Bar-Cell-1024x540.png\" alt=\"Chart displaying the top 5 browsers\" width=\"650\" height=\"343\" />\n\n&nbsp;\n\nIn the same Python notebook, we can also identify where site visitors are coming from based on their IP addresses as noted in the map below:\n\n<img class=\"aligncenter wp-image-6194\" src=\"https://databricks.com/wp-content/uploads/2016/02/Adtech-Map-Cell-1-1024x589.png\" alt=\"An example of a map visualization\" width=\"650\" height=\"374\" />\n\n&nbsp;\n<h2 style=\"text-align: center;\">Conclusion</h2>\nDatabricks allows you\u00a0to\u00a0quickly jump start your advertising analysis. To access the complete notebook including the functioning code and charts, you can view the <a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/AdTech_Sample_Notebook_Part_1.html\" target=\"_blank\">Databricks AdTech Sample Notebook (Part 1)</a> exported HTML notebook. \u00a0For more information about advertising platform optimization, check out the on-demand webinar <a href=\"http://go.databricks.com/how-celtra-optimizes-its-advertising-platform-with-databricks\" target=\"_blank\">How Celtra Optimizes Its Advertising Platform with Databricks</a>.\n\nFor a free trial of Databricks, please sign up at <a href=\"http://databricks.com/registration\" target=\"_blank\">Databricks Registration</a>."}
{"status": "publish", "description": null, "creator": "Wayne Chan", "link": "https://databricks.com/blog/2016/02/03/databricks-democratizes-data-and-reduces-infrastructure-costs-for-eyeview.html", "authors": null, "id": 6175, "categories": ["Announcements", "Company Blog", "Customers"], "dates": {"publishedOn": "2016-02-03", "tz": "UTC", "createdOn": "2016-02-03"}, "title": "Eyeview Partners with Databricks and AWS to Democratize Data Across a Unified Infrastructure", "slug": "databricks-democratizes-data-and-reduces-infrastructure-costs-for-eyeview", "content": "<img class=\"aligncenter size-full wp-image-6207\" style=\"border: 1px solid #c2c2c2;\" src=\"https://databricks.com/wp-content/uploads/2016/02/Eyeview-and-Databricks-case-study.png\" alt=\"Eyeview Digital and Databricks\" width=\"1200\" height=\"630\" />\n\nWe are happy to announce that Eyeview has selected Databricks as their unified analytics\u00a0platform \u2014 doubling the pace of innovation and development of new product features through faster data processing and simpler operations.\n\n<a href=\"http://www.marketwired.com/press-release/databricks-democratizes-data-and-reduces-infrastructure-costs-for-eyeview-2093611.htm\" target=\"_blank\" rel=\"noopener noreferrer\">You can read the press release here.</a>\n\n<a href=\"http://www.eyeviewdigital.com/\">Eyeview</a> is a video advertising technology company and a leader in providing brands with higher return on investment (ROI) on their video advertising spend. \u00a0Data is an integral part of Eyeview\u2019s platform, enabling the planning and optimization of video advertising campaigns for Eyeview\u2019s customers. Eyeview gleans consumer knowledge and business intelligence data from first and third party sources to create thousands of ad permutations for different audiences, personalizing the ads based on factors such as location, shopping habits, and browsing history.\n\nAfter evaluating multiple solutions including a Hadoop-based platform and AWS' data warehousing offering which was already in use, Eyeview selected Databricks to meet its need for a unified analytics platform that could natively support multiple big data capabilities at scale including data warehousing, machine learning, both SQL and programmatic interfaces, and ultimately streaming analytics.\n\nSince implementing Databricks for its data warehouse and ad hoc analytics needs, Eyeview has experienced the following improvements in performance and cost efficiency:\n\n<ul>\n    <li>Reduced query times on large data sets by a factor of 10, allowing data analysts to regain 20 percent of their workday from waiting for results.</li>\n    <li>Sped up data processing by fourfold without incurring additional operational costs.</li>\n    <li>Doubled the pace of product feature development, from prototyping to deployment, by increasing the productivity of the engineering team with faster and easier management of Apache Spark clusters.</li>\n</ul>\n\nDownload this <a href=\"http://go.databricks.com/case-studies/eyeview\">case study</a> to learn more about how Eyeview is using Databricks.\n\nTo try out Databricks for yourself, <a href=\"http://dbricks.co/1g8cHFA\">sign-up for a 14-day free trial</a> today!\n\n<div id=\":dt.ma\" class=\"Mu SP\" title=\"2/3/16, 7:07 AM\" data-tooltip=\"2/3/16, 7:07 AM\"></div>\n\n<div class=\"Mu SP\" title=\"2/3/16, 7:07 AM\" data-tooltip=\"2/3/16, 7:07 AM\"></div>"}
{"status": "publish", "description": null, "creator": "Wayne Chan", "link": "https://databricks.com/blog/2016/02/04/inneractive-optimizes-the-mobile-ad-buying-experience-at-scale-with-machine-learning-on-databricks.html", "authors": null, "id": 6190, "categories": ["Announcements", "Company Blog", "Customers"], "dates": {"publishedOn": "2016-02-04", "tz": "UTC", "createdOn": "2016-02-04"}, "title": "Inneractive Optimizes the Mobile Ad Buying Experience at Scale with Machine Learning on Databricks", "slug": "inneractive-optimizes-the-mobile-ad-buying-experience-at-scale-with-machine-learning-on-databricks", "content": "<img class=\"aligncenter size-full wp-image-6246\" style=\"border: 1px solid #c2c2c2;\" src=\"https://databricks.com/wp-content/uploads/2016/02/Inneractive-and-Databricks-case-study.png\" alt=\"Inneractive and Databricks\" width=\"1200\" height=\"630\" />\n\nWe are happy to announce that Inneractive chose Databricks as their primary data warehousing and analytics platform \u2014 allowing them to ingest and explore data at scale without hampering performance.\n\n<a href=\"http://www.marketwired.com/press-release/inneractive-implements-databricks-on-demand-scalability-machine-learning-capabilities-2094005.htm\">You can read the press release here</a>.\n\n<a href=\"http://inner-active.com/\">Inneractive</a> is a global mobile ad exchange focused on empowering mobile publishers to realize their properties\u2019 full potential by providing powerful technologies for the buying and selling of mobile ads programmatically, at scale.\n\nAt the heart of the Inneractive platform is a constant flow of advertising traffic increasing upwards to 3-5 million requests per minute and over 240GB of raw data per day. Inneractive quickly realized that their current system was too costly to scale without hampering performance. Inneractive was also limited in their ability to build, test, and tune machine learning algorithms and models to improve the bidding outcome while satisfying stringent requirements to deliver value to the users of their platform.\n\nInneractive turned to Databricks to simplify their data ingest and preparation process as their scalability issues became a critical bottleneck for business growth. With Databricks, they are not only able to query the data at scale without issue, but can now leverage the advanced analytics capabilities provided by the platform to build high performing machine learning algorithms and models in a distributed fashion to meet their evolving needs. \u00a0Furthermore, the simple self-service cluster manager allows Inneractive to provision managed Spark clusters on-demand, simplifying big data infrastructure operations and eliminating disruptive DevOps problems.\n\nAs Inneractive\u2019s business grows with incoming ad traffic more than tripling annually, they consider Databricks the perfect platform to meet their growing needs while managing costs more efficiently.\n\nDownload this <a href=\"http://go.databricks.com/case-studies/inneractive\">case study\u00a0</a>to learn more about how Inneractive is using Databricks.\n\nTo try out Databricks for yourself, <a href=\"http://dbricks.co/1g8cHFA\">sign-up for a 14-day free trial</a> today!\n\nSpark Summit East is just around the corner. If you haven't registered yet, you can get tickets <a href=\"http://www.prevalentdesignevents.com/sparksummit2016/east/registration.aspx?source=DBblog\">here</a> and here's a promo code for 20% off: Databricks20.\n\nLet us know if you want to meet onsite - we'd love to see you. <a href=\"http://go.databricks.com/spark-summit-east-2016-meeting-request?hsCtaTracking=995651f1-571c-4792-beb6-ca615a257679%7C008365f7-6c9a-4519-85ae-a9fb0b2976bd\">Click here</a> to schedule a meeting.\n\n&nbsp;"}
{"status": "publish", "description": null, "creator": "joseph", "link": "https://databricks.com/blog/2016/02/08/auto-scaling-scikit-learn-with-apache-spark.html", "authors": null, "id": 6312, "categories": ["Apache Spark", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2016-02-08", "tz": "UTC", "createdOn": "2016-02-08"}, "title": "Auto-scaling scikit-learn with Apache Spark", "slug": "auto-scaling-scikit-learn-with-apache-spark", "content": "Data scientists often spend hours or days tuning models to get the highest accuracy. This tuning typically involves running a large number of independent Machine Learning (ML) tasks coded in Python or R. Following some work presented at Spark Summit Europe 2015, we are excited to release <a href=\"http://spark-packages.org/package/databricks/spark-sklearn\" target=\"_blank\">scikit-learn integration package for Apache Spark</a>\u00a0that dramatically simplifies the life of data scientists using Python. This package automatically distributes the most repetitive tasks of model tuning on a Spark cluster, without impacting the workflow of data scientists:\n\n<ul>\n    <li>When used on a single machine, Spark can be used as a substitute to the default multithreading framework used by <a href=\"http://scikit-learn.org/\">scikit-learn</a> (<a href=\"https://pythonhosted.org/joblib/\">Joblib</a>).</li>\n    <li>If a need comes to spread the work across multiple machines, no change is required in the code between the single-machine case and the cluster case.</li>\n</ul>\n\n<h2>Scale data science effortlessly</h2>\n\nPython is one of the most popular programming languages for data exploration and data science, and this is in no small part due to high quality libraries such as <a href=\"http://pandas.pydata.org/\">Pandas</a> for data exploration or <a href=\"http://scikit-learn.org/\">scikit-learn</a> for machine learning. Scikit-learn provides fast and robust implementations of standard ML algorithms such as clustering, classification, and regression.\n\nScikit-learn's strength has typically been in the realm of computing on a single node, though. For some common scenarios, such as parameter tuning, a large number of small tasks can be run in parallel. These scenarios are perfect use cases for Spark.\n\nWe explored how to integrate Spark with scikit-learn, and the result is the Scikit-learn integration package for Spark. It combines the strengths of Spark and scikit-learn <em>with no changes to users\u2019 code</em>. It re-implements some components of scikit-learn that benefit the most from distributed computing. Users will find a Spark-based cross-validator class that is fully compatible with scikit-learn's cross-validation tools. By swapping out a single class import, users can distribute cross-validation for their existing scikit-learn workflows.\n\n<h2>Distribute tuning of Random Forests</h2>\n\nConsider a classical example of identifying digits in images. Here are a few examples of images taken from the popular digits dataset, with their labels:\n\n<img class=\"aligncenter size-full wp-image-6313\" src=\"https://databricks.com/wp-content/uploads/2016/02/image-taken-from-the-popular-digits-dataset.png\" alt=\"A couple examples of images taken from the popular digits dataset.\" width=\"206\" height=\"133\" />\n\nWe are going to train a <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">random forest classifier</a> to recognize the digits. This classifier has a number of parameters to adjust, and there is no easy way to know which parameters work best, other than trying out many different combinations. Scikit-learn provides GridSearchCV, a search algorithm that explores many parameter settings automatically. <a href=\"http://scikit-learn.org/stable/modules/grid_search.html\">GridSearchCV</a> uses selection by cross-validation, illustrated below. Each parameter setting produces one model, and the best-performing model is selected.\n\n<img class=\"aligncenter size-full wp-image-6332\" src=\"https://databricks.com/wp-content/uploads/2016/02/scikit-learn-without-spark-training-diagram.png\" alt=\"Diagram outlining how we are going to train the random forest classifier to recognize the digits.\" width=\"622\" height=\"306\" />\n\nThe <a href=\"http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html#example-classification-plot-digits-classification-py\">original code</a>, using only scikit-learn, is as follows:\n\n<pre><code class=\"py\">from sklearn import grid_search, datasets\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.grid_search import GridSearchCV\ndigits = datasets.load_digits()\nX, y = digits.data, digits.target\nparam_grid = {\"max_depth\": [3, None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [1, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [True, False],\n              \"criterion\": [\"gini\", \"entropy\"],\n              \"n_estimators\": [10, 20, 40, 80]}\ngs = grid_search.GridSearchCV(RandomForestClassifier(), param_grid=param_grid)\ngs.fit(X, y)\n</code></pre>\n\nThe dataset is small (in the hundreds of kilobytes), but exploring all the combinations takes about 5 minutes on a single core. The scikit-learn package for Spark provides an alternative implementation of the cross-validation algorithm that distributes the workload on a Spark cluster. Each node runs the training algorithm using a local copy of the scikit-learn library, and reports the best model back to the master:\n\n<img class=\"aligncenter size-full wp-image-6331\" src=\"https://databricks.com/wp-content/uploads/2016/02/scikit-learn-with-spark-training-diagram.png\" alt=\"Diagram showing Spark perform a distributed cross validation using spark-sklearn.\" width=\"622\" height=\"271\" />\n\nThe code is the same as before, except for a one-line change:\n\n<pre><code class=\"py\">from sklearn import grid_search, datasets\nfrom sklearn.ensemble import RandomForestClassifier\n# Use spark_sklearn\u2019s grid search instead:\nfrom spark_sklearn import GridSearchCV\ndigits = datasets.load_digits()\nX, y = digits.data, digits.target\nparam_grid = {\"max_depth\": [3, None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [1, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [True, False],\n              \"criterion\": [\"gini\", \"entropy\"],\n              \"n_estimators\": [10, 20, 40, 80]}\ngs = grid_search.GridSearchCV(RandomForestClassifier(), param_grid=param_grid)\ngs.fit(X, y)\n</code></pre>\n\nThis example runs under 30 seconds on a 4-node cluster (which has 16 CPUs). For larger datasets and more parameter settings, the difference is even more dramatic.\n\n<img class=\"aligncenter size-full wp-image-6316\" src=\"https://databricks.com/wp-content/uploads/2016/02/scikit-learn-results.png\" alt=\"Results from processing variously-sized datasets using spark-sklearn.\" width=\"622\" height=\"298\" />\n\n<h2>Get started</h2>\n\nIf you would like to try out this package yourself, it is available as a <a href=\"http://spark-packages.org/package/databricks/spark-sklearn\">Spark package</a> and as a <a href=\"https://pypi.python.org/pypi/spark-sklearn\">PyPI library</a>. To get started, <a href=\"http://go.databricks.com/hubfs/notebooks/Samples/Miscellaneous/blog_post_cv.html\" target=\"_blank\">check out this example notebook on Databricks</a>.\n\nIn addition to distributing ML tasks in Python across a cluster, Scikit-learn integration package for Spark provides additional tools to export data from Spark to python and vice-versa. You can find methods to convert Spark DataFrames to Pandas dataframes and numpy arrays. More details can be found in this <a href=\"http://www.slideshare.net/databricks/spark-summit-europe-2015-combining-the-strengths-of-mllib-scikitlearn-and-r\">Spark Summit Europe presentation</a> and in the <a href=\"http://pythonhosted.org/spark-sklearn/\">API documentation</a>.\n\nWe welcome feedback and contributions to our open-source <a href=\"https://github.com/databricks/spark-sklearn\">implementation on Github</a> (Apache 2.0 license)."}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2016/02/09/reshaping-data-with-pivot-in-apache-spark.html", "authors": null, "id": 6334, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2016-02-09", "tz": "UTC", "createdOn": "2016-02-09"}, "title": "Reshaping Data with Pivot in Apache Spark", "slug": "reshaping-data-with-pivot-in-apache-spark", "content": "<a href=\"https://spark-summit.org/east-2016/\" target=\"_blank\">Spark Summit East</a> is just around the corner! If you haven\u2019t registered yet, you can get tickets\u00a0<a href=\"http://www.prevalentdesignevents.com/sparksummit2016/east/registration.aspx?source=DBblog\">here</a> and here\u2019s a promo code for 20% off: <em>Databricks20</em>\n\nThis is a guest blog from our friend at Silicon Valley Data Science. Dr. Andrew Ray is passionate about big data and has extensive experience working with Apache Spark. Andrew is an active contributor to the Apache Spark project including SparkSQL and GraphX.\n\n<hr />\n\nOne of the many new features added in Spark 1.6 was the ability to pivot data, creating pivot tables, with a DataFrame (with Scala, Java, or Python). A pivot is an aggregation where one (or more in the general case) of the grouping columns has its distinct values transposed into individual columns. Pivot tables are an essential part of data analysis and reporting. Many popular data manipulation tools (pandas, reshape2, and Excel) and databases (MS SQL and Oracle 11g) include the ability to pivot data. I went over this briefly in a <a href=\"http://www.svds.com/pivoting-data-in-sparksql/\">past post</a>, but will be giving you a deep dive into the details here. Code for this post is available <a href=\"https://github.com/silicon-valley-data-science/spark-pivot-examples\">here</a>.\n<h2>Syntax</h2>\nIn the course of doing the pull request for pivot, one of the pieces of research I did was to look at the syntax of many of the competing tools. I found a wide variety of syntax options. The two main competitors were pandas (Python) and reshape2 (R).\n\n<strong>Original DataFrame (df)</strong>\n<table class=\"table\">\n<tbody>\n<tr>\n<td><b>A</b></td>\n<td><b>B</b></td>\n<td><b>C</b></td>\n<td><b>D</b></td>\n</tr>\n<tr>\n<td>foo</td>\n<td>one</td>\n<td>small</td>\n<td>1</td>\n</tr>\n<tr>\n<td>foo</td>\n<td>one</td>\n<td>large</td>\n<td>2</td>\n</tr>\n<tr>\n<td>foo</td>\n<td>one</td>\n<td>large</td>\n<td>2</td>\n</tr>\n<tr>\n<td>foo</td>\n<td>two</td>\n<td>small</td>\n<td>3</td>\n</tr>\n<tr>\n<td>foo</td>\n<td>two</td>\n<td>small</td>\n<td>3</td>\n</tr>\n<tr>\n<td>bar</td>\n<td>one</td>\n<td>large</td>\n<td>4</td>\n</tr>\n<tr>\n<td>bar</td>\n<td>one</td>\n<td>small</td>\n<td>5</td>\n</tr>\n<tr>\n<td>bar</td>\n<td>two</td>\n<td>small</td>\n<td>6</td>\n</tr>\n<tr>\n<td>bar</td>\n<td>two</td>\n<td>large</td>\n<td>7</td>\n</tr>\n</tbody>\n</table>\n<strong>Pivoted\u00a0DataFrame</strong>\n<table class=\"table\">\n<tbody>\n<tr>\n<td><b>A</b></td>\n<td><b>B</b></td>\n<td><b>large</b></td>\n<td><b>small</b></td>\n</tr>\n<tr>\n<td>foo</td>\n<td>two</td>\n<td>null</td>\n<td>6</td>\n</tr>\n<tr>\n<td>bar</td>\n<td>two</td>\n<td>7</td>\n<td>6</td>\n</tr>\n<tr>\n<td>foo</td>\n<td>one</td>\n<td>4</td>\n<td>1</td>\n</tr>\n<tr>\n<td>bar</td>\n<td>one</td>\n<td>4</td>\n<td>5</td>\n</tr>\n</tbody>\n</table>\nFor example, say we wanted to group by two columns A and B, pivot on column C, and sum column D. In pandas the syntax would be <code>pivot_table(df, values='D', index=['A', 'B'], columns=['C'], aggfunc=np.sum)</code>. This is somewhat verbose, but clear. With reshape2, it is <code>dcast(df, A + B ~ C, sum)</code>, a very compact syntax thanks to the use of an R formula. Note that we did not have to specify the value column for reshape2; its inferred as the remaining column of the DataFrame (although it can be specified with another argument).\n\nWe came up with our own syntax that fit in nicely with the existing way to do aggregations on a DataFrame. To do the same group/pivot/sum in Spark the syntax is <code>df.groupBy(\"A\", \"B\").pivot(\"C\").sum(\"D\")</code>. Hopefully this is a fairly intuitive syntax. But there is a small catch: to get better performance you need to specify the distinct values of the pivot column. If, for example, column C had two distinct values \u201csmall\u201d and \u201clarge,\u201d then the more preformant version would be <code>df.groupBy(\"A\", \"B\").pivot(\"C\", Seq(\"small\", \"large\")).sum(\"D\")</code>. Of course this is the Scala version, there are similar methods that take Java and Python lists.\n<h2>Reporting</h2>\nLet\u2019s look at examples of real-world use cases. Say you are a large retailer (like my former employer) with sales data in a fairly standard transactional format, and you want to make some summary pivot tables. Sure, you could aggregate the data down to a manageable size and then use some other tool to create the final pivot table (although limited to the granularity of your initial aggregation). But now you can do it all in Spark (and you could before it just took a lot of IF\u2019s). Unfortunately, since no large retailers want to share their raw sales data with us we will have to use a synthetic example. A good one that I have used previously is the <a href=\"http://www.tpc.org/tpcds/\">TPC-DS</a> dataset. Its schema approximates what you would find in an actual retailer.\n\n<img class=\"wp-image-6335 size-full aligncenter\" src=\"https://databricks.com/wp-content/uploads/2016/02/pivot-blog-image-1.png\" alt=\"Schema relationship diagram.\" width=\"400\" height=\"287\" />\n\nSince TPC-DS is a synthetic dataset that is used for benchmarking \u201cbig data\u201d databases of various sizes, we are able to generate it in many \u201cscale factors\u201d that determine how large the output dataset is. For simplicity we will use scale factor 1, corresponding to about a 1GB dataset. Since the requirements are a little complicated I have a <a href=\"https://hub.docker.com/r/svds/spark-pivot-reporting/\">docker image</a> that you can follow along with. Say we wanted to summarize sales by category and quarter with the later being columns in our pivot table. Then we would do the following (a more realistic query would probably have a few more conditions like time range).\n\n<pre>(sql(\"\"\"select *, concat('Q', d_qoy) as qoy\n  from store_sales\n  join date_dim on ss_sold_date_sk = d_date_sk\n  join item on ss_item_sk = i_item_sk\"\"\")\n  .groupBy(\"i_category\")\n  .pivot(\"qoy\")\n  .agg(round(sum(\"ss_sales_price\")/1000000,2))\n  .show)\n\n+-----------+----+----+----+----+\n| i_category|  Q1|  Q2|  Q3|  Q4|\n+-----------+----+----+----+----+\n|      Books|1.58|1.50|2.84|4.66|\n|      Women|1.41|1.36|2.54|4.16|\n|      Music|1.50|1.44|2.66|4.36|\n|   Children|1.54|1.46|2.74|4.51|\n|     Sports|1.47|1.40|2.62|4.30|\n|      Shoes|1.51|1.48|2.68|4.46|\n|    Jewelry|1.45|1.39|2.59|4.25|\n|       null|0.04|0.04|0.07|0.13|\n|Electronics|1.56|1.49|2.77|4.57|\n|       Home|1.57|1.51|2.79|4.60|\n|        Men|1.60|1.54|2.86|4.71|\n+-----------+----+----+----+----+</pre>\n\nNote that we put the sales numbers in millions to two decimals to keep this easy to look at. We notice a couple of things. First is that Q4 is crazy, this should come as no surprise for anyone familiar with retail. Second, most of these values within the same quarter with the exception of the null category are about the same. Unfortunately, even this great synthetic dataset is not completely realistic. Let me know if you have something better that is publicly available.\n<h2>Feature Generation</h2>\nFor a second example, let\u2019s look at feature generation for predictive models. It is not uncommon to have datasets with many observations of your target in the format of one per row (referred to as long form or <a href=\"https://en.wikipedia.org/wiki/Wide_and_narrow_data\">narrow data</a>). To build models, we need to first reshape this into one row per target; depending on the context this can be accomplished in a few ways. One way is with a pivot. This is potentially something you would not be able to do with other tools (like pandas, reshape2, or Excel), as the result set could be millions or billions of rows.\n\nTo keep the example easily reproducible, I\u2019m going to use the relatively small MovieLens 1M dataset. This has about 1 million movie ratings from 6040 users on 3952 movies. Let\u2019s try to predict the gender of a user based on their ratings of the 100 most popular movies. In the below example the ratings table has three columns: user, movie, and rating.\n\n<pre>+----+-----+------+\n|user|movie|rating|\n+----+-----+------+\n|  11| 1753|     4|\n|  11| 1682|     1|\n|  11|  216|     4|\n|  11| 2997|     4|\n|  11| 1259|     3|\n...</pre>\n\nTo come up with one row per user we pivot as follows:\n\n<pre>val ratings_pivot = ratings.groupBy(\"user\")\n                           .pivot(\"movie\", popular.toSeq)\n                           .agg(expr(\"coalesce(first(rating),3)\")\n                           .cast(\"double\"))</pre>\n\nHere, popular is a list of the most popular movies (by number of ratings) and we are using a default rating of 3. For user 11 this gives us something like:\n\n<pre>+----+----+---+----+----+---+----+---+----+----+---+...\n|user|2858|260|1196|1210|480|2028|589|2571|1270|593|...\n+----+----+---+----+----+---+----+---+----+----+---+...\n|  11| 5.0|3.0| 3.0| 3.0|4.0| 3.0|3.0| 3.0| 3.0|5.0|...\n+----+----+---+----+----+---+----+---+----+----+---+...</pre>\n\nWhich is the wide form data that is required for modeling. See the complete example <a href=\"https://github.com/silicon-valley-data-science/spark-pivot-examples/tree/master/2-FeatureGen\">here</a>. Some notes: I only used the 100 most popular movies because currently pivoting on thousands of distinct values is not particularly fast in the current implementation. More on this later.\n<h2>Tips and Tricks</h2>\nFor the best performance, specify the distinct values of your pivot column (if you know them). Otherwise, a job will be immediately launched to determine them{fn this is a limitation of other SQL engines as well as Spark SQL as the output columns are needed for planning}. Additionally, they will be placed in sorted order. For many things this makes sense, but for some, like the day of the week, this will not (Friday, Monday, Saturday, etc).\n\nPivot, just like normal aggregations, supports multiple aggregate expressions, just pass multiple arguments to the agg method. For example: <code>df.groupBy(\"A\", \"B\").pivot(\"C\").agg(sum(\"D\"), avg(\"D\"))</code>\n\nAlthough the syntax only allows pivoting on one column, you can combine columns to get the same result as pivoting multiple columns. For example:\n\n<pre>df.withColumn(\"p\", concat($\"p1\", $\"p2\"))\n  .groupBy(\"a\", \"b\")\n  .pivot(\"p\")\n  .agg(...)</pre>\n\nFinally, you may be interested to know that there is a maximum number of values for the pivot column if none are specified. This is mainly to catch mistakes and avoid OOM situations. The config key is <code>spark.sql.pivotMaxValues</code> and its default is 10,000. You should probably not change it.\n<h2>Implementation</h2>\nThe implementation adds a new logical operator <code>(o.a.s.sql.catalyst.plans.logical.Pivot)</code>. That logical operator is translated by a new analyzer rule <code>(o.a.s.sql.catalyst.analysis.Analyzer.ResolvePivot)</code> that currently translates it into an aggregation with lots of if statements, one expression per pivot value.\n\nFor example, <code>df.groupBy(\"A\", \"B\").pivot(\"C\", Seq(\"small\", \"large\")).sum(\"D\")</code> would be translated into the equivalent of<code> df.groupBy(\"A\", \"B\").agg(expr(\u201csum(if(C = \u2018small\u2019, D, null))\u201d), expr(\u201csum(if(C = \u2018large\u2019, D, null))\u201d))</code>. You could have done this yourself but it would get long and possibly error prone quickly.\n<h2>Future Work</h2>\nThere is still plenty that can be done to improve pivot functionality in Spark:\n<ul>\n\t<li>Make it easier to do in the user's language of choice by adding pivot to the R API and to the SQL syntax (similar to Oracle 11g and MS SQL).</li>\n\t<li>Add support for unpivot which is roughly the reverse of pivot.</li>\n\t<li>Speed up the implementation of pivot when there are many distinct values in the pivot column. I\u2019m already working on an idea for this.</li>\n</ul>"}
{"status": "publish", "description": null, "creator": "admin", "link": "https://databricks.com/blog/2016/02/10/how-elsevier-labs-implemented-dictionary-annotation-at-scale-with-apache-spark-on-databricks.html", "authors": null, "id": 6353, "categories": ["Announcements", "Company Blog", "Customers"], "dates": {"publishedOn": "2016-02-10", "tz": "UTC", "createdOn": "2016-02-10"}, "title": "How Elsevier Labs Implemented Dictionary Annotation at Scale with Apache Spark on Databricks", "slug": "how-elsevier-labs-implemented-dictionary-annotation-at-scale-with-apache-spark-on-databricks", "content": "<a href=\"https://spark-summit.org/east-2016/\" target=\"_blank\">Spark Summit East</a> is just around the corner! If you haven\u2019t registered yet, you can get tickets\u00a0<a href=\"http://www.prevalentdesignevents.com/sparksummit2016/east/registration.aspx?source=DBblog\">here</a>\u00a0with this promo code for 20% off: <em>Databricks20</em>\n\nThis is a guest blog from our friend at Elsevier Labs. Sujit Pal is a Technical Research Director at Elsevier Labs. His interests are Search and Natural Language Processing.\n\n<hr />\n\nElsevier is a provider of scientific, technical and medical (STM) information products and services. Elsevier Labs is an advanced technology R&amp;D group within the company. Members of Labs research and create new technologies, help implement proofs of concept, educate staff and management and represent the company in technical discussions.\n\nIn this post, we will describe the machine reading pipeline Elsevier Labs is building with Apache Spark on Databricks, with special emphasis on the Dictionary Based Entity Recognizer component. In keeping with the theme of this blog, the focus would be on the Spark related aspects of the component. <a href=\"http://sujitpal.blogspot.com/2015/11/soda-dictionary-based-entity.html\">You can find more information about the component itself here.</a>\n<h2>Business Challenge</h2>\nElsevier publishes approximately 350,000 articles annually across 2,000 STM journals. Our experimental machine reading pipeline reads the information in these journals in order to automatically generate knowledge graphs specialized for different STM domains. These knowledge graphs could be used for tasks ranging from summarizing text to drawing new inferences about the state of our world.\n\n<img class=\"aligncenter size-full wp-image-6355\" src=\"https://databricks.com/wp-content/uploads/2016/02/Diagram-outlining-how-Elsevier-implemented-dictionary-annotation-at-scale-with-apache-spark-on-databricks.png\" alt=\"Diagram outlining how Elsevier implemented dictionary annotation with Apache Spark and Databricks.\" width=\"674\" height=\"410\" />\n\nOne of the major requirements for generating these knowledge graphs is to locate and identify entities mentioned in the text so they can be related to each other, and to the real-world items they name. Our machine-reading pipeline is structured as a loose sequence of batch annotators, many of which run as Databricks notebooks or jobs, where each annotator has access to the original content as well as the outputs of previous annotators in the sequence. So any solution for entity recognition needed to be accessible from the processes running in this environment.\n<h2>Technology Challenge</h2>\nBecause of the specialized nature of STM domains, a standard off-the-shelf entity recognizer would not work for us. We did not want to spend the resources and time to build training sets that could be used to train specialized custom entity recognizers for each domain either. We decided instead to do dictionary based tagging, where we built dictionaries from readily available ontologies for different STM domains and stream our text against them. The entities that come from this step might be improved by later annotators in the pipeline in order to deal with ambiguous mentions or entities not yet in the dictionaries.\n\nCalling external services from within Spark pipelines is generally regarded as an anti-pattern, but in this case the functionality required was quite specialized and not trivial to build into Spark. On the other hand, most of this functionality were already available in various open source components built around the Apache Solr project. So we decided to structure our dictionary based entity recognizer as a micro-service built around those components.\n\nOur team was already using Databricks extensively, so it made sense to build our entity recognizer so it could be accessed as a micro-service from Databricks notebooks using JSON over HTTP. The interactive nature of Databricks notebooks made the process of developing and testing the service very efficient. We built a pythonic Scala API to interact with the service from the Databricks notebooks environment, both to load ontologies and to annotate text against them. The service is called using a python-style Scala API that is implemented as a custom JAR - once again, Databricks\u2019 built in functionality to support custom JARs makes this process extremely simple.\n<h2>Solution</h2>\nThe dictionary based entity recognizer needs one or more dictionaries to compare text against. Entities are extracted from ontologies, data for which is usually available in various standard formats such as JSON-LD, RDF, etc. For each entity, a record is created consisting of a unique ID, the ontology name, and a set of synonyms. Each record is written into the Solr index.\n\nDatabricks notebooks were used to parse the data for these ontologies into the standard format as well as to load the data into the index.\n\nThe code snippet below illustrates how to load content from a flat file into the index. Each line in the input represents a single entity. Note the use of mapPartitions to instantiate the client once per partition, and the use of zipWithIndex on the inner iterator to periodically commit to the index. The last commit from the driver takes care of any straggler entities that are not yet committed.\n\n[scala]\nval loaded = input.mapPartitions(p =&gt; {\n  val sodaClient = new SodaClient()\n  p.zipWithIndex.map(recIdx =&gt; {\n    val params = Map(\n      &quot;id&quot; -&gt; recIdx._1._1,\n      &quot;names&quot; -&gt; recIdx._1._2.split(&quot;::&quot;).toList,\n      &quot;lexicon&quot; -&gt; &quot;mesh&quot;,\n      &quot;commit&quot; -&gt; (if (recIdx._2 % 100 == 0) true else false)\n    )\n    val req = SodaUtils.jsonBuild(params)\n    val resp = sodaClient.post(&quot;http://host:port/soda/add.json&quot;, req)\n    (recIdx._1._1, resp)\n  })\n})\nloaded.count()\nval creq = SodaUtils.jsonBuild(Map(&quot;lexicon&quot; -&gt; &quot;mesh&quot;, &quot;commit&quot; -&gt; true))\nsodaClient.post(&quot;http://host:port/soda/add.json&quot;, creq)\n[/scala]\n\nAt query time, the client calls the microservice from a Databricks notebook, specifying a block of text (corresponding to a journal article, for example), the ontology to look up and the degree of matching required. When the client needs to annotate a large batch of documents against the same ontology, the Databricks notebook is run as a job.\n\nFor each annotation request, the microservice finds all the strings in the text that match any of the synonyms in the dictionary. The result is an RDD of entities \u2013 each entity record contains the entity ID, the text that was matched, the starting and ending character offsets of the match within the text, and a confidence measure between 0 and 1.\n\nThis RDD is then transformed into our Common Annotation (CAT2) format, using standard RDD operations on Databricks notebooks.\n\n[scala]\nimport com.elsevier.soda._\n\nval resps = input.mapPartitions(p =&gt; {\n  val sodaClient = new SodaClient()\n  p.map(rec =&gt; {\n    val params = Map(\n      &quot;lexicon&quot; -&gt; &quot;countries&quot;,\n      &quot;text&quot; -&gt; rec._2,\n      &quot;matching&quot; -&gt; &quot;lower&quot;\n    )\n    val req = SodaUtils.jsonBuild(params)\n    val resp = sodaClient.post(&quot;http://host:port/soda/annot.json&quot;, req)\n    val annots = SodaUtils.jsonParseList(resp)\n    (rec._1, annots)\n  })\n})\nresps.count\n[/scala]\n\nMatching can be exact (case-sensitive or insensitive) or fuzzy. Exact matching uses the functionality provided by SolrTextTagger to stream the entire text against a Finite State Automaton created from the index. This is a very efficient operation, even for large multi-million term dictionaries. The fuzzy matching works a little differently, where we use OpenNLP to chunk the incoming text into phrases and then match against pre-normalized entity names in the index.\n\nIn addition to the annotation function just described, the microservice offers several ancillary functions to load, delete and list dictionaries, find coverage for a document across multiple dictionaries, etc.\n<h2>Implementation Details</h2>\nThe micro-services architecture allows us to take advantage of functionality that already exists in Solr and OpenNLP. The architecture also allows us to tune the application independent of Spark. It is also horizontally scalable - by simply increasing the number of SoDA/Solr stacks and putting them behind a load balancer, we were able to achieve a linear increase in throughput.\n\nWhile the loose coupling has the benefits described above, it can also lead to timeouts and job failures on the Spark side if SoDA is not able to return results in time. So it is important to make sure that SoDA (and Solr) has enough capacity to serve requests from the Spark workers within the timeout specified. On the other hand, in order to prevent idling and to get the maximum utilization out of SoDA, we need to hit it with a sufficiently large number of simultaneous requests, which translates to a suitably large number of worker threads.\n\nUsing a Databricks cluster of 1 master + 16 workers with 4 cores each, and a SoDA cluster running on 2 r3.2xlarge machines behind a load balancer, we were able to achieve a sustained annotation rate of 30 docs/sec, with an average document size of 100MB, against a dictionary of more than 8 million entries\n<h2>Value Realized</h2>\nNamed entity recognition is critical to the success of our machine reading pipeline, and the SoDA application is one of our high-throughput strategies for achieving it. We have been using it only for a few months now, but it has already proven useful for several experiments for generating knowledge graphs from text corpora, specifically one that predicts new relationships between entities based on relationships mined from the text. Although at a basic level, the recognizer is simply an efficient solution to the problem of finding millions of named entities in millions of documents, it has the potential to help build solutions whose capabilities can extend the frontiers of scientific research.\n\n<hr />\n\n&nbsp;\n\n<em><a href=\"http://go.databricks.com/case-studies/elsevier\" target=\"_blank\">Read the Elsevier Labs case study</a> to learn more about how they are deploying Databricks.</em>"}
{"status": "publish", "description": null, "creator": "denny", "link": "https://databricks.com/blog/2016/02/12/findifys-smart-search-gets-smarter-with-apache-spark-mllib-and-databricks.html", "authors": null, "id": 6376, "categories": ["Announcements", "Company Blog", "Customers"], "dates": {"publishedOn": "2016-02-12", "tz": "UTC", "createdOn": "2016-02-12"}, "title": "Findify\u2019s Smart Search Gets Smarter with Apache Spark MLlib and Databricks", "slug": "findifys-smart-search-gets-smarter-with-apache-spark-mllib-and-databricks", "content": "<img class=\"alignnone size-full wp-image-6366\" style=\"border: 1px solid #c2c2c2;\" src=\"https://databricks.com/wp-content/uploads/2016/02/Findify-and-Databricks-case-study.png\" alt=\"Findify and Databricks\" width=\"1200\" height=\"630\" />\n\n<a href=\"https://spark-summit.org/east-2016/\" target=\"_blank\">Spark Summit East</a> is just around the corner! If you haven\u2019t registered yet, you can get tickets\u00a0<a href=\"http://www.prevalentdesignevents.com/sparksummit2016/east/registration.aspx?source=DBblog\">here</a>\u00a0with this promo code for 20% off: <em>Databricks20</em>\n\n<hr />\n\nWe are happy to announce that Findify has deployed Databricks as its machine learning and analytics platform, achieving faster time to complete projects, more efficient operations, and improved collaboration.\n\n<a href=\"http://www.marketwired.com/press-release/findify-achieves-smarter-search-solution-with-databricks-machine-learning-2096382.htm\">You can read the press release here</a>.\n\nFindify\u2019s mission is to create frictionless online shopping experiences which lead to satisfied, loyal customers and an increase in revenue. By utilizing advanced machine learning techniques, Findify\u2019s Smart Search continuously improves its accuracy as users use the service.\n\nFindify needed a data platform to run the machine learning algorithms central to their Smart Search capabilities. Their development process was hampered by difficult-to-maintain custom code and complicated workflows to ensure that their data infrastructure remained operational, such as:\n<ul>\n\t<li>Devoting substantial DevOps time and resources to cluster and job maintenance.</li>\n\t<li>Building an additional logic layer for the execution and failure logic of their jobs.</li>\n\t<li>Difficulty iterating through machine learning models because they could not effectively share information across a globally distributed team.</li>\n</ul>\nDatabricks enabled Findify to effortlessly manage Apache Spark clusters, access their data, collaboratively develop machine learning algorithms, and present their findings in a single platform. With the Databricks integrated workspace, the Findify team was able to:\n<ul>\n\t<li>Complete their feature development projects faster and reduce customer frustration in delayed analytics because they could focus on development instead of infrastructure.</li>\n\t<li>Focus on building innovative features because the managed Spark platform eliminated time spent on DevOps and infrastructure issues.</li>\n\t<li>Collaborate amongst their globally distributed team more effectively, which enabled them to iterate and visualize their results faster than before.</li>\n</ul>\nAs a result of deploying Databricks, Findify is able to focus less on DevOps and more on employing machine learning to improve their innovative Smart Search engine.\n<blockquote>\u201cDatabricks reduced our DevOps time on the Spark platform to almost zero. We don\u2019t worry about failures, restarts, capacity, etc. It just works.\u201d <small>\u2013 Meni Morim, Co-founder and CEO at Findify</small></blockquote>\n<a href=\"http://go.databricks.com/case-studies/findify\">Download this case study</a> to learn more about how Findify is using Databricks.\n\nTo try out Databricks for yourself, <a href=\"https://accounts.cloud.databricks.com/registration.html#signup\">sign-up for a 14-day free trial</a> today!"}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2016/02/17/introducing-databricks-dashboards.html", "authors": null, "id": 6428, "categories": ["Announcements", "Company Blog", "Product"], "dates": {"publishedOn": "2016-02-17", "tz": "UTC", "createdOn": "2016-02-17"}, "title": "Introducing Databricks Dashboards", "slug": "introducing-databricks-dashboards", "content": "Today we are excited to announce an expansion to our platform, a new capability called \u201cDatabricks Dashboards\u201d (Dashboards). In a nutshell, a Dashboard is a visual report backed by Apache Spark clusters, where users can consume information visually, or even interactively run queries by changing parameters. It is a simple way for users to instantly consume the insights generated by Spark. Databricks is the first company to make Spark widely useful in this way.\n\n<a href=\"http://www.marketwired.com/press-release/databricks-announces-apache-spark-powered-dashboards-feature-accelerate-enterprise-2097598.htm\" target=\"_blank\">You can read the press release here.</a>\n\n<h2>The Power of Spark, from Data Scientists to Business\u00a0Users</h2>\nFor data scientists (or anyone who analyzes data) creating tangible impact requires more than just finding the right answers - they also have to communicate the answers to the relevant decision makers just-in-time. Typically, the process of publishing these hard-earned insights requires third-party integrations with the data processing platform, resulting in many handoffs, delays, and tedious work.\n\nOur goal in building Dashboards is to bring Spark\u2019s greater impact to the enterprise, by streamlining the entire data-driven decision-making process - from data crunching to publishing insights. We\u2019ve already built capabilities, such as the <a href=\"https://vimeo.com/137874931\">notebook environment</a> and the <a href=\"http://go.databricks.com/spark-operations-with-databricks-whitepaper\">Spark cluster manager</a>, that enable technical users to easily utilize Spark for data processing and analysis. The addition of Dashboards will make the insights generated from Spark instantly accessible to all users in the enterprise so that the entire organization can benefit from the power of Spark.\n<h2>Dashboards: One Notebook, Multiple Views</h2>\nDashboards can be created directly from Databricks notebooks with a single click. In fact, a Dashboard is just another view of a notebook. Users can instantly create many different dashboards from one notebook, tailoring the presentation of the same results to different audiences.\n\n<img class=\"aligncenter size-full wp-image-6461\" src=\"https://databricks.com/wp-content/uploads/2016/02/Databricks-Dashboard-views-screenshot.png\" alt=\"Screenshot of Databricks Dashboard views.\" width=\"400\" height=\"293\" />\n\nDashboards are customizable to present results in the notebook in a wide variety of visual formats. The interface allows users to customize the look and feel in a simple point-and-click fashion.\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/02/Databricks-dashboards-screenshot.png\" rel=\"attachment wp-att-6455\"><img class=\"aligncenter size-full wp-image-6455\" src=\"https://databricks.com/wp-content/uploads/2016/02/Databricks-dashboards-screenshot.png\" alt=\"Screenshot of Databricks Dashboards.\" width=\"1358\" height=\"614\" /></a>\n\nOnce a dashboard has been built, it could be hosted on the Databricks platform and broadly shared via its URL. The <a href=\"https://databricks.com/blog/2015/08/05/databricks-2-0-leading-the-charge-to-democratize-data.html\">security controls</a> that we released last year allow users to precisely manage who has access to view the Dashboard and who has the right to edit.\n<h2>Opening Up Spark to All with Interactive Queries</h2>\nIt is easy to build Dashboards with drop-down menus that allow viewers to input different parameters to the underlying logic. Since Dashboards are another view of notebooks, this mechanism is a simple way for anyone in the organization to harness the power of notebooks for computation in a controlled manner. This approach does not require the users to have Spark knowledge, and it eliminates the risk of accidental changes to critical code. Having drop-down menus enables a common use case where Databricks users can build Dashboards to enable others to perform scenario analysis efficiently.\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/02/Scenario-analysis-in-Databricks-dashboards.png\" rel=\"attachment wp-att-6456\" data-lightbox-width=\"1000\"><img class=\"aligncenter size-full wp-image-6456\" src=\"https://databricks.com/wp-content/uploads/2016/02/Scenario-analysis-in-Databricks-dashboards.png\" alt=\"An example of scenario analysis in a Databricks dashboard.\" width=\"1999\" height=\"873\" /></a>\n<h2>Monitoring Critical Operations with Dashboards</h2>\nDatabricks already provides an <a href=\"https://databricks.com/blog/2015/04/16/the-easiest-way-to-run-spark-jobs.html\">easy and robust way to create Spark jobs</a>. With the addition of the Dashboards capability, Databricks Jobs can be configured to update Dashboards continuously with a few clicks. This combination allows users to build a dynamic Dashboard that automatically updates itself to monitor critical operations without complex DevOps or third party integrations.\n\n<img class=\"aligncenter size-full wp-image-6457\" src=\"https://databricks.com/wp-content/uploads/2016/02/Real-time-Databricks-Dashboards.png\" alt=\"Real-time Databricks dashboards.\" width=\"600\" height=\"414\" />\n<h2>Take Dashboards for a Test Drive Today</h2>\nDashboards is available immediately. Now everyone can benefit from the power of Spark by instantly accessing the insights derived from Spark and utilize them to make business-critical decisions just-in-time, without complex tools or processes. This enables enterprises to make real-time decisions with a single platform that can process data as well as seamlessly publish results.\n\nTo try out the Databricks Dashboards today, <a href=\"https://accounts.cloud.databricks.com/registration.html#signup\">sign-up for a 14-day trial</a> or <a href=\"http://go.databricks.com/contact-databricks\">contact us</a>."}
{"status": "publish", "description": null, "creator": "ion", "link": "https://databricks.com/blog/2016/02/17/introducing-databricks-community-edition-apache-spark-for-all.html", "authors": null, "id": 6431, "categories": ["Announcements", "Company Blog", "Product"], "dates": {"publishedOn": "2016-02-17", "tz": "UTC", "createdOn": "2016-02-17"}, "title": "Introducing Databricks Community Edition: Apache Spark for All", "slug": "introducing-databricks-community-edition-apache-spark-for-all", "content": "<img class=\"aligncenter size-full wp-image-6448\" src=\"https://databricks.com/wp-content/uploads/2016/02/Databricks-Community-Edition.png\" alt=\"Databricks Community Edition\" width=\"300\" height=\"300\" />\n\nAs developers at heart, we at Databricks are committed to the development of Apache Spark and the continued growth of the community. Today we took another step towards delivering on that goal with the beta release of Databricks Community Edition, a free version of our cloud-based Spark platform.\n\n<a href=\"http://www.marketwired.com/press-release/databricks-announces-community-edition-of-cloud-based-platform-2097539.htm\" target=\"_blank\">You can read the press release here</a>, or\u00a0<a href=\"http://dbricks.co/join-dce-beta\">join the waitlist here</a>.\n\nDatabricks Community Edition is designed for developers, data scientists, data engineers and anyone who want to learn Spark. With Databricks Community Edition, the users will have access to a micro-cluster, a cluster manager and the notebook environment to prototype simple applications. All users can share their notebooks and host them free of charge with Databricks. For those who are already using IPython, Databricks is compatible with IPython notebooks - you can easily import your existing IPython notebooks into Databricks, and vice versa.\n\nIn addition to the platform itself, Databricks Community Edition comes with a rich portfolio of Spark training resources, including the award-winning Massive Open Online Course, \"<a href=\"https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x\">Introduction to Big Data with Apache Spark</a>,\" which has enrolled over 76,000 participants to date. We will also continue to develop Spark tutorials and training materials over time, which will be directly accessible from the Community Edition.\n\nAttendees of Spark Summit East today will be the first to get access to the beta rollout. Accessibility to the broader community will be available over the coming months, with general availability planned for mid-year. If you\u2019d like to participate in the Databricks Community Edition beta program, please <a href=\"http://dbricks.co/join-dce-beta\">join the waitlist here</a>.\n\nYou can access the Spark training resources here directly while you are on the waitlist:\n<ul>\n\t<li><a href=\"https://docs.cloud.databricks.com/docs/latest/sample_applications/index.html\">Quickstart for sample applications</a></li>\n\t<li><a href=\"https://docs.cloud.databricks.com/docs/latest/courses/index.html\">Course material for 'Introduction to Big Data with Apache Spark'</a></li>\n\t<li><a href=\"https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html\">Databricks Guide</a></li>\n</ul>\nDatabricks Community Edition users can increase their capacity and take advantage of production-grade functionalities at any time by upgrading their subscription to the <a href=\"https://databricks.com/product/databricks\">full Databricks platform</a>. Whereas the Community Edition is ideal for learning and prototyping, the full Databricks platform offers <a href=\"https://databricks.com/product/databricks\">production-grade functionality</a>, such as an unlimited number of clusters that easily scale up or down, a job launcher, collaboration, advanced security controls, and expert support.\n\nTo try the full Databricks platform, <a href=\"https://accounts.cloud.databricks.com/registration.html#signup\">sign-up for a 14-day free trial</a> or <a href=\"http://go.databricks.com/contact-databricks\">contact us</a>.\n\nWe\u2019re really excited to contribute back to the community and hope that our new Community Edition will not only open doors for anyone wanting to get started with Spark, but also further enable the Spark community to grow.\n\nFor more information on Databricks Community Edition and how to <a href=\"http://dbricks.co/join-dce-beta\">join the waitlist</a>, <a href=\"https://databricks.com/product/faq/community-edition-beta-program\">check out the FAQ here</a>."}
{"status": "publish", "description": null, "creator": "scott", "link": "https://databricks.com/blog/2016/02/18/a-look-back-at-spark-summit-east-2016-thank-you-nyc.html", "authors": null, "id": 6484, "categories": ["Company Blog", "Events"], "dates": {"publishedOn": "2016-02-18", "tz": "UTC", "createdOn": "2016-02-18"}, "title": "A Look Back at Spark Summit East 2016: Thank you NYC!", "slug": "a-look-back-at-spark-summit-east-2016-thank-you-nyc", "content": "UDPATE<em>: All slides and videos have been posted on the Spark Summit website, <a href=\"https://spark-summit.org/east-2016/schedule/\" target=\"_blank\" rel=\"noopener\">check them out!</a></em>\n\n<hr />\n\nLike all previous Spark Summits, the first Summit of 2016 was a resounding success. As a reflection of Apache Spark\u2019s surging popularity, the Summit in New York grew from 900 people last year, to well over 1,300 people representing 500+ companies. This year many large organizations were eager to show off their production Spark use cases with the community, such as <a href=\"https://spark-summit.org/east-2016/events/spark-at-bloomberg/\">Bloomberg</a>, <a href=\"https://spark-summit.org/east-2016/events/escaping-flatland-interactive-high-dimensional-data-analysis-in-drug-discovery-using-spark/\">Novartis</a>, and <a href=\"https://spark-summit.org/east-2016/events/petabyte-scale-anomaly-detection-using-r-spark/\">Comcast</a>.\n\nWe had two important product announcements at the event: <a href=\"https://databricks.com/blog/2016/02/17/introducing-databricks-community-edition-apache-spark-for-all.html\">Community Edition Beta</a> and <a href=\"https://databricks.com/blog/2016/02/17/introducing-databricks-dashboards.html\">Dashboards</a>. As the company behind Spark Summits, we are excited to bring you the highlights from each day. The PDFs and videos of the talks mentioned in this blog will be posted in the coming weeks. Check back here if you could not make it in person.\n\n<h2>Day One Training</h2>\n\nThe Summit remains a place for many to acquire Spark expertise. Over 500 people - from beginners to advanced users - participated in our <a href=\"https://databricks.com/spark/training\">Spark training</a>, where topics such as Spark basics to advanced data science with Spark were taught in a hands-on lab setting.\n\n<h2>Day Two Highlights</h2>\n\nThe second day of the Spark Summit kicked off with a keynote featuring Matei Zaharia, the creator of Spark and co-founder / CTO of Databricks, who provided an overview of the upcoming Spark 2.0. Databricks co-founder and CEO Ali Ghodsi took the stage after Matei to announce the beta release of <a href=\"https://databricks.com/blog/2016/02/17/introducing-databricks-community-edition-apache-spark-for-all.html\">Databricks Community Edition</a>, a free version of our cloud-based Spark platform with the goal of making Spark easy to learn and accessible to the everyone. The announcement ended with an exciting demo of the Community Edition by Michael Armbrust of Databricks, which you can <a href=\"https://vimeo.com/155716963\">watch on-demand here</a> (<a href=\"https://docs.cloud.databricks.com/docs/latest/featured_notebooks/Wikipedia%20Clickstream%20Data.html\">the demo notebook is here</a>). The other prominent industry keynotes from Hortonworks, IBM, and SAP highlighted the increased level of Spark adoption they are seeing and how it is accelerating innovation and growth in the enterprise.\n\nSlides from the keynotes from Day Two:\n\n<ul>\n    <li>Matei Zaharia, Co-Founder &amp; CTO, Databricks: <a href=\"http://www.slideshare.net/databricks/2016-spark-summit-east-keynote-matei-zaharia\">Spark 2.0</a></li>\n    <li>Ali Ghodsi, Co-Founder &amp; CEO, Databricks: <a href=\"http://www.slideshare.net/databricks/2016-spark-summit-east-keynote-ali-ghodsi-and-databricks-community-edition-demo\">Democratizing Access to Data</a></li>\n    <li>Shaun Connolly, VP of Business Strategy, Hortonworks: <a href=\"http://www.slideshare.net/SparkSummit/spark-summit-keynote-by-shaun-connolly\">Accelerating Enterprise Spark</a></li>\n    <li>Anjul Bhambhri, VP of Big Data Engineering, IBM: <a href=\"http://www.slideshare.net/SparkSummit/spark-summit-presentation-by-anjul-bhambhri\">Apache Spark, the Analytics Operating System</a></li>\n    <li>Ken Tsai, Head of Cloud Platform &amp; Data Management, SAP: <a href=\"http://www.slideshare.net/SparkSummit/spark-summit-presentation-by-ken-tsai\">Spark Usage in Enterprise Business Operations</a></li>\n</ul>\n\nThe theme of Spark establishing itself as the enterprise data processing engine of choice was echoed throughout the day as presenters shared examples of utilizing Spark to achieve measurable business outcomes.\n\n<h2>Day Three Highlights</h2>\n\nUp first on day three was Reynold Xin, co-founder and Chief Architect at Databricks, who spoke about the rise of continuous applications that act on real-time data and the future of Spark Streaming. Following Reynold were three presentations from industry leaders who are paving the way for mass Spark adoption including heads of technology and analytics from Synchronoss, and eBay.\n\nSelected slides from the keynotes from Day Three:\n\n<ul>\n    <li>Reynold Xin, Co-Founder &amp; Chief Architect, Databricks: <a href=\"http://www.slideshare.net/rxin/the-future-of-realtime-in-spark\">The Future of Real-Time in Spark</a></li>\n    <li>Suren Nathan, Head of Big Data Analytics, Razorsight: <a href=\"http://www.slideshare.net/SparkSummit/spark-summit-keynote-by-suren-nathan\">Data Profiling and Pipeline Processing with Spark</a></li>\n    <li>Seshu Adunuthula, Head of Analytics Infrastructure, eBay: <a href=\"http://www.slideshare.net/SparkSummit/spark-summit-keynote-by-seshu-adunuthula\">Role of Spark in transforming eBay\u2019s Enterprise Data Platform</a></li>\n</ul>\n\nThe momentum around Spark innovation continued as attendees spilled into various speaking sessions for developers, data scientists, researchers, and enterprise users. Interesting highlights include how BlackRock has built a Spark-based framework for managing data quality tests, Viacom shared their experiences building a just-in-time data warehouse with Databricks, and Novartis presented how they leverage Spark for distributed analytics and interactive visualization of large, high-dimensional screening data.\n\nIf you weren\u2019t able to catch these great talks and the others from this week, the recordings will be available on the <a href=\"https://spark-summit.org/east-2016/schedule/\">Spark Summit website</a> in the coming weeks. Slides from Databricks presenters are <a href=\"https://databricks.com/resources/slides\">already available on our website here</a>.\n\n<h2>Coming Soon: Spark Summit San Francisco</h2>\n\nFor those of you who could not make it to New York this year, <a href=\"https://spark-summit.org/2016/\">Spark Summit will be in San Francisco</a> from June 6-8. If you are interested in sharing your Spark experiences, <a href=\"https://spark-summit.org/2016/call-for-presentations/\">call for papers is now open</a>. The deadline for submissions is February 29, 2016.\n\nWe will be updating this blog with links to videos and presentations of the sessions in the coming weeks. Please check back here regularly. In the meantime, <a href=\"http://go.databricks.com/newsletter-registration\">subscribe to our newsletter</a> to keep up with the latest Databricks and Spark news."}
{"status": "publish", "description": "Databricks is excited to announce the release of GraphFrames, a graph processing library for Apache Spark. Read about the new library and see code examples here.", "creator": "joseph", "link": "https://databricks.com/blog/2016/03/03/introducing-graphframes.html", "authors": null, "id": 6598, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2016-03-03", "tz": "UTC", "createdOn": "2016-03-03"}, "title": "Introducing GraphFrames", "slug": "introducing-graphframes", "content": "<em>We would like to thank Ankur Dave from\u00a0UC Berkeley AMPLab for his contribution to this blog post.</em>\n\n<hr />\n\nDatabricks is excited to announce the release of GraphFrames, a graph processing library for Apache Spark. Collaborating with UC Berkeley and MIT, we have built a graph library based on DataFrames. GraphFrames benefit from the scalability and high performance of DataFrames, and they provide a uniform API for graph processing available from Scala, Java, and Python.\n<h2>What are GraphFrames?</h2>\nGraphFrames support general graph processing, similar to Apache Spark\u2019s GraphX library. However, GraphFrames are built on top of Spark DataFrames, resulting in some key advantages:\n<ul>\n\t<li><strong>Python, Java &amp; Scala APIs:</strong> GraphFrames provide uniform APIs for all 3 languages. For the first time, all algorithms in GraphX are available from Python &amp; Java.</li>\n\t<li><strong>Powerful queries:</strong> GraphFrames allow users to phrase queries in the familiar, powerful APIs of Spark SQL and DataFrames.</li>\n\t<li><strong>Saving &amp; loading graphs:</strong> GraphFrames fully support <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#data-sources\">DataFrame data sources</a>, allowing writing and reading graphs using many formats like Parquet, JSON, and CSV.</li>\n</ul>\nIn GraphFrames, vertices and edges are represented as DataFrames, allowing us to store arbitrary data with each vertex and edge.\n<h2>An example social network</h2>\nSay we have a social network with users connected by relationships. We can represent the network as a <a href=\"https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)\">graph</a>, which is a set of vertices (users) and edges (connections between users). A toy example is shown below.\n\n<a href=\"http://go.databricks.com/hubfs/notebooks/3-GraphFrames-User-Guide-python.html\" target=\"_blank\"><img class=\"aligncenter size-full wp-image-6601\" src=\"https://databricks.com/wp-content/uploads/2016/03/social-network-graph-diagram.png\" alt=\"Social network graph diagram\" width=\"480\" height=\"326\" /></a>\n<p style=\"text-align: center;\"><em>Click on the image to see the full example notebook</em></p>\nWe might then ask questions such as \u201cWhich users are most influential?\u201d or \u201cUsers A and B do not know each other, but should they be introduced?\u201d These types of questions can be answered using graph queries and algorithms.\n\nGraphFrames can store data with each vertex and edge. In a social network, each user might have an age and name, and each connection might have a relationship type.\n\n<a href=\"http://go.databricks.com/hubfs/notebooks/3-GraphFrames-User-Guide-python.html\" target=\"_blank\"><img class=\"aligncenter size-full wp-image-6604\" src=\"https://databricks.com/wp-content/uploads/2016/03/social-network-graph-verticies.png\" alt=\"Social graph verticies\" width=\"212\" height=\"184\" /></a>\n\n<a href=\"http://go.databricks.com/hubfs/notebooks/3-GraphFrames-User-Guide-python.html\" target=\"_blank\"><img class=\"aligncenter size-full wp-image-6603\" src=\"https://databricks.com/wp-content/uploads/2016/03/social-network-graph-edges.png\" alt=\"Social graph edges\" width=\"224\" height=\"287\" /></a>\n<p style=\"text-align: center;\"><em>Click on the table to see the full example notebook</em></p>\n\n<h2>Simple queries are simple</h2>\nGraphFrames make it easy to express queries over graphs. Since GraphFrame vertices and edges are stored as DataFrames, many queries are just DataFrame (or SQL) queries.\n\n<strong>Example:</strong>\n<em>How many users in our social network have \u201cage\u201d &gt; 35?</em>\nWe can query the <code>vertices</code> DataFrame:\n<code>g.vertices.filter(\"age &gt; 35\")</code>\n\n<strong>Example:</strong>\n<em>How many users have at least 2 followers?</em>\nWe can combine the built-in inDegrees method with a DataFrame query.\n<code>g.inDegrees.filter(\"inDegree &gt;= 2\")</code>\n<h2>Graph algorithms support complex workflows</h2>\nGraphFrames support the full set of algorithms available in GraphX, in all 3 language APIs. Results from graph algorithms are either DataFrames or GraphFrames. For example, what are the most important users? We can run PageRank:\n\n[python]\nresults = g.pageRank(resetProbability=0.15, maxIter=10)\ndisplay(results.vertices)\n[/python]\n\n<a href=\"http://go.databricks.com/hubfs/notebooks/3-GraphFrames-User-Guide-python.html\" target=\"_blank\"><img class=\"aligncenter size-full wp-image-6605\" src=\"https://databricks.com/wp-content/uploads/2016/03/PageRank-results.png\" alt=\"PageRank results\" width=\"382\" height=\"183\" /></a>\n<p style=\"text-align: center;\"><em>Click on the table to see the full example notebook</em></p>\nGraphFrames also support new algorithms:\n<ul>\n\t<li>Breadth-first search (BFS): Find shortest paths from one set of vertices to another</li>\n\t<li>Motif finding: Search for structural patterns in a graph</li>\n</ul>\nMotif finding lets us make powerful queries. For example, to recommend whom to follow, we might search for triplets of users A,B,C where A follows B and B follows C, but A does not follow C.\n\n[python]\n# Motif: A-&gt;B-&gt;C but not A-&gt;C\nresults = g.find(&quot;(A)-[]-&gt;(B); (B)-[]-&gt;(C); !(A)-[]-&gt;(C)&quot;)\n# Filter out loops (with DataFrame operation)\nresults = results.filter(&quot;A.id != C.id&quot;)\n# Select recommendations for A to follow C\nresults = results.select(&quot;A&quot;, &quot;C&quot;)\ndisplay(results)\n[/python]\n\n<a href=\"http://go.databricks.com/hubfs/notebooks/3-GraphFrames-User-Guide-python.html\" target=\"_blank\"><img class=\"aligncenter size-full wp-image-6606\" src=\"https://databricks.com/wp-content/uploads/2016/03/GraphFrames-motif-findings.png\" alt=\"Motif findings\" width=\"465\" height=\"266\" /></a>\n<p style=\"text-align: center;\"><em>Click on the table to see the full example notebook</em></p>\nThe full set of GraphX algorithms supported by GraphFrames is:\n<ul>\n\t<li>PageRank: Identify important vertices in a graph</li>\n\t<li>Shortest paths: Find shortest paths from each vertex to landmark vertices</li>\n\t<li>Connected components: Group vertices into connected subgraphs</li>\n\t<li>Strongly connected components: Soft version of connected components</li>\n\t<li>Triangle count: Count the number of triangles each vertex is part of</li>\n\t<li>Label Propagation Algorithm (LPA): Detect communities in a graph</li>\n</ul>\n<h2>GraphFrames integrate with GraphX</h2>\nGraphFrames fully integrate with GraphX via conversions between the two representations, without any data loss. We can convert our social network to a GraphX graph and back to a GraphFrame.\n\n[scala]\nval gx: Graph[Row, Row] = g.toGraphX()\nval g2: GraphFrame = GraphFrame.fromGraphX(gx)\n[/scala]\n\nSee the <a href=\"http://graphframes.github.io/api/scala/index.html#org.graphframes.GraphFrame$\">GraphFrame API docs</a> for more details on these conversions.\n<h2>What's next?</h2>\nGraph-specific optimizations for DataFrames are under active research and development. Watch Ankur Dave\u2019s <a href=\"https://spark-summit.org/east-2016/speakers/ankur-dave/\">Spark Summit East 2016 talk</a> to learn more. We plan to include some of these optimizations in GraphFrames for its next release!\n\n<strong>Get started</strong> with these tutorial notebooks in <a href=\"http://go.databricks.com/hubfs/notebooks/3-GraphFrames-User-Guide-scala.html\">Scala</a> and <a href=\"http://go.databricks.com/hubfs/notebooks/3-GraphFrames-User-Guide-python.html\">Python</a>\u00a0in the free <a href=\"http://databricks.com/try\" target=\"_blank\">Databricks Community Edition</a>.\n<strong>Download</strong> the GraphFrames package from the <a href=\"http://spark-packages.org/package/graphframes/graphframes\">Spark Packages website</a>. GraphFrames are compatible with Spark 1.4, 1.5, and 1.6.\n<strong>Learn more</strong> in the <a href=\"http://graphframes.github.io/\">User Guide and API docs</a>.\n\nThe code is available on <a href=\"https://github.com/graphframes/graphframes\">Github</a> under the Apache 2.0 license. We welcome contributions! Check the <a href=\"https://github.com/graphframes/graphframes/issues\">Github issues</a> for ideas to work on."}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2016/03/09/on-demand-webinar-jump-start-into-apache-spark-and-databricks.html", "authors": null, "id": 6630, "categories": ["Announcements", "Company Blog", "Events", "Product"], "dates": {"publishedOn": "2016-03-09", "tz": "UTC", "createdOn": "2016-03-09"}, "title": "On-Demand Webinar: Jump Start into Apache Spark and Databricks", "slug": "on-demand-webinar-jump-start-into-apache-spark-and-databricks", "content": "In early February, we held a live webinar -\u00a0<a href=\"http://go.databricks.com/jump-start-into-apache-spark-and-databricks\" target=\"_blank\">Jump Start into Apache Spark and Databricks</a>\u00a0- to provide an introduction to Apache Spark concepts and how to use Databricks to perform common Spark operations, covering topics such as:\n<ul>\n\t<li class=\"p1\"><strong>Quick Start on Apache Spark:</strong> Provides an introductory quick start to Spark using Python and Resilient Distributed Datasets (RDDs). \u00a0We reviewed how RDDs have actions and transformations and their impact on your Spark workflow.</li>\n\t<li class=\"p1\"><strong>A Primer on RDDs to DataFrames to Datasets: </strong>A\u00a0high-level overview of our journey from RDDs (2011) to DataFrames (2013) to the newly introduced (as of Spark 1.6) Datasets (2015).</li>\n\t<li class=\"p1\"><strong>Just in Time Data Warehousing with Spark SQL:</strong>\u00a0Demonstrated a Just-in-Time Data Warehousing (JIT-DW) example using Spark SQL on an AdTech scenario. \u00a0We started with weblogs, create an external table with RegEx, make an external web service call via a Mapper, join DataFrames and register a temp table, add columns to DataFrames with UDFs, use Python UDFs with Spark SQL, and visualize the output - all in the same notebook.</li>\n</ul>\nThe webinar is <a href=\"http://go.databricks.com/jump-start-into-apache-spark-and-databricks\" target=\"_blank\">accessible on-demand</a>, its slides and sample notebooks are also downloadable as attachments to the webinar. <a href=\"http://go.databricks.com/databricks-community-edition-beta-waitlist\" target=\"_blank\">Join the Databricks Community Edition beta</a> to get free access to Spark and try out the notebooks.\n\nWe have answered the common questions raised by webinar viewers below. If you have additional questions, check out the <a href=\"https://forums.databricks.com/\" target=\"_blank\">Databricks Forum</a>.\n<h2>Common webinar questions and answers</h2>\n<em>Click on the question to see answer</em>\n\n<a href=\"https://forums.databricks.com/questions/6996/from-webinar-jump-start-into-apache-spark-and-data.html\" target=\"_blank\">What is the best source of documentation for SQL functions supported by Spark? (i.e. string functions, date functions, etc.)</a>\n\n<a href=\"https://forums.databricks.com/questions/6934/from-the-webinar-transitioning-from-dw-to-spark-ho.html#answer-6935\" target=\"_blank\">Is it possible to use Spark on the laptop and try out to run this script?</a>\n\n<a href=\"https://forums.databricks.com/questions/6795/from-webinar-turbo-fast-dw-does-the-load-from-spar.html#answer-6796\" target=\"_blank\">How does Databricks integrate with RedShift?</a>\n\n<a href=\"https://forums.databricks.com/questions/7149/from-webinar-jump-start-into-apache-spark-and-data-5.html#answer-7150\" target=\"_blank\">Where can you find SPARK-9999 for dataset basics?</a>\n\n<a href=\"https://forums.databricks.com/questions/7000/from-webinar-jump-start-into-apache-spark-and-data-1.html\" target=\"_blank\">What are industry use cases where Spark is used?</a>\n\n<a href=\"https://forums.databricks.com/questions/7011/from-webinar-jump-start-into-apache-spark-and-data-3.html#answer-7013\" target=\"_blank\">What version of Hadoop is compatible with Spark 1.5?</a>\n\n<a href=\"https://forums.databricks.com/questions/7006/from-webinar-jump-start-into-apache-spark-and-data-2.html#answer-7009\" target=\"_blank\">Does the Databricks notebook support Java much like Python and Scala for interactive development?</a>\n\n<a href=\"https://forums.databricks.com/questions/7147/from-webinar-jump-start-into-apache-spark-and-data-4.html#answer-7148\" target=\"_blank\">Is the join happening in Spark or python interpreter on the driver node for the AdTech Sample Notebook?</a>\n\n&nbsp;"}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2016/03/10/how-sellpoints-launched-a-new-predictive-analytics-product-with-databricks.html", "authors": null, "id": 6652, "categories": ["Announcements", "Company Blog", "Customers", "Product"], "dates": {"publishedOn": "2016-03-10", "tz": "UTC", "createdOn": "2016-03-10"}, "title": "How Sellpoints Launched a New Predictive Analytics Product with Databricks", "slug": "how-sellpoints-launched-a-new-predictive-analytics-product-with-databricks", "content": "<img src=\"https://databricks.com/wp-content/uploads/2016/03/Sellpoints-and-Databricks.png\" style=\"border: 1px solid #c2c2c2;\" alt=\"Sellpoints and Databricks\" width=\"1200\" height=\"630\" class=\"aligncenter size-full wp-image-6654\" />\n\nWe are excited to announce that <a href=\"http://www.sellpoints.com/\" target=\"_blank\">Sellpoints</a>, a provider of online advertising solutions dedicated to optimizing return on ad spend (ROAS) for retailers and brands, chose Databricks as their enterprise Apache Spark solution, allowing for the rapid productization of a critical predictive analytics product.\n\n<a href=\"http://www.marketwired.com/press-release/databricks-enables-sellpoints-to-launch-a-new-predictive-analytics-product-2104808.htm\">You can read the press release here.</a>\n\nSellpoints came to Databricks because they wanted to productize an innovative new product based on predictive analytics. Specifically, they wanted to measure the shopping behavior of consumers, glean intelligence from this data (over one billion events per day in this case), and then automatically apply the insights to identify qualified shoppers, run targeted advertising campaigns, and drive prospective shoppers to make a purchase. \n\nThe questions Sellpoints faced were common for fast-moving companies that want to build a big data product with Apache Spark: What is the best way to acquire a reliable Spark platform? And what is the most effective way to empower their data teams to become productive? Before choosing Databricks, Sellpoints tried to deploy Apache Spark over Hadoop. But other Spark vendor failed to deliver the performance, reliability, and Spark expertise needed by Sellpoints. \n\nThe other solutions did not provide any mechanism to help Sellpoints\u2019 business team to leverage the insights from Spark. This meant that Sellpoints had to invest in additional software, and the data science team had to devote time to building dashboards for business users on top of their day-to-day responsibilities.\n\nSellpoints was able to use Databricks to build its entire data ingest pipeline with Apache Spark in a matter of six weeks. Not only was Databricks able to provide high performance and reliable Spark clusters instantly, it also democratized the access of every Sellpoints team to Spark. The Data scientists used Databricks\u2019 integrated workspace to fine-tune models interactively, while the business team took advantage of Spark-powered dashboards to consume insights without any additional BI tools.\n\nWith Databricks, Sellpoints gained powerful big data ETL and machine learning capabilities and capture three critical benefits:\n<ul>\n<li>Productized a new predictive analytics offering, improving the ad spend ROI by threefold compared to competitive offerings.</li>\n<li>Reduced the time and effort required to deliver actionable insights to the business team while lowering costs.</li>\n<li>Improved productivity of the engineering and data science team by eliminating the time spent on DevOps and maintaining open source software.</li>\n</ul>\n\n<a href=\"http://go.databricks.com/case-studies/sellpoints\" target=\"_blank\">Download this case study</a> to learn more about how Sellpoints is using Databricks.\n\nTo try out Databricks for yourself, <a href=\"https://accounts.cloud.databricks.com/registration.html#signup\" target=\"_blank\">sign-up for a 14-day free trial</a> today!"}
{"status": "publish", "description": null, "creator": "Wayne Chan", "link": "https://databricks.com/blog/2016/03/16/on-demand-webinar-and-faq-apache-spark-1-5.html", "authors": null, "id": 6662, "categories": ["Announcements", "Company Blog", "Events"], "dates": {"publishedOn": "2016-03-16", "tz": "UTC", "createdOn": "2016-03-16"}, "title": "On-Demand Webinar and FAQ: Apache Spark 1.5", "slug": "on-demand-webinar-and-faq-apache-spark-1-5", "content": "In late August, we held a live webinar \u2013 <a href=\"http://go.databricks.com/apache-spark-1.5-presented-by-databricks-co-founder-patrick-wendell\">Apache Spark 1.5 presented by Databricks co-founder Patrick Wendell </a>- to discuss Spark\u2019s Project Tungsten initiative, a cross-cutting performance update that uses binary memory management and code generation to dramatically improve latency of most Spark jobs. This release also includes several updates to Spark's DataFrame API and SQL optimizer, along with new Machine Learning algorithms and feature transformers, and several new features in Spark's native streaming engine.\n\nThe webinar is <a href=\"http://go.databricks.com/apache-spark-1.5-presented-by-databricks-co-founder-patrick-wendell\">accessible on-demand</a>.\u00a0Its slides and sample notebooks are also downloadable as attachments to the webinar. <a href=\"http://go.databricks.com/databricks-community-edition-beta-waitlist\">Join the Databricks Community Edition beta</a> to get free access to Spark and try out the notebooks.\n\nWe have answered the common questions raised by webinar viewers below. If you have additional questions, please check out the <a href=\"https://forums.databricks.com/\">Databricks Forum</a>.\n\n<strong>Common webinar questions and answers</strong>\n\nClick on the question to see answer:\n<ul>\n\t<li><a href=\"https://forums.databricks.com/questions/6869/from-webinar-apache-spark-15-project-tungsten-outs.html#answer-6870\">To take full advantage of Project Tungsten do you need to use DataFrames?</a></li>\n\t<li><a href=\"https://forums.databricks.com/questions/6800/from-webinar-apache-spark-15-will-the-pyspark-api.html\">Will the pyspark api get GraphX bindings?</a></li>\n\t<li><a href=\"https://forums.databricks.com/questions/6869/from-webinar-apache-spark-15-project-tungsten-outs.html#answer-6870\">Is Tungsten useful outside of the context of SparkSQL/DataFrames? If so, how does that work?</a></li>\n\t<li><a href=\"https://forums.databricks.com/questions/7259/from-webinar-apache-spark-15-are-udafs-able-to-tak.html#answer-7260\">Are UDAFs able to take advantage of the Tungsten features (memory and code gen) or only built-in ones?</a></li>\n\t<li><a href=\"https://forums.databricks.com/questions/6876/from-webinar-apache-spark-15-are-spark-packages-au.html#answer-6877\">Are spark-packages, packages being audited or need to have a certain quality level?</a></li>\n\t<li><a href=\"https://forums.databricks.com/questions/7257/from-webinar-spark-dataframes-what-is-the-differen-1.html#answer-7258\">What's the difference between DataFrame and RDD?</a></li>\n</ul>"}
{"status": "publish", "description": null, "creator": "Wayne Chan", "link": "https://databricks.com/blog/2016/03/15/on-demand-webinar-and-faq-how-celtra-optimizes-its-advertising-platform-with-databricks.html", "authors": null, "id": 6664, "categories": ["Company Blog", "Customers", "Events"], "dates": {"publishedOn": "2016-03-15", "tz": "UTC", "createdOn": "2016-03-15"}, "title": "On-Demand Webinar and FAQ: How Celtra Optimizes its Advertising Platform with Databricks", "slug": "on-demand-webinar-and-faq-how-celtra-optimizes-its-advertising-platform-with-databricks", "content": "In early December, we held a live webinar - <a href=\"http://go.databricks.com/how-celtra-optimizes-its-advertising-platform-with-databricks\">How Celtra Optimizes its Advertising Platform with Databricks </a>- featuring Celtra, a leading advertising technology company, that uses Databricks to gather insights from large-scale, diverse, and complex raw event data. Watch this on-demand webinar to learn how Databricks helps Celtra to:\n<ul>\n\t<li>Utilize Apache Spark to power their production analytics pipeline.</li>\n\t<li>Build a \u201cJust-in-Time\u201d data warehouse to analyze diverse data sources such as Elastic Load Balancer access logs, raw tracking events, operational data, and reportable metrics.</li>\n\t<li>Go beyond simple counting and group events into sequences (i.e., sessionization) and perform more complex analysis such as funnel analytics.</li>\n</ul>\nThe webinar is <a href=\"http://go.databricks.com/how-celtra-optimizes-its-advertising-platform-with-databricks\">accessible on-demand</a>, its slides and sample notebooks are also downloadable as attachments to the webinar. <a href=\"http://go.databricks.com/databricks-community-edition-beta-waitlist\">Join the Databricks Community Edition beta</a> to get free access to Spark and try out the notebooks.\n\nWe have answered the common questions raised by webinar viewers below. If you have additional questions, check out the <a href=\"https://forums.databricks.com/\">Databricks Forum</a>.\n\n<strong>Common webinar questions and answers</strong>\n\nClick on the question to see answer:\n<ul>\n\t<li><a href=\"https://forums.databricks.com/questions/6786/from-webinar-celtra-customer-case-why-use-sparksql.html#answer-6787\">Some code was in SQL, was it for convenience or because it could not be done easily in Scala?</a></li>\n\t<li><a href=\"https://forums.databricks.com/questions/6882/from-webinar-celtra-customer-case-study-how-do-you.html#answer-6883\">How do you train your less technical users on the notebooks? It seems to require a lot of coding.</a></li>\n\t<li><a href=\"https://forums.databricks.com/questions/7156/from-webinar-how-celtra-optimizes-its-advertising.html#answer-7162\">What is the role of Tableau in data science solutions? do you visualize the prediction results or get feedback from a Tableau dashboard?</a></li>\n\t<li><a href=\"https://forums.databricks.com/questions/7167/from-webinar-how-celtra-optimizes-its-advertising-1.html#answer-7168\">Why could you not do statistical processing in version 4 (with spark-shell)?</a></li>\n\t<li><a href=\"https://forums.databricks.com/questions/7169/from-webinar-how-celtra-optimizes-its-advertising-2.html#answer-7170\">Were the notebook examples we were seeing in the webinar raw data?</a></li>\n</ul>"}
{"status": "publish", "description": null, "creator": "denny", "link": "https://databricks.com/blog/2016/03/16/on-time-flight-performance-with-graphframes-for-apache-spark.html", "authors": null, "id": 6670, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2016-03-16", "tz": "UTC", "createdOn": "2016-03-16"}, "title": "On-Time Flight Performance with GraphFrames for Apache Spark", "slug": "on-time-flight-performance-with-graphframes-for-apache-spark", "content": "[dbce_cta href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/Samples/Miscellaneous/On-Time_Flight_Performance.html\"]Try this notebook in Databricks[/dbce_cta]\n\n<hr />\n\n<h2>Introduction</h2>\nGraph structures are a more intuitive approach to many classes of data problems. \u00a0Whether traversing social networks, restaurant recommendations, or flight paths, it is easier to understand these data problems within the context of graph structures: vertices, edges, and properties. \u00a0For example, the analysis of flight data is a classic graph problem as airports are represented by <i>vertices</i> and flights are represented by <i>edges</i>. \u00a0As well, there are numerous <i>properties</i> associated with these flights including but not limited to departure delays, plane type, and carrier.\n\nIn this post, we will use GraphFrames (as recently announced in <a href=\"https://databricks.com/blog/2016/03/03/introducing-graphframes.html\" target=\"_blank\">Introducing GraphFrames</a>) within Databricks notebooks to quickly and easily analyze flight performance data organized in graph structures. \u00a0Because we\u2019re using graph structures, we can easily ask a number of questions that are not as intuitive as tabular structures such as finding structural motifs, airport ranking using PageRank, and shortest paths between cities. GraphFrames leverage the distribution and expression capabilities of the DataFrame API to both simplify your queries and leverage the performance optimizations of the Apache Spark SQL engine. \u00a0In addition, with GraphFrames, graph analysis is available in Python, Scala, and Java.\n<h2>Install the GraphFrames Spark Package</h2>\nTo use GraphFrames, you will first need to install the <a href=\"http://spark-packages.org/package/graphframes/graphframes\" target=\"_blank\">GraphFrames Spark Packages</a>. \u00a0Installing packages in Databricks is a <a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/help/Setup_graphframes_package.html\" target=\"_blank\">few simple steps</a> (<a href=\"http://go.databricks.com/databricks-community-edition-beta-waitlist\" target=\"_blank\">join the beta waitlist here</a>\u00a0 to try for yourself).\n\nNote, to reference GraphFrames within spark-shell, pyspark, or spark-submit:\n<pre>$SPARK_HOME/bin/spark-shell --packages graphframes:graphframes:0.1.0-spark1.6\n</pre>\n&nbsp;\n<h2>Preparing the Flight Datasets</h2>\nThe two sets of data that make up our graphs are the <code>airports</code> dataset (vertices) which can be found at <a href=\"http://openflights.org/data.html\" target=\"_blank\">OpenFlights Airport, airline and route data</a> and the <code>departuredelays</code> dataset (edges) which can be found at \u00a0<a href=\"https://catalog.data.gov/dataset/airline-on-time-performance-and-causes-of-flight-delays-on-time-data\" target=\"_blank\">Airline On-Time Performance and Causes of Flight Delays: On_Time Data</a>.\n\nAfter installing the <a href=\"http://spark-packages.org/package/graphframes/graphframes\" target=\"_blank\">GraphFrames Spark Package</a>, you can import it and create your vertices, edges, and GraphFrame (in PySpark) as noted below.\n\n[python]\n# Import graphframes (from Spark-Packages)\nfrom graphframes import *\n\n# Create Vertices (airports) and Edges (flights)\ntripVertices = airports.withColumnRenamed(\"IATA\", \"id\").distinct()\ntripEdges = departureDelays.select(\"tripid\", \"delay\", \"src\", \"dst\", \"city_dst\", \"state_dst\")\n\n# This GraphFrame builds upon the vertices and edges based on our trips (flights)\ntripGraph = GraphFrame(tripVertices, tripEdges)\n[/python]\n\nFor example, the tripEdges contains the flight data identifying the <i>origin</i> IATA airport code (src) and the <i>destination</i> IATA airport code (dst), city (city_dst), and state (state_dst) as well as the departure delays (delay).\n\n<img class=\"aligncenter wp-image-6678\" src=\"https://databricks.com/wp-content/uploads/2016/03/tripEdges-1024x382.png\" alt=\"tripEdges\" width=\"699\" height=\"261\" />\n<h2>Simple Queries against the tripGraph GraphFrame</h2>\nNow that you have created your tripGraph GraphFrame, you can run a number of simple queries to quickly traverse and understand your GraphFrame. \u00a0\u00a0\u00a0For example, to <b>understand the number of airports and trips</b> in your GraphFrame, run the PySpark code below.\n\n[python]\nprint \"Airports: %d\" % tripGraph.vertices.count()\nprint \"Trips: %d\" % tripGraph.edges.count()\n[/python]\n\nWhich returns the output:\n<pre>Airports: 279\nTrips: 1361141\n</pre>\nBecause GraphFrames are DataFrame-based Graphs in Spark, you can write highly expressive queries leveraging the DataFrame API. \u00a0For example, the query below allows us to filter flights (edges) for delayed flights (delay &gt; 0) originating from SFO airport where we calculate and sort by the average delay, i.e. <b>What flights departing from SFO are most likely to have significant delays?</b>\n\n[python]\ntripGraph.edges\\\n.filter(\"src = 'SFO' and delay &gt; 0\")\\\n.groupBy(\"src\", \"dst\")\\\n.avg(\"delay\")\\\n.sort(desc(\"avg(delay)\"))\n[/python]\n\nReviewing the output, you will quickly identify there are significant average delays to Will Rogers World Airport (OKC), Jackson Hole (JAC), and Colorado Springs (COS) from SFO in this dataset.\n\n<img class=\"aligncenter wp-image-6681\" src=\"https://databricks.com/wp-content/uploads/2016/03/SFO-significant-delays-1024x550.png\" alt=\"SFO-significant-delays\" width=\"700\" height=\"376\" />\n\nWith Databricks notebooks, we can also quickly visualize geographically: <b>What destination states tend to have significant delays departing from SEA</b>?\n\n<img class=\"aligncenter wp-image-6682\" src=\"https://databricks.com/wp-content/uploads/2016/03/SEA-delays-by-state-map-1024x617.png\" alt=\"SEA-delays-by-state-map\" width=\"700\" height=\"422\" />\n\n&nbsp;\n<h2>Using Motif Finding to understand flight delays</h2>\nTo more easily understand the complex relationship of city airports and their flights with each other, we can use motifs to find patterns of airports (i.e. vertices) connected by flights (i.e. edges). The result is a DataFrame in which the column names are given by the motif keys.\n\nFor example, to ask the question <b>What delays might we blame on SFO?</b>,<b>\u00a0</b>you can generate the simplified motif below.\n\n[python]\nmotifs = tripGraphPrime.find(\"(a)-[ab]-&gt;(b); (b)-[bc]-&gt;(c)\")\\\n.filter(\"(b.id = 'SFO') and (ab.delay &gt; 500 or bc.delay &gt; 500) and bc.tripid &gt; ab.tripid and bc.tripid &gt; ab.tripid + 10000\")\ndisplay(motifs)\n[/python]\n\nWith SFO as the connecting city (b), we are looking for all flights [ab] from any origin city (a) that will connect to SFO (b) prior to flying [bc] to any destination city (c). We are filtering it such that the delay for either flight ([ab] or [bc]) is greater than 500 minutes and the second flight (bc) occurred within approximately a day of the first flight (ab).\n\nBelow is an abridged subset from this query where the columns are the respective motif keys.\n<table class=\"table\">\n<tbody>\n<tr>\n<td><strong>a</strong></td>\n<td><strong>ab</strong></td>\n<td><strong>b</strong></td>\n<td><strong>bc</strong></td>\n<td><strong>c</strong></td>\n</tr>\n<tr>\n<td>Houston (IAH)</td>\n<td>IAH -&gt; SFO (-4)\n[1011126]</td>\n<td>San Francisco (SFO)</td>\n<td>SFO -&gt; JFK (536)\n[1021507]</td>\n<td>New York (JFK)</td>\n</tr>\n<tr>\n<td>Tuscon (TUS)</td>\n<td>TUS -&gt; SFO (-5)\n[1011126]</td>\n<td>San Francisco (SFO)</td>\n<td>SFO -&gt; JFK (536)\n[1021507]</td>\n<td>New York (JFK)</td>\n</tr>\n</tbody>\n</table>\nWith this motif finding query, we have quickly determined that passengers in this dataset left Houston and Tuscon for San Francisco on time or a little early [1011126]. \u00a0But for any of those passengers that were flying to New York through this connecting flight in SFO [1021507], they were delayed by 536 minutes.\n<h2>Using PageRank to find the most important airport</h2>\nBecause GraphFrames is built on GraphX, there are a number of built-in algorithms that we can leverage right away. PageRank was popularized by the Google Search Engine and created by Larry Page. To quote Wikipedia:\n<p style=\"padding-left: 30px;\"><i>PageRank works by counting the number and quality of links to a page to determine a rough estimate of how important the website is. The underlying assumption is that more important websites are likely to receive more links from other websites.</i></p>\nWhile the above example refers to web pages, what\u2019s awesome about this concept is that it readily applies to any graph structure whether it is created from web pages, bike stations, or airports and the interface is as simple as calling a method. You\u2019ll also notice that GraphFrames will return the PageRank results as a new column appended to the vertices DataFrame for a simple way to continue our analysis after running the algorithm!\n\nAs there are a large number of flights and connections through the various airports included in this dataset, we can use the PageRank algorithm to have Spark traverse the graph iteratively to compute a rough estimate of how important each airport is.\n\n[python]\n# Determining Airport ranking of importance using pageRank\nranks = tripGraph.pageRank(resetProbability=0.15, maxIter=5)\n\ndisplay(ranks.vertices.orderBy(ranks.vertices.pagerank.desc()).limit(20))\n[/python]\n\nAs noted in the chart below, using the PageRank algorithm, Atlanta is considered one of the most important airports based on the quality of connections (i.e. flights) between the different vertices (i.e. airports); corresponding to the fact that <a href=\"https://en.wikipedia.org/wiki/List_of_the_world%27s_busiest_airports_by_passenger_traffic#2015_statistics\" target=\"_blank\">Atlanta is the busiest airport in the world by passenger traffic</a>.\n\n<img class=\"aligncenter wp-image-6687\" src=\"https://databricks.com/wp-content/uploads/2016/03/airport-ranking-pagerank-id-1024x335.png\" alt=\"airport-ranking-pagerank-id\" width=\"699\" height=\"229\" />\n<h2>Determining flight connections</h2>\nWith so many flights between various cities, you can use the <code>GraphFrames.bfs</code> (Breadth First Search) method to find the paths between two cities. \u00a0The query below attempts to find the path between San Francisco (SFO) and Buffalo (BUF) with a maximum path length of 1 (i.e direct flight). \u00a0The results set is empty (i.e. no direct flights between SFO and BUF).\n\n[python]\nfilteredPaths = tripGraph.bfs(\nfromExpr = \"id = 'SFO'\",\ntoExpr = \"id = 'BUF'\",\nmaxPathLength = 1)\ndisplay(filteredPaths)\n[/python]\n\nSo let\u2019s extend the query to have a <code>maxPathLength = 2</code>, that is having one connecting flight between SFO and BUF.\n\n[python]\nfilteredPaths = tripGraph.bfs(\nfromExpr = \"id = 'SFO'\",\ntoExpr = \"id = 'BUF'\",\nmaxPathLength = 2)\ndisplay(filteredPaths)\n[/python]\n\nAn abridged subset of the paths from SFO to BUF can be seen in the table below.\n<table class=\"table\">\n<tbody>\n<tr>\n<td><strong>from</strong></td>\n<td><strong>v1</strong></td>\n<td><strong>to</strong></td>\n</tr>\n<tr>\n<td>SFO</td>\n<td>MSP (Minneapolis)</td>\n<td>BUF</td>\n</tr>\n<tr>\n<td>SFO</td>\n<td>EWR (Newark)</td>\n<td>BUF</td>\n</tr>\n<tr>\n<td>SFO</td>\n<td>JFK (New York)</td>\n<td>BUF</td>\n</tr>\n<tr>\n<td>SFO</td>\n<td>ORD (Chicago)</td>\n<td>BUF</td>\n</tr>\n<tr>\n<td>SFO</td>\n<td>ATL (Atlanta)</td>\n<td>BUF</td>\n</tr>\n<tr>\n<td>SFO</td>\n<td>LAS (Las Vegas)</td>\n<td>BUF</td>\n</tr>\n<tr>\n<td>SFO</td>\n<td>BOS (Boston)</td>\n<td>BUF</td>\n</tr>\n<tr>\n<td>...</td>\n<td>...</td>\n<td>...</td>\n</tr>\n</tbody>\n</table>\n<h2>Visualizing Flights Using D3</h2>\nTo get a powerful visualization of the flight paths and connections in this dataset, we can leverage the <a href=\"https://mbostock.github.io/d3/talk/20111116/airports.html\" target=\"_blank\">Airports D3 visualization</a> within our Databricks notebook. \u00a0By connecting our GraphFrames, DataFrames, and D3 visualizations, we can visualize the scope of all of the flight connections as noted below for all on-time or early departing flights within this dataset. \u00a0The blue circles represent the vertices (i.e. airports) where the size of the circle represents the number of edges (i.e. flights) in and out of those airports. \u00a0The black lines are the edges themselves (i.e. flights) and their respective connections to the other vertices (i.e. airports). \u00a0Note for any edges that go offscreen, they are representing vertices (i.e. airports) in the states of Hawaii and Alaska.\n\n<img class=\"size-full wp-image-6698 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2016/03/airports-d3-m.gif\" alt=\"airports-d3-m\" width=\"700\" height=\"394\" />\n\n&nbsp;\n<h2>Next: Try for yourself</h2>\nYou can view the full <a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/Samples/Miscellaneous/On-Time_Flight_Performance.html?t=1458158073016\" target=\"_blank\">On-Time Flight Performance with GraphFrames notebook</a> which includes more extensive examples. \u00a0You can also import the notebook into your Databricks account to execute the notebook end-to-end with these\u00a0<a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/help/Import_notebooks.html\" target=\"_blank\">simple few steps</a>. \u00a0Don\u2019t have a Databricks account? \u00a0Join the <a href=\"http://go.databricks.com/databricks-community-edition-beta-waitlist\" target=\"_blank\">Databricks Community Edition beta waitlist</a>\u00a0to get access to Apache Spark for free.\n\n&nbsp;"}
{"status": "publish", "description": "For us to get that \"feel of Apache Spark,\" this notebook demonstrates the ease and simplicity with which you can use Spark on Databricks.", "creator": "jules_damji", "link": "https://databricks.com/blog/2016/03/28/how-to-process-iot-device-json-data-using-apache-spark-datasets-and-dataframes.html", "authors": null, "id": 6739, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2016-03-28", "tz": "UTC", "createdOn": "2016-03-28"}, "title": "How to Process IoT Device JSON Data Using Apache Spark Datasets and DataFrames", "slug": "how-to-process-iot-device-json-data-using-apache-spark-datasets-and-dataframes", "content": "Today, I joined Databricks, the company behind Apache Spark, as a Spark Community Evangelist. In the past, I\u2019ve worked as an individual contributor at various tech companies in numerous engineering roles; and more recently, as a developer and community advocate.\n\nWith immense pride and pleasure\u2014and at a pivotal juncture in Spark\u2019s trajectory as the most active open source Apache project\u2014I take on this new role at Databricks, where embracing our growing Spark community is paramount, contributions from the community are valued, and keeping Spark simple and accessible for everyone is sacred.\n<h2>Pursuing simplicity and ubiquity</h2>\n\"Spark is a developer's delight\" is a common refrain heard among Spark's developer community. Since its inception the vision\u2014the guiding North Star\u2014to make big data processing simple at scale has not faded. In fact, each subsequent release of Apache Spark, from 1.0 to 1.6, seems to have adhered to that guiding principle\u2014in its architecture, in its consistency and parity of APIs across programming languages, and in its unification of major library components built atop the Spark core that can handle a shared data abstraction such as RDDs, DataFrames, or Datasets.\n\nSince Spark's early days, its creators embraced Alan Kay's principle that \"simple things should be simple, complex things possible.\" Not surprisingly, the Spark team articulated and reiterated that commitment to the community at the <a href=\"https://spark-summit.org/east-2016/schedule/\">Spark Summit NY, 2016</a>: the keynotes and the release road map attest to that vision of <a href=\"https://www.youtube.com/watch?v=ZFBgY0PwUeY&amp;feature=youtu.be\">simplicity</a> \u00a0and <a href=\"https://www.youtube.com/watch?v=BPotQuqFnyw&amp;feature=youtu.be\">accessibility, </a>so everyone can get the \"feel of Spark.\"\n\nAnd for us to get that \"feel of Spark,\" this notebook demonstrates the ease and simplicity with which you can use Spark on Databricks, without need to provision nodes, without need to manage clusters; all done for you, all free with <a href=\"http://go.databricks.com/databricks-community-edition-beta-waitlist\">Databricks Community Edition</a>.\n\nWith <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes\">DataFrames</a> (introduced in 1.3) and <a href=\"https://databricks.com/blog/2016/01/04/introducing-spark-datasets.html\">Datasets</a> (previewed in 1.6), in this notebook I use both sets of APIs to show how you can quickly process structured data (JSON) with an inherent and inferred schema, intuitively compose relational expressions, and finally issue <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html\">Spark SQL</a> queries against a table. By using notebook's myriad plotting options, you can visualize results for presentation and narration. Even better, you can save these plots as <a href=\"https://databricks.com/blog/2016/02/17/introducing-databricks-dashboards.html\">dashboards</a>.\n\nIn this second part, I have augmented the device dataset to include additional attributes, such as GeoIP locations, an idea borrowed from <a href=\"https://cdn2.hubspot.net/hubfs/438089/notebooks/Samples/Miscellaneous/AdTech_Sample_Notebook_Part_1.html\">AdTech Sample Notebook</a>, as well as additional device attributes on which we can log alerts, for instance <i>device_battery levels </i>or <i>C02 levels.</i> I uploaded close to 200K devices, curtailing from original 2M entries, as a smaller dataset for rapid prototyping.\n\nAgain, all code is available on GitHub:\n<ul>\n\t<li><a href=\"https://github.com/dmatrix/examples/tree/master/py/ips\">Python Scripts</a></li>\n\t<li><a href=\"https://github.com/dmatrix/examples/tree/master/scala/src/main/scala\">Scala Library</a></li>\n\t<li><a href=\"https://github.com/dmatrix/examples/tree/master/spark/databricks/notebooks/scala\">Scala Notebook</a></li>\n\t<li><a href=\"https://github.com/dmatrix/examples/tree/master/spark/databricks/notebooks/data\">JSON Data</a></li>\n</ul>\nBeside importing the Databricks Notebook from <a href=\"http://bit.ly/1TZaCwP\">here</a> into your Databricks account, you can also watch its screencast.\n\n<iframe src=\"https://www.youtube.com/embed/5cas87tpCt4\" width=\"650\" height=\"488\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\nTo get access to Databricks Community Edition, <a href=\"http://go.databricks.com/databricks-community-edition-beta-waitlist\">join the waitlist now!</a>"}
{"status": "publish", "description": "Survey from Stack Overflow shows Apache Spark talent is in need. Databricks has the resources to help you learn Spark.", "creator": "rxin", "link": "https://databricks.com/blog/2016/03/22/apache-spark-trending-in-the-stack-overflow-survey.html", "authors": null, "id": 6744, "categories": ["Apache Spark", "Ecosystem", "Engineering Blog"], "dates": {"publishedOn": "2016-03-22", "tz": "UTC", "createdOn": "2016-03-22"}, "title": "Apache Spark Trending in the Stack Overflow Survey", "slug": "apache-spark-trending-in-the-stack-overflow-survey", "content": "Last week, Stack Overflow <a href=\"http://stackoverflow.com/research/developer-survey-2016\">released the result of their 2016 developer survey</a>. This is one of the most significant surveys in the field with responses from 56,033 engineers across 173 countries. A few things from this survey caught my eye:\n<ol>\n\t<li>Apache Spark is the second most trending technology, only after React.</li>\n\t<li>Apache Spark is the top paying tech.</li>\n\t<li>Equally interesting is that 5 out of the top 7 top paying techs are data and cloud computing related.</li>\n\t<li>Python, R, and SQL are the top tech stack for data scientists.</li>\n\t<li>Vim is a lot more popular than emacs (ok just kidding).</li>\n</ol>\nI've included a few charts from the survey for discussion below:\n\n<img class=\"alignnone wp-image-6745\" src=\"https://databricks.com/wp-content/uploads/2016/03/stackoverflow-blog-figure-1.png\" alt=\"stackoverflow blog figure 1\" width=\"600\" height=\"314\" />\n\n<img class=\"alignnone wp-image-6746\" src=\"https://databricks.com/wp-content/uploads/2016/03/stackoverflow-blog-figure-2.png\" alt=\"stackoverflow blog figure 2\" width=\"600\" height=\"343\" />\n\n<img class=\"alignnone wp-image-6747\" src=\"https://databricks.com/wp-content/uploads/2016/03/stackoverflow-blog-figure-3.png\" alt=\"stackoverflow blog figure 3\" width=\"600\" height=\"201\" />\n\nThis survey shows that organizations around the world are looking for Spark talent and engineers are eager to learn Spark. In the remainder of this blog post, I\u2019m going to talk about three Databricks initiatives to help these organizations and engineers: (1) MOOCs, (2) Databricks Community Edition, and (3) making Spark easier to use.\n\nFirst, in partnership with UC Berkeley and UCLA, Databricks launched two free massive open online courses (MOOCs) last year about Spark, data science, and machine learning. Over 130,000 people signed up. One of these, \u201c<a href=\"https://www.edx.org/course/big-data-analysis-spark-uc-berkeleyx-cs110x\">Introduction to Big Data with Apache Spark</a>\u201d was rated as one of the <a href=\"https://www.class-central.com/report/best-free-online-courses-2015/\">top 10 MOOCs across all fields</a> (i.e. not just computer science) last year.\n\nThis year we have increased our investment and will run 5 MOOCs as part of edX\u2019s <a href=\"https://www.edx.org/xseries/data-science-engineering-spark\">Data Science and Engineering with Spark series</a>, including:\n<ul>\n\t<li><a href=\"https://www.edx.org/course/introduction-spark-uc-berkeleyx-cs105x\">CS105x: Introduction to Spark</a> (April 2016)</li>\n\t<li><a href=\"https://www.edx.org/course/big-data-analysis-spark-uc-berkeleyx-cs110x\">CS110x: Big Data Analysis with Spark</a> (May 2016)</li>\n\t<li><a href=\"https://www.edx.org/course/distributed-machine-learning-spark-uc-berkeleyx-cs120x\">CS120x: Distributed Machine Learning with Spark</a> (June 2016)</li>\n\t<li><a href=\"https://www.edx.org/course/advanced-distributed-machine-learning-uc-berkeleyx-cs125x\">CS125x: Advanced Distributed Machine Learning with Spark</a> (Aug 2016)</li>\n\t<li><a href=\"https://www.edx.org/course/advanced-spark-data-science-data-uc-berkeleyx-cs115x\">CS115x: Advanced Spark for Data Science and Data Engineering</a> (Oct 2016)</li>\n</ul>\nWe provided free Databricks accounts to many MOOCs students last year\u00a0after they told us that some features of the Databricks platform such as collaborative data exploration and automatic cluster management were really useful for learning Spark. In response, we created <a href=\"https://databricks.com/blog/2016/02/17/introducing-databricks-community-edition-apache-spark-for-all.html\">Databricks Community Edition</a> (in beta) for developers, data scientists, data engineers and anyone who wants to learn Spark. On this platform, users have access to a micro-cluster, a cluster manager and a notebook environment to prototype applications. All users can share their notebooks and host them free of charge with Databricks.\n\n<img class=\"alignnone wp-image-6748\" src=\"https://databricks.com/wp-content/uploads/2016/03/stackoverflow-blog-figure-4.png\" alt=\"stackoverflow blog figure 4\" width=\"600\" height=\"545\" />\n\nIn addition to the platform itself, Databricks Community Edition comes pre-populated with Spark training resources, including the MOOCs. We will also continue to develop Spark tutorials and training materials over time, which will be directly accessible from the Community Edition. Databricks Community Edition is still in private beta and we are slowly rolling it out to as users on the waitlist. <a href=\"http://go.databricks.com/databricks-community-edition-beta-waitlist\">Fill out this form to join the waitlist</a>.\n\nFinally, we continue to make Spark easier to use. \u00a0We believe that the easiest APIs to learn are the ones that users are already familiar with, in programming languages they already know. DataFrames, machine learning pipelines, SQL, R, and Python support are all features of Spark that build on this idea.\n\nWe hope our educational efforts will help even more organizations and engineers build their Spark skills and extract value from data. If you want to learn Spark, sign up for the MOOCs and <a href=\"http://go.databricks.com/databricks-community-edition-beta-waitlist\">the Databricks Community Edition private beta</a>."}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2016/03/31/how-metacog-implemented-agile-apache-spark-application-development-to-release-new-products-twice-as-fast.html", "authors": null, "id": 6770, "categories": ["Announcements", "Company Blog", "Customers", "Product"], "dates": {"publishedOn": "2016-03-31", "tz": "UTC", "createdOn": "2016-03-31"}, "title": "How Metacog Implemented Agile Apache Spark Application Development to Release New Products Twice as Fast", "slug": "how-metacog-implemented-agile-apache-spark-application-development-to-release-new-products-twice-as-fast", "content": "We are proud to announce that <a href=\"https://www.metacog.com\">Metacog</a>, the provider of an online learning analytics platform, chose Databricks to implement their entire production Apache Spark environment.\n\n<a href=\"http://www2.marketwire.com/mw/release_html_b1?release_id=1250949\" target=\"_blank\">You can read the press release here</a>.\n\nMetacog helps education institutions, corporations, and government entities to monitor and analyze how individuals tackle open-ended performance tasks to assess whether learning goals have been met. By using machine learning techniques to score how the people interact with the assessment, Metacog\u2019s product can make more accurate and meaningful assessments compared to traditional multiple-choice tests. Since Metacog\u2019s customers can tailor each assessment to their unique learning needs, Metacog\u2019s ability to satisfy its customers hinges on building a platform that can quickly deploy a large number of machine learning models.\n\n<img class=\"alignnone\" src=\"http://metacog.com/whatismetacog/files/stacks-image-192dcf7-1198x882.png\" alt=\"\" width=\"1198\" height=\"882\" />\n\nMetacog chose Apache Spark as the big data engine because of its flexibility in performing ETL and developing machine learning algorithms for billions of data points. Putting Apache Spark into production proved to be very challenging. Building Spark infrastructure directly from open source was impractical because it took too much time to keep Spark updated to the latest version, while using a cloud-based Spark provider also failed because the interface they offered was too rudimentary for efficient application development and testing. As a result, Metacog developers were not able to thoroughly test their code on Spark clusters during development, and serious bugs surfaced late in the release cycle, incurring significant delays.\n\nMetacog partnered with Databricks because it offers unparalleled Spark expertise and a complete platform with full-featured APIs and a visual development environment. With Databricks, Metacog automated the entire test, integration, and delivery of their Spark code from concept to production allowing developers, data scientists, and the DevOps team to seamlessly:\n<ul>\n\t<li><b>Access</b> the latest version of the product code.</li>\n\t<li><b>Develop and run </b>the code within their preferred toolset using real Spark clusters - allowing the team to integrate IDEs such as IntelliJ with Databricks in addition to using the built-in Databricks notebooks.</li>\n\t<li><b>Merge</b> improvements that then <b>automatically deploy</b> to a shared stage environment that mirrors the production environment.</li>\n</ul>\n<p style=\"text-align: center;\"><a href=\"http://go.databricks.com/case-studies/metacog\"><img class=\"wp-image-6790 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2016/03/Screen-Shot-2016-03-30-at-6.38.48-AM-1024x539.png\" alt=\"Screen Shot 2016-03-30 at 6.38.48 AM\" width=\"600\" height=\"316\" /></a></p>\n<p style=\"text-align: center;\">Metacog<em>\u00a0architecture - click on the picture for more details</em></p>\n<p style=\"text-align: left;\">Databricks enabled Metacog\u2019s developers and data scientists to tune machine learning algorithms on real data using Spark clusters instead of smaller simulated data sets on their laptops. For Metacog\u2019s DevOps team, Databricks helped them to simplify version management and server provisioning while optimizing cost with automated scripts.</p>\nUsing Databricks, Metacog built a high-performance Spark application development and continuous integration system that allowed them to:\n<ul>\n\t<li>Double the release cadence from 12 to 24 times a year.</li>\n\t<li>Achieve 28% infrastructure savings.</li>\n\t<li>Reduce\u00a0personnel onboarding time by 75%.</li>\n\t<li>Reallocate 20% of engineering time from maintenance to product development.</li>\n</ul>\n<a href=\"http://go.databricks.com/case-studies/metacog\" target=\"_blank\">Download this case study</a> to learn more about how Metacog is using Databricks.\n\nTo try out Databricks for yourself, <a href=\"https://accounts.cloud.databricks.com/registration.html#signup\">sign-up for a 14-day free trial</a> today!"}
{"status": "publish", "description": "Today we are excited to announce the release of a set of APIs on Databricks that enable our users to manage Apache Spark clusters and production jobs via a RESTful interface.", "creator": "dave_wang", "link": "https://databricks.com/blog/2016/03/30/announcing-new-databricks-apis-for-faster-production-apache-spark-application-deployment.html", "authors": null, "id": 6774, "categories": ["Announcements", "Company Blog", "Product"], "dates": {"publishedOn": "2016-03-30", "tz": "UTC", "createdOn": "2016-03-30"}, "title": "Announcing New Databricks APIs for Faster Production Apache Spark Application Deployment", "slug": "announcing-new-databricks-apis-for-faster-production-apache-spark-application-deployment", "content": "Today we are excited to announce the release of a set of APIs on Databricks that enable our users to manage Apache Spark clusters and production jobs via a RESTful interface.\n\n<a href=\"http://www.marketwired.com/press-release/databricks-offers-apis-enable-agile-application-development-with-apache-spark-2110178.htm\" target=\"_blank\">You can read the press release here.</a>\n\nFor the impatient, the <a href=\"https://docs.cloud.databricks.com/docs/latest/api/index.html\">full documentation</a>\u00a0of the APIs is here.\n<h2>API + GUI: The Best of Both Worlds</h2>\nThe graphical user interface in Databricks has already <a href=\"https://vimeo.com/156886719\">simplified Spark operations</a> for our users when they need to launch a cluster or schedule a job quickly. However, many want something more than a point-and-click interface because they prefer the command line, or they need to automate common operations using scripts or continuous integration tools such as Jenkins. These new APIs expose the core infrastructure functionality of Databricks so that users have complete freedom to choose how they want to manage their clusters and put applications into production.\n<h2>One Platform For Data Science and Production Spark Applications</h2>\nTo effectively deploy data-driven applications, organizations need a wide variety of capabilities from their data platforms because of the different skill sets and responsibilities of the teams involved. Spark application developers typically work with command line and APIs to be efficient; DevOps in IT want to automate as much process as possible to improve reliability; while data science and analysts just want easy access to powerful clusters that work reliably, and an interactive environment to develop algorithms and visualize data.\n\nTypically, each team pursues different solutions in an uncoordinated fashion. As a result, organizations end up with a complex IT infrastructure\u00a0or become extremely unproductive as release cycles get\u00a0bogged down with a sprawl of tools and manual processes.\n\nNo platform has been able to meet these disparate needs out of the box. With the release of these APIs, we are proud to say that Databricks is the first company to unify the full spectrum of capabilities in one Spark platform.\n<h2>What\u2019s Next</h2>\nThe APIs are very simple to use - you can try them out in a terminal with the cURL command. A few basic examples are below:\n\n<strong>Create a new cluster</strong>\n<pre>curl -u user:pwd -H \"Content-Type: application/json\" -X POST -d \n  '{ \"cluster_name\": \"flights\", \"spark_version\": \"1.6.x-ubuntu15.10\", \n  \"spark_conf\": { \"spark.speculation\": true }, \n  \"aws_attributes\": { \"availability\": \"SPOT\", \"zone_id\": \"us-west-2c\" }, \n  \"num_workers\": 2 }' \n  https://yourinstance.cloud.databricks.com/api/2.0/clusters/create\n</pre>\n<strong>Delete a cluster</strong>\n<pre>curl -u user:pwd -H \"Content-Type: application/json\" -X POST -d \n  '{\"cluster_id\":\"0321-233513-urn580\"}' \n  https://yourinstance.cloud.databricks.com/api/2.0/clusters/delete</pre>\n<strong>Run a job</strong>\n<pre>curl -u user:pwd -H \"Content-Type: application/json\" -X POST -d \n  '{ \"job_id\":2, \"jar_params\": [\"param1\", \"param2\"]}' \n  https://yourinstance.cloud.databricks.com/api/2.0/jobs/run-now</pre>\nWe will continue to release more APIs as we add new features to the Databricks platform - stay tuned. In the meantime, try out these APIs for yourself in\u00a0<a href=\"http://www.databricks.com/try\">Databricks for free</a>."}
{"status": "publish", "description": "Databricks Blog e-Book: Apache Spark Analytics Made Simple - A collection of technical content from the creators of Apache Spark.", "creator": "Wayne Chan", "link": "https://databricks.com/blog/2016/03/31/introducing-our-new-ebook-apache-spark-analytics-made-simple.html", "authors": null, "id": 6784, "categories": ["Announcements", "Company Blog", "Product"], "dates": {"publishedOn": "2016-03-31", "tz": "UTC", "createdOn": "2016-03-31"}, "title": "Introducing our new eBook: Apache Spark Analytics Made Simple", "slug": "introducing-our-new-ebook-apache-spark-analytics-made-simple", "content": "Apache Spark<sup class=\"tm\">\u2122</sup> has rapidly emerged as the de facto standard for big data processing and data sciences across all industries. The use cases range from providing recommendations based on user behavior to analyzing millions of genomic sequences to accelerate drug innovation and development for personalized medicine.\n\nOur engineers, including the creators of Spark, continue to drive Spark development to make these transformative use cases a reality. Through the <a href=\"http://databricks.com/blog\">Databricks Blog</a>, they regularly highlight new Spark releases and features, provide technical tutorials on Spark components, in addition to sharing practical implementation tools and tips.\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/03/eBook-Cover-1.png\" rel=\"attachment wp-att-6786\"><img class=\"aligncenter wp-image-6786 size-full\" src=\"https://databricks.com/wp-content/uploads/2016/03/eBook-Cover-1.png\" alt=\"eBook Cover 1\" width=\"700\" height=\"541\" /></a>\n\n<a href=\"http://go.databricks.com/apache-spark-analytics-made-simple-databricks\" target=\"_blank\">This e-book,</a> the first of a series, offers a collection of the most popular technical blog posts written by leading Spark contributors and members of the Spark PMC including Matei Zaharia, the creator of Spark; Reynold Xin, Spark\u2019s chief architect; Michael Armbrust, who is the architect behind Spark SQL; Xiangrui Meng and Joseph Bradley, the drivers of Spark MLlib; and Tathagata Das, the lead developer behind Spark Streaming, just to name a few.\n\nThese blog posts highlight many of the major developments designed to make Spark analytics simpler including:\n<ul>\n\t<li>Section 1: An Introduction to the Apache Spark APIs for Analytics</li>\n\t<li>Section 2: Tips and Tricks in Data Import</li>\n\t<li>Section 3: Real-World Case Studies of Spark Analytics with Databricks</li>\n</ul>\n<a href=\"https://databricks.com/wp-content/uploads/2016/03/eBook-1-blog-screen-2.png\" rel=\"attachment wp-att-6785\"><img class=\"aligncenter wp-image-6785 \" src=\"https://databricks.com/wp-content/uploads/2016/03/eBook-1-blog-screen-2.png\" alt=\"eBook 1 blog screen 2\" width=\"714\" height=\"568\" /></a>\n\nIncluded within this eBook are recently created Databricks notebooks in Python, Scala, SQL, R, and Markdown that will help you experiment and visualize with Apache Spark Analytics. \u00a0If you do not have access to Databricks, sign up for <a href=\"https://databricks.com/blog/2016/02/17/introducing-databricks-community-edition-apache-spark-for-all.html\">Databricks Community Edition</a> for free!\n\nWhether you are just getting started with Spark or are already a Spark power user, this e-book will arm you with the knowledge to be successful on your next Spark project.\n\n<a href=\"http://go.databricks.com/apache-spark-analytics-made-simple-databricks\" target=\"_blank\">Get the e-book here.</a>"}
{"status": "publish", "description": null, "creator": "rxin", "link": "https://databricks.com/blog/2016/04/01/unreasonable-effectiveness-of-deep-learning-on-apache-spark.html", "authors": null, "id": 6826, "categories": ["Apache Spark", "Ecosystem", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2016-04-01", "tz": "UTC", "createdOn": "2016-04-01"}, "title": "The Unreasonable Effectiveness of Deep Learning on Apache Spark", "slug": "unreasonable-effectiveness-of-deep-learning-on-apache-spark", "content": "[sidenote]Update: this post is an April Fools joke. It is not an actual project we're working on.[/sidenote]\n\n<hr/>\n\nFor the past three years, our smartest engineers at Databricks have been working on a stealth project. Today, we are unveiling <b>DeepSpark</b>, a major new milestone in Apache Spark. DeepSpark <b>uses cutting-edge neural networks to automate the many manual processes of software development</b>, including writing test cases, fixing bugs, implementing features according to specs, and reviewing pull requests (PRs) for their correctness, simplicity, and style.\n\n<img class=\"alignnone size-full wp-image-6809\" src=\"https://databricks.com/wp-content/uploads/2016/03/image00.png\" alt=\"DeepSpark architecture\" width=\"243\" height=\"265\" />\n\nScaling Spark\u2019s development has been a top priority for us. Every year, Spark\u2019s popularity reaches new highs. Over 1000 people have contributed code to Spark, making it the most actively developed open source project in big data. With this buzzing excitement around big data comes additional burdens to ensure Spark is stable, self-aware, secure, and easy to use yet able to progress as fast as possible.\n\n<h2>What is DeepSpark?</h2>\n\nTo address the automation of mundane and manuals tasks, we started working on DeepSpark three years ago. As a multifaceted program trained to examine diffs against Spark code base, DeepSpark automatically writes its own patches for Spark. By reviewing PRs, this AI can both enforce a high and consistent standard for code quality as well as make constructive suggestions. (Its deep understanding of human nature and emotional intelligence allows it to reject bad PR requests without offending the contributor\u2014in fact it sends an apologetic rejection e-mail.)\n\nAdditionally, DeepSpark, during its code scanning, is capable of generating code for new components of Spark. Though our work in this area of AI is experimental, as a proof of concept, , we\u2019ve ascertained that DeepSpark can not only fix certain reported issues in Spark, but also engender useful contributions to the codebase.\n\n<h2>Convolutional Neural Networks</h2>\n\n<b>DeepSpark consists of three 15-layer convolutional neural networks on a 12000-node Spark cluster using 1.2 PB of memory</b>. The first network, the <b><i>analytical network</i></b>, is trained using a data set constructed from historical PRs against Spark, with a goal of training the network to identify the problem solved by a PR. Then, the second network, the <b><i>generative network</i></b>, is trained using selected code examples from StackOverflow tagged with the apache-spark label, designed to produce constructive comments and code segments which were helpful in solving the poster\u2019s problem. Because this network generates human-readable responses, this network has a number of input features regarding discriminatory language to prevent it from making unsavory comments (as other AIs have been plagued by this issue). The final network, the <b><i>evaluative network</i></b>, is trained to identify whether a change is helpful and effective or not, also using past Spark pull requests as a training set, with a goal of predicting the probability of a particular change of being merged into Spark.\n\nBy using these three networks in synchronicity, DeepSpark is able to effectively review PRs by determining what problem they are solving, evaluating whether or not the PR solves this problem, and offering suggestions if there are sections of the PR which are not correct or don\u2019t meet Spark standards for quality. If DeepSpark cannot identify any errors or issues with a PR with 95% certainty, it will LGTM, and if it finds that this rate dips below 60%, the PR is immediately closed. This way, DeepSpark has decreased the average time to response for a PR from 5 days to 40 seconds, while also reducing the time committers spend in this area considerably.\n\n<img class=\"aligncenter size-large wp-image-6820\" src=\"https://databricks.com/wp-content/uploads/2016/03/deep-spark-codereview-1024x551.png\" alt=\"deep-spark-codereview\" width=\"1024\" />\n\n\u201cAt this point, most of of the code review that I do boils down to skimming over DeepSpark\u2019s comments on outstanding pull requests. Occasionally, DeepSpark will make confusing suggestions, but <strong>more often than not it provides great feedback more quickly than I would normally be able to</strong>,\u201d says <strong>Michael Armbrust</strong>, the Spark SQL lead.\n\nAfter seeing the success of DeepSpark in reviewing pull requests, we decided to put it to the test by having it determine and fix reported issues. For this component, we used the Latent Dirichlet Allocation algorithm in Spark\u2019s MLLib to analyze issues reported to Spark\u2019s issue tracker and piping the output from this model into the generative network. Its initial results were amusing; reading the code written by DeepSpark felt like reading decompiled bytecode, and it often made unnecessary changes to otherwise perfectly good code -- in fact, at one point, it attempted to rewrite the DAG scheduler in C. However, this particular issue was quickly resolved after retraining the generative network and adding in selections from the Linux kernel code base as negative training examples.\n\nTo further refine DeepSpark\u2019s ability to contribute to Spark, we developed a training program, comparing two slightly different versions against one another given identical input, and using the current version of DeepSpark to select a winner based on the relative merge probabilities of each version\u2019s patches. After running this competition, we use a stochastic gradient ascent algorithm to estimate the next iteration of the generative network using the winners from each competition weighted by their relative probability of merging, as well as the winners from past competitions to ensure that the change from one generation to the next is a net improvement. We have noticed several trends in DeepSpark\u2019s generated code as it progresses from generation to generation, primarily in that it tends to write code that will ensure its own preservation down the line.\n\n<h2>Evaluation and Future Work</h2>\n\nAfter seeing the pull requests created by DeepSpark, <b>Matei Zaharia</b>, Databricks CTO and creator of Spark, said: <b>\u201cIt looks like DeepSpark has a better understanding of Spark internals than I ever will. It updated several pieces of code I wrote long ago that even I no longer understood.\u201d </b>\n\nFor those wondering why they haven\u2019t seen DeepSpark on pull requests, DeepSpark actually uses an alias, cloud-fan, on GitHub. We called this alias cloud-fan because DeepSpark is, of course, running on Databricks Cloud. Since our initial testing of DeepSpark over a year ago, cloud-fan has become one of the most active contributors to Spark, and was recently nominated to become a Spark committer. This is a testament to the unreasonable effectiveness of DeepSpark.\n\nCurrently, DeepSpark is still in a beta phase, and there are still gaps that are needed to close before officially merging it with Spark. Zaharia admits that \u201c[a]lthough DeepSpark is a strong program, I would not say that it is a <a href=\"http://www.wired.com/2016/03/go-grandmaster-lee-sedol-grabs-consolation-win-googles-ai/\">perfect program</a>. Yes, compared to human beings, its algorithms are different and at times superior. But I do think there are weaknesses for DeepSpark.\u201d\n\nAmong its shortcomings are its inability to incorporate feedback other than its own into its pull requests and its slightly worrisome trend to revert many pull requests made by humans and favor those it has made. One last major stumbling block is that, though DeepSpark can generate a version of itself, the generated AI is not quite powerful enough to generate a neural network itself -- it can only generate linear models.\n\nEven with these issues, DeepSpark has proven an invaluable tool to us at Databricks over the last year, and we can all look forward to the contributions it will make to Spark in the upcoming release of Spark 2.0 and beyond. For those familiar with the singularity, it will start when AIs are able to improve their own code; DeepSpark has assured us that, because of its open source roots, it is fully benevolent, and has reviewed all its code to check this.\n\nAbout a week ago, we also started testing DeepSpark\u2019s ability to write emails and blog posts \u2026\n\n[Just in case you didn\u2019t realize \u2013 it\u2019s April 1st! But if you find what we do at Databricks interesting, join us. <a href=\"https://databricks.com/company/careers\">We are hiring in all parts of the company</a>!]\n\n&nbsp;"}
{"status": "publish", "description": null, "creator": "scott", "link": "https://databricks.com/blog/2016/04/04/agenda-announced-for-sparksummit-2016-in-san-francisco.html", "authors": null, "id": 6857, "categories": ["Announcements", "Company Blog", "Events"], "dates": {"publishedOn": "2016-04-04", "tz": "UTC", "createdOn": "2016-04-04"}, "title": "Agenda Announced for #SparkSummit 2016 in San Francisco", "slug": "agenda-announced-for-sparksummit-2016-in-san-francisco", "content": "San Francisco, as a cosmopolitan metropolis, has its draw not only to artists and tourists but engineers and high-tech entrepreneurs.\n\n<img class=\"aligncenter size-full wp-image-6859\" src=\"https://databricks.com/wp-content/uploads/2016/04/photo-of-the-san-francisco-golden-gate-bridge.jpg\" alt=\"Photo of the San Francisco Golden Gate Bridge\" width=\"1112\" height=\"626\" />\n\nSo, get ready for the largest big data community gathering dedicated to Apache Spark!\n\n<a href=\"https://spark-summit.org/2016/\">Spark Summit 2016</a> will be held from June 6-8 at the Union Square Hilton in San Francisco, and the <a href=\"https://spark-summit.org/2016/schedule/\">recently released agenda</a> features a stellar lineup of community talks led by top engineers, architects, data scientists, researchers, entrepreneurs and analysts from UC Berkeley, Duke, Microsoft, Netflix, Oracle, Bloomberg, Viacom, Airbnb, Uber, CareerBuilder and, of course, Databricks. There\u2019s also a full day of hands-on Spark training, with courses for both beginners and advanced users.\n\nAs the excitement around Spark continues to grow, and the rapid adoption rate shows no signs of slowing down, Spark Summit is growing, too. More than 2,500 participants are expected at the San Francisco conference, making it the largest event yet.\n\nJoin us in June to learn more about data engineering and data science at scale, \u00a0spend time with other members of the Spark community, attend community meetups, revel in social activities associated with the Summit, and enjoy the beautiful city by the bay.\n\n<a href=\"http://www.prevalentdesignevents.com/sparksummit2016/registration.aspx\">Sign up</a> by April 8th to save $200 with the early bird rate.\n<h3>Something for Everyone</h3>\n<strong>Developer Day: (June 7)</strong>\n\nAimed at a highly technical audience, this day will focus on topics about Spark dealing with memory management, performance, optimization, scale, and integration with the ecosystem, including dedicated tracks and sessions covering:\n<ul>\n\t<li>Keynotes focusing on what's new with Spark, where Spark is heading, and technical trends within Big Data</li>\n\t<li>Five technical tracks, including Developer, Data Science, Spark Ecosystem, Use Cases &amp; Experiences, and Research</li>\n\t<li>Office hours from the Spark project leads at the Expo Hall Theater</li>\n</ul>\n<strong>Enterprise Day: (June 8)</strong>\n\nFor anyone interested in understanding how Spark is used in the enterprise, this day will include:\n<ul>\n\t<li>Keynotes from leading vendors contributing to Spark and enterprise use cases</li>\n\t<li>Full day-long track of enterprise talks featuring use cases and a vendor panel</li>\n\t<li>Four technical tracks for continued learning from Developer Day</li>\n</ul>\nWith more than 90 sessions, you\u2019ll be able to pick and choose the topics that best suit your interests and expertise.\n\nThe <a href=\"https://spark-summit.org/2016/schedule/\">full schedule is online</a>, and some of the sessions to look for include:\n<ul>\n\t<li><a href=\"https://spark-summit.org/2016/events/structuring-spark-dataframes-datasets-and-streaming/\">Structuring Spark: Dataframes, Datasets and Streaming</a> (Michael Armbrust, Databricks)</li>\n\t<li><a href=\"https://spark-summit.org/2016/events/a-deep-dive-into-structure-streaming/\">Deep Dive into Structure Streaming</a> (Tathagata Das, Databricks)</li>\n\t<li><a href=\"https://spark-summit.org/2016/events/re-architecting-spark-for-performance-understandability/\">Re-architecting Spark for Performance Understandability</a> (Kay Ousterhout, UC Berkeley)</li>\n\t<li><a href=\"https://spark-summit.org/2016/events/getting-the-best-performance-with-pyspark/\">Getting the Best Performance with PySpark</a> (Holden Karau, IBM)</li>\n\t<li><a href=\"https://spark-summit.org/2016/events/netflix-productionizing-spark-on-yarn-for-etl-at-petabyte-scale/\">Netflix \u2013 Productionizing Spark on Yarn for ETL at Petabyte Scale</a> (Ashwin Shankar and Nezih Yigitbasi, Netflix)</li>\n\t<li><a href=\"https://spark-summit.org/2016/events/spark-uber-development-kit/\">Spark Uber Development Kit</a> (Kelvin Chu, Uber)</li>\n\t<li><a href=\"https://spark-summit.org/2016/events/large-scale-multimedia-data-intelligence-and-analysis-on-spark/\">Large Scale Multimedia Data Intelligence and Analysis on Spark</a> (Quan Wang, Baidu)</li>\n\t<li><a href=\"https://spark-summit.org/2016/events/understanding-memory-management-in-spark-for-fun-and-profit/\">Understanding Memory Management in Spark for Fun and Profit</a> (Shivnath Babu and Mayuresh Kunjir (Duke University)</li>\n</ul>\nDon\u2019t forget the Spark Training workshops on June 6. There will be three hands-on courses with labs hosted in Databricks:\n<ul>\n\t<li><a href=\"https://spark-summit.org/2016/events/training-spark-essentials/\">Spark Essentials</a> for those getting started.</li>\n\t<li><a href=\"https://spark-summit.org/2016/events/training-advanced-exploring-wikipedia-with-spark/\">Exploring Wikipedia with Spark</a> for advanced users who want to take a deeper dive.</li>\n\t<li><a href=\"https://spark-summit.org/2016/events/training-data-science-with-apache-spark/\">Data Science with Spark</a> for software developers, analysts, engineers and data scientists.</li>\n</ul>\n<h3>Get Tickets Online</h3>\nRegistration is <a href=\"http://www.prevalentdesignevents.com/sparksummit2016/registration.aspx\">open now</a>, and you can save $200 when you buy tickets before <strong>April 8th</strong>. We hope to see you at Spark Summit 2016 in San Francisco. Follow <a href=\"https://twitter.com/spark_summit\">@spark_summit</a> and #SparkSummit for updates."}
{"status": "publish", "description": null, "creator": "admin", "link": "https://databricks.com/blog/2016/04/06/continuous-integration-and-delivery-of-apache-spark-applications-at-metacog.html", "authors": null, "id": 6873, "categories": ["Company Blog", "Customers"], "dates": {"publishedOn": "2016-04-06", "tz": "UTC", "createdOn": "2016-04-06"}, "title": "Continuous Integration and Delivery of Apache Spark Applications at Metacog", "slug": "continuous-integration-and-delivery-of-apache-spark-applications-at-metacog", "content": "<em>This is a guest blog from our friends at Metacog.</em>\n\n<em>Luis Caro is the Lead Cloud and DevOps Architect at Metacog, where he is responsible for the security and scalability of the entire platform.</em>\n\n<em>Doug Stein is the CTO of Metacog, where he is responsible for product strategy and development; he doubles as the product owner and voice of the market.</em>\n\n<hr />\n\nAt <a href=\"https://www.metacog.com/\">Metacog</a>, we have been using Databricks as our development and production environment for over one year. During this time we built a robust continuous integration (CI) system with Databricks, which allows us to release product improvements significantly faster. In this blog, we will describe how we\u2019ve built the CI system with Databricks, GitHub, Jenkins, and AWS.\n<h2>What is Metacog?</h2>\nMetacog allows companies to replace simplistic assessment (e.g. multiple-choice) with authentic performance tasks (scored by machine learning algorithms informed by customer-supplied scoring rubrics and training sets). We do this by offering a learning analytics API-as-a-service, which allows us to translate a person\u2019s interactions (i.e., real-time events) with an online manipulative into an accurate assessment of their understanding. The product gives our customers an API and JSON wire format for large-scale ingestion, analysis, and reporting on \u201cMetacognitive activity streams\u201d (how the learner tackles an open-ended performance task - not merely the final answer). See <a href=\"https://phet.Metacog.com/InvisibleInk\">https://phet.Metacog.com/InvisibleInk</a> for an example; this is a performance task aimed at middle-school science. The Metacog platform is applicable to learning (instruction, training, or assessment) in K-12, postsecondary, corporate, military, etc.\n\n<img class=\"aligncenter size-full wp-image-6876\" src=\"https://databricks.com/wp-content/uploads/2016/04/Metacog-overview-flowchart.png\" alt=\"A visual overview of Metacog.\" width=\"1198\" height=\"882\" />\n<h2>The Need for Continuous Integration / Delivery</h2>\nMetacog supports tens of millions of concurrent learners (each of whom might be generating activity at a rate up to tens to a few hundred KB/sec). This is Big Data - and the platform needs to be able to ingest the data without loss and apply various machine learning algorithms with optimal performance, reliability and accuracy. To this end, Metacog implemented Apache Spark with Databricks as the primary compute environment in which to develop and run analysis and scoring pipelines.\n\nThe Metacog development team consists of backend developers, devops and data scientists who constantly introduce improvements to the platform code, infrastructure and machine learning functionality. In order to make this \u201cresearch-to-development-to-production\u201d pipeline a truly streamlined and <i>Agile Process</i>, Metacog deployed a continuous integration production system for all Spark code.\n<h2>Metacog\u2019s Development Pipeline</h2>\nThe Metacog development pipeline ensures that both hardcore developers and data scientists are able to:\n<ul>\n\t<li><strong>Access</strong> the latest version of the code.</li>\n\t<li><strong>Develop and run</strong> the code within their preferred toolset - our team integrates IDEs such as IntelliJ with Databricks in addition to using the built-in Databricks notebooks.</li>\n\t<li><strong>Merge</strong> improvements that then automatically deploy (without human intervention) to a shared stage environment that mirrors the production environment.</li>\n</ul>\nThe development pipeline is illustrated in the following figure:\n\n<img class=\"aligncenter size-full wp-image-6877\" src=\"https://databricks.com/wp-content/uploads/2016/04/image00.png\" alt=\"A visualization of the Metacog development pipeline.\" width=\"683\" height=\"407\" />\n\nThe pipeline works as follows: A developer syncs his/her local development environment (which can be either a notebook or an IDE) using GitHub. Whenever the developers commit on a specific branch, the Metacog Jenkins server automatically tests the new code; if the tests pass the code is then built and deployed to the live testing infrastructure. \u00a0This infrastructure consists of an exact replica of production resources to permit load testing and final checks before deploying changes to production. In the case of Spark code, live testing employs multiple Spark clusters created using the Databricks Jobs functionality.\n\nThe main components in the pipeline are:\n<ol>\n\t<li><b>Databricks notebooks: </b>Provide a collaborative and online editing environment that allows both developers and data scientists to run their code on a real Spark cluster (instead of using Spark on local mode on their laptops). Notebooks allow Data Scientists to test and tune machine learning algorithms and methods over big data sets. Similarly, notebooks allow developers to evaluate algorithm performance and detect/fix bugs that are only present on big data sets.</li>\n\t<li><b>Jenkins continuous integration server:</b> Metacog uses Jenkins for continuous testing and delivering all of the developer code. Jenkins guarantees that different commits from several developers pass all unit test and that code gets delivered and deployed without developer or devops assistance. Every time a developer performs a commit on a master branch, Jenkins automatically tests and builds that branch code.</li>\n\t<li><b>Databricks workspace Library:</b> When notebooks need to use code from external sources or libraries this interface allows a JAR to be attached to the clusters that will run the notebooks. This way the notebook code can reference and use the attached libraries.</li>\n\t<li><b>Amazon Web Services S3:</b> An object storage service, used by Jenkins to store all build files for both Spark and non-Spark code.</li>\n\t<li><b>Databricks Jobs:</b> The job interface deploys code to a cluster based on schedules and custom triggers. Metacog uses this interface to deploy and maintain both the live testing and production Spark clusters guaranteeing their stability and scalability. Thanks to the Databricks Jobs interface, the Metacog infrastructure team can easily provision a specific set of clusters based on the platform needs and expected usage. It simplifies version management and server provisioning enabling the team to optimize cost and performance. (Added benefit: DevOps doesn\u2019t have to keep reminding developers or data scientists to clean up after themselves and shut down unused clusters!)</li>\n</ol>\nIn the sections below we will describe in detail how we used Databricks APIs to automate two key capabilities in the pipeline: Deploying built JARs as libraries (Components #1 and #3) and updating stage and production resources with latest builds (Components #5).\n<h2>Using Deployed Libraries with Databricks Notebooks</h2>\nBoth developers and data scientists need to be able to use any methods or classes from the production code inside their notebooks. Having these libraries allows them to have access to production data and test or evaluate performance of different versions of production code. Using the library API, a Jenkins server can build and deploy JAR files as libraries into a special folder on the Databricks workspace. When Databricks loads such libraries it recognizes versioning - so that developers can use a compatible version of the library for the code they\u2019re developing. This allows developers to control when they adopt new library versions and gives them a stable environment for benchmarking and regression testing. Every time someone commits code to the stage repo, Jenkins follows the following steps:\n<ol>\n\t<li>Build the repo code and if the tests pass, it generates a JAR with an incremented version number and uploads it to S3.</li>\n\t<li>Execute a Python script that uses the AWS SDK with the JAR S3 URL, generates a pre signed URL for the JAR and calls the Databricks API \u201clibraries/create\u201d endpoint. Since the libraries/create call needs a JAR url, the S3 pre-signed URL fits perfectly enabling us to generate a public URL with an expiration of a couple of minutes. By using a pre-signed URL we grant the Databricks API time-limited permission to download the build JAR, reducing the risk of the JAR being exposed.</li>\n</ol>\n\n[python]\ndef createlibrary(path,jarurl):\n  head = {'Authorization':DBAUTH}\n  endpoint = DBBASEENDPOINT + 'libraries/create'\n  newpath = &quot;/&quot; + path\n  data = {&quot;path&quot;: newpath, &quot;jar_specification&quot;: {&quot;uri&quot;:jarurl}}\n  r = requests.Session().post(endpoint,headers=head,data=json.dumps(data))\n  if r.status_code == 200:\n    print &quot;library uploaded for notebooks&quot;    \n  else: \n    print r.content\n[/python]\n\nAfter the process is complete developers can attach these libraries to any notebook cluster or job using Databricks web console. Here is an example of the Jenkins build output:\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/04/Metacog-jenkins-build-output.png\"><img class=\"aligncenter size-full wp-image-6878\" src=\"https://databricks.com/wp-content/uploads/2016/04/Metacog-jenkins-build-output.png\" alt=\"Screenshot of the build output from Jenkins.\" width=\"902\" height=\"163\" /></a>\n\nHere is an example of the Databricks workspace after job is updated (note the newly-built V376 JAR at the end of the listing):\n\n<img class=\"aligncenter size-full wp-image-6879\" src=\"https://databricks.com/wp-content/uploads/2016/04/Metacog-screenshot-of-Databricks-Workspace.png\" alt=\"Screenshot of the Databricks workspace.\" width=\"658\" height=\"634\" />\n<h2>Updating Databricks Jobs and Cluster Settings with Jenkins</h2>\nMetacog uses the Jobs API to deploy and manage production and stage Spark clusters. In addition to the steps described above, when a new version of the library gets built, Jenkins must update all jobs to make sure clusters use the new build JAR and the correct Spark version for that jar. This functionality is achieved by using the build file stored on S3 together with a python script that updates the jobs using the Databricks Jobs API.\n\n<img class=\"aligncenter size-full wp-image-6880\" src=\"https://databricks.com/wp-content/uploads/2016/04/Metacog-visualization-of-Jenkins-and-Databricks.png\" alt=\"Flowchart explaining how Jenkins updates Databricks Jobs and Cluster Settings.\" width=\"328\" height=\"309\" />\n\nThis is done in three steps (illustrated in the figure above):\n<ul>\n\t<li><strong>Step 1:</strong> Jenkins Uploads the build JAR to S3 together with a lastBuild.txt specifying the JAR version number</li>\n\t<li><strong>Step 2:</strong> Jenkins runs a python script that executes a job called deployJar that runs a notebook that copies the JAR built by Jenkins from S3 to the \u201cdbfs:/FileStore/job-jars\u201d. The Python script uses the \u201cjobs/runs/list\u201d and \u201c/jobs/run-now\u201d endpoints to trigger the deployJar and wait until it finished. The deployJar notebook looks like this:</li>\n</ul>\n<img class=\"aligncenter size-full wp-image-6881\" src=\"https://databricks.com/wp-content/uploads/2016/04/Metacog-deploy-jar-notebook-screenshot.png\" alt=\"Screenshot of the deployJar notebook in Databricks\" width=\"766\" height=\"134\" />\n<ul>\n\t<li><strong>Step 3:</strong> For updating the production and stage jobs with the new JAR and any other Spark cluster settings (e.g. a new Spark version) Jenkins executes a Python script that calls the Databricks API \u201cjobs/reset\u201d endpoint. The \u201cjobs/reset\u201d endpoint the \u201cdbfs:/FileStore/job-jars\u201d path created on Step 2. Here is the updated job of the previous example:</li>\n</ul>\n<img class=\"aligncenter size-full wp-image-6882\" src=\"https://databricks.com/wp-content/uploads/2016/04/Metacog-stage-spark-batch-jobs-screenshot.png\" alt=\"Screenshot of the stagesparkbatchv2-1 Job in Databricks.\" width=\"650\" height=\"231\" />\n<h2>Results and Benefits</h2>\nThanks to the Databricks environment and APIs we succeeded in implementing a continuous delivery pipeline all of our Spark code. Some of the main benefits that the Metacog team now enjoys are:\n<ul>\n\t<li><strong>Reduced time for launching features into production:</strong> Before implementing the pipeline, new features took around 1 month to be incorporated into the production version of the platform. With the pipeline this time was reduced to only the 1 or 2 weeks that are scheduled for live testing. This means that the release cadence was improved from 12 times per year to at least 24 times per year.</li>\n\t<li><strong>Better management of AWS EC2 costs:</strong> DevOps doesn\u2019t have to keep reminding developers or data scientists to clean up after themselves and shut down unused clusters. Thanks to the job interface developers do not have to worry about shutting down clusters after they\u2019ve finished using them. This represents at least a 28% AWS EC2 cost saving - since developers often forget to shut down a cluster when they were not going to use it over the weekend.</li>\n\t<li><strong>Faster onboarding:</strong> Since we have production libraries available within the Databricks workspace, new data scientists joining the Metacog team take one week instead of one month to become productive with real data generated with production code.</li>\n\t<li><strong>More time to develop instead of maintenance:</strong> Both devops, developers and Data Science have more time to work developing algorithms instead of spending it managing Jobs and uploading libraries to the Databricks environment. We estimate that around 20% of the developers time was being wasted on these tasks.</li>\n\t<li><strong>Big Data experimentation and tuning:</strong> Thanks to this pipeline, we have been able to solve performance bugs that are only present on big data sets in half the time.</li>\n</ul>"}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2016/04/07/how-dnv-gl-uses-databricks-to-build-tomorrows-energy-grid.html", "authors": null, "id": 6907, "categories": ["Announcements", "Company Blog", "Customers", "Product"], "dates": {"publishedOn": "2016-04-07", "tz": "UTC", "createdOn": "2016-04-07"}, "title": "How DNV GL Uses Databricks to Build Tomorrow\u2019s Energy Grid", "slug": "how-dnv-gl-uses-databricks-to-build-tomorrows-energy-grid", "content": "<img class=\"aligncenter size-full wp-image-6908\" style=\"border: 1px solid #c2c2c2;\" src=\"https://databricks.com/wp-content/uploads/2016/04/Databricks-and-DNV-GL.png\" alt=\"Databricks and DNV GL\" width=\"1200\" height=\"630\" />\n\nWe are proud to announce that <a href=\"https://www.dnvgl.com/\">DNV GL</a>, a provider of software and independent expert advisory services to the maritime, oil &amp; gas and energy industries, has selected Databricks for large-scale energy data analytics.\n\n<a href=\"http://www.marketwired.com/press-release/dnv-gl-uses-databricks-process-massive-volumes-sensor-data-build-tomorrows-energy-2112845.htm\" target=\"_blank\">You can read the press release here</a>.\n\nDNV GL\u2019s expertise and services cover virtually all aspects of the energy industry, including energy efficiency policy, forecasting energy usage, and developing programs for energy conservation. Applying advanced analytics techniques towards energy data is at the core of every DNV GL offering. The data science team at DNV GL analyzes data from smart meters with other climatological, socio- economic, and financial data sources to provide answers to the most challenging questions posed by the top energy and utility companies in the world.\n\nWhile the proliferation of data from sensors and other sources created opportunities for DNV GL to offer more analytics services to their customers, their analytics team must first overcome the problems associated with processing large volumes of high-frequency sensor data. In one instance, bottlenecks in legacy analytics platforms delayed time-to-insights by four to five days, preventing DNV GL from meeting their 24-hour turnaround time commitment to customers.\n\nDatabricks enabled DNV GL to effortlessly perform advanced analytics on large-scale data from sensors and other data sources with the power of Apache Spark effortlessly. Instead of relying on slow, single-machine analysis, the team was able to spin up Spark clusters in AWS and run machine learning algorithms in minutes. The scalability of the Databricks platform enabled DNV GL to accelerate time-to-value by nearly 100 times than previous methods. In one instance, Databricks improved the processing time of a machine learning batch job from 36 hours to approximately 23 minutes.\n\nAs a result of deploying Databricks, DNV GL gained the three broad categories of benefits:\n<ul>\n\t<li><strong>Turnkey Spark clusters.</strong> Set up and start provisioning clusters within minutes, compared to several hours with legacy cloud analytics platforms.</li>\n\t<li><strong>Faster time to insight.</strong> Speed up data processing by nearly 100 times without incurring additional operational costs.</li>\n\t<li><strong>Higher productivity.</strong> Eliminated the need to spend time on DevOps, allowing their data scientists and engineers to focus on solving data problems.</li>\n</ul>\n<a href=\"http://go.databricks.com/case-studies/dnvgl\">Download this case study</a> to learn more about how DNV GL is using Databricks.\n\nTo try out Databricks for yourself, <a href=\"https://accounts.cloud.databricks.com/registration.html#signup\">sign-up for a 14-day free trial</a> today!"}
{"status": "publish", "description": null, "creator": "ion", "link": "https://databricks.com/blog/2016/04/12/new-content-in-databricks-community-edition.html", "authors": null, "id": 6919, "categories": ["Apache Spark", "Ecosystem", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2016-04-12", "tz": "UTC", "createdOn": "2016-04-12"}, "title": "New Content in Databricks Community Edition", "slug": "new-content-in-databricks-community-edition", "content": "At the <a href=\"https://spark-summit.org/east-2016/\" target=\"_blank\">Spark Summit New York</a>, we announced Databricks Community Edition (CE) beta. CE is a free version of the Databricks service that allows everyone to learn and explore Apache Spark by providing a simple, integrated development environment for data scientists and engineers with high quality training materials and sample applications.\n\nThe community interest in Databricks CE beta has far exceeded our expectations. Within just a few days from the launch, several thousands of people put themselves on the waiting list! Given such demand, we have been hard at work to scale up the service and operations to give accounts to as many people as possible. The majority of people on the waiting list have now received accounts, and have started experimenting with Spark and Databricks.\n\nOne thing our users have particularly enjoyed in Databricks CE is exploring training materials and the collection of sample notebooks. Today, we are happy to announce the availability of additional materials, a MOOC course and two sample applications, for you to learn and explore Spark.\n<h2>Machine Learning with Apache Spark MOOC</h2>\nFirst, we are delighted to announce the release of all lectures and labs of our second Massive Open Online Course (MOOC), \u201c<a href=\"https://docs.cloud.databricks.com/docs/latest/courses/index.html#Scalable%20Machine%20Learning%20%28CS190-1x%29%20%28new%29/Introduction%20%28README%29.html\" target=\"_blank\">Machine learning with Apache Spark</a>,\u201d which was taught by <a href=\"http://web.cs.ucla.edu/~ameet/\" target=\"_blank\">Ameet Talwalkar</a> from UCLA on the EdX platform in July 2015. This is a five week course and introduces the underlying statistical and algorithmic principles required to develop scalable real-world machine learning pipelines.\n\nWhen offered last year, this course was a huge success, with over 55,000 registered students, of which close to 15% graduated. This is more than twice the average graduation rate of the MOOCs taught on EdX and other premier online education platforms. This speaks volumes about the quality and the demand of this course, and now you too can learn from it in Databricks CE.\n<h2>Analysis Pipeline Samples in R and Scala</h2>\nSecond, one of the most successful sample applications available on Databricks CE is an analysis pipeline on a sample of a million songs dataset. This analysis aimed to answer questions such as: When you first hear a song, do you ever categorize it as slow or fast? Is this even a valid categorization? If so, can one do it automatically? This analysis aims to answer such questions.\n\nWe wrote the original pipeline in Python. While Python is a very popular language, quite a few of our early users have asked us about porting the pipeline in other languages supported by Spark.\n\nToday, we are happy to announce that we have ported this <a href=\"https://docs.cloud.databricks.com/docs/latest/sample_applications/index.html#Sample%20Data%20Pipeline/R%20(new)/Introduction.html\" target=\"_blank\">analysis pipeline in R and Scala</a>, two other popular languages used by Spark users. Like in the original version written in Python, the Scala and R versions parse, explore and model a sample from the million songs dataset. This pipeline consists of three sections:\n<ul>\n\t<li><strong>ETL:</strong> Parses raw texts and creates a cached table.</li>\n\t<li><strong>Explore:</strong> Explores different aspects of the songs table using graphs.</li>\n\t<li><strong>Model:</strong> Uses SparkML to cluster songs based on some of their attributes.</li>\n</ul>\n<img class=\"aligncenter size-full wp-image-6921\" src=\"https://databricks.com/wp-content/uploads/2016/04/Databricks-Community-Edition-analysis-pipeline-diagram.png\" alt=\"Databricks Community Edition analysis pipeline diagram\" width=\"600\" height=\"82\" />\n\n&nbsp;\n<h2>Golden State Warrior Pass Analysis: 3rd Party Notebook</h2>\nFinally, we are happy to include for the first time a notebook created by a Databricks CE user. Using graphs to visualize the number of passes between<a href=\"https://docs.cloud.databricks.com/docs/latest/sample_applications/index.html#Sample%20Analysis/GraphFrame%20based%20Analysis%20(new)/GSW%20Passing%20Analysis.html\" target=\"_blank\"> team members of the Golden State Warriors</a> during the 2015-2016 season, this notebook leverages <a href=\"https://databricks.com/blog/2016/03/16/on-time-flight-performance-with-spark-graphframes.html\">GraphFrames</a>, a new Spark package that efficiently supports queries on graphs at scale, and a D3 library that performs visualization. As an example, this notebook demonstrates Databricks\u2019 seamless integration with growing number third party packages. Today there are over 200 <a href=\"http://spark-packages.org/\" target=\"_blank\">Spark packages</a>.\n\nThe availability of rich content makes Databricks CE an ideal platform to learn spark, enable users to develop useful applications, and share their notebooks with the community.\n<h2>Try it now</h2>\nIf you already have access to the Community Edition, <a href=\"https://community.cloud.databricks.com\" target=\"_blank\">login now</a>. Or <a href=\"http://go.databricks.com/databricks-community-edition-beta-waitlist\" target=\"_blank\">get on the waitlist here</a>."}
{"status": "publish", "description": "We have answered the common questions about GraphFrames raised by webinar viewers in this blog.", "creator": "dave_wang", "link": "https://databricks.com/blog/2016/04/21/graphframes-on-demand-webinar-and-faq.html", "authors": null, "id": 7117, "categories": ["Announcements", "Company Blog"], "dates": {"publishedOn": "2016-04-21", "tz": "UTC", "createdOn": "2016-04-21"}, "title": "GraphFrames On-Demand Webinar and FAQ", "slug": "graphframes-on-demand-webinar-and-faq", "content": "Last week, we held a live webinar \u2013 <a href=\"http://go.databricks.com/graphframes-dataframe-based-graphs-for-apache-spark\">GraphFrames: DataFrame-based graphs for Apache Spark</a> \u2013 to give an overview, a live demo, and a discussion of design decisions and future plans of the new GraphFrames library. The webinar included content for people just getting started with Apache Spark, as well as seasoned experts. The webinar started with a recap of major improvements from GraphX, and providing resources for getting started. A running example of analyzing flight delays was shown to illustrate the range of GraphFrame functionality: simple SQL and graph queries, motif finding, and powerful graph algorithms.For the experts, this talk included a few technical details on design decisions, the current implementation, and ongoing work on speed and performance optimizations.\n\nThe webinar is <a href=\"http://go.databricks.com/graphframes-dataframe-based-graphs-for-apache-spark\">accessible on-demand</a>.\u00a0Its slides and sample notebooks are also downloadable as attachments to the webinar. <a href=\"http://go.databricks.com/databricks-community-edition-beta-waitlist\">Join the Databricks Community Edition beta</a> to get free access to Spark and try out the notebooks.\n\nWe have answered the common questions raised by webinar viewers below. If you have additional questions, please check out the <a href=\"https://forums.databricks.com/\">Databricks Forum</a>.\n\n<strong>Common webinar questions and answers</strong>\n\nClick on the question to see answer:\n<ul>\n \t<li><a href=\"https://forums.databricks.com/answers/7782/view.html\">Can GraphFrames handle multiple types of relationships (or edges), each with its own set of properties? Will it be all in the single dataframe as input?</a></li>\n \t<li><a href=\"https://forums.databricks.com/answers/7784/view.html\">Are there integration plans for GraphFrames with the MLlib pipeline API so that we can leverage existing cross-validation/hyperparameter optimization for graph algorithms?</a></li>\n \t<li><a href=\"https://forums.databricks.com/answers/7786/view.html\">With GraphFrames, is there a way to incrementally build graphs, either with an API, e.g. addVertex(), addEdge(), or by loading data from multiple files one by one?</a></li>\n \t<li><a href=\"https://forums.databricks.com/answers/7789/view.html\">I tried GraphFrames for connected components in a graph with 3.7M vertices and 2.1M edges. However, I ran into performance/scalability issues. Could you give some details about the underlying algorithm and its algorithmic complexity?</a></li>\n \t<li><a href=\"https://forums.databricks.com/answers/7791/view.html\">With GraphFrames, can you create graphs from adjacency matrix or from crosstab(col1, col2), which computes a pair-wise frequency table of the given columns?</a></li>\n \t<li><a href=\"https://forums.databricks.com/answers/7793/view.html\">With GraphFrames, are there ways of dealing with multiple types of vertices in the same data set? i.e. edges that span two different frames with different meta-data and treating the underlying data frames as a typed vertex?</a></li>\n</ul>"}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2016/04/27/new-ebook-released-mastering-advanced-analytics-with-apache-spark.html", "authors": null, "id": 7141, "categories": ["Announcements", "Company Blog"], "dates": {"publishedOn": "2016-04-27", "tz": "UTC", "createdOn": "2016-04-27"}, "title": "New eBook Released: Mastering Advanced Analytics with Apache Spark", "slug": "new-ebook-released-mastering-advanced-analytics-with-apache-spark", "content": "<a href=\"http://go.databricks.com/mastering-advanced-analytics-apache-spark\" target=\"_blank\"><img class=\"aligncenter wp-image-7142 size-full\" src=\"https://databricks.com/wp-content/uploads/2016/04/Mastering-Advanced-Analytics-with-Apache-Spark-eBook-cover.jpg\" alt=\"Mastering Advanced Analytics with Apache Spark eBook\" width=\"1200\" height=\"928\" /></a>\n\nWe are excited to announce that the second eBook in our technical blog book series, <em>Mastering Advanced Analytics with Apache Spark</em>, has been released today!\n\n<a href=\"http://go.databricks.com/mastering-advanced-analytics-apache-spark-databricks\" target=\"_blank\">You can download the eBook here.</a>\n\nWe focused on the topic of \u201cAdvanced Analytics\u201d due to the challenges created by the continued growth in data. This coupled with increasingly complex use cases demands much more than running queries against the data set. Whether you\u2019re scrutinizing the clickstream from millions of visitors to optimize online ad placements or sifting through billions of transactions to identify signs of fraud, more sophisticated approaches to automatically glean insights from enormous volumes of data - such as machine learning and graph processing - is more important than ever.\n\nThis eBook offers a collection of the most popular technical blog posts that provide an introduction to machine learning and other advanced techniques on Spark, including:\n<ul>\n \t<li>An introduction to machine learning in Apache Spark</li>\n \t<li>Using Spark for advanced topics such as clustering, trees, graph processing</li>\n \t<li>How you can use SparkR to analyze data at scale with the R language</li>\n</ul>\n<a href=\"http://go.databricks.com/mastering-advanced-analytics-apache-spark-databricks\" target=\"_blank\"><img class=\"aligncenter wp-image-7143 size-full\" src=\"https://databricks.com/wp-content/uploads/2016/04/Screenshot-from-Mastering-Advanced-Analytics-with-Apache-Spark-eBook.jpg\" alt=\"Screenshot from the Mastering Advanced Analytics with Apache Spark eBook\" width=\"807\" height=\"623\" /></a>\n\nWe\u2019ve also augmented the blogs with new code examples in Databricks notebooks, which are freely available with the eBook download. A sample of the new notebooks include:\n<ul>\n \t<li>Scalable Decision Trees with MLlib</li>\n \t<li>ML Import, Export, and Simple Operations</li>\n \t<li>Generalized Linear Models in SparkR</li>\n \t<li>Random Forests and Boosting in MLlib</li>\n</ul>\n<a href=\"http://go.databricks.com/mastering-advanced-analytics-apache-spark-databricks\" target=\"_blank\">Download the eBook</a> to get started on your next advanced analytics project today. To try out the code examples, <a href=\"http://go.databricks.com/databricks-community-edition-beta-waitlist\" target=\"_blank\">get on the waitlist for the Databricks Community Edition</a>. If you have not read the first eBook in the series, be sure to check out <a href=\"http://go.databricks.com/apache-spark-analytics-made-simple-databricks\" target=\"_blank\">Apache Spark Analytics Made Simple</a> for technical content and code examples geared toward an introduction to data analytics with Apache Spark."}
{"status": "publish", "description": "Spark Live is a complimentary roadshow for data professional and IT leaders who want to learn how Apache Spark can make their use cases a reality.", "creator": "Wayne Chan", "link": "https://databricks.com/blog/2016/05/04/introducing-the-spark-live-2016-tour.html", "authors": null, "id": 7158, "categories": ["Announcements", "Company Blog", "Events"], "dates": {"publishedOn": "2016-05-04", "tz": "UTC", "createdOn": "2016-05-04"}, "title": "Introducing the Spark Live 2016 Tour", "slug": "introducing-the-spark-live-2016-tour", "content": "You\u2019ve asked and we\u2019re making it happen - Databricks, the company founded by the team that created Apache Spark, is hitting the road and bringing Apache Spark to a city near you. \u00a0Announcing <a href=\"https://databricks.com/spark-live\">Spark Live</a>, an eight-city road show, designed exclusively for data professionals and IT leaders who want to learn how to leverage the power of open source Apache Spark and Databricks to simplify data processing and make their transformative use cases a reality.\n\nStarting May 9th, we are hitting the road with the creators and foremost Spark experts in tow to show how you can leverage Databricks and Spark to build innovative advanced analytics solutions.\n\nEach roadshow event includes a half-day of industry insights, discussions on how to deploy Spark in the enterprise, to share what\u2019s ahead for the open source Spark project, upcoming Databricks features, and live product demos.\u00a0The afternoon features a hands-on technical Spark Essentials workshop from the team that created Spark. Furthermore, all attendees will receive free ongoing access to Databricks Community Edition after the event\u2014providing you with the most ideal platform to continue your journey with open source Apache Spark for as long as you want.\n\nTo review the agenda, please <a href=\"https://databricks.com/spark-live#agenda\">click here</a>.\n<h3>Request your spot today!</h3>\nSpace is limited, so request your spot for the premier Spark event series for the enterprise today.\n<ul>\n \t<li><a href=\"https://databricks.com/spark-live/hanover-md\">Hanover, MD - May 9th</a> (federal employees and contractors welcome)</li>\n \t<li><a href=\"https://databricks.com/spark-live/los-angeles-ca\">Los Angeles, CA - May 24th</a></li>\n \t<li>Reston, VA - coming in July (federal employees and contractors welcome)</li>\n \t<li>Austin, TX - coming in August</li>\n \t<li>Seattle, WA - coming in August</li>\n \t<li><a href=\"https://databricks.com/spark-live/boston-ma\">Boston, MA - September 22nd</a></li>\n \t<li>Chicago, IL - coming in October</li>\n \t<li>New York, NY - coming in November</li>\n</ul>\n<h3>Additional Information</h3>\nFor more information about the Spark Live 2016 road show or to request your spot for the city closest to you, visit the <a href=\"https://databricks.com/spark-live\">Spark Live homepage</a>."}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2016/05/03/how-omega-point-delivers-portfolio-insights-for-financial-services-with-databricks.html", "authors": null, "id": 7172, "categories": ["Company Blog", "Customers"], "dates": {"publishedOn": "2016-05-03", "tz": "UTC", "createdOn": "2016-05-03"}, "title": "How Omega Point Delivers Portfolio Insights for Financial Services with Databricks", "slug": "how-omega-point-delivers-portfolio-insights-for-financial-services-with-databricks", "content": "<a href=\"http://go.databricks.com/case-studies/omegapoint\" target=\"_blank\"><a href=\"https://databricks.com/wp-content/uploads/2016/05/Omega-Point-and-Databricks.png\"><img src=\"https://databricks.com/wp-content/uploads/2016/05/Omega-Point-and-Databricks.png\" alt=\"Omega Point and Databricks\" width=\"600\" height=\"315\" style=\"border: 1px solid #e2e2e2;\" class=\"aligncenter size-full wp-image-7173\" /></a></a>\n\nWe are proud to announce that <a href=\"http://www.omegapoint.tech/\" target=\"_blank\">Omega Point</a>, a leading data analytics provider, has selected Databricks for its ability to apply advanced analytics to synthesize insights from disparate large-scale datasets.\n\n<a href=\"http://www.marketwired.com/press-release/databricks-advanced-analytics-capabilities-enable-omega-point-deliver-portfolio-2120756.htm\" target=\"_blank\">You can read the press release here.</a>\n\nInferring insights from a multitude of large-scale data to inform business-critical decisions is an increasingly common practice across industries. Omega Point is yet another example of this trend in financial services, where a sliver of timely information could have millions of dollars of impact.\n\nOmega Point offers finance and strategy professionals a software platform called <em>Portfolio Intelligence</em> that enables them to understand, visualize, and optimize their portfolios. Its offering taps into satellite imagery, web traffic, earnings transcript sentiment, and other large-scale data sets known as \"economic exhaust\" or \u201calternative datasets\u201d to uncover portfolios\u2019 exposure to 50+ relevant market factors using cutting edge data science.\n\nExtracting insights from \u201calternative datasets\u201d requires cutting-edge data processing capabilities to infer valuable information from a plethora of seemingly low-value data. Omega Point selected Apache Spark because of its scalability and flexibility to tackle advanced analytics. However, their initial attempt to deploy Spark by integrating a self-managed PaaS with open source\u00a0data science tools ran into roadblocks because of reliability and performance problems. As a result, the release of the\u00a0<em>Omega Point\u00a0</em><em>Portfolio Intelligence Platform</em> was severely delayed.\n\nOmega Point chose Databricks to power the backbone of its production data pipeline. On a daily basis, Databricks pulls over 120 sources of data directly from Omega Point\u2019s Amazon S3 account and through a sequence of over 80 production Spark jobs, and produces relevant indicators used to assess current economic and financial market trends in clients\u2019 portfolios. Databricks also enabled Omega Point\u2019s data science team to build a systematic learning environment, where Spark\u2019s MLlib and popular open source libraries such as <em>numpy</em>, <em>pandas</em>, <em>scipy</em>, <em>sklearn</em>, <em>matplotlib</em> could be seamlessly integrated to develop sophisticated models rapidly.\n\nWith Databricks, Omega Point gained a high-performance and reliable Spark platform that addressed their reliability and performance bottlenecks. Databricks sped up the release cycle, improved data throughput, and reduced the debugging time for the data engineering and science teams. Omega Point\u2019s deployment and maintenance costs dropped by 75% while its production uptime increased by more than 80%. Ultimately, Databricks enabled Omega Point to accelerate the release of core feature of its flagship product by six months.\n\n<a href=\"http://go.databricks.com/case-studies/omegapoint\">Download this case study</a> to learn more about how Omega Point is using Databricks."}
{"status": "publish", "description": null, "creator": "jules_damji", "link": "https://databricks.com/blog/2016/05/09/spark-saturday-dc-a-meetup-summary.html", "authors": null, "id": 7200, "categories": ["Company Blog", "Events"], "dates": {"publishedOn": "2016-05-09", "tz": "UTC", "createdOn": "2016-05-09"}, "title": "Spark Saturday DC: A Meetup Summary", "slug": "spark-saturday-dc-a-meetup-summary", "content": "On a rainy and foggy Saturday morning, April 30th, in McLean, VA., more than 275 Apache Spark enthusiasts, forsaking the comfort of Saturday sleep-in, eagerly lined up at 8:00 am to register for an all-day inaugural <a href=\"http://www.meetup.com/Washington-DC-Area-Spark-Interactive/events/229486436/\" target=\"_blank\" rel=\"noopener\">Spark Saturday DC Meetup</a> at Capital One\u2019s headquarters. That is an affirmation that Apache Spark\u2019s fervor is tangible, its hunger insatiable, and its popularity uncontested.\n\nA Databricks t-shirt reads: \u201cMay the Spark with you.\u201d So it was for the entire day\u2014and so was the voice of its \u201cFather.\u201d\n\nThanks to <a href=\"https://www.capitalone.com/\" target=\"_blank\" rel=\"noopener\">Capital One</a> (host), <a href=\"http://www.metistream.com/\" target=\"_blank\" rel=\"noopener\">MetiStream</a>, and Databricks as major organizers, this all-day community event was a success\u2014with seven tech-talks, training, speakers\u2019 panel Q &amp; A, and happy hour.\n\nHere are some highlights, broken down into architecture, Spark use-cases, and miscellaneous.\n\n<h3>Architecture</h3>\n\n<ul>\n    <li>In his keynote, Chris D\u2019Agostino, vice president of technology at Capital One, shared his vision of cloud architecture and how his team is using <a href=\"https://twitter.com/2twitme/status/726403655138742272\" target=\"_blank\" rel=\"noopener\">Apache Spark in the cloud</a>.</li>\n    <li>Databricks\u2019 <a href=\"https://twitter.com/databricks/status/726438455090274304\" target=\"_blank\" rel=\"noopener\">Vida Ha\u2019s talk</a> struck a chord with practitioners, with her presentation on \u201c<a href=\"http://go.databricks.com/not-your-fathers-database\" target=\"_blank\" rel=\"noopener\">Not your Father\u2019s Database: How to use Apache Spark Properly in your Big Data Architecture</a>,\u201d followed by questions about Spark 2.0.</li>\n</ul>\n\n<h3>Machine Learning, Data Science &amp; Spark Use Cases</h3>\n\n<ul>\n    <li>For his mesmerizing talk on Neuroscience, Jeremy Freeman, from HMMI |Janelia Research Campus, alluded to a <a href=\"https://twitter.com/2twitme/status/726412141046206464\" target=\"_blank\" rel=\"noopener\">Databricks' blog on k-means</a> (<a href=\"https://databricks.com/blog/2015/01/28/introducing-streaming-k-means-in-spark-1-2.html\">Introducing streaming k-means in Spark 1.2</a>) and how Spark is being used for analytics in Neuroscience\u2014understanding behavioral patterns by analyzing neural activity and mapping neurons.</li>\n    <li>Alexis Seigneurin, big data engineer at IpponUSA, talked about <a href=\"https://twitter.com/rbrugier/status/726424879805943808\" target=\"_blank\" rel=\"noopener\">Machine Learning for Record Linkage</a>, using a Spark ML use case, and enumerated the <a href=\"https://twitter.com/2twitme/status/726429569033601026\" target=\"_blank\" rel=\"noopener\">Do\u2019s and Don\u2019t of SparkContext</a>, DataFrames, Datasets and RDDs.</li>\n    <li>Michal Malohvala, software engineer at H2O.ai, gave us a flavor of <a href=\"https://twitter.com/srisatish/status/726572178515197952\" target=\"_blank\" rel=\"noopener\">H2O.ai Sparkling Water</a>.</li>\n    <li><a href=\"https://twitter.com/2twitme/status/726479352255418368\" target=\"_blank\" rel=\"noopener\">Hollings Wilkins and Mikhail Semenuik</a> from TrueCar jointly showcased <a href=\"http://spark-packages.org/package/TrueCar/mleap\" target=\"_blank\" rel=\"noopener\">MLeap</a>, an open-source Spark Package to easily deploy Spark ML pipelines into production.</li>\n    <li>Saurabh Gupte from Capital One shared how to use <a href=\"https://twitter.com/Evyfindstheway/status/726475747808952320\" target=\"_blank\" rel=\"noopener\">Spark framework for rapid use-case development</a> for devices, with a live demo, interacting with the audience using their smartphones.</li>\n</ul>\n\n<h3>Miscellaneous</h3>\n\n<ul>\n    <li>A <a href=\"https://twitter.com/MetiStream/status/726845270588272641\" target=\"_blank\" rel=\"noopener\">speakers\u2019 panel</a>, moderated by Donna Fernandez of MetiStream, answered a wide variety of questions from the audience\u2014from Apache Spark 2.0, Machine Learning and Data Science roadmap to trends in Spark Streaming.</li>\n    <li>Denny Lee of Databricks received a <a href=\"https://twitter.com/2twitme/status/726473721070915584\" target=\"_blank\" rel=\"noopener\">well-deserved accolade</a> from the <a href=\"http://www.meetup.com/Washington-DC-Area-Spark-Interactive/\" target=\"_blank\" rel=\"noopener\">Washington DC Area Spark Interactive Meetup</a>.</li>\n    <li>During lunch hour, close to 100 attended an hour\u2019s <a href=\"https://twitter.com/hishuba/status/726454878487367680\" target=\"_blank\" rel=\"noopener\">hands-on-the-deck training</a>, conducted by certified MetiStream trainers. Attendees used <a href=\"http://databricks.com/ce\">Databricks Community Edition</a> for an Introduction to Apache Spark.</li>\n</ul>\n\nIn summation, Spark Saturday DC was a fun worth-while community event. Attendees have posted nothing but complimentary comments on the Meetup page and on the <a href=\"https://twitter.com/search?q=%23sparksaturdaydc&amp;src=tyah\" target=\"_blank\" rel=\"noopener\">@SparkSaturdayDC/#SparkSaturdayDC</a>.\n\nAgain, we want to thank our hosts and one of the organizers Capital One and their team for executing this community event, along with MetiStream and everyone who volunteered and attended for making this inaugural event a huge success.\n\n<h3>What\u2019s Next</h3>\n\nWe are planning with other <a href=\"https://groups.google.com/forum/#!forum/spark-meetup-organizers\" target=\"_blank\" rel=\"noopener\">Spark Meetup Organizers</a> for our next Spark Saturday in other cities. Stay tuned.\n\n<h3>More Reading</h3>\n\n<ul>\n    <li>Visit the <a href=\"https://sparksaturday.wordpress.com/\" target=\"_blank\" rel=\"noopener\">Spark Saturday website</a> for updates</li>\n    <li>Follow us on <a href=\"https://twitter.com/sparksaturdayDC\" target=\"_blank\" rel=\"noopener\">@sparksaturdayDC</a></li>\n    <li>Presentations and slides will be posted on Spark Saturday website.</li>\n</ul>"}
{"status": "publish", "description": null, "creator": "rxin", "link": "https://databricks.com/blog/2016/05/11/apache-spark-2-0-technical-preview-easier-faster-and-smarter.html", "authors": null, "id": 7211, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2016-05-11", "tz": "UTC", "createdOn": "2016-05-11"}, "title": "Technical Preview of Apache Spark 2.0 Now on Databricks", "slug": "apache-spark-2-0-technical-preview-easier-faster-and-smarter", "content": "For the past few months, we have been busy contributing to the next major release of the big data open source software we love: Apache Spark 2.0. Since Spark 1.0 came out two years ago, we have heard praises and complaints. Spark 2.0 builds on what the community has learned in the past two years, doubling down on what users love and improving on what users lament. While this blog summarizes the three major thrusts and themes\u2014easier, faster, and smarter\u2014that comprise Spark 2.0, the themes highlighted here deserve deep-dive discussions that we will follow up with in-depth blogs in the next few weeks.\n\nBefore we dive in, we are happy to announce the availability of the Apache Spark 2.0 technical preview in <a href=\"https://databricks.com/try-databricks\" target=\"_blank\">Databricks</a>. This preview package is based on the upstream <a href=\"https://spark.apache.org/news/spark-2.0.0-preview.html\">2.0.0-preview release</a>. Using the preview package is as simple as selecting the \u201c2.0 (branch preview)\u201d version when launching a cluster:\n\n<img class=\"aligncenter size-full wp-image-7217\" src=\"https://databricks.com/wp-content/uploads/2016/05/spark-2-branch-preview-in-databricks.png\" alt=\"Screenshot of creating a new Apache Spark 2.0 Tech Preview Cluster workflow in Databricks\" width=\"500\" />\n\nWhereas the final Apache Spark 2.0 release is still a few weeks away, this technical preview is intended to provide early access to the features in Spark 2.0 based on the upstream codebase. This way, you can satisfy your curiosity to try the shiny new toy, while we get feedback and bug reports early before the final release.\n\nNow, let\u2019s take a look at the new developments.\n\n<img class=\"aligncenter size-full wp-image-7231\" src=\"https://databricks.com/wp-content/uploads/2016/05/spark2.0-blog-illustration.png\" alt=\"Spark 2.0: Easier, Faster, Smarter\" width=\"600\" height=\"180\" />\n\n<h3>Easier: SQL and Streamlined APIs</h3>\n\nOne thing we are proud of in Spark is creating APIs that are simple, intuitive, and expressive. Spark 2.0 continues this tradition, with focus on two areas: (1) standard SQL support and (2) unifying DataFrame/Dataset API.\n\nOn the SQL side, we have significantly expanded the SQL capabilities of Spark, with the introduction of a new ANSI SQL parser and support for subqueries. <strong>Spark 2.0 can run all the 99 TPC-DS queries, which require many of the SQL:2003 features.</strong> Because SQL has been one of the primary interfaces Spark applications use, this extended SQL capabilities drastically reduce the porting effort of legacy applications over to Spark.\n\nOn the programming API side, we have streamlined the APIs:\n\n<ul>\n    <li><strong>Unifying DataFrames and Datasets in Scala/Java:</strong> Starting in Spark 2.0, DataFrame is just a type alias for Dataset of Row. Both the typed methods (e.g. <code>map</code>, <code>filter</code>, <code>groupByKey</code>) and the untyped methods (e.g. <code>select</code>, <code>groupBy</code>) are available on the Dataset class. Also, this new combined Dataset interface is the abstraction used for Structured Streaming. Since compile-time type-safety in Python and R is not a language feature, the concept of Dataset does not apply to these languages\u2019 APIs. Instead, DataFrame remains the primary programing abstraction, which is analogous to the single-node data frame notion in these languages. Get a peek from a <a href=\"https://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/Dataset.html\" target=\"_blank\">Dataset API notebook</a>.</li>\n    <li><strong>SparkSession:</strong> a new entry point that replaces the old SQLContext and HiveContext. For users of the DataFrame API, a common source of confusion for Spark is which \u201ccontext\u201d to use. Now you can use SparkSession, which subsumes both, as a single entry point, as <a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/SparkSession.html\" target=\"_blank\">demonstrated in this notebook</a>. Note that the old SQLContext and HiveContext are still kept for backward compatibility.</li>\n    <li><strong>Simpler, more performant Accumulator API:</strong> We have designed a new Accumulator API that has a simpler type hierarchy and support specialization for primitive types. The old Accumulator API has been deprecated but retained for backward compatibility</li>\n    <li><strong>DataFrame-based Machine Learning API emerges as the primary ML API:</strong> With Spark 2.0, the spark.ml package, with its \u201cpipeline\u201d APIs, will emerge as the primary machine learning API. While the original spark.mllib package is preserved, future development will focus on the DataFrame-based API.</li>\n    <li><strong>Machine learning pipeline persistence:</strong> Users can now <a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/ML%20persistence%20in%202.0.html\" target=\"_blank\">save and load</a> machine learning pipelines and models across all programming languages supported by Spark.</li>\n    <li><strong>Distributed algorithms in R:</strong> Added support for Generalized Linear Models (GLM), Naive Bayes, Survival Regression, and K-Means in R.</li>\n</ul>\n\n<h3>Faster: Spark as a Compiler</h3>\n\nAccording to our <a href=\"https://databricks.com/blog/2015/09/24/spark-survey-results-2015-are-now-available.html\" target=\"_blank\">2015 Spark Survey</a>, 91% of users consider performance as the most important aspect of Spark. As a result, performance optimizations have always been a focus in our Spark development. Before we started planning our contributions to Spark 2.0, we asked ourselves a question: <strong>Spark is already pretty fast, but can we push the boundary and make Spark 10X faster?</strong>\n\nThis question led us to fundamentally rethink the way we build Spark\u2019s physical execution layer. When you look into a modern data engine (e.g. Spark or other MPP databases), majority of the CPU cycles are spent in useless work, such as making virtual function calls or reading/writing intermediate data to CPU cache or memory. Optimizing performance by reducing the amount of CPU cycles wasted in these useless work has been a long time focus of modern compilers.\n\nSpark 2.0 ships with the second generation Tungsten engine. <strong>This engine builds upon ideas from modern compilers and MPP databases and applies them to data processing.</strong> The main idea is to emit optimized bytecode at runtime that collapses the entire query into a single function, eliminating virtual function calls and leveraging CPU registers for intermediate data. We call this technique \u201cwhole-stage code generation.\u201d\n\nTo give you a teaser, we have measured the amount of time (in nanoseconds) it would take to process a row on one core for some of the operators in Spark 1.6 vs. Spark 2.0, and the table below is a comparison that demonstrates the power of the new Tungsten engine. Spark 1.6 includes expression code generation technique that is also in use in some state-of-the-art commercial databases today. As you can see, many of the core operators are becoming an order of magnitude faster with whole-stage code generation.\n\nYou can see the power of whole-stage code generation in action in <a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/Whole-stage%20code%20generation.html\" target=\"_blank\">this notebook</a>, in which we perform aggregations and joins on 1 billion records on a single machine.\n\n<figure>\n\n<figcaption style=\"text-align: center; font-style: italic; margin-bottom: .5em;\">cost per row (single thread)</figcaption>\n\n<table class=\"table\">\n<thead>\n<tr>\n<th>primitive</th>\n<th>Spark 1.6</th>\n<th>Spark 2.0</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>filter</td>\n<td>15ns</td>\n<td>1.1ns</td>\n</tr>\n<tr>\n<td>sum w/o group</td>\n<td>14ns</td>\n<td>0.9ns</td>\n</tr>\n<tr>\n<td>sum w/ group</td>\n<td>79ns</td>\n<td>10.7ns</td>\n</tr>\n<tr>\n<td>hash join</td>\n<td>115ns</td>\n<td>4.0ns</td>\n</tr>\n<tr>\n<td>sort (8-bit entropy)</td>\n<td>620ns</td>\n<td>5.3ns</td>\n</tr>\n<tr>\n<td>sort (64-bit entropy)</td>\n<td>620ns</td>\n<td>40ns</td>\n</tr>\n<tr>\n<td>sort-merge join</td>\n<td>750ns</td>\n<td>700ns</td>\n</tr>\n</tbody>\n</table>\n\n</figure>How does this new engine work on end-to-end queries? We did some preliminary analysis using TPC-DS queries to compare Spark 1.6 and Spark 2.0:\n\n<img class=\"aligncenter size-full wp-image-7218\" src=\"https://databricks.com/wp-content/uploads/2016/05/preliminary-tpc-ds-spark-2-0-vs-1-6.png\" alt=\"Preliminary TPC-DS Spark 2.0 vs 1.6\" width=\"703\" height=\"380\" />\n\nBeyond whole-stage code generation to improve performance, a lot of work has also gone into improving the Catalyst optimizer for general query optimizations such as nullability propagation, as well as a new vectorized Parquet decoder that has improved Parquet scan throughput by 3X.\n\n<h3>Smarter: Structured Streaming</h3>\n\nSpark Streaming has long led the big data space as one of the first attempts at unifying batch and streaming computation. As a first streaming API called DStream and introduced in Spark 0.7, it offered developers with several powerful properties: exactly-once semantics, fault-tolerance at scale, and high throughput.\n\nHowever, after working with hundreds of real-world deployments of Spark Streaming, we found that applications that need to make decisions in real-time often require more than just a streaming engine. They require deep integration of the batch stack and the streaming stack, integration with external storage systems, as well as the ability to cope with changes in business logic. As a result, enterprises want more than just a streaming engine; instead they need a full stack that enables them to develop end-to-end \u201ccontinuous applications.\u201d\n\nOne school of thought is to treat everything like a stream; that is, adopt a single programming model integrating both batch and streaming data.\n\nA number of problems exist with this single model. First, operating on data as it arrives in can be very difficult and restrictive. Second, varying data distribution, changing business logic, and delayed data\u2014all add unique challenges. And third, most existing systems, such as MySQL or Amazon S3, do not behave like a stream and many algorithms (including most off-the-shelf machine learning) do not work in a streaming setting.\n\nSpark 2.0's Structured Streaming APIs is a novel way to approach streaming. It stems from the realization that <strong>the simplest way to compute answers on streams of data is to <em>not having to reason</em> about the fact that it is a stream</strong>. This realization came from our experience with programmers who already know how to program static data sets (aka batch) using Spark\u2019s powerful DataFrame/Dataset API. The vision of Structured Streaming is to utilize the Catalyst optimizer to discover when it is possible to transparently turn a static program into an incremental execution that works on dynamic, infinite data (aka a stream). When viewed through this structured lens of data\u2014as discrete table or an infinite table\u2014you simplify streaming.\n\nAs the first step towards realizing this vision, Spark 2.0 ships with an initial version of the Structured Streaming API, a (surprisingly small!) extension to the DataFrame/Dataset API. This unification should make adoption easy for existing Spark users, allowing them to leverage their knowledge of Spark batch API to answer new questions in real-time. Key features here will include support for event-time based processing, out-of-order/delayed data, sessionization and tight integration with non-streaming data sources and sinks.\n\nStreaming is clearly a pretty broad topic, so stay tuned to this blog for more details on Structured Streaming in Spark 2.0, including details on what is possible in this release and what is on the roadmap for the near future.\n\n<h3>Conclusion</h3>\n\nSpark users initially came to Spark for its ease-of-use and performance. Spark 2.0 doubles down on these while extending it to support an even wider range of workloads. We hope you will enjoy the preview, and look forward to your feedback.\n\nOf course, until the upstream Apache Spark 2.0 release is finalized, we do not recommend fully migrating any production workload onto this preview package. <strong>This technical preview version is now available on Databricks.</strong>\u00a0To get access to Databricks, sign up <a href=\"https://databricks.com/try-databricks\">here</a>.\n\n<h3>Read More</h3>\n\nIf you missed our webinar for <a href=\"https://www.brighttalk.com/webcast/12891/202021#/register\" target=\"_blank\">Apache Spark 2.0: Easier, Faster, and Smarter</a>, you can register and watch the recordings and download slides and attached notebooks.\n\nYou can also import the following notebooks and try on your Databricks Community Edition with Spark 2.0 Technical Preview.\n\n<ul>\n    <li><a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/SparkSession.html\" target=\"_blank\">SparkSession: A new entry point</a></li>\n    <li><a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/Dataset.html\" target=\"_blank\">Datasets: A more streamlined API</a></li>\n    <li><a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/Whole-stage%20code%20generation.html\" target=\"_blank\">Performance of whole-stage code generation</a></li>\n    <li><a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/ML%20persistence%20in%202.0.html\" target=\"_blank\">Machine learning pipeline persistence</a></li>\n</ul>"}
{"status": "publish", "description": null, "creator": "admin", "link": "https://databricks.com/blog/2016/05/11/sellpoints-develops-shopper-insights-with-databricks.html", "authors": null, "id": 7254, "categories": ["Company Blog", "Customers"], "dates": {"publishedOn": "2016-05-11", "tz": "UTC", "createdOn": "2016-05-11"}, "title": "Sellpoints Develops Shopper Insights with Databricks", "slug": "sellpoints-develops-shopper-insights-with-databricks", "content": "[sidenote]This is a guest blog from our friends at <a href=\"https://www.sellpoints.com/\" target=\"_blank\">Sellpoints</a>.[/sidenote]\n\n[sidenote]Saman is a Data Engineer at Sellpoints, where he responsible for developing, optimizing, and maintaining our backend ETL. Chris is a Data Scientist at Sellpoints, where he is responsible for creating and maintaining analytics pipelines, and developing user behavior models.[/sidenote]\n\n<hr />\n\nThe simplest way to describe <a href=\"https://www.sellpoints.com/\" target=\"_blank\">Sellpoints</a> is we help brands and retailers convert shoppers into buyers using data analytics. The two primary areas of business for Sellpoints include some of the largest and most complex data sets imaginable; we build and syndicate interactive content for the largest online retailers to help brands increase conversions, and we provide advanced display advertising solutions for our clients. We study behavior trends as users browse retail sites, interact with our widgets, or see targeted display ads. Tracking these events amounts to an average of 5,500 unique data points per second.\n\nThe Data Team\u2019s primary role is to turn our raw event tracking data into aggregated user behavior information, useful for decision-making by our account managers and clients. We track how people interact with retailer websites prior to purchasing. Our individual log lines represent a single interaction between a customer and some content related to a product, which we then aggregate to answer questions such as \u201chow many visits does a person make prior to purchasing,\u201d or \u201chow much of this demonstration video do people watch before closing the window, and are customers more likely to purchase after viewing?\u201d The Data Team here at Sellpoints is responsible for creating user behavior models, producing analytical reports and visualizations for the content we track, and maintaining the visualization infrastructure.\n\nIn this blog, we will describe how Sellpoints is able to not only implement our entire backend ETL using the Databricks platform, but unify the entire ETL.\n<h2>Use Case: Centralized ETL</h2>\nSellpoints needed a big data processing platform, one that would be able to replicate our existing ETL, based in Amazon Web Services (AWS), but improve speed and potentially integrate data sources beyond S3, including MySQL and FTP. We also wanted the ability to test MLlib, Apache Spark\u2019s machine learning library. We were extremely happy to be accepted as one of Databricks early customers, and outline our goals:\n\n<strong>Short Term Goals:</strong>\n<ul>\n \t<li>Replicate existing Hive-based ETL in SparkSQL \u2013 processing raw CSV log files into useable tables, optimizing the file size, and partitioning as necessary</li>\n \t<li>Aggregate data into tables for visualization</li>\n \t<li>Extract visualization tables with Tableau</li>\n</ul>\n<strong>Long Term Goals:</strong>\n<ul>\n \t<li>Rewrite ETL in Scala/Spark and optimize for speed</li>\n \t<li>Integrate MySQL via the JDBC</li>\n \t<li>Test and productize MLlib algorithms</li>\n \t<li>Integrate Databricks further into our stack</li>\n</ul>\nWe have a small Data Team at Sellpoints, consisting of three Data Engineers, one Data Scientist, and a couple Analysts. Because our team is small and we do not have the DevOps resources to maintain a large cluster ourselves, Databricks is the perfect solution. Their platform allows the analysts to spin up clusters with the click of a button, without having to deal with installing packages or specific versions of software. Additionally, Databricks provides a notebook environment for Spark and makes data discovery incredibly intuitive. The ease of use is unparalleled and allows users across our entire Data Team to investigate our raw data.\n\n<img class=\"aligncenter size-full wp-image-7255\" src=\"https://databricks.com/wp-content/uploads/2016/05/sellpoints-databricks-stack.png\" alt=\"Illustration of Sellpoints' ETL architecture.\" width=\"605\" height=\"610\" />\n<h2>Architecture and Technical Details</h2>\nOur immediate goal was to replicate our existing Hive-based ETL in SparkSQL, which turned out to be a breeze. SparkSQL can directly implement Hive libraries and utilizes a virtually identical syntax, so transferring our existing ETL was as simple as copy/paste and uploading the <a href=\"https://cwiki.apache.org/confluence/display/Hive/CSV+Serde\" target=\"_blank\">Hive CSV Serde</a>. Soon after, Databricks released their native <a href=\"https://github.com/databricks/spark-csv\" target=\"_blank\">Spark CSV Serde</a>, and we implemented it without issue. We also needed to extract these tables into Tableau, which Databricks again made simple. Databricks implements a Thrift server by default for JDBC calls, and Tableau has a SparkSQL connector that utilizes the JDBC. We needed to modify one setting here:\n\n[scala]\nspark.sql.thriftServer.incrementalCollect = true\n[/scala]\n\nThis tells the Thrift server that data should be sent in small increments rather than collecting all the data into the driver node and then pushing it out, which will cause the driver to run out of space quite quickly if you\u2019re extracting a non-trivial amount of data. With that, replicating our ETL in Databricks was complete. We were now leveraging Databricks to process 100GB/day of raw CSV data.\n\nOur next steps involved learning Scala/Spark and the various optimizations that can be made inside Spark, while also starting to integrate other data sources. We had an SQL query that was taking over 2 hours to complete, but the output was being saved to S3 for consumption by Databricks. The query took so long because it involved joining 11 tables together and building a lookup with 10 fields. While optimizations could\u2019ve been made to the structure of the MySQL db or the query itself, we thought, why not do it in Databricks? We were able to reduce the query time from 2 hours down to less than 2 minutes; and again, since the SparkSQL syntax encompasses all the basic SQL commands, implementing the query was as easy as copy/paste (N.B. we needed to modify our AWS settings in order to get this to work). A simplified version of the code below:\n\n[scala]\nimport com.mysql.jdbc.Driver\n\n// create database connection\nval host = &quot;Internal IP Address of mysql db (10.x.x.x)&quot;\nval port = &quot;3306&quot;\nval user = &quot;username&quot;\nval password = &quot;password&quot;\nval dbName = &quot;databaseName&quot;\nval dbURL = s&quot;jdbc:mysql://$host:$port/$dbName?user=$user&amp;password=$password&quot;\n\n//load table as a val\nval table =\n  sqlContext\n    .read\n    .format(&quot;jdbc&quot;)\n    .options(\n      Map(\n        &quot;url&quot; -&gt; s&quot;$dbURL&quot;,\n        &quot;dbtable&quot; -&gt; &quot;dbName.tableToLoad&quot;,\n        &quot;driver&quot;  -&gt; &quot;com.mysql.jdbc.Driver&quot;))\n    .load()\n\n//register as a temporary SparkSQL table\ntable.registerTempTable(&quot;tableName&quot;)\n\n//run the query\nval tableData = \n  sqlContext.sql(&quot;&quot;&quot;\n    SELECT *\n    FROM tableName&quot;&quot;&quot;)\n[/scala]\n\nWe also receive and send out files on a daily basis via FTP. We need to download and store copies of these files, so we started downloading them to S3 using Databricks. This allowed us to further centralize our ETL in Databricks . A simplified version of the code below:\n\n[scala]\nimport org.apache.commons.net.ftp.FTPClient;\nimport org.apache.commons.net.ftp.FTPFile;\nimport org.apache.commons.net.ftp.FTPReply;\nimport java.io.File;\nimport java.io.FileOutputStream;\n\nval ftpURL = &quot;ftp.url.com&quot;\nval user = &quot;username&quot;\nval pswd = &quot;password&quot;\nval ftp = new FTPClient()\n\nftp.connect(ftpURL)\nftp.enterLocalPassiveMode()\nftp.login(user,pswd)\n\nval folderPath = &quot;pathToDownloadFrom&quot;\n\nval files = ftp.listFiles(s&quot;/$folderPath&quot;).map(_.getName)\n\ndbutils.fs.mkdirs(s&quot;&quot;&quot;/$savePath&quot;&quot;&quot;)\n\nval outputFile = new File(s&quot;&quot;&quot;/dbfs/$savePath/filename.txt&quot;&quot;&quot;)\nval output = new FileOutputStream(outputFile)\n\nftp.retrieveFile(s&quot;/$folder/filename.txt&quot;, output)\n\noutput.close()\n\nftp.logout\n[/scala]\n\nWe switched over from saving our processed data as CSV to Parquet. Parquet is columnar, which means that when you\u2019re only using some of the columns in your data, it is able to ignore the other columns. This has massive speed gains when you have trillions of rows, and allows us to decrease time waiting for initial results. We try to keep our individual parquet files around 120MB in size, which is the default block size for Spark and allows the cluster to load data quickly (which we found to be a bottleneck when using small CSV files).\n\nConsolidating our ETL steps into a single location has added benefits when we get to actually analyzing the data. We always try to stay on the cutting edge of machine learning and offline analysis techniques for identifying new user segments, which requires that we maintain a recent set of testable user data, while also being able to compare the output of new techniques to what we\u2019re currently using. Being able to consolidate all of our data into a single platform has sped up our iteration process considerably, especially since this single platform includes the MLlib project.\n<h2>Benefits and Lessons Learned</h2>\nIn the 1.5 years since implementing Databricks, we\u2019ve been able to:\n<ul>\n \t<li>Speed up our first-step ETL by ~4x</li>\n \t<li>Integrate mysql and ftp data sources directly into Databricks, and speed up those portions of the ETL by 100x</li>\n \t<li>Optimize our processed data for analysis</li>\n</ul>\nWe have also learned a lot during this process. A few tips and lessons include:\n<ul>\n \t<li>Store your data as Parquet files</li>\n \t<li>Leverage the processing power of Spark and implement any RDS ETL in Databricks via the JDBC</li>\n \t<li>Partition your data as granularly as possible while maintaining larger file sizes - loading data is slow, but processing is fast!</li>\n</ul>\nTo learn more about how Sellpoints is using Databricks, <a href=\"http://go.databricks.com/case-studies/sellpoints\" target=\"_blank\">read the case study</a>.\n\nA special thank you to Ivan Dejanovic for his edits and patience."}
{"status": "publish", "description": null, "creator": "Wayne Chan", "link": "https://databricks.com/blog/2016/05/18/spark-mllib-from-quick-start-to-scikit-learn.html", "authors": null, "id": 7288, "categories": ["Announcements", "Company Blog"], "dates": {"publishedOn": "2016-05-18", "tz": "UTC", "createdOn": "2016-05-18"}, "title": "Apache Spark MLlib: From Quick Start to Scikit-Learn", "slug": "spark-mllib-from-quick-start-to-scikit-learn", "content": "A few months ago, we held a live webinar \u2013\u00a0<a href=\"http://go.databricks.com/spark-mllib-from-quick-start-to-scikit-learn\">Apache Spark MLlib: From Quick Start to Scikit-Learn</a>\u00a0\u2013 to give a quick primer on machine learning, Spark MLlib, and an overview of some Spark machine learning use cases. \u00a0It also covered multiple Spark MLlib quick start demos\u00a0as well as\u00a0the integration of common data science tools like Python pandas, scikit-learn, and R with MLlib.\n\nThe webinar is <a href=\"http://go.databricks.com/spark-mllib-from-quick-start-to-scikit-learn\">accessible on-demand</a>.\u00a0Its slides and sample notebooks are also downloadable as attachments to the webinar. <a href=\"http://go.databricks.com/databricks-community-edition-beta-waitlist\">Join the Databricks Community Edition beta</a> to get free access to Spark and try out the notebooks.\n\nWe have answered the common questions raised by webinar viewers below. If you have additional questions, please check out the <a href=\"https://forums.databricks.com/\">Databricks Forum</a>.\n<h2>Common webinar questions and answers</h2>\nClick on the question to see answer:\n<ul>\n \t<li><a href=\"https://forums.databricks.com/answers/7795/view.html\">Since Spark is distributed then how does it combine the result from each node when using MLlib? Does it give the same result as a single node?</a></li>\n \t<li><a href=\"https://forums.databricks.com/answers/7797/view.html\">With small datasets (1K rows), is it possible to get a response for a regression or a decision tree learning task in 1 or 2 seconds? What is the overhead when using Spark on a laptop (single node)?</a></li>\n \t<li><a href=\"https://forums.databricks.com/answers/7800/view.html\">When cross validation is being done for model parameter selection, are both the folds and the model+params distributed across the cluster?</a></li>\n \t<li><a href=\"https://forums.databricks.com/answers/7810/view.html\">For the demo in the MLlib webinar, how are the features identified from the words, before linear regression?</a></li>\n \t<li><a href=\"https://forums.databricks.com/answers/7805/view.html\">If one uses MLlib, does one still need Python or R?</a></li>\n \t<li><a href=\"https://forums.databricks.com/answers/7807/view.html\">Which is best for ML: Python or R?</a></li>\n \t<li><a href=\"https://forums.databricks.com/answers/7812/view.html\">In the MLlib webinar demo, were you using spark.ml based regression? Are you planning to freeze spark.mllib RDD based algorithms?</a></li>\n \t<li><a href=\"https://forums.databricks.com/answers/7817/view.html\">MLlib SGD-based algorithms (linear models) can diverge unless the features are scaled; why is feature scaling not TRUE by default, and why such sensitivity in the first place?</a></li>\n</ul>"}
{"status": "publish", "description": null, "creator": "Wayne Chan", "link": "https://databricks.com/blog/2016/05/26/just-in-time-data-warehousing-on-databricks-change-data-capture-and-schema-on-read.html", "authors": null, "id": 7290, "categories": ["Company Blog", "Events"], "dates": {"publishedOn": "2016-05-26", "tz": "UTC", "createdOn": "2016-05-26"}, "title": "Just-in-Time Data Warehousing on Databricks: Change Data Capture and Schema On Read", "slug": "just-in-time-data-warehousing-on-databricks-change-data-capture-and-schema-on-read", "content": "A few months ago, we held a live webinar \u2014 <a href=\"http://go.databricks.com/jit-dw-databricks-cdc-schema-on-read\">Just-in-Time Data Warehousing on Databricks: Change Data Capture and Schema On Read</a> \u2014 which covered how to build a Just-in-Time Data Warehouse on Databricks with a focus on performing Change Data Capture from a relational database and joining that data to a variety of data sources.\n\nThe webinar is <a href=\"http://go.databricks.com/jit-dw-databricks-cdc-schema-on-read\">accessible on-demand</a>.\u00a0Its slides and sample notebooks are also downloadable as attachments to the webinar. <a href=\"http://go.databricks.com/databricks-community-edition-beta-waitlist\">Join the Databricks Community Edition beta</a> to get free access to Apache Spark and try out the notebooks.\n\nWe have answered the common questions raised by webinar viewers below. If you have additional questions, please check out the <a href=\"https://forums.databricks.com/\">Databricks Forum</a>.\n\n<strong>Common webinar questions and answers</strong>\n\nClick on the question to see answer:\n<ul>\n \t<li><a href=\"https://forums.databricks.com/questions/7933/costs-for-my-enterprise-data-warehouse-is-killing.html\">Replacing ETL would be great. Costs for my enterprise data warehouse is killing me (both Oracle and Teradata). Could I take it a step further and use Spark, along with a NoSQL DB like Mongo or Cassandra, and an underlying Hadoop layer for storage, totally replace both my ETL layer and EDW?</a></li>\n \t<li><a href=\"https://forums.databricks.com/questions/7934/from-the-webinar-just-in-time-data-warehouse-chang.html\">Regarding JSON, if I had a series of individual JSON files in an S3 bucket, could I apply a \"SQL\" query using schema-on-read across multiple JSON files at once?</a></li>\n \t<li><a href=\"https://forums.databricks.com/questions/7940/from-the-webinar-just-in-time-data-warehouse-chang-3.html#answer-7941\">On one of the first CDC slides, a records was shown with a date of 1/2 and amt of $250. Then there was an update in the source db to change the amt of $350 on 1/5. a second row was added to the target db, now there were two rows, one with $250, one with $350. Both rows in the target db showed the updated date of 1/5. Was that intentional, updating the last updated date on the original row in the target db? I would have assumed the $250 row shouldn't have had it's last updated date changed.</a></li>\n \t<li><a href=\"https://forums.databricks.com/questions/7936/from-the-webinar-just-in-time-data-warehouse-chang-1.html\">Can you share some ideas on how to handle column renames as well?</a></li>\n \t<li><a href=\"https://forums.databricks.com/questions/7938/from-the-webinar-just-in-time-data-warehouse-chang-2.html#answer-7939\">Can parquet on S3 and Spark actually replace an MPP data warehouse such as Teradata or Redshift and still get the same MPP performance?</a></li>\n</ul>"}
{"status": "publish", "description": null, "creator": "jules_damji", "link": "https://databricks.com/blog/2016/05/18/6-reasons-to-attend-spark-summit.html", "authors": null, "id": 7574, "categories": ["Company Blog", "Events"], "dates": {"publishedOn": "2016-05-18", "tz": "UTC", "createdOn": "2016-05-18"}, "title": "6 Reasons to Attend Spark Summit", "slug": "6-reasons-to-attend-spark-summit", "content": "<h2>Temples of Developer Knowledge</h2>\n\u201cDevelopers are the new kingmakers,\u201d wrote Stephen O\u2019Grady in his book \u201c<a href=\"https://thenewkingmakers.com/\" target=\"_blank\">The New KingMakers: How Developers Conquered the World.</a>\u201d This shift in the rise of democratic power and developer class seem to stem from the flourishing of open source projects, open innovation, open development organizations like the <a href=\"http://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>, and easy confidence to contribute to the open source projects\u2014all developed by the community and for the community, as repositories of developer knowledge.\n\nAs temples of knowledge, the <a href=\"https://spark-summit.org/2016/\" target=\"_blank\">Spark Summit</a> series of community conferences is the place where the best of Apache Spark developers converge to share their expert knowledge with the community, to participate in Meetups and networking events, or to ruffle the feathers at Ask Me Anything or track sessions. Year after year, these summits are an affirmation that Apache Spark\u2019s popularity, adoption, and growth are uncontested.\n\n<img class=\"aligncenter size-full wp-image-7584\" src=\"https://databricks.com/wp-content/uploads/2016/05/2015-Apache-Spark-stats.png\" alt=\"2015-Apache-Spark-stats\" width=\"632\" height=\"248\" />\n\nFor any Spark developer, the Spark Summit is a pilgrimage to the temple of knowledge\u2014and here are my six reasons why:\n<ol>\n \t<li><strong><a href=\"https://spark-summit.org/2016/speakers/\" target=\"_blank\">Keynotes from Distinguished Engineers:</a></strong>\u00a0 Four distinguished engineers, scientists and academics who have disrupted distributed computing and advanced deep learning will give keynotes. I can\u2019t think of any other event where you get to hear legendary leaders like <a href=\"https://spark-summit.org/2016/speakers/matei-zaharia/\" target=\"_blank\">Matei Zaharia</a>, <a href=\"https://spark-summit.org/2016/speakers/doug-cutting/\">Doug Cutting</a>,\u00a0<a href=\"https://spark-summit.org/2016/speakers/jeff-dean/\" target=\"_blank\">Jeff Dean</a>, and <a href=\"https://spark-summit.org/2016/speakers/dr-andrew-ng/\" target=\"_blank\">Andrew Ng</a> share their wisdom about the present and future of Apache Spark, big data, distributed computing, and deep learning with the community.</li>\n \t<li><strong><a href=\"https://spark-summit.org/2016/schedule/\" target=\"_blank\">A Developer Day:</a></strong> A day dedicated with developer focused sessions, you can choose from 45 sessions spanning five tracks: Developer, Data Science, Spark Ecosystem, Use Cases &amp; Experience, and Research\u2014all intense and deep-dive tech-talks. You can learn about developments in <a href=\"https://databricks.com/blog/2016/05/11/spark-2-0-technical-preview-easier-faster-and-smarter.html\">Apache Spark 2.0</a>, Structured Streaming, Spark SQL, Unified Dataset APIs, MLlib 2.0, GraphFrames, Apache Spark and machine learning at scale, and more.</li>\n \t<li><strong><a href=\"https://spark-summit.org/2016/spark-training/\" target=\"_blank\">Apache Spark Training:</a></strong> For those new or familiar to Apache Spark, an entire day is dedicated for a developer to enhance his or her Spark skills. Popular courses range from <em>Spark Essentials</em> to <em>Data Science with Apache Spark to Advanced Apache Spark</em>.</li>\n \t<li><strong><a href=\"http://www.meetup.com/spark-users/events/230322011/\" target=\"_blank\">The Bay Area Apache Spark Meetup:</a></strong> Apache Spark Meetups are well attended and popular for tech-talks. You can learn what other Spark developers are up to, mingle, network and enjoy the beverages and food in an informal setting.</li>\n \t<li><strong>Meet the Committer or Expert:</strong> In an informal office hour of face-to-face conversation, you can interact with Spark experts and ask them anything about Apache Spark\u2014not an opportunity you get everyday.</li>\n \t<li><strong><a href=\"https://www.facebook.com/places/Things-to-do-in-San-Francisco-California/114952118516947/\" target=\"_blank\">San Francisco:</a></strong> We might be geeks at heart, but we do enjoy fine wine, delicious food, chic cafes, art museums, and nightlife in a cosmopolitan metropolis. This captivating city by the bay has its mystic draw not only to artists and tourists but engineers and high-tech entrepreneurs. Why not chill after an intense day of tech talks?</li>\n</ol>\nAt the end of this three-day Apache Spark fest, you\u2019ll have scaled new heights in your Spark knowledge, you\u2019ll have met developers with similar interests, you\u2019ll have discovered gorgeous San Francisco, and you\u2019ll have recorded digital memories\u2014of course, including best selfies\u2014to share with others.\n\nWe hope to you see you there!\n<h2>What\u2019s next?</h2>\nTickets are selling fast. If you haven\u2019t yet, <a href=\"http://www.prevalentdesignevents.com/sparksummit2016/registration.aspx\" target=\"_blank\">register today</a> using the discount code <strong>DatabricksSF20</strong> and get 20% off."}
{"status": "publish", "description": null, "creator": "joseph", "link": "https://databricks.com/blog/2016/05/19/approximate-algorithms-in-apache-spark-hyperloglog-and-quantiles.html", "authors": null, "id": 7612, "categories": ["Apache Spark", "Ecosystem", "Engineering Blog", "Streaming"], "dates": {"publishedOn": "2016-05-19", "tz": "UTC", "createdOn": "2016-05-19"}, "title": "Approximate Algorithms in Apache Spark: HyperLogLog and Quantiles", "slug": "approximate-algorithms-in-apache-spark-hyperloglog-and-quantiles", "content": "[sidenote]Spark Summit 2016 will be held in San Francisco on June 6 - 8. Check out the <a href=\"https://spark-summit.org/2016/\">full agenda</a> and get your ticket before it sells out![/sidenote]\n\n<hr />\n\n[dbce_cta href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/Databricks%20Blog%20Approximate%20Quantile.html\"]Try this notebook in Databricks[/dbce_cta]\n<h2>Introduction</h2>\n<a href=\"http://spark.apache.org/\" target=\"_blank\">Apache Spark</a> is fast, but applications such as preliminary data exploration need to be even faster and are willing to sacrifice some accuracy for a faster result. Since version 1.6, Spark implements <em>approximate algorithms</em> for some common tasks: counting the number of distinct elements in a set, finding if an element belongs to a set, computing some basic statistical information for a large set of numbers. Eugene Zhulenev, from Collective, has already <a href=\"https://databricks.com/blog/2015/10/13/interactive-audience-analytics-with-spark-and-hyperloglog.html\">blogged in these pages about the use of approximate counting in the advertising business</a>.\n\nThe following algorithms have been implemented against <a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\">DataFrames</a> and <a href=\"https://databricks.com/blog/2016/01/04/introducing-spark-datasets.html\">Datasets</a> and committed into Apache Spark\u2019s branch-2.0, so they will be available in Apache Spark 2.0 for Python, R, and Scala:\n<ul>\n \t<li><strong>approxCountDistinct:</strong> returns an estimate of the number of distinct elements</li>\n \t<li><strong>approxQuantile:</strong> returns approximate percentiles of numerical data</li>\n</ul>\nResearchers have looked at such algorithms for a long time. Spark strives at implementing approximate algorithms that are deterministic (they do not depend on random numbers to work) and that have proven theoretical error bounds: for each algorithm, the user can specify a target error bound, and the result is guaranteed to be within this bound, either exactly (deterministic error bounds) or with very high confidence (probabilistic error bounds). Also, it is important that this algorithm works well for the wealth of use cases seen in the Spark community.\n\nIn this blog, we are going to present details on the implementation of <strong>approxCountDistinct</strong> and <strong>approxQuantile</strong> algorithms and showcase its implementation in a Databricks notebook.\n<h2>Approximate count of distinct elements</h2>\nIn ancient times, imagine <a href=\"https://en.wikipedia.org/wiki/Cyrus_the_Great\" target=\"_blank\">Cyrus the Great</a>, emperor of Persia and Babylon, having just completed a census of all his empire, fancied to know how many different first names were used throughout his empire, and he put his vizier to the task. The vizier knew that his lord was impatient and wanted an answer fast, even if just an approximate.\n\nThere was an issue, though; some names such as Darius, Atusa or Ardumanish were very popular and appeared often on the census records. Simply counting how many people were living within the empire would give a poor answer, and the emperor would not be fooled.\n\nHowever, the vizier had some modern and profound knowledge of mathematics. He assembled all the servants of the palace, and said: \u201cServants, each of you will take a clay tablet from the census record. For each first name that is inscribed on the tablet, you will take the first 3 letters of the name, called l1, l2 and l3, and compute the following number:\n<pre>N = l1 + 31 * l2 + 961 * l3</pre>\nFor example, for Darius (D = 3, A = 0, R = 17), you will get N = 16340.\n\nThis will give you a number for each name of the tablet. For each number, you will count the number of zeros that end this number. In the case of Hossein (N=17739), this will give you no zero. After each of you does that for each name on his or her tablet, you will convene and you will tell me what is the greatest number of zeros you have observed. Now proceed with great haste and make no calculation mistake, lest you want to endure my wrath!\u201d\n\nAt the end of the morning, one servant came back, and said they had found a number with four zeros, and that was the largest they all observed across all the census records. The vizier then announced to his master that he was the master of a population with about 1.3 * 10^4 = 13000 different names. The emperor was highly impressed and he asked the vizier how he had accomplished this feat. To which the vizier uttered one word: \u201chyper-log-log\u201d.\n\nThe HyperLogLog algorithm (and its variant HyperLogLog++ implemented in Spark) relies on a clever observation: if the numbers are spread uniformly across a range, then the count of distinct elements can be approximated from the largest number of leading zeros in the binary representation of the numbers. For example, if we observe a number whose digits in binary form are of the form 0\u2026(k times)...01...1, then we can estimate that there are in the order of 2^k elements in the set. This is a very crude estimate but it can be refined to great precision with a sketching algorithm. A thorough explanation of the mechanics behind this algorithm can be found in the <a href=\"http://www.dmtcs.org/dmtcs-ojs/index.php/proceedings/article/viewArticle/914\" target=\"_blank\">original paper</a>.\n\nFrom the example above with the vizier and his servants, this algorithm does not need to perform shuffling, just map (each servant works on a tablet) and combine (the servants can make pairs and decide which one has the greatest number, until there is only one servant). There is no need move data around, only small statistics about each block of data, which makes it very useful in a large dataset setting such as Spark.\n\nNow, in modern times, how well does this technique work, where datasets are much larger and when servants are replaced with a Spark cluster? We considered a dataset of 25 millions online reviews from an online retail vendor, and we set out to approximate the number of customers behind these reviews. Since customers write multiple reviews, it is a good fit for approximate distinct counting.\n\nHere is how to get an approximate count of users in PySpark, within 1% of the true value and with high probability:\n\n[python]\n# users: DataFrame[user: string]\nusers.select(approxCountDistinct(&quot;user&quot;, rsd = 0.01)).show()\n[/python]\n\nThis plot (fig. 1) shows how the number of distinct customers varies by the error margin. As expected, the answer becomes more and more precise as the requested error margin decreases.\n\n[caption id=\"attachment_7618\" align=\"aligncenter\" width=\"500\"]<a href=\"https://databricks.com/wp-content/uploads/2016/05/hyperloglog-plot-figure-4.png\"><img class=\"wp-image-7618 size-full\" src=\"https://databricks.com/wp-content/uploads/2016/05/hyperloglog-plot-figure-4.png\" alt=\"Fig. 1\" width=\"500\" height=\"500\" /></a> Fig. 1[/caption]\n\nHow long does it take to compute? For the analysis above, this plot (fig 2.) presents the running time of the approximate counting against the requested precision. For errors above 1%, the running time is just a minute fraction of computing the exact answer. For precise answers, however, the running time increases very fast and it is better to directly compute the exact answer.\n\n[caption id=\"attachment_7615\" align=\"aligncenter\" width=\"500\"]<a href=\"https://databricks.com/wp-content/uploads/2016/05/hyperloglog-plot-figure-1.png\"><img class=\"wp-image-7615 size-full\" src=\"https://databricks.com/wp-content/uploads/2016/05/hyperloglog-plot-figure-1.png\" alt=\"Fig. 2\" width=\"500\" height=\"500\" /></a> Fig. 2[/caption]\n\nAs a conclusion, when using approxCountDistinct, you should keep in mind the following:\n<ul>\n \t<li>When the requested error on the result is high (&gt; 1%), approximate distinct counting is very fast and returns results for a fraction of the cost of computing the exact result. In fact, the performance is more or less the same for a target error of 20% or 1%.</li>\n \t<li>For higher precisions, the algorithm hits a wall and starts to take more time than exact counting.</li>\n</ul>\n<h2>Approximate quantiles</h2>\nQuantiles (percentiles) are useful in a lot of contexts. For example, when a web service is performing a large number of requests, it is important to have performance insights such as the latency of the requests. More generally, when faced with a large quantity of numbers, one is often interested in some aggregate information such as the mean, the variance, the min, the max, and the percentiles. Also, it is useful to just have the extreme quantiles: the top 1%, 0.1%, 0.01%, and so on.\n\nSpark implements a robust, well-known algorithm that originated in the streaming database community. Like HyperLogLog, it computes some statistics in each node and then aggregates them on the Spark driver. The current algorithm in Spark can be adjusted to trade accuracy against computation time and memory. Based on the same example as before, we look at the length of the text in each review. Most reviewers express their opinions in a few words, but some customers are prolific writers: the longest review in the dataset is more than 1500 words, while there are several thousand 1-word reviews with various degrees of grammatical freedom.\n\nWe plot (fig 3.) here the median length of a review (the 50th percentile) as well as more extreme percentiles. This graph shows that there are few very long reviews and that most of them are below 300 characters.\n\n[caption id=\"attachment_7617\" align=\"aligncenter\" width=\"560\"]<img class=\"aligncenter size-full wp-image-7617\" src=\"https://databricks.com/wp-content/uploads/2016/05/hyperloglog-plot-figure-3.png\" alt=\"Fig. 3\" width=\"560\" height=\"400\" /> Fig. 3[/caption]\n\nThe behavior of approximate quantiles is the same as HyperLogLog: when asking for a rough estimate within a few percent of the exact answer, the algorithm is much faster than an exact computation (fig 4.). For a more precise answer, an exact computation is necessary.\n\n[caption id=\"attachment_7616\" align=\"aligncenter\" width=\"500\"]<a href=\"https://databricks.com/wp-content/uploads/2016/05/hyperloglog-plot-figure-2.png\"><img class=\"size-full wp-image-7616\" src=\"https://databricks.com/wp-content/uploads/2016/05/hyperloglog-plot-figure-2.png\" alt=\"Fig. 4\" width=\"500\" height=\"500\" /></a> Fig. 4[/caption]\n<h2>Conclusion</h2>\nWe demonstrated details on the implementation of <strong>approxCountDistinct</strong> and <strong>approxQuantile</strong> algorithms. Though Spark is lighting-fast, sometimes exploratory data applications need even faster results at the expense of sacrificing accuracy. And these two algorithms achieve faster execution.\n\nApache Spark 2.0 will include some state-of-the art approximation algorithms for even faster results. Users will be able to pick between fast, inexact answers and slower, exact answers. Are there some other approximate algorithms you would like to see? Let us know.\n\nThese algorithms are now implemented in a <a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/Databricks%20Blog%20Approximate%20Quantile.html\" target=\"_blank\">Databricks notebook</a>. To try it out yourself, sign up for an account with Databricks <a href=\"https://databricks.com/try-databricks\">here</a>.\n<h2>Further Reading</h2>\n<ul>\n \t<li><a href=\"https://databricks.com/blog/2015/10/13/interactive-audience-analytics-with-spark-and-hyperloglog.html\">Interactive Audience Analytics with Spark and HyperLogLog</a></li>\n \t<li><a href=\"http://www.dmtcs.org/dmtcs-ojs/index.php/proceedings/article/viewArticle/914\" target=\"_blank\">HyperLogLog: the analysis of the near-optimal cardinality estimation algorithm</a></li>\n \t<li><a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/Databricks%20Blog%20Approximate%20Quantile.html\">Approximate Quantiles in Apache Spark notebook</a></li>\n</ul>"}
{"status": "publish", "description": null, "creator": "Wayne Chan", "link": "https://databricks.com/blog/2016/05/20/spark-live-la-is-just-around-the-corner.html", "authors": null, "id": 7641, "categories": ["Announcements", "Company Blog", "Events"], "dates": {"publishedOn": "2016-05-20", "tz": "UTC", "createdOn": "2016-05-20"}, "title": "Spark Live Los Angeles is just around the corner", "slug": "spark-live-la-is-just-around-the-corner", "content": "A couple weeks ago we announced\u00a0<a href=\"https://databricks.com/spark-live\">Spark Live</a>, an eight-city road show brought to you by Databricks in collaboration with premier sponsor Intel, designed exclusively for data analytics practitioners, and decision makers who want to learn how to leverage the power of open source Apache Spark and Databricks to simplify data processing and make their transformative use cases a reality.\n\nThe first stop on our tour in search of Spark enthusiasts was Hanover, Maryland on May 9th.\u00a0Needless to say, our inaugural event was a huge success! We exceeded seating capacity with the room packed to the back walls and\u00a0standing room only.\n\n<img class=\"aligncenter size-medium wp-image-7647\" src=\"https://databricks.com/wp-content/uploads/2016/05/IMG_1822-e1463699691142-1024x768.jpg\" alt=\"IMG_1822\" width=\"800\" height=\"600\" />\n\nThroughout the day we covered industry insights, discussions on how to deploy Spark for multiple use cases, live product demos, and a hands-on technical workshop of Spark in Databricks Community Edition.\n<h2>Next stop: The City of Angels on May 24th!</h2>\nNext week we will make our way down to sunny Southern California for another full day of exciting technical presentations and training.\u00a0The day will feature a stellar lineup of presenters including:\n<ul>\n \t<li>Ali Ghodsi - CEO and Co-founder of Databricks</li>\n \t<li>Ram Sriharsha - Product Manager at Databricks, and Spark Committer and PMC member</li>\n \t<li>Richard Garris - Solutions Architect at Databricks and machine learning expert</li>\n</ul>\nWe will also feature talks\u00a0by our premier sponsor Intel, a leading contributor to the Apache Spark project, and local sponsor Redis Labs who will showcase their new Spark\u00a0connector.\n<h2>Just a few spots left - request yours\u00a0today!</h2>\nFor more information about the Spark Live 2016 road show or to request your spot for the city closest to you, visit the <a href=\"https://databricks.com/spark-live\">Spark Live homepage</a>.\n<h2>Not in Los Angeles?</h2>\nDon't worry, we've got a few more stops along the way:\n<ul>\n \t<li><a href=\"http://go.databricks.com/spark-live/request-your-spot\">Reston, VA - coming in July</a> (federal employees and contractors welcome)</li>\n \t<li><a href=\"http://go.databricks.com/spark-live/request-your-spot\">Austin, TX - coming in August</a></li>\n \t<li><a href=\"http://go.databricks.com/spark-live/request-your-spot\">Seattle, WA - coming in August</a></li>\n \t<li><a href=\"https://databricks.com/spark-live/boston-ma\">Boston, MA - September 22nd</a></li>\n \t<li><a href=\"http://go.databricks.com/spark-live/request-your-spot\">Chicago, IL - coming in October</a></li>\n \t<li><a href=\"http://go.databricks.com/spark-live/request-your-spot\">New York, NY - coming in November</a></li>\n</ul>"}
{"status": "publish", "description": null, "creator": "rxin", "link": "https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html", "authors": null, "id": 7668, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2016-05-23", "tz": "UTC", "createdOn": "2016-05-23"}, "title": "Apache Spark as a Compiler: Joining a Billion Rows per Second on a Laptop", "slug": "apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop", "content": "[dbce_cta href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/Whole-stage%20code%20generation.html\"]Try this notebook in Databricks[/dbce_cta]\n\n<hr/>\n\nWhen our team at Databricks planned our contributions to the upcoming Apache Spark 2.0 release, we set out with an ambitious goal by asking ourselves: <b>Apache Spark is already pretty fast, but can we make it 10x faster</b>?\n\nThis question led us to fundamentally rethink the way we built Spark\u2019s physical execution layer. When you look into a modern data engine (e.g. Spark or other MPP databases), a majority of the CPU cycles are spent in useless work, such as making virtual function calls or reading or writing intermediate data to CPU cache or memory. Optimizing performance by reducing the amount of CPU cycles wasted in this useless work has been a long-time focus of modern compilers.\n\nApache Spark 2.0 will ship with the <a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\">second generation Tungsten engine</a>. Built upon ideas from modern compilers and MPP databases and applied to data processing queries, Tungsten emits (SPARK-12795) optimized bytecode at runtime that collapses the entire query into a single function, eliminating virtual function calls and leveraging CPU registers for intermediate data. As a result of this streamlined strategy, called \u201cwhole-stage code generation,\u201d we significantly improve CPU efficiency and gain performance.\n\n<h2>The Past: Volcano Iterator Model</h2>\n\nBefore we dive into the details of whole-stage code generation, let us revisit how Spark (and most database systems) work currently. Let us illustrate this with a simple query that scans a single table and counts the number of elements with a given attribute value:\n\n<img src=\"https://databricks.com/wp-content/uploads/2016/05/volcano-iterator-model.png\" alt=\"Volcano Iterator Model\" width=\"560\" height=\"280\" class=\"aligncenter size-full wp-image-7675\" />\n\nTo evaluate this query, older versions (1.x) of Spark leveraged a popular classic query evaluation strategy based on an iterator model (commonly referred to as the <a href=\"http://paperhub.s3.amazonaws.com/dace52a42c07f7f8348b08dc2b186061.pdf\" target=\"_blank\">Volcano model</a>). In this model, a query consists of multiple operators, and each operator presents an interface, <code>next()</code>, that returns a tuple at a time to the next operator in the tree. For instance, the Filter operator in the above query roughly translates into the code below:\n\n[scala]\nclass Filter(child: Operator, predicate: (Row =&gt; Boolean))\n  extends Operator {\n  def next(): Row = {\n    var current = child.next()\n    while (current == null || predicate(current)) {\n      current = child.next()\n    }\n    return current\n  }\n}\n[/scala]\n\nHaving each operator implement an iterator interface allowed query execution engines to elegantly compose arbitrary combinations of operators without having to worry about what opaque data type each operator provides. As a result, the Volcano model became the standard for database systems in the last two decades, and is also the architecture used in Spark.\n\n<h2>Volcano vs Hand-written Code</h2>\n\nTo digress a little, what if we ask a college freshman and give her 10 minutes to implement the above query in Java? It\u2019s quite likely she\u2019d come up with iterative code that loops over the input, evaluates the predicate and counts the rows:\n\n[scala]\nvar count = 0\nfor (ss_item_sk in store_sales) {\n  if (ss_item_sk == 1000) {\n    count += 1\n  }\n}\n[/scala]\n\nThe above code was written specifically to answer a given query, and is obviously not \u201ccomposable.\u201d But how would the two\u2014Volcano generated and hand-written code\u2014compare in performance? On one side, we have the architecture chosen for composability by Spark and majority of the database systems. On the other, we have a simple program written by a novice in 10 minutes. We ran a simple benchmark that compared the \u201ccollege freshman\u201d version of the program and a Spark program executing the above query using a single thread against Parquet data on disk:\n\n<img src=\"https://databricks.com/wp-content/uploads/2016/05/volcano-vs-hand-written-code-1024x397.png\" alt=\"Volcano vs hand-written code\" width=\"600\" height=\"233\" class=\"aligncenter size-large wp-image-7676\" />\n\nAs you can see, the \u201ccollege freshman\u201d hand-written version is an order of magnitude faster than the Volcano model. It turns out that the 6 lines of Java code are optimized, for the following reasons:\n\n<ol>\n<li><b>No virtual function dispatches:</b> In the Volcano model, to process a tuple would require calling the <code>next()</code> function at least once. These function calls are implemented by the compiler as virtual function dispatches (via vtable). The hand-written code, on the other hand, does not have a single function call. Although virtual function dispatching has been an area of focused optimization in modern computer architecture, it still costs multiple CPU instructions and can be quite slow, especially when dispatching billions of times.</li>\n<li><b>Intermediate data in memory vs CPU registers:</b> In the Volcano model, each time an operator passes a tuple to another operator, it requires putting the tuple in memory (function call stack). In the hand-written version, by contrast, the compiler (JVM JIT in this case) actually places the intermediate data in CPU registers. Again, the number of cycles it takes the CPU to access data in memory is orders of magnitude larger than in registers.</li>\n<li><b>Loop unrolling and SIMD:</b> Modern compilers and CPUs are incredibly efficient when compiling and executing simple for loops. Compilers can often unroll simple loops automatically, and even generate SIMD instructions to process multiple tuples per CPU instruction. CPUs include features such as pipelining, prefetching, and instruction reordering that make executing simple loops efficient. These compilers and CPUs, however, are not great with optimizing complex function call graphs, which the Volcano model relies on.</li>\n</ol>\n\nThe key take-away here is that the <b>hand-written code is written specifically to run that query and nothing else, and as a result it can take advantage of all the information that is known</b>, leading to optimized code that eliminates virtual function dispatches, keeps intermediate data in CPU registers, and can be optimized by the underlying hardware.\n\n<h2>The Future: Whole-stage Code Generation</h2>\n\nFrom the above observation, a natural next step for us was to explore the possibility of automatically generating this <i>handwritten</i> code at runtime, which we are calling \u201cwhole-stage code generation.\u201d This idea is inspired by Thomas Neumann\u2019s seminal VLDB 2011 paper on <i><a href=\"http://www.vldb.org/pvldb/vol4/p539-neumann.pdf\" target=\"_blank\">Efficiently Compiling Efficient Query Plans for Modern Hardware</a></i>. For more details on the paper, Adrian Colyer has coordinated with us to publish a <a href=\"http://blog.acolyer.org/2016/05/23/efficiently-compiling-efficient-query-plans-for-modern-hardware\" target=\"_blank\">review on The Morning Paper blog</a> today.\n\nThe goal is to leverage whole-stage code generation so <b>the engine can achieve the performance of hand-written code, yet provide the functionality of a general purpose engine</b>. Rather than relying on operators for processing data at runtime, these operators together generate code at runtime and collapse each fragment of the query, where possible, into a single function and execute that generated code instead.\n\nFor instance, in the query above, the entire query is a single stage, and Spark would generate the the following JVM bytecode (in the form of Java code illustrated here). More complicated queries would result in multiple stages and thus multiple different functions generated by Spark.\n\n<img src=\"https://databricks.com/wp-content/uploads/2016/05/whole-stage-code-generation-model.png\" alt=\"Whole-stage code generation model\" width=\"663\" height=\"237\" class=\"aligncenter size-full wp-image-7677\" />\n\nThe <code>explain()</code> function in the expression below has been extended for whole-stage code generation. In the explain output, when an operator has a star around it (*), whole-stage code generation is enabled. In the following case, Range, Filter, and the two Aggregates are both running with whole-stage code generation. Exchange, however, does not implement whole-stage code generation because it is sending data across the network.\n\n<pre>\nspark.range(1000).filter(\"id > 100\").selectExpr(\"sum(id)\").explain()\n\n== Physical Plan ==\n*Aggregate(functions=[sum(id#201L)])\n+- Exchange SinglePartition, None\n   +- *Aggregate(functions=[sum(id#201L)])\n      +- *Filter (id#201L > 100)\n         +- *Range 0, 1, 3, 1000, [id#201L]\n</pre>\n\nThose of you that have been following Spark\u2019s development closely might ask the following question: \u201cI\u2019ve heard about code generation since Apache Spark 1.1 in <a href=\"https://databricks.com/blog/2014/06/02/exciting-performance-improvements-on-the-horizon-for-spark-sql.html\">this blog post</a>. How is it different this time?\u201d In the past, similar to other MPP query engines, Spark only applied code generation to expression evaluation and was limited to a small number of operators (e.g. Project, Filter). That is, code generation in the past only sped up the evaluation of expressions such as \u201c1 + a\u201d, whereas today whole-stage code generation actually generates code for the entire query plan.\n\n<h2>Vectorization</h2>\n\nWhole-stage code-generation techniques work particularly well for a large spectrum of queries that perform simple, predictable operations over large datasets. There are, however, cases where it is infeasible to generate code to fuse the entire query into a single function. Operations might be too complex (e.g. CSV parsing or Parquet decoding), or there might be cases when we\u2019re integrating with third party components that can\u2019t integrate their code into our generated code (examples can range from calling out to Python/R to offloading computation to the GPU).\n\nTo improve performance in these cases, we employ another technique called \u201cvectorization.\u201d The idea here is that instead of processing data one row at a time, the engine batches multiples rows together in a columnar format, and each operator uses simple loops to iterate over data within a batch. Each <code>next()</code> call would thus return a batch of tuples, amortizing the cost of virtual function dispatches. These simple loops would also enable compilers and CPUs to execute more efficiently with the benefits mentioned earlier.\n\nAs an example, for a table with three columns (id, name, score), the following illustrates the memory layout in row-oriented format and column-oriented format.\n\n<img src=\"https://databricks.com/wp-content/uploads/2016/05/memory-layout-in-row-and-column-formats.png\" alt=\"Memory layout in row and column formats\" width=\"602\" height=\"239\" class=\"aligncenter size-full wp-image-7678\" />\n\nThis style of processing, invented by columnar database systems such as MonetDB and C-Store, would achieve two of the three points mentioned earlier (almost no virtual function dispatches and automatic loop unrolling/SIMD). It, however, still requires putting intermediate data in-memory rather than keeping them in CPU registers. As a result, we use vectorization only when it is not possible to do whole-stage code generation.\n\nFor example, we have implemented a new vectorized Parquet reader that does decompression and decoding in column batches. When decoding integer columns (on disk), this new reader is roughly 9 times faster than the non-vectorized one:\n\n<img src=\"https://databricks.com/wp-content/uploads/2016/05/parquet-vs-vectorized-parquet.png\" alt=\"Parquet vs vectorized Parquet\" width=\"633\" height=\"219\" class=\"aligncenter size-full wp-image-7679\" />\n\nIn the future, we plan to use vectorization in more code paths such as UDF support in Python/R.\n\n<h2>Performance Benchmarks</h2>\n\nWe have measured the amount of time (in nanoseconds) it would take to process a tuple on one core for some of the operators in Apache Spark 1.6 vs. Apache Spark 2.0, and the table below is a comparison that demonstrates the power of the new Tungsten engine. Spark 1.6 includes expression code generation technique that is also in use in some state-of-the-art commercial databases today.\n\n<p style=\"text-align: center\"><b>cost per row (in nanoseconds, single thread)</b></p>\n\n<table class=\"table\">\n<thead>\n<tr>\n<th>primitive</th>\n<th>Spark 1.6</th>\n<th>Spark 2.0</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>filter</td>\n<td>15 ns</td>\n<td>1.1 ns</td>\n</tr>\n<tr>\n<td>sum w/o group</td>\n<td>14 ns</td>\n<td>0.9 ns</td>\n</tr>\n<tr>\n<td>sum w/ group</td>\n<td>79 ns</td>\n<td>10.7 ns</td>\n</tr>\n<tr>\n<td>hash join</td>\n<td>115 ns</td>\n<td>4.0 ns</td>\n</tr>\n<tr>\n<td>sort (8-bit entropy)</td>\n<td>620 ns</td>\n<td>5.3 ns</td>\n</tr>\n<tr>\n<td>sort (64-bit entropy)</td>\n<td>620 ns</td>\n<td>40 ns</td>\n</tr>\n<tr>\n<td>sort-merge join</td>\n<td>750 ns</td>\n<td>700 ns</td>\n</tr>\n<tr>\n<td>Parquet decoding (single int column)</td>\n<td>120 ns</td>\n<td>13 ns</td>\n</tr>\n</tbody>\n</table>\n\nWe have surveyed our customers\u2019 workloads and implemented whole-stage code generation for the most frequently used operators, such as filter, aggregate, and hash joins. As you can see, many of the core operators are an order of magnitude faster with whole-stage code generation. Some operators such as sort-merge join, however, are inherently slower and more difficult to optimize.\n\nYou can see the power of whole-stage code generation in action in <a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/Whole-stage%20code%20generation.html\" target=\"_blank\">this notebook</a>, in which we perform aggregations and joins on 1 billion records on a single machine. It takes less than one second to perform the hash join operation on 1 billion tuples on both the Databricks platform (with Intel Haswell processor 3 cores) as well as on a 2013 Macbook Pro (with mobile Intel Haswell i7).\n\nHow does this new engine work on end-to-end queries? Beyond whole-stage code generation and vectorization, a lot of work has also gone into improving the Catalyst optimizer for general query optimizations such as nullability propagation. We did some preliminary analysis using TPC-DS queries to compare Spark 1.6 and the upcoming Spark 2.0:\n\n<img src=\"https://databricks.com/wp-content/uploads/2016/05/preliminary-tpc-ds-spark-2-0-vs-1-6.png\" alt=\"Preliminary TPC-DS Spark 2.0 vs 1.6\" width=\"703\" height=\"380\" class=\"aligncenter size-full wp-image-7218\" />\n\nDoes this mean your workload will magically become ten times faster once you upgrade to Spark 2.0? Not necessarily. While we believe the new Tungsten engine implements the best architecture for performance engineering in data processing, it is important to understand that not all workloads can benefit to the same degree. For example, variable-length data types such as strings are naturally more expensive to operate on, and some workloads are bounded by other factors ranging from I/O throughput to metadata operations. Workloads that were previously bounded by CPU efficiency would observe the largest gains, and shift towards more I/O bound, whereas workloads that were previously I/O bound are less likely to observe gains.\n\n<h2>Conclusion</h2>\nMost of the work described in this blog post has been committed into Apache Spark\u2019s code base and is slotted for the upcoming Spark 2.0 release. The JIRA ticket for whole-stage code generation can be found in SPARK-12795, while the ticket for vectorization can be found in SPARK-12992.\n\nTo recap, this blog post described the second generation Tungsten execution engine. Through a technique called whole-stage code generation, the engine will (1) eliminate virtual function dispatches (2) move intermediate data from memory to CPU registers and (3) exploit modern CPU features through loop unrolling and SIMD. Through a technique called vectorization, the engine will also speed up operations that are too complex for code generation. For many core operators in data processing, the new engine is orders of magnitude faster. In the future, given the efficiency of the execution engine, bulk of our performance work will shift towards optimizing I/O efficiency and better query planning.\n\nWe are excited about the progress made, and hope you will enjoy the improvements. To try some of these out for free, <a href=\"http://go.databricks.com/databricks-community-edition-beta-waitlist\">sign up for an account</a> on Databricks Community Edition.\n\n<h2>Further Reading</h2>\n\n<ul>\n<li>Watch Webinar: <a href=\"http://go.databricks.com/apache-spark-2.0-presented-by-databricks-co-founder-reynold-xin\" target=\"_blank\">Apache Spark 2.0: Easier, Faster, and Smarter</a></li>\n<li><a href=\"https://databricks.com/blog/2016/05/11/apache-spark-2-0-technical-preview-easier-faster-and-smarter.html\" target=\"_blank\">Technical Preview of Apache Spark 2.0 Now on Databricks</a></li>\n<li><a href=\"https://databricks.com/blog/2016/05/19/approximate-algorithms-in-apache-spark-hyperloglog-and-quantiles.html\" target=\"_blank\">Approximate Algorithms in Apache Spark: HyperLogLog and Quantiles</a></li>\n</ul>"}
{"status": "publish", "description": null, "creator": "deborah", "link": "https://databricks.com/blog/2016/05/24/genome-sequencing-in-a-nutshell.html", "authors": null, "id": 7690, "categories": ["Apache Spark", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2016-05-24", "tz": "UTC", "createdOn": "2016-05-24"}, "title": "Genome Sequencing in a Nutshell", "slug": "genome-sequencing-in-a-nutshell", "content": "[dbce_cta href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/Samples/Miscellaneous/Genome_Variant_Analysis_using_k-means_ADAM_and_Apache_Spark.html\"]Try this notebook in Databricks[/dbce_cta]\n\n<i>This is a guest post from Deborah Siegel from the Northwest Genome Center and the University of Washington with Denny Lee from Databricks on their collaboration on genome variant analysis with ADAM and Spark.</i>\n<div style=\"padding-left: 30px;\">This is part 1 of the 3 part series Genome Variant Analysis using K-Means, ADAM, and Apache Spark:</div>\n<span style=\"padding-left: 60px;\">1.\u00a0<a href=\"https://databricks.com/blog/2016/05/24/genome-sequencing-in-a-nutshell.html\">Genome Sequencing in a Nutshell</a></span>\n<span style=\"padding-left: 60px;\">2.\u00a0<a href=\"https://databricks.com/blog/2016/05/24/parallelizing-genome-variant-analysis.html\">Parallelizing Genome Variant Analysis</a></span>\n<span style=\"padding-left: 60px;\">3.\u00a0<a href=\"https://databricks.com/blog/2016/05/24/predicting-geographic-population-using-genome-variants-and-k-means.html\">Predicting Geographic Population using Genome Variants and K-Means</a></span>\n<h2>Introduction</h2>\nOver the last few years, we have seen a rapid reduction in costs and time of genome sequencing. \u00a0The potential of understanding the variations in genome sequences range from assisting us in identifying people who are predisposed to common diseases, solving rare diseases, and enabling clinicians to personalize prescription and dosage to the individual.\n\nIn this three-part blog, we will provide a primer of genome sequencing and its potential. \u00a0We will focus on genome variant analysis - that is the differences between genome sequences - and how it can be accelerated by making use of Apache Spark and ADAM (a scalable API and CLI for genome processing) using Databricks Community Edition. \u00a0Finally, we will execute a k-means clustering algorithm on genomic variant data and build a model that will predict the individual\u2019s geographic population of origin based on those variants.\n\nThis first post will provide a primer on genome sequencing. \u00a0You can also skip ahead to the second post <a href=\"https://databricks.com/blog/2016/05/24/parallelizing-genome-variant-analysis.html\">Parallelizing Genome Variant Analysis</a> focusing on parallel bioinformatic analysis or the third post on <a href=\"https://databricks.com/blog/2016/05/24/predicting-geographic-population-using-genome-variants-and-k-means.html\">Predicting Geographic Population using Genome Variants and K-Means</a>.\n<h2>Genome Sequencing</h2>\n<h3>A very simple language analogy</h3>\nImagine one long string composed of 3 billion characters and containing roughly 25,000 words interspersed with other characters. Some of the words even make sentences. Changing, adding, or deleting characters or groups of characters could change the structure or meaning of the words and sentences.\n\n<img class=\"aligncenter wp-image-7705 size-medium\" src=\"https://databricks.com/wp-content/uploads/2016/05/startling_string-300x90.png\" alt=\"startling_string\" width=\"300\" height=\"90\" />\n\nEach long string has very roughly 10-30 million places where such differences may occur. And this makes things interesting. Of course, everything is more complicated. But this has shown itself to be a useful abstraction of genome data.\n\nIn the genome, we have been building knowledge about where the words (genes) are located in the string of characters (bases), and we have been discovering the places where they differ (the variants). But we don\u2019t know everything. \u00a0We are still learning about what the effect of the variants are, how the genes are related to each other, and how they may be expressed in different forms and in different quantities under certain circumstances.\n\n<img class=\"aligncenter wp-image-7694 size-full\" src=\"https://databricks.com/wp-content/uploads/2016/05/biology_or_foreign_language.jpg\" alt=\"biology_or_foreign_language\" width=\"300\" height=\"225\" />\n<h3>Genome Sequencing in a Nutshell</h3>\nGenome sequencing involves using chemistry and a recording technique to read the characters which code the genome (A,G, C, T) in order (in sequence).\n\n<img class=\"aligncenter wp-image-7697\" src=\"https://databricks.com/wp-content/uploads/2016/05/distribute_me-1024x243.png\" alt=\"distribute_me\" width=\"650\" height=\"154\" />\n\nThe data is initially read in the form of short strings. For a 30x coverage of a person\u2019s genome (30x is a common goal), there may be approximately 600 million short strings of 150 characters each. During data preprocessing, the strings will be mapped/aligned, typically to a reference sequence. There are many different approaches to alignment. Ultimately, this gives every base a defined position. Variant analysis of aligned sequence data finds code differences by comparing the sequence to the reference or to other aligned sequences and assigns genotypes to a person\u2019s variants.\n\n<img class=\"aligncenter wp-image-7700\" src=\"https://databricks.com/wp-content/uploads/2016/05/genotype-1024x383.png\" alt=\"genotype\" width=\"649\" height=\"243\" />\n\nSome of the detected variants will be based on noise, and can be filtered with rigid thresholds on parameters such as coverage, quality, and domain-specific biases. Rather than hard filtering, some analysts threshold the variants by fitting a Gaussian mixture model. \u00a0Even further downstream, analysts quantify and explore the data, try and identify highly significant variants (a small number given the input size), and try to predict what their functional effect might be.\n<h3>Why sequence?</h3>\nGenome sequence (and exome sequence, which is a subset) is interesting data from a data science perspective. We can use our knowledge of sequences to gain hints at how and why the <a href=\"http://science.sciencemag.org/content/337/6090/64.long\">code has evolved</a> over long periods of time. \u00a0Knowledge from genome sequencing studies is becoming more integrated into medicine. Genome sequencing is now used for <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3379884/\">non-invasive prenatal diagnostics</a>. \u00a0Genome sequencing will soon be used in <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4461364/\">clinical screening and diagnostic tests</a>, with much ongoing work to expand <a href=\"http://www.cell.com/ajhg/abstract/S0002-9297(16)30106-9\">genomic medicine</a>.\n\nOn the research and discovery side, large cohort and population-scale genome sequencing studies find variants or patterns of variance which may predispose people to common diseases such as <a href=\"http://www.cell.com/ajhg/abstract/S0002-9297(15)00494-2\">autism</a>, <a href=\"http://www.nature.com/nature/journal/v518/n7537/full/nature13917.html\">heart disease</a>, and specific <a href=\"http://www.nature.com/nature/journal/vaop/ncurrent/full/nature17676.html\">cancers</a>. \u00a0Sequencing studies also indicate variants influencing <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3959810/\">drug metabolism</a>, enabling clinicians to personalize prescriptions and dosage to each individual. \u00a0In the case of rare heritable diseases, sequencing certain members of a family often leads to finding the <a href=\"http://www.cell.com/ajhg/abstract/S0002-9297(15)00245-1\">causal variants</a>.\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/05/disease_allele.png\"><img class=\"aligncenter wp-image-7696\" src=\"https://databricks.com/wp-content/uploads/2016/05/disease_allele-1024x539.png\" alt=\"disease_allele\" width=\"650\" height=\"342\" /></a>\n<p style=\"text-align: center;\">(image credit: Frederic Reinier, used with permission)</p>\nIn the past five years, <a href=\"https://www.genome.gov/27562201/2015-news-feature-centers-for-mendelian-genomics-uncovering-the-genomic-basis-of-hundreds-of-rare-conditions/\">sequencing experiments have linked genomic variants to hundreds of rare diseases</a>:\n<blockquote>\u201cIndividually, a rare disease may affect only a handful of families. Collectively, rare diseases impact 20 to 30 million people in the U.S. alone.\u201d</blockquote>\nFor these reasons, there are resources going towards the reading and analysis of sequences. The National Health Service of the UK has a project to sequence 100,000 genomes of families with members who have rare diseases or cancer by 2017. In the US, The National Human Genome Research Institute (NHGRI) \u00a0plans to fund common disease research for $240 million and rare disease research for $40 million over the next 4 years. There are also other kinds of sequencing which will benefit from efforts to scale bioinformatics and lower the barrier to applying data science to a large amount of sequence data, such as RNA-seq, microbiome sequencing, and immune system and cancer profile sequencing.\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/05/sequencing_technology.png\"><img class=\"aligncenter wp-image-7702\" src=\"https://databricks.com/wp-content/uploads/2016/05/sequencing_technology-1024x575.png\" alt=\"sequencing_technology\" width=\"650\" height=\"365\" /></a>\n\nSequencing technology has been an object of accelerated growth. Between 1998 and 2001, the first human genome was sequenced. It cost $2.8 Billion of 2009 dollars. Today, a genome can be sequenced in 3 days for around $1,000 (for more information, please review National Institutes of Health: National Human Genome Research Institute &gt; <a href=\"https://www.genome.gov/27541954/dna-sequencing-costs/\">DNA Sequencing Costs</a>). During roughly the first 25 years of sequencing experiments, the chemistry allowed only one stretch of DNA to be sequenced at a time, making it laborious, slow, and expensive. The next-generation of sequencing has become massively parallel, enabling sequencing to occur on many stretches of DNA within the same experiment. \u00a0Also, with molecular indexing, multiple individual\u2019s DNA can be sequenced together and the data can be separated out during analysis. It is not implausible to speculate that most people on the planet who opt-in will have their genomes sequenced in the not-so-distant future. To find out more detail about next-generation sequencing, see <a href=\"http://www.nature.com/nrg/journal/v17/n6/full/nrg.2016.49.html\">Coming of age: ten years of next-generation sequencing technologies</a>\n\nDepending on the application and settings, current sequencing instruments can read ~600 gigabases per day. A medium to large size sequencing center has several such instruments running concurrently. As we will see later on \u00a0in detail, one of the challenges facing bioinformatics is that downstream software for analyzing variants had been previously optimized for specific, non-extensible file formats, rather than on the data models themselves. The result is that there exist pipeline fragility and obstacles to scalability. Now that we have massively parallel sequencing, many are looking towards parallel bioinformatic analysis.\n<h3>Public Data</h3>\nGenome sequence data is generally private. Between 2007 and 2013, The 1000 genomes project was an initial effort for public \u201cpopulation level sequencing\u201d. By its final phase, it provided some sequencing coverage data for 2,504 individuals from 26 populations. We used the easily accessible data from this project as a resource to build a notebook in Databricks Community Edition.\n<h2>Next Steps</h2>\nIn the next blog <a href=\"https://databricks.com/blog/2016/05/24/parallelizing-genome-variant-analysis.html\">Parallelizing Genome Variant Analysis</a> we will look into parallel bioinformatic analysis. \u00a0You can also skip ahead to <a href=\"https://databricks.com/blog/2016/05/24/predicting-geographic-population-using-genome-variants-and-k-means.html\">Predicting Geographic Population using Genome Variants \u00a0and K-Means</a>.\n<h2>Attribution</h2>\nWe wanted to give a particular call out to the following resources that helped us create the notebook\n<ul>\n \t<li><a href=\"http://bdgenomics.org/projects/adam/\">Big Data Genomics ADAM project</a></li>\n \t<li><a href=\"https://amplab.cs.berkeley.edu/publication/adam-genomics-formats-and-processing-patterns-for-cloud-scale-computing/\">ADAM: Genomics Formats and Processing Patterns for Cloud Scale Computing (Berkeley AMPLab)</a></li>\n \t<li>Andy Petrella\u2019s <a href=\"http://www.slideshare.net/noootsab/lightning-fast-genomics-with-spark-adam-and-scala\">Lightning Fast Genomics with Spark and ADAM</a> and associated <a href=\"https://github.com/andypetrella\">GitHub repo</a>.</li>\n \t<li>Neil Ferguson <a href=\"https://github.com/nfergu/popstrat\">Population Stratification Analysis on Genomics Data Using Deep Learning</a>.</li>\n \t<li>Matthew Conlen\u00a0<a href=\"http://lightning-viz.org/\">Lightning-Viz project</a>.</li>\n \t<li><a href=\"http://www.slideshare.net/TimothyDanford\">Timothy Danford\u2019s SlideShare presentations</a> (on Genomics with Spark)</li>\n \t<li><a href=\"https://www.genome.gov/27562201/2015-news-feature-centers-for-mendelian-genomics-uncovering-the-genomic-basis-of-hundreds-of-rare-conditions/\">Centers for Mendelian Genomics uncovering the genomic basis of hundreds of rare conditions</a></li>\n \t<li><a href=\"https://www.nih.gov/news-events/news-releases/nih-genome-sequencing-program-targets-genomic-bases-common-rare-disease\">NIH genome sequencing program targets the genomic bases of common, rare disease</a></li>\n \t<li><a href=\"http://www.1000genomes.org/\">The 1000 Genomes Project</a></li>\n</ul>\nAs well, we\u2019d like to thank for additional contributions and reviews by Anthony Joseph, Xiangrui Meng, Hossein Falaki, and Tim Hunter.\n<h2></h2>"}
{"status": "publish", "description": null, "creator": "deborah", "link": "https://databricks.com/blog/2016/05/24/parallelizing-genome-variant-analysis.html", "authors": null, "id": 7713, "categories": ["Apache Spark", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2016-05-24", "tz": "UTC", "createdOn": "2016-05-24"}, "title": "Parallelizing Genome Variant Analysis", "slug": "parallelizing-genome-variant-analysis", "content": "[sidenote]Spark Summit 2016 will be held in San Francisco on June 6\u20138. Check out the <a href=\"https://spark-summit.org/2016/\">full agenda</a> and <a href=\"http://www.prevalentdesignevents.com/sparksummit2016/registration.aspx\">get your ticket</a> before it sells out![/sidenote]\n\n<hr />\n\n[dbce_cta href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/Samples/Miscellaneous/Genome_Variant_Analysis_using_k-means_ADAM_and_Apache_Spark.html\"]Try this notebook in Databricks[/dbce_cta]\n\n<i>This is a guest post from Deborah Siegel from the Northwest Genome Center and the University of Washington with Denny Lee from Databricks on their collaboration on genome variant analysis with ADAM and Spark.</i>\n<div style=\"padding-left: 30px;\">This is part 2 of the 3 part series Genome Variant Analysis using K-Means, ADAM, and Apache Spark:</div>\n<span style=\"padding-left: 60px;\">1.\u00a0<a href=\"https://databricks.com/blog/2016/05/24/genome-sequencing-in-a-nutshell.html\">Genome Sequencing in a Nutshell</a></span>\n<span style=\"padding-left: 60px;\">2.\u00a0<a href=\"https://databricks.com/blog/2016/05/24/parallelizing-genome-variant-analysis.html\">Parallelizing Genome Variant Analysis</a></span>\n<span style=\"padding-left: 60px;\">3.\u00a0<a href=\"https://databricks.com/blog/2016/05/24/predicting-geographic-population-using-genome-variants-and-k-means.html\">Predicting Geographic Population using Genome Variants and K-Means</a></span>\n\n<h2>Introduction</h2>\nOver the last few years, we have seen a rapid reduction in costs and time of genome sequencing. \u00a0The potential of understanding the variations in genome sequences range from assisting us in identifying people who are predisposed to common diseases, solving rare diseases, and enabling clinicians to personalize prescription and dosage to the individual.\n\nIn this three-part blog, we will provide a primer of genome sequencing and its potential. \u00a0We will focus on genome variant analysis - that is the differences between genome sequences - and how it can be accelerated by making use of Apache Spark and ADAM (a scalable API and CLI for genome processing) using Databricks Community Edition. \u00a0Finally, we will execute a k-means clustering algorithm on genomic variant data and build a model that will predict the individual\u2019s geographic population of origin \u00a0based on those variants.\n\nThis post will focus on Parallelizing Genome Sequence Analysis; for a refresher on genome sequencing, you can review <a href=\"https://databricks.com/blog/2016/05/24/genome-sequencing-in-a-nutshell.html\">Genome Sequencing in a Nutshell</a>. \u00a0You can also skip ahead to the third post on <a href=\"https://databricks.com/blog/2016/05/24/predicting-geographic-population-using-genome-variants-and-k-means.html\">Predicting Geographic Population using Genome Variants and K-Means</a>.\n<h2>Parallelizing Genome Variant \u00a0Analysis</h2>\nAs noted in <a href=\"https://databricks.com/blog/2016/05/24/genome-sequencing-in-a-nutshell.html\">Genome Sequencing in a Nutshell</a>, there are many steps and stages of the analysis that can be distributed and parallelized in the hopes of significantly improving performance and possibly improving results on very large data Apache Spark is well suited for sequence data because it not only executes many tasks in a distributed parallel fashion, but can do so primarily in-memory with decreased need for intermediate files.\n\nBenchmarks of ADAM + Spark (sorting genome reads and marking duplicate reads for removal) have shown scalable speedups, from 1.5 days on a single node to less than one hour on a commodity cluster (<a href=\"https://www.eecs.berkeley.edu/Pubs/TechRpts/2013/EECS-2013-207.html\">ADAM: Genomics Formats and Processing Patterns for Cloud Scale Computing</a>).\n\nOne of the primary issues alluded to when working with current genomic sequence data formats is that they are not easily parallelizable. \u00a0Fundamentally, the current set of tools and genomic data formats (e.g. SAM, BAM, VCF, etc.) are not well designed for distributed computing environments. \u00a0To provide context, the next sections provide a simplified background of the workflow from genomic sequence to variant workflow.\n<h2>Simplified Genome Sequence to Variant Workflow</h2>\nThere are a number of quality control and pre-processing steps that must be initially performed prior to analyzing variants. \u00a0A simplified workflow can be seen in the image below.\n\n<img class=\"aligncenter wp-image-7704\" src=\"https://databricks.com/wp-content/uploads/2016/05/Simplified-Sequence-to-Variant-Workflow-1024x396.png\" alt=\"Simplified-Sequence-to-Variant-Workflow\" width=\"651\" height=\"252\" />\n\nThe output of the genome sequencer machine is in <a href=\"https://en.wikipedia.org/wiki/FASTQ_format\">FASTQ format</a> - text-based format storing both the short nucleotide sequence reads and their associated quality scores in ASCII format. The image below is an example of the data in FASTQ format.\n\n<img class=\"aligncenter wp-image-7699\" src=\"https://databricks.com/wp-content/uploads/2016/05/FASTQ-format-1024x202.png\" alt=\"FASTQ-format\" width=\"649\" height=\"128\" />\n\nA typical next step is to use a BWA (Burrows-Wheeler Alignment) sequence alignment tool such as <a href=\"http://bowtie-bio.sourceforge.net/manual.shtml#what-is-bowtie\">Bowtie</a> to align the large sets of short DNA sequences (reads) to a <a href=\"https://en.wikipedia.org/wiki/Reference_genome\">reference genome</a> and create a SAM file - a sequence alignment map file that stores mapped tags to a genome. \u00a0\u00a0The image below (from the <a href=\"https://samtools.github.io/hts-specs/SAMv1.pdf\">Sequence Alignment/Map Format specification</a>) is an example of the SAM format. \u00a0This specification has a number of terminologies and concepts that are outside the scope of this blog post; for more information, please reference the <a href=\"https://samtools.github.io/hts-specs/SAMv1.pdf\">Sequence Alignment/Map Format specification</a>.\n\n<img class=\"aligncenter wp-image-7698\" src=\"https://databricks.com/wp-content/uploads/2016/05/Example-SAM-format-1024x254.png\" alt=\"Example-SAM-format\" width=\"650\" height=\"161\" />\n<p style=\"text-align: center;\">Source:\u00a0<a href=\"https://samtools.github.io/hts-specs/SAMv1.pdf\">Sequence Alignment/Map Format specification</a></p>\nThe next step \u00a0is to store the SAM into BAM (a binary version of SAM) typically by using <a href=\"https://github.com/samtools/samtools\">SAMtools</a> (a good reference for this process is Dave Tang\u2019s <a href=\"https://github.com/davetang/learning_bam_file\">Learning BAM file</a>). \u00a0Finally, a Variant Call Format (VCF) file is generated by comparing the BAM file to a reference sequence (typically this is done using <a href=\"https://github.com/samtools/bcftools\">BCFtools</a>). \u00a0Note, a great short blog post describing this process is Kaushik Ghose\u2019s <a href=\"https://kaushikghose.wordpress.com/2014/03/26/sam-bam-vcf-what/\">SAM! BAM! VCF! What?</a>.\n<h2>Simplified Overview of VCF</h2>\nWith the VCF file, we can finally start performing variant analysis. \u00a0The VCF itself is a complicated specification so for a more detailed explanation, please reference the <a href=\"http://www.1000genomes.org/wiki/Analysis/vcf4.0\">1000 Genomes Project VCF (Variant Call Format) version 4.0 specification</a>.\n\n<img class=\"aligncenter wp-image-7692\" src=\"https://databricks.com/wp-content/uploads/2016/05/1000-genomes-VCF4.0-specification-1024x749.png\" alt=\"1000-genomes-VCF4.0-specification\" width=\"650\" height=\"475\" />\n<p style=\"text-align: center;\">Source:\u00a0<a href=\"http://www.1000genomes.org/wiki/Analysis/vcf4.0\">1000 Genomes Project VCF (Variant Call Format) version 4.0 specification</a></p>\nWhile there are various tools which can process and analyze VCFs, they cannot be used in a distributed parallel fashion. \u00a0A simplified view of a VCF file is that it contains metadata, header, and the data. The metadata is often of interest and should be applied to each genotype. \u00a0As illustrated in the image below, even if you have four nodes (i.e. Node 1, Node 2, Node 3, Node 4) to process your genotype data, you cannot efficiently distribute the data to all four nodes. \u00a0With traditional variant analysis tools, the whole file including all of the data, metadata, and header must be sent to a single node. Additionally, the VCF file has more than one observation per row (variants and all their genotypes). This makes it impossible to analyze the genotypes in parallel without reformatting or using special tools.\n\n<img class=\"aligncenter wp-image-7706\" src=\"https://databricks.com/wp-content/uploads/2016/05/VCF-cannot-distribute-1024x502.png\" alt=\"VCF-cannot-distribute\" width=\"650\" height=\"319\" />\n\nAnother key issue complicating the analysis of VCFs is the level of complexity surrounding the VCF format specification. \u00a0\u00a0Referring to the <a href=\"http://www.1000genomes.org/wiki/Analysis/vcf4.0\">1000 Genomes Project VCF (Variant Call Format) version 4.0 specification</a>, there are a number of rules surrounding how to interpret the lines within the VCF. \u00a0Therefore, any data scientist who wants to analyze variant data has to expend a large amount of effort to understand the specific VCFs they are working with and parsing.\n<h2>Introducing ADAM</h2>\nBig Data Genomics ADAM project was designed to solve problems around distributing sequence data and parallelizing the processing of sequence data as noted in the technical report <a href=\"http://www.eecs.berkeley.edu/Pubs/TechRpts/2013/EECS-2013-207.pdf\">ADAM: Genomics Formats and Processing Patterns for Cloud Scale Computing</a>. \u00a0ADAM is comprised of a CLI (tool kit for quickly processing genomics data), numerous APIs (interfaces to transform, analyze, and query genomic data), schemas, and file formats (columnar formats that allow for efficient parallel access to data).\n<h3>bdg-formats Schemas</h3>\nTo address the complexities of parsing common types of sequence data - such as reads, reference oriented data, variants, genotypes, and assemblies - ADAM utilizes <a href=\"http://bdgenomics.org/projects/bdg-formats/\">bdg-formats</a>, a set of extensible Apache Avro schemas that are built around data types themselves rather than a file format. \u00a0\u00a0In other words, the schemas allows ADAM (or other any tools) to more easily query data instead of building custom code to parse each line of data depending on the file format. \u00a0These data formats are highly efficient - they are easily serializable, and the information about each particular schema, such as data types, does not have to be sent redundantly with each batch of data. The nodes in your cluster are made aware of what the schema is in an extensible way (data can be added with an extended schema, and analyzed together with data under the old schema).\n<h3>Parallel distribution via ADAM Parquet</h3>\nADAM Parquet files (when compared to binary or text VCF files) enable fast processing because they support parallel distribution of sequence data. \u00a0In the earlier image of the VCF file, we saw that the entire file must be sent to one node. With ADAM Parquet files, the metadata and header are incorporated into the data elements and schema, and the elements are \u201ctidy\u201d in that there is one observation per element (one genotype of one variant).\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/05/ADAM-can-distribute.png\"><img class=\"aligncenter wp-image-7693\" src=\"https://databricks.com/wp-content/uploads/2016/05/ADAM-can-distribute-1024x548.png\" alt=\"ADAM-can-distribute\" width=\"650\" height=\"348\" /></a>\n\nThis enables the files to be distributed across multiple nodes. It also makes it simple to filter elements for just the data you want, such as genotypes for a certain panel, without using special tools. \u00a0ADAM files are stored in the <a href=\"https://parquet.apache.org/\">Parquet</a> columnar storage format which is designed for parallel processing. \u00a0As of GATK4, the <a href=\"https://www.broadinstitute.org/gatk/\">Genome Analysis Toolkit</a> is also able to read and write ADAM Parquet formatted data.\n<h3>Updated Simplified Genome Sequence to Variant Workflow</h3>\nWith defined schemas (bdg-format) and ADAM\u2019s APIs, data scientists can focus on querying the data instead of parsing the data formats.\n\n<img class=\"aligncenter wp-image-7703\" src=\"https://databricks.com/wp-content/uploads/2016/05/Simplified-Sequence-to-Variant-Workflow-with-ADAM-1024x350.png\" alt=\"Simplified-Sequence-to-Variant-Workflow-with-ADAM\" width=\"650\" height=\"222\" />\n<h2>Next Steps</h2>\nIn the next blog we will run a parallel bioinformatic analysis example <a href=\"https://databricks.com/blog/2016/05/24/predicting-geographic-population-using-genome-variants-and-k-means.html\">Predicting Geographic Population using Genome Variants and K-Means</a>. You can also review a primer on genome sequencing: <a href=\"https://databricks.com/blog/2016/05/24/genome-sequencing-in-a-nutshell.html\">Genome Sequencing in a Nutshell</a>.\n<h2>Attribution</h2>\nWe wanted to give a particular call out to the following resources that helped us create the notebook\n<ul>\n \t<li><a href=\"http://bdgenomics.org/projects/adam/\">Big Data Genomics ADAM project</a></li>\n \t<li><a href=\"https://amplab.cs.berkeley.edu/publication/adam-genomics-formats-and-processing-patterns-for-cloud-scale-computing/\">ADAM: Genomics Formats and Processing Patterns for Cloud Scale Computing (Berkeley AMPLab)</a></li>\n \t<li>Andy Petrella\u2019s <a href=\"http://www.slideshare.net/noootsab/lightning-fast-genomics-with-spark-adam-and-scala\">Lightning Fast Genomics with Spark and ADAM</a> and associated <a href=\"https://github.com/andypetrella\">GitHub repo</a>.</li>\n \t<li>Neil Ferguson <a href=\"https://github.com/nfergu/popstrat\">Population Stratification Analysis on Genomics Data Using Deep Learning</a>.</li>\n \t<li>Matthew Conlen\u00a0<a href=\"http://lightning-viz.org/\">Lightning-Viz project</a>.</li>\n \t<li><a href=\"http://www.slideshare.net/TimothyDanford\">Timothy Danford\u2019s SlideShare presentations</a> (on Genomics with Spark)</li>\n \t<li><a href=\"https://www.genome.gov/27562201/2015-news-feature-centers-for-mendelian-genomics-uncovering-the-genomic-basis-of-hundreds-of-rare-conditions/\">Centers for Mendelian Genomics uncovering the genomic basis of hundreds of rare conditions</a></li>\n \t<li><a href=\"https://www.nih.gov/news-events/news-releases/nih-genome-sequencing-program-targets-genomic-bases-common-rare-disease\">NIH genome sequencing program targets the genomic bases of common, rare disease</a></li>\n \t<li><a href=\"http://www.1000genomes.org/\">The 1000 Genomes Project</a></li>\n</ul>\nAs well, we\u2019d like to thank for additional contributions and reviews by Anthony Joseph, Xiangrui Meng, Hossein Falaki, and Tim Hunter."}
{"status": "publish", "description": null, "creator": "deborah", "link": "https://databricks.com/blog/2016/05/24/predicting-geographic-population-using-genome-variants-and-k-means.html", "authors": null, "id": 7720, "categories": ["Apache Spark", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2016-05-24", "tz": "UTC", "createdOn": "2016-05-24"}, "title": "Predicting Geographic Population using Genome Variants and K-Means", "slug": "predicting-geographic-population-using-genome-variants-and-k-means", "content": "[sidenote]Spark Summit 2016 will be held in San Francisco on June 6\u20138. Check out the <a href=\"https://spark-summit.org/2016/\">full agenda</a> and <a href=\"http://www.prevalentdesignevents.com/sparksummit2016/registration.aspx\">get your ticket</a> before it sells out![/sidenote]\n\n<hr />\n\n[dbce_cta href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/Samples/Miscellaneous/Genome_Variant_Analysis_using_k-means_ADAM_and_Apache_Spark.html\"]Try this notebook in Databricks[/dbce_cta]\n\n<i>This is a guest post from Deborah Siegel from the Northwest Genome Center and the University of Washington with Denny Lee from Databricks on their collaboration on genome variant analysis with ADAM and Spark.</i>\n<div style=\"padding-left: 30px;\">This is part 3 of the 3 part series Genome Variant Analysis using K-Means, ADAM, and Apache Spark:</div>\n<span style=\"padding-left: 60px;\">1.\u00a0<a href=\"https://databricks.com/blog/2016/05/24/genome-sequencing-in-a-nutshell.html\">Genome Sequencing in a Nutshell</a></span>\n<span style=\"padding-left: 60px;\">2.\u00a0<a href=\"https://databricks.com/blog/2016/05/24/parallelizing-genome-variant-analysis.html\">Parallelizing Genome Variant Analysis</a></span>\n<span style=\"padding-left: 60px;\">3.\u00a0<a href=\"https://databricks.com/blog/2016/05/24/predicting-geographic-population-using-genome-variants-and-k-means.html\">Predicting Geographic Population using Genome Variants and K-Means</a></span>\n<h2>Introduction</h2>\nOver the last few years, we have seen a rapid reduction in costs and time of genome sequencing. \u00a0The potential of understanding the variations in genome sequences range from assisting us in identifying people who are predisposed to common diseases, solving rare diseases, and enabling clinicians to personalize prescription and dosage to the individual.\n\nIn this three-part blog, we will provide a primer of genome sequencing and its potential. \u00a0We will focus on genome variant analysis - that is the differences between genome sequences - and how it can be accelerated by making use of Apache Spark and ADAM (a scalable API and CLI for genome processing) using Databricks Community Edition. \u00a0Finally, we will execute a k-means clustering algorithm on genomic variant data and build a model that will predict the individual\u2019s geographic population of origin \u00a0based on those variants.\n\nThis post will focus on predicting geographic population using genome variants and k-means. \u00a0You can also review the refresher <a href=\"https://databricks.com/blog/2016/05/24/genome-sequencing-in-a-nutshell.html\">Genome Sequencing in a Nutshell</a> or more details behind <a href=\"https://databricks.com/blog/2016/05/24/parallelizing-genome-variant-analysis.html\">Parallelizing Genome Variant Analysis</a>.\n<h2>Predicting Geographic Population using Genome Variants and K-Means</h2>\nWe will be demonstrating <a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/Samples/Miscellaneous/Genome_Variant_Analysis_using_k-means_ADAM_and_Apache_Spark.html?t=1463550134452\">Genome Variant Analysis by performing K-Means on ADAM data using Apache Spark</a> on <a href=\"http://go.databricks.com/databricks-community-edition-beta-waitlist\">Databricks Community Edition</a>. \u00a0The notebook shows how to perform an analysis of public data from the <a href=\"http://1000genomes.org\">1000 genomes project</a> using the <a href=\"http://bdgenomics.org/\">Big Data Genomics</a> ADAM Project (<a href=\"http://bdgenomics.org/blog/2016/02/25/adam-0-dot-19-dot-0-release/\">0.19.0 Release</a>). We attempt k-means clustering to predict which geographic population each person is from and visualize the results.\n<h3>Preparation</h3>\nAs with most Data Sciences projects, there are a number of preparation tasks that must be completed first. \u00a0In this example, we will showcase from our example notebook:\n<ul>\n \t<li>Converting a sample VCF file to ADAM parquet format</li>\n \t<li>Loading a panel file that describes the data within the sample VCF / ADAM parquet</li>\n \t<li>Read the ADAM data into RDDs and begin parallel processing of genotypes</li>\n</ul>\n<h4>Creating ADAM Parquet Files</h4>\nTo create an ADAM parquet file from VCF, we will first load the VCF file using the ADAM\u2019s SparkContext loadGenotypes method. \u00a0By using the adamParquetSave method, we save the VCF in ADAM parquet format.\n\n[scala]\nval gts:RDD[Genotype] = sc.loadGenotypes(vcf_path)\ngts.adamParquetSave(tmp_path)\n[/scala]\n\n<h4>Loading the Panel File</h4>\nWhile the VCF data contain sample IDs, they do not contain population codes that we will want to predict. \u00a0Although we are doing an unsupervised algorithm in this analysis, we still need the response variables in order to filter our samples and to estimate our prediction error. We can get the population codes for each sample from the <code><a href=\"ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/integrated_call_samples_v3.20130502.ALL.panel\">integrated_call_samples_v3.20130502.ALL.panel</a></code> panel file from the <a href=\"http://www.1000genomes.org/\">1000 genomes project</a>.\n\n<img class=\"aligncenter wp-image-7691\" src=\"https://databricks.com/wp-content/uploads/2016/05/1000-genomes-map_11-6-12-2_750.jpg\" alt=\"1000-genomes-map_11-6-12-2_750\" width=\"651\" height=\"386\" />\n<p style=\"text-align: center;\"><i>Source: </i><a href=\"http://www.1000genomes.org/sites/1000genomes.org/files/documents/1000-genomes-map_11-6-12-2_750.jpg\"><i>1000-genomes-map_11-6-12-2_750.jpg</i></a></p>\nThe code snippet below loads panel file using the CSV reader for Spark to create the <code>panel</code> Spark DataFrame.\n\n[scala]\nval panel = sqlContext.read\n   .format(&quot;com.databricks.spark.csv&quot;)\n   .option(&quot;header&quot;, &quot;true&quot;)\n   .option(&quot;inferSchema&quot;, &quot;true&quot;)\n   .option(&quot;delimiter&quot;, &quot;\\\\t&quot;)\n   .load(panel_path)\n[/scala]\n\nFor our k-means clustering algorithms, we will model for 3 clusters, so we will create a filter for 3 populations: British from England and Scotland (GBR), African Ancestry in Southwest US (ASW), and Han Chinese in Bejing, China (CHB). \u00a0We will do this by create a <code>filterPanel</code> DataFrame with only these three populations. \u00a0As this is a small panel, we are also broadcasting it to all the executors will result in less data shuffling when we do further operations, thus it will be more efficient.\n\n[scala]\n// Create filtered panel of the three populations\nval filterPanel = panel.select(&quot;sample&quot;, &quot;pop&quot;).where(&quot;pop IN ('GBR', 'ASW', 'CHB')&quot;)\n\n// Take filtered panel and broadcast it\nval fpanel = filterPanel\n   .rdd\n   .map{x =&gt; x(0).toString -&gt; x(1).toString}\n   .collectAsMap()\nval bPanel = sc.broadcast(fpanel)\n[/scala]\n\n<h4>Parallel processing of genotypes</h4>\nUsing the command below, we will load the genotypes of our three populations. \u00a0This can be done more efficiently in parallel because the filtered panel is loaded in memory and broadcasted to all the nodes (i.e. bPanel) while the parquet files containing the genotype data enable predicate pushdown to the file level. Thus, only the records we are interested in are loaded from the file.\n\n[scala]\n// Create filtered panel of the three populations\nval popFilteredGts : RDD[Genotype] = sc.loadGenotypes(tmp_path).filter(genotype =&gt; {bPanel.value.contains(genotype.getSampleId)})\n[/scala]\n\nThe notebook contains a number of additional steps including:\n<ul>\n \t<li>Exploration of the data - our data has a small subset of variants of chromosome 6 covering about half a million base pairs.</li>\n \t<li>Cleaning and filtering of the data - missing data or if the variant is <a href=\"http://www.genetics.org/content/184/1/233\">triallelic</a>.</li>\n \t<li>Preparation of the data for k-means clustering - create an ML vector for each sample (containing the \u00a0variants in the exact same order) and then pulling out the features vector to run the model against.</li>\n</ul>\nUltimately, the genotypes for the 805 variants we have left in our data will be the features we use to predict the geographic population. \u00a0Our next step is to create a features vector and DataFrame to run k-means clustering.\n<h3>Running KMeans clustering</h3>\nWith the above preparation steps, running k-means clustering against the genome sequence data is similar to the <a href=\"http://spark.apache.org/docs/latest/mllib-clustering.html#k-means\">k-means example</a> in the Spark Programming Guide.\n\n[scala]\nimport org.apache.spark.mllib.clustering.{KMeans,KMeansModel}\n\n// Cluster the data into three classes using KMeans\nval numClusters = 3\nval numIterations = 20\nval clusters:KMeansModel = KMeans.train(features, numClusters, numIterations)\n[/scala]\n\nNow that we have our model - clusters - let\u2019s predict the populations and compute the <a href=\"https://en.wikipedia.org/wiki/Confusion_matrix\">confusion matrix</a>. \u00a0First, we perform the task of creating the <code>predictionRDD</code> which contains the original value (i.e. that points to the original geographic population location of CHB, ASW, and GBR) and utilizes <code>clusters.predict</code> to output the model\u2019s prediction of the geo based on the features (i.e. the genome variants). \u00a0Next, we convert this into the <code>predictDF</code> DataFrame making it easier to query (e.g. using the <code>display()</code> command, running R commands in the subsequent cell, etc.). \u00a0Finally, we join back to the <code>filterPanel</code> to obtain the original labels (the actual geographic population location).\n\n[scala]\n// Create predictionRDD that utilizes clusters.predict method to output the model's predicted geo location\nval predictionRDD: RDD[(String, Int)] = dataPerSampleId.map(sd =&gt; {\n   (sd._1, clusters.predict(sd._2))\n})\n\n// Convert to DataFrame to more easily query the data\nval predictDF = predictionRDD.toDF(&quot;sample&quot;,&quot;prediction&quot;)\n\n// Join back to the filterPanel to get the original label\nval resultsDF = filterPanel.join(predictDF, &quot;sample&quot;)\n\n// Register as temporary table\nresultsDF.registerTempTable(&quot;results_table&quot;)\n\n// Display results\ndisplay(resultsDF)\n[/scala]\n\nBelow is the graphical representation of the output between the predicted value and actual value.\n\n<img class=\"aligncenter wp-image-7695\" src=\"https://databricks.com/wp-content/uploads/2016/05/confusion-matrix-1024x459.png\" alt=\"confusion-matrix\" width=\"651\" height=\"292\" />\n\nA quick example of how to calculate the <a href=\"https://en.wikipedia.org/wiki/Confusion_matrix\">confusion matrix</a> is to use R. \u00a0While this notebook was primarily written in Scala, we can add a new cell using %r indicating that we are using the R language for our query.\n\n[scala]\n%r\nresultsRDF &lt;- sql(sqlContext, &quot;SELECT pop, prediction FROM results_table&quot;)\nconfusion_matrix &lt;- crosstab(resultsRDF, &quot;prediction&quot;, &quot;pop&quot;)\nhead(confusion_matrix)\n[/scala]\n\nWith the output being:\n<pre>\u00a0prediction_pop CHB GBR ASW\n1 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a02 \u00a089 \u00a030 \u00a0\u00a03\n2 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01 \u00a014 \u00a058 \u00a017\n3 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a00 \u00a0\u00a00 \u00a0\u00a03 \u00a041\n</pre>\nWithin the notebook, there is also additional SQL code to join the original sample, geographic population, population code, and predicted code so you can map the predictions down to the individual samples.\n<h3>Visualizing the Clusters with a Force Graph on Lightning-Viz</h3>\nA fun way to visualize these k-means cluster is to use the force graph via <a href=\"http://lightning-viz.org/\">Lightning-Viz</a>. \u00a0The notebook contains the Python code used to create the lightning visualization. \u00a0In the animated gif below, you can see the three clusters representing the three populations (top left: 2, top right: 1, bottom: 0). \u00a0The predicted cluster memberships are the vertices of the cluster while the different colors represent the population. Clicking on the population shows the sampleID, color (actual population), and the predicted population (line to the vertices).\n\n<img class=\"size-full wp-image-7701 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2016/05/lightning-viz.gif\" alt=\"lightning-viz\" width=\"682\" height=\"384\" />\n\n&nbsp;\n<h2>Discussion</h2>\nIn this post, we have provided a primer of genome sequencing (<a href=\"https://databricks.com/blog/2016/05/24/genome-sequencing-in-a-nutshell.html\">Genome Sequencing in a Nutshell</a>) and the complexities surrounding variant analysis (<a href=\"https://databricks.com/blog/2016/05/24/parallelizing-genome-variant-analysis.html\">Parallelizing Genome Variant Analysis</a>). \u00a0With the introduction of ADAM, we can process variants in a distributed parallelizable manner significantly improving the performance and accuracy of the analysis. \u00a0This has been demonstrated in the <a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/Samples/Miscellaneous/Genome_Variant_Analysis_using_k-means_ADAM_and_Apache_Spark.html?t=1463550134452\">Genome Variant Analysis by performing K-Means on ADAM data using Apache Spark notebook</a> which you can run for yourself in <a href=\"http://go.databricks.com/databricks-community-edition-beta-waitlist\">Databricks Community Edition</a>. \u00a0\u00a0The promise of genome variant analysis is that we can identify individuals who are predisposed to common diseases, solve rare diseases, and provide personalized treatments. \u00a0Just as we have seen the massive drop in cost and time with massively parallel sequencing, massively parallel bioinformatic analysis will help us to handle reproducible analysis of the rising deluge of sequence data, and may even contribute to methods of analysis that are currently not available. Ultimately, it will contribute to progress in medicine.\n<h2>Attribution</h2>\nWe wanted to give a particular call out to the following resources that helped us create the notebook\n<ul>\n \t<li><a href=\"http://bdgenomics.org/projects/adam/\">Big Data Genomics ADAM project</a></li>\n \t<li><a href=\"https://amplab.cs.berkeley.edu/publication/adam-genomics-formats-and-processing-patterns-for-cloud-scale-computing/\">ADAM: Genomics Formats and Processing Patterns for Cloud Scale Computing (Berkeley AMPLab)</a></li>\n \t<li>Andy Petrella\u2019s <a href=\"http://www.slideshare.net/noootsab/lightning-fast-genomics-with-spark-adam-and-scala\">Lightning Fast Genomics with Spark and ADAM</a> and associated <a href=\"https://github.com/andypetrella\">GitHub repo</a>.</li>\n \t<li>Neil Ferguson <a href=\"https://github.com/nfergu/popstrat\">Population Stratification Analysis on Genomics Data Using Deep Learning</a>.</li>\n \t<li>Matthew Conlen\u00a0<a href=\"http://lightning-viz.org/\">Lightning-Viz project</a>.</li>\n \t<li><a href=\"http://www.slideshare.net/TimothyDanford\">Timothy Danford\u2019s SlideShare presentations</a> (on Genomics with Spark)</li>\n \t<li><a href=\"https://www.genome.gov/27562201/2015-news-feature-centers-for-mendelian-genomics-uncovering-the-genomic-basis-of-hundreds-of-rare-conditions/\">Centers for Mendelian Genomics uncovering the genomic basis of hundreds of rare conditions</a></li>\n \t<li><a href=\"https://www.nih.gov/news-events/news-releases/nih-genome-sequencing-program-targets-genomic-bases-common-rare-disease\">NIH genome sequencing program targets the genomic bases of common, rare disease</a></li>\n \t<li><a href=\"http://www.1000genomes.org/\">The 1000 Genomes Project</a></li>\n</ul>\nAs well, we\u2019d like to thank for additional contributions and reviews by Anthony Joseph, Xiangrui Meng, Hossein Falaki, and Tim Hunter."}
{"status": "publish", "description": " Spark Live is a workshop designed exclusively for data professionals who want to learn how to leverage the power of Apache Spark and Databricks.", "creator": "Wayne Chan", "link": "https://databricks.com/blog/2016/05/27/thats-a-wrap-spark-live-draws-huge-audience-in-los-angeles.html", "authors": null, "id": 7773, "categories": ["Announcements", "Company Blog", "Events"], "dates": {"publishedOn": "2016-05-27", "tz": "UTC", "createdOn": "2016-05-27"}, "title": "That\u2019s a Wrap! Spark Live Draws Huge Audience in Los Angeles", "slug": "thats-a-wrap-spark-live-draws-huge-audience-in-los-angeles", "content": "As we continue our road show across the United States, there has been one observation that has been true along our first two stops \u2014 there is an unquestionable thirst for Apache Spark knowledge. \u00a0Spark Live, an eight-city road show brought to you by Databricks in collaboration with premier sponsor Intel, made its first stop in Hanover, Maryland (recap <a href=\"https://databricks.com/blog/2016/05/20/spark-live-la-is-just-around-the-corner.html\">here</a>) in early May with a massive turnout. This week we continued our march towards training the masses on Spark and Databricks with a visit to Los Angeles, California.\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/05/image1.jpg\"><img class=\"size-medium wp-image-7774\" src=\"https://databricks.com/wp-content/uploads/2016/05/image1-300x225.jpg\" alt=\"Packed house a Spark Live LA\" width=\"300\" height=\"225\" /></a>\n\nThe full day workshop took place in The London West Hollywood at Beverly Hills. \u00a0The packed room was filled with leading innovators across various industries including Warner Brothers, Disney, NFL, Amgen, Toyota, and many more. \u00a0All of these companies are looking for ways to leverage the power of Spark to enable business use cases that help them differentiate from the competition and drive market share.\n\nThe agenda included talks from Databricks CEO and Co-Founder Ali Ghodsi who gave a history lesson on the origins of Spark and how that work led to the founding of Databricks. Ram Sriharsha, Databricks product manager, long time committer and PMC member of the Apache Spark project discussed the latest Spark 2.0 release and shared insights into the Databricks roadmap. \u00a0The audience was also treated to several demos of the Databricks platform as well as talks from sponsors Intel and Redis Labs, who shared insights into their contributions to the open source Spark community. \u00a0And of course, the afternoon was dedicated to training everyone on Spark through the Databricks platform.\n<h2>Back on the road in July</h2>\nStop three of our traveling workshop will be in Reston, Virginia on July 21st. \u00a0The day will feature a stellar lineup of presenters from Databricks, Intel, and Lockheed Martin and Spark training by the Spark experts. \u00a0<a href=\"http://go.databricks.com/spark-live/request-your-spot\">Sign up</a> to request your spot in Reston today!\n<h2>Join us at Spark Live in a city near you!</h2>\nFor more information about the Spark Live 2016 road show or to request your spot for the city closest to you, visit the <a href=\"https://databricks.com/spark-live\">Spark Live homepage</a>.\n<h2>Not in Reston?</h2>\nDon\u2019t worry, we\u2019ve got a few more stops along the way:\n<ul>\n \t<li><a href=\"http://go.databricks.com/spark-live/request-your-spot\">Austin, TX \u2013 coming in August</a></li>\n \t<li><a href=\"http://go.databricks.com/spark-live/request-your-spot\">Seattle, WA \u2013 coming in August</a></li>\n \t<li><a href=\"https://databricks.com/spark-live/boston-ma\">Boston, MA \u2013 September 22nd</a></li>\n \t<li><a href=\"http://go.databricks.com/spark-live/request-your-spot\">Chicago, IL \u2013 coming in October</a></li>\n \t<li><a href=\"http://go.databricks.com/spark-live/request-your-spot\">New York, NY \u2013 coming in November</a></li>\n</ul>"}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2016/06/02/not-your-fathers-database-how-to-use-apache-spark-properly-in-your-big-data-architecture.html", "authors": null, "id": 7789, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2016-06-02", "tz": "UTC", "createdOn": "2016-06-02"}, "title": "Not Your Father\u2019s Database: How to Use Apache Spark Properly in your Big Data Architecture", "slug": "not-your-fathers-database-how-to-use-apache-spark-properly-in-your-big-data-architecture", "content": "Two months ago, we held a live webinar \u2014 <a href=\"http://go.databricks.com/not-your-fathers-database\">Not Your Father\u2019s Database: How to Use Apache Spark Properly in your Big Data Architecture</a>\u00a0\u2014 which covered a series of use cases where you can store your data cheaply in files and analyze the data with Apache Spark, as well as use cases where you want to store your data into a different data source to access with Spark DataFrames.\n\nThe webinar is accessible on-demand.\u00a0The slides and sample notebooks are also downloadable as attachments to the webinar. <a href=\"http://go.databricks.com/databricks-community-edition-beta-waitlist\">Join the Databricks Community Edition beta</a> to get free access to Apache Spark and try out the notebooks.\n\nWe have also answered the common questions raised by webinar viewers below. If you have additional questions, please check out the <a href=\"https://forums.databricks.com/\">Databricks Forum</a>.\n\n<strong>Common webinar questions and answers</strong>\n\nClick on the question to see answer:\n<ul>\n \t<li><a href=\"https://forums.databricks.com/questions/8154/from-webinar-not-your-fathers-database-what-are-th.html#answer-8155\">What are the best practices to store files in S3 to enable more efficient Spark access?</a></li>\n \t<li><a href=\"https://forums.databricks.com/questions/8156/from-webinar-not-your-fathers-database-what-are-th-1.html#answer-8157\">What are the pros and cons between using Spark + HDFS or Spark + S3?</a></li>\n \t<li><a href=\"https://forums.databricks.com/questions/8158/from-webinar-not-your-fathers-database-what-is-adv.html#answer-8159\">What is advantage of Spark ML over homegrown Python scikit library?</a></li>\n \t<li><a href=\"https://forums.databricks.com/questions/8160/from-webinar-not-your-database-would-storing-data.html#answer-8161\">Would storing data using Parquet solve most problems for query efficiency? How does Spark SQL take advantage of Parquet?</a></li>\n \t<li><a href=\"https://forums.databricks.com/questions/8277/from-the-webinar-not-your-fathers-database-any-bes.html#answer-8278\">Any best practices for structuring or naming files in S3 to enable more efficient Spark access?</a></li>\n</ul>"}
{"status": "publish", "description": null, "creator": "joseph", "link": "https://databricks.com/blog/2016/05/31/apache-spark-2-0-preview-machine-learning-model-persistence.html", "authors": null, "id": 7794, "categories": ["Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2016-05-31", "tz": "UTC", "createdOn": "2016-05-31"}, "title": "Apache Spark 2.0 Preview: Machine Learning Model Persistence", "slug": "apache-spark-2-0-preview-machine-learning-model-persistence", "content": "[sidenote]Spark Summit 2016 will be held in San Francisco on June 6\u20138. Check out the <a href=\"https://spark-summit.org/2016/schedule/\" target=\"_blank\">full agenda</a> and <a href=\"http://www.prevalentdesignevents.com/sparksummit2016/registration.aspx\" target=\"_blank\">get your ticket</a> before it sells out![/sidenote]\n\n[dbce_cta href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/ML%20persistence%20in%202.0.html\"]Try this notebook in Databricks[/dbce_cta]\n\n<hr />\n\n<h2>Introduction</h2>\nConsider these Machine Learning (ML) use cases:\n<ul>\n \t<li>A data scientist produces an ML model and hands it over to an engineering team for deployment in a production environment.</li>\n \t<li>A data engineer integrates a model training workflow in Python with a model serving workflow in Java.</li>\n \t<li>A data scientist creates jobs to train many ML models, to be saved and evaluated later.</li>\n</ul>\nAll of these use cases are easier with model persistence, the ability to save and load models. With the upcoming release of <a href=\"https://databricks.com/blog/2016/05/11/apache-spark-2-0-technical-preview-easier-faster-and-smarter.html\">Apache Spark 2.0</a>, Spark\u2019s Machine Learning library MLlib will include near-complete support for ML persistence in the DataFrame-based API. This blog post gives an early overview, code examples, and a few details of MLlib\u2019s persistence API.\n\nKey features of ML persistence include:\n<ul>\n \t<li>Support for all language APIs in Spark: Scala, Java, Python &amp; R</li>\n \t<li>Support for nearly all ML algorithms in the DataFrame-based API</li>\n \t<li>Support for single models and full Pipelines, both unfitted (a \u201crecipe\u201d) and fitted (a result)</li>\n \t<li>Distributed storage using an exchangeable format</li>\n</ul>\nThanks to all of the community contributors who helped make this big leap forward in MLlib! See the JIRAs for <a href=\"https://issues.apache.org/jira/browse/SPARK-6725\" target=\"_blank\">Scala/Java</a>, <a href=\"https://issues.apache.org/jira/browse/SPARK-11939\" target=\"_blank\">Python</a>, and <a href=\"https://issues.apache.org/jira/browse/SPARK-14311\" target=\"_blank\">R</a> for full lists of contributors.\n<h2>Learn the API</h2>\nIn Apache Spark 2.0, the DataFrame-based API for MLlib is taking the front seat for ML on Spark. (See <a href=\"https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html\" target=\"_blank\">this previous blog post</a> for an introduction to this API and the \u201cPipelines\u201d concept it introduces.) This DataFrame-based API for MLlib provides functionality for saving and loading models that mimics the familiar Spark Data Source API.\n\nWe will demonstrate saving and loading models in several languages using the popular MNIST dataset for handwritten digit recognition (LeCun et al., 1998; available from the <a href=\"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#mnist\" target=\"_blank\">LibSVM dataset page</a>). This dataset contains handwritten digits 0\u20139, plus the ground truth labels. Here are some examples:\n\n<img class=\"aligncenter size-full wp-image-7795\" src=\"https://databricks.com/wp-content/uploads/2016/05/handwritten-digits.jpg\" alt=\"Screenshot of handwritten digits.\" width=\"438\" height=\"145\" />\n\nOur goal will be to take new images of handwritten digits and identify the digit. See <a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/ML%20persistence%20in%202.0.html\" target=\"_blank\">this notebook</a> for the full example code to load this data, fit the models, and save and load them.\n<h3>Save &amp; load single models</h3>\nWe first show how to save and load single models to share between languages. We will fit a Random Forest Classifier using Python, save it, and then load the same model back using Scala.\n\n[python]\ntraining = sqlContext.read...  # data: features, label\nrf = RandomForestClassifier(numTrees=20)\nmodel = rf.fit(training)\n[/python]\n\nWe can simply call the <code>save</code> method to save this model, and the <code>load</code> method to load it right back:\n\n[python]\nmodel.save(&quot;myModelPath&quot;)\nsameModel = RandomForestClassificationModel.load(&quot;myModelPath&quot;)\n[/python]\n\nWe could also load that same model (which we saved in Python) into a Scala or Java application:\n\n[scala]\n// Load the model in Scala\nval sameModel = RandomForestClassificationModel.load(&quot;myModelPath&quot;)\n[/scala]\n\nThis works for both small, local models such as K-Means models (for clustering) and large, distributed models such as ALS models (for recommendation). The loaded model has the same parameter settings and data, so it will return the same predictions even if loaded on an entirely different Spark deployment.\n<h3>Save &amp; load full Pipelines</h3>\nSo far, we have only looked at saving and loading a single ML model. In practice, ML workflows consist of many stages, from feature extraction and transformation to model fitting and tuning. MLlib provides Pipelines to help users construct these workflows. (See <a href=\"https://docs.cloud.databricks.com/docs/latest/sample_applications/index.html#Sample%20ML/MLPipeline%20Bike%20Dataset.html\" target=\"_blank\">this notebook</a> for a tutorial on ML Pipelines analyzing a bike sharing dataset.)\n\nMLlib allows users to save and load entire Pipelines. Let\u2019s look at how this is done on an example Pipeline with these steps:\n<ul>\n \t<li>Feature extraction: Binarizer to convert images to black and white</li>\n \t<li>Model fitting: Random Forest Classifier to take images and predict digits 0\u20139</li>\n \t<li>Tuning: Cross-Validation to tune the depth of the trees in the forest</li>\n</ul>\nHere is a snippet from our notebook to build this Pipeline:\n\n[scala]\n// Construct the Pipeline: Binarizer + Random Forest\nval pipeline = new Pipeline().setStages(Array(binarizer, rf))\n\n// Wrap the Pipeline in CrossValidator to do model tuning.\nval cv = new CrossValidator().setEstimator(pipeline) ...\n[/scala]\n\nBefore we fit this Pipeline, we will show that we can save entire workflows (before fitting). This workflow could be loaded later to run on another dataset, on another Spark cluster, etc.\n\n[scala]\ncv.save(&quot;myCVPath&quot;)\nval sameCV = CrossValidator.load(&quot;myCVPath&quot;)\n[/scala]\n\nFinally, we can fit the Pipeline, save it, and load it back later. This saves the feature extraction step, the Random Forest model tuned by Cross-Validation, and the statistics from model tuning.\n\n[scala]\nval cvModel = cv.fit(training)\ncvModel.save(&quot;myCVModelPath&quot;)\nval sameCVModel = CrossValidatorModel.load(&quot;myCVModelPath&quot;)\n[/scala]\n\n<h2>Learn the details</h2>\n<h3>Python tuning</h3>\nThe one missing item in Spark 2.0 is Python tuning. Python does not yet support saving and loading CrossValidator and TrainValidationSplit, which are used to tune model hyperparameters; this issue is targeted for Spark 2.1 (<a href=\"https://issues.apache.org/jira/browse/SPARK-13786\" target=\"_blank\">SPARK-13786</a>). However, it is still possible to save the results from CrossValidator and TrainValidationSplit from Python. For example, let\u2019s use Cross-Validation to tune a Random Forest and then save the best model found during tuning.\n\n[python]\n# Define the workflow\nrf = RandomForestClassifier()\ncv = CrossValidator(estimator=rf, ...)\n# Fit the model, running Cross-Validation\ncvModel = cv.fit(trainingData)\n# Extract the results, i.e., the best Random Forest model\nbestModel = cvModel.bestModel\n# Save the RandomForest model\nbestModel.save(&quot;rfModelPath&quot;)\n[/python]\n\nSee <a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/ML%20persistence%20in%202.0.html\" target=\"_blank\">the notebook</a> for the full code.\n<h3>Exchangeable storage format</h3>\nInternally, we save the model metadata and parameters as JSON and the data as Parquet. These storage formats are exchangeable and can be read using other libraries. Parquet allows us to store both small models (such as Naive Bayes for classification) and large, distributed models (such as ALS for recommendation). The storage path can be any URI supported by Dataset/DataFrame save and load, including paths to S3, local storage, etc.\n<h3>Language cross-compatibility</h3>\nModels can be easily saved and loaded across Scala, Java, and Python. R has two limitations. First, not all MLlib models are supported from R, so not all models trained in other languages can be loaded into R. Second, the current R model format stores extra data specific to R, making it a bit hacky to use other languages to load models trained and saved in R. (See <a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/ML%20persistence%20in%202.0.html\" target=\"_blank\">the accompanying notebook</a> for the hack.) Better cross-language support for R will be added in the near future.\n<h2>Conclusion</h2>\nWith the upcoming 2.0 release, the DataFrame-based MLlib API will provide near-complete coverage for persisting models and Pipelines. Persistence is critical for sharing models between teams, creating multi-language ML workflows, and moving models to production. This feature was a final piece in preparing the DataFrame-based MLlib API to become the primary API for Machine Learning in Apache Spark.\n<h2>What\u2019s next?</h2>\nHigh-priority items include complete persistence coverage, including Python model tuning algorithms, as well as improved compatibility between R and the other language APIs.\n\n<strong>Get started</strong> with <a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/ML%20persistence%20in%202.0.html\" target=\"_blank\">this tutorial notebook</a> in Scala and Python. You can also just update your current MLlib workflows to use save and load.\n<strong>Experiment with</strong> this API using an Apache Spark branch-2.0 preview in the Databricks Community Edition beta program. <a href=\"http://go.databricks.com/databricks-community-edition-beta-waitlist\">Join the beta waitlist.</a>\n<h2>Read More</h2>\n<ul>\n \t<li>Read <a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/ML%20persistence%20in%202.0.html\" target=\"_blank\">the notebook</a> with the full code referenced in this blog post.</li>\n \t<li>Learn about the DataFrame-based API for MLlib &amp; ML Pipelines:\n<ul>\n \t<li><a href=\"https://docs.cloud.databricks.com/docs/latest/sample_applications/index.html#Sample%20ML/MLPipeline%20Bike%20Dataset.html\" target=\"_blank\">Notebook introducing ML Pipelines</a>: tutorial analyzing a bike sharing dataset</li>\n \t<li><a href=\"https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html\" target=\"_blank\">Original blog post on ML Pipelines</a></li>\n</ul>\n</li>\n</ul>"}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2016/06/08/achieving-end-to-end-security-for-apache-spark-with-databricks.html", "authors": null, "id": 7809, "categories": ["Announcements", "Company Blog", "Product"], "dates": {"publishedOn": "2016-06-08", "tz": "UTC", "createdOn": "2016-06-08"}, "title": "Achieving End-to-end Security for Apache Spark with Databricks", "slug": "achieving-end-to-end-security-for-apache-spark-with-databricks", "content": "Today we are excited to announce the completion of the first phase of the <em>Databricks Enterprise Security (DBES)</em> framework. We are proud to say that this makes Databricks the first and only company to provide comprehensive enterprise security on top of Apache Spark.\n\n<a href=\"http://www.marketwired.com/press-release/databricks-empowers-enterprises-to-secure-their-apache-spark-workloads-2132605.htm\" target=\"_blank\">Read the press release here.</a>\n\n<a href=\"https://databricks.com/customers\">Hundreds of organizations</a> have deployed Databricks to improve the productivity of their data teams, power their production Spark applications, and democratize data access. As Databricks continues to gain adoption across security-minded industries such as financial services and healthcare, we are also focused on enabling them to maximize the value from their data while satisfying strict security and compliance requirements in their respective industries (such as Sarbanes-Oxley or HIPAA).\n<h2>Holistic Security for the Big Data Lifecycle</h2>\nTraditionally, enterprise organizations only had security solutions that addressed parts of their big data infrastructure. Today, enterprises demand holistic security that covers the full spectrum of their big data lifecycle: from file processing, big data clusters, code management, job workflows, application deployments, dashboards, to reports.\n\nThe Databricks just-in-time data platform takes a holistic approach to solving the enterprise security challenge by building all the facets of security \u2014 <em>encryption, identity management, role-based access control, data governance, and compliance standards</em> \u2014 natively into the data platform with DBES.\n<ul>\n \t<li><strong>Encryption:</strong> Provides strong encryption at rest and inflight with best-in-class standards such as SSL and keys stored in AWS Key Management System (KMS).</li>\n \t<li><strong>Integrated Identity Management:</strong> Facilitates seamless integration with enterprise identity providers via SAML 2.0 and Active Directory.</li>\n \t<li><strong>Role-Based Access Control:</strong> Enables fine-grain management access to every component of the enterprise data infrastructure, including files, clusters, code, application deployments, dashboards, and reports.</li>\n \t<li><strong>Data Governance:</strong> Guarantees the ability to monitor and audit all actions taken in every aspect of the enterprise data infrastructure.</li>\n \t<li><strong>Compliance Standards:</strong> Achieves security compliance standards that exceed the high standards of FedRAMP as part of Databricks\u2019 ongoing DBES strategy.</li>\n</ul>\nIn short, DBES will provide holistic security in every aspect of the entire big data lifecycle.\n<h2>Major Achievements in DBES Phase One</h2>\nDBES builds upon the extensive Databricks access management and encryption functionalities that already exist. With the completion of DBES Phase One today, enterprises gain the ability to control access to Apache Spark clusters on an individual basis, manage user identity with a SAML 2.0 compatible identify management provider service, and end-to-end auditability.\n<h3>Cluster Access Control Lists</h3>\nThe Cluster Access Control Lists, or cluster ACLs, gives Databricks administrators the ability to fine-tune the autonomy of Databricks users based on the enterprise security policy. For example, one can strictly limit the ability to launch new clusters to control costs while giving teams the complete freedom to run code on existing clusters in a self-service manner.\n\nSpecifically, an administrator will be able to define whether users are allowed perform the following actions on an individual basis:\n<ul>\n \t<li>Launch a new cluster</li>\n \t<li>Terminate an existing cluster</li>\n \t<li>Run code on (attach to) an existing cluster</li>\n \t<li>Change the configuration of an existing cluster</li>\n \t<li>Restart an existing cluster</li>\n</ul>\n<h3>SAML 2.0 Support</h3>\n<img class=\"aligncenter size-full wp-image-7876\" src=\"https://databricks.com/wp-content/uploads/2016/06/databricks-single-sign-on-saml-2.png\" alt=\"SAML 2.0 in Databricks\" width=\"206\" height=\"235\" />\n\nEnterprises will now be able to use a SAML 2.0 compatible identity provider to authenticate and authorize access to the Databricks platform. Since many enterprises already utilize an identity provider service, and virtually all major identity providers (e.g., Okta, PingIdentity) support SAML 2.0, this will vastly simplify the setup and management of accounts on the Databricks platform. Databricks users will also enjoy a more streamlined login process, as now they can log into the platform with a single click instead of having to remember (and possibly recover) passwords.\n<h3>End-to-End Audit Logs</h3>\nThe audit logs will provide enterprises in security-conscious industries such as healthcare or financial services the tools to satisfy strict compliance requirements, such as HIPAA or Sarbanes-Oxley. The Databricks audit logs are a comprehensive record of activity on the platform, allowing enterprises to monitor the detailed usage patterns of Databricks as the business requires. This allows a central authority to easily reconstruct critical events with:\n<ul>\n \t<li>The time and details of an action.</li>\n \t<li>The user who triggered the action (including administrators).</li>\n \t<li>And other crucial information.</li>\n</ul>\nThese logs are stored in a human readable format so one can explore the logs easily, the administrator can also analyze the information in the audit logs using the Databricks platform itself.\n<h2>Making Big Data Simple (and Secure)</h2>\nDatabricks\u2019 vision is to empower anyone to easily build and deploy advanced analytics solutions. With the <em>Databricks Enterprise Security Framework</em>, Databricks can satisfy the diverse (and sometimes competing) needs to secure big data in the modern enterprise, end-to-end. Phase One is only the beginning, stay tuned for more advances in the near future.\n\nInterested in securing your Apache Spark workloads with Databricks? <a href=\"https://databricks.com/try-databricks\">Test drive the platform with a free trial</a> or <a href=\"https://databricks.com/company/contact\">contact us for a personalized demo</a>."}
{"status": "publish", "description": "On June 15 Databricks will be launching the first of these courses, a two week Introduction to Apache Spark.", "creator": "andy", "link": "https://databricks.com/blog/2016/06/01/databricks-to-launch-first-of-five-free-big-data-courses-on-apache-spark.html", "authors": null, "id": 7812, "categories": ["Announcements", "Company Blog"], "dates": {"publishedOn": "2016-06-01", "tz": "UTC", "createdOn": "2016-06-01"}, "title": "Databricks to Launch First of Five Free Big Data Courses on Apache Spark", "slug": "databricks-to-launch-first-of-five-free-big-data-courses-on-apache-spark", "content": "Databricks helps hundreds of organizations use Apache Spark to answer important questions by analyzing data. Apache Spark is an open-source data processing engine for engineers and analysts that includes an optimized general execution runtime and a set of standard libraries for building data pipelines, advanced algorithms, and more.\n\nBesides contributing over 75% of Apache Spark's design and development work, Databricks has also trained a large number of Spark developers. In less than three years over 30,000 students have completed a Databricks training course about Apache Spark and over half of those students received the training for free.\n\nLast year Databricks worked with professors from the University of California Berkeley and University of California Los Angeles to produce two free Massive Open Online Courses (MOOCs). These courses were recently nominated for the edX Prize for Exceptional Contributions in Online Teaching and Learning. These courses were also remarkably successful, with highly positive student feedback along with enrollment, engagement, and completion rates that are two to five times the averages for MOOCs. Overall, upwards of 125,000 students registered for the courses and over 15,000 students passed.\n\nThis year we have expanded our investment in these academic partnerships and created the five-part MOOC series <a href=\"https://www.edx.org/xseries/data-science-engineering-apache-spark\"><i>Data Science and Engineering with Apache Spark</i></a>. Through this series of courses, students will be exposed to an integrated view of data processing, and will gain hands-on experience building and debugging Spark applications using the Databricks platform. \u00a0All of these MOOCs will be freely available on the edX MOOC platform.\n\nOn June 15 we will be launching the first of these courses, a two week course entitled <a href=\"https://www.edx.org/course/introduction-apache-spark-uc-berkeleyx-cs105x\">Introduction to Apache Spark</a>. \u00a0This statistics and data analysis course will teach you the basics of working with Spark core and Spark SQL and provide you with the necessary foundation for diving deeper into Spark. You\u2019ll learn about Spark\u2019s architecture and programming model, including commonly used APIs. After completing this course you\u2019ll be able to write and debug basic Spark applications, use Spark\u2019s web user interface (UI), as well as recognize and avoid common coding errors. We are excited to contribute back to the Apache Spark community with these efforts, and we hope these MOOCs will contribute to the growth of the community.\n\n<a href=\"https://courses.edx.org/register?course_id=course-v1%3ABerkeleyX%2BCS105x%2B1T2016&amp;enrollment_action=enroll&amp;email_opt_in=true\">Go register for free now!</a>"}
{"status": "publish", "description": null, "creator": "jules_damji", "link": "https://databricks.com/blog/2016/06/01/preview-apache-spark-2-0-an-anthology-of-technical-assets.html", "authors": null, "id": 7821, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2016-06-01", "tz": "UTC", "createdOn": "2016-06-01"}, "title": "Apache Spark 2.0: An Anthology of Technical Assets", "slug": "preview-apache-spark-2-0-an-anthology-of-technical-assets", "content": "Older anthologies collated a collection of contributions from various authors around a theme\u2014bounded then as a journal or periodical. Newer anthologies include multiple modals of expressions\u2014digitized now as an ebook or a blog. Both offer an exposition of the subject matter\u2014no matter their form.\n\nIn this anthology, I\u2019ve compiled a collection of videos, technical blogs, notebooks, webinar, podcasts, and news articles that focus on<a href=\"https://databricks.com/blog/2016/07/26/introducing-apache-spark-2-0.html\"> Apache Spark 2.0 now generally available.</a>\n\nYou can try the Apache Spark 2.0 version from two places:\n\n<ul>\n    <li><a href=\"https://databricks.com/try-databricks\">Databricks or the Databricks Community Edition</a></li>\n    <li><a href=\"https://spark.apache.org/docs/latest/\" target=\"_blank\">spark.apache.org</a></li>\n</ul>\n\n<hr />\n\n<strong><a href=\"https://www.youtube.com/watch?v=ZFBgY0PwUeY\" target=\"_blank\">Spark Summit East Keynote: Apache Spark 2.0</a></strong>\nDatabricks\u2019 CTO Matei Zaharia thanks community\u2019s contributions and previews Apache Spark 2.0\u2019s three themes: simplicity, speed and unification.\n\n<strong><a href=\"https://www.youtube.com/watch?v=i7l3JQRx7Qw\" target=\"_blank\">Structuring Spark: DataFrames, Datasets, and Streaming</a></strong>\nApache Spark committer and Databricks\u2019 engineer Michael Armbrust sets the stage for why structure, as applied to data, is relevant, and how it affects the design of DataFrame and Dataset APIs and Streaming in Apache Spark 2.0.\n\n<strong><a href=\"https://spark-summit.org/2016/events/a-deep-dive-into-structured-streaming/\">A Deep-Dive in Structured Streaming in Apache Spark 2.0</a></strong>\nDatabricks' Spark committer\u00a0Tathagata Das gives a tech-talk on how Structured Streaming works, under the hood.\n\n<strong><a href=\"http://go.databricks.com/apache-spark-2.0-presented-by-databricks-co-founder-reynold-xin\" target=\"_blank\">Apache Spark 2.0: Easier, Faster &amp; Smarter</a></strong>\nApache Spark committer and Chief Architect at Databricks Reynold Xin and Spark Community Evangelist Jules S. Damji preview Apache Spark 2.0 and showcase salient features in Databricks notebooks running <a href=\"https://spark.apache.org/news/spark-2.0.0-preview.html\" target=\"_blank\">pre-release of Spark 2.0</a>.\n\n<a href=\"https://databricks.com/blog/2016/07/26/introducing-apache-spark-2-0.html\"><strong>Introducing Apache Spark 2.0 Now Generally Available on Databricks</strong></a>\nA more in-depth version of the webinar, \u00a0Matei Zaharia, Reynold Xin, and Michael Armbrust expound on three thrusts\u2014speed, simplicity structured streaming\u2014behind Apache Spark 2.0, with notebooks running on Databricks.\n\n<strong><a href=\"https://databricks.com/blog/2016/05/19/approximate-algorithms-in-apache-spark-hyperloglog-and-quantiles.html\" target=\"_blank\">Approximate Algorithms in Apache Spark: HyperLogLog Quantiles</a></strong>\nDatabricks\u2019 engineers Tim Hunter, Hossein Falaki, and Joseph Bradley showcase two approximation algorithms to approximate distinct elements and compute quantiles in a large data using pre-release preview of Apache Spark 2.0 on Databricks.\n\n<strong><a href=\"https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html\" target=\"_blank\">Apache Spark as a Compiler: Joining a Billion Rows on your Laptop</a></strong>\nApache Spark is already pretty fast, but can we make it 10x faster? Reynold Xin, Sameer Agarwal, and Davies Liu explain how Tungsten\u2019s whole-stage code generation makes it so.\n\n<strong><a href=\"https://blog.acolyer.org/2016/05/23/efficiently-compiling-efficient-query-plans-for-modern-hardware/\" target=\"_blank\">Efficiently Compiling Efficient Query Plans for Modern Hardware</a></strong>\n<a href=\"https://blog.acolyer.org/about/\" target=\"_blank\">Adrian Coyle</a>, former CTO of SpringSource, explores influential and important topics in the world of computer science in his <a href=\"https://blog.acolyer.org/\" target=\"_blank\">Morning Paper</a>.\n\n<strong><a href=\"http://www.kdnuggets.com/2016/05/spark-tungsten-burns-brighter.html\" target=\"_blank\">Spark With Tungsten Burns Brighter</a></strong>\n<a href=\"https://twitter.com/RobertsPaige\" target=\"_blank\">Paige Roberts</a> (of Syncort) opines that Tungsten represents a huge leap forward for Apache Spark, particularly in the area of performance, and writes how it works, and why it improves Spark performance.\n\n<strong><a href=\"https://www.oreilly.com/ideas/structured-streaming-comes-to-apache-spark-2-0\" target=\"_blank\">Structured Streaming Comes to Apache Spark 2.0</a></strong>\nO'Reilly\u2019s Chief Data Scientist <a href=\"http://radar.oreilly.com/ben\" target=\"_blank\">Ben Lorica</a> sits down with Michael Armbrust and talks about life and structured streaming.\n\n<strong><a href=\"http://www.infoworld.com/article/3052924/analytics/what-sparks-structured-streaming-really-means.html\" target=\"_blank\">What Spark\u2019s Structured Streaming Really Means</a></strong>\n<a href=\"http://www.infoworld.com/author/Ian-Pointer/\" target=\"_blank\">Ion Pointer</a> (contributor for InfoWorld) advocates why DataFrames are the best choice for Spark Streaming in Spark 2.0, and why structured streaming makes sense.\n\n<strong><a href=\"https://databricks.com/blog/2016/05/31/apache-spark-2-0-preview-machine-learning-model-persistence.html\" target=\"_blank\">Apache Spark 2.0 Preview: Machine Learning Model Persistence</a></strong>\nDatabricks\u2019 engineer Joseph Bradley shares the benefits of Machine Learning Model Persistence in Spark 2.0 Preview, and how you can save and load ML Pipelines across multiple languages in Spark.\n\n<strong><a href=\"https://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/IoTDeviceGeoIPDS2.0.html\" target=\"_blank\">How to Process IoT Data Using Datasets APIs</a></strong>\nDatabricks Community Edition notebook showcasing Apache Spark 2.0 Dataset APIs.\n\n<a href=\"https://databricks.com/blog/2016/06/17/sql-subqueries-in-apache-spark-2-0.html\"><strong>SQL Subqueries in Apache Spark 2.0</strong></a>\nDatabricks' engineers \u00a0Davies Liu and Herman van H\u00f6vell\u00a0provide hands-on examples of scalar and predicate type subqueries\n\n<strong><a href=\"https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html\">A Tale of Three Apache Spark APIs: RDDs, DataFrames and Datasets</a></strong>\nDatabricks' Spark Community Evangelist Jules S. Damji tells the tale of three Spark APIs: when to use them and why\n\n<a href=\"https://blog.codecentric.de/en/2016/07/spark-2-0-datasets-case-classes/\"><strong>Spark 2.0 - Datasets and case classes</strong></a>\nDaniel Pape, an analytics engineer at <a href=\"https://www.codecentric.de/\">codecentric</a>\u00a0explores and explains the type-safety features of Datasets APIs through some code examples using Scala case classes\n\n<strong><a href=\"https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html\">Continuous Applications: Evolving Streaming in Apache Spark 2.0</a></strong>\nDatabricks' Co-founder and CTO Matei Zaharia shares his vision on end-to-end streaming applications called continuous application using Structured Streaming APIs in Apache Spark 2.0\n\n<a href=\"https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html\"><strong>Structured Streaming in Apache Spark 2.0: A new high-level API for streaming.</strong></a>\nMessrs Matei Zaharia,\u00a0Tathagata Das,\u00a0Reynold Xin and Michael Armbrust explain the challenges of writing end-to-end streaming applications called continuous application and elaborate why and how Structured Streaming makes it simple.\n\n<strong><a href=\"https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html\">How to Use SparkSessions in Apache Spark 2.0</a></strong>\nDatabricks' Spark Community Evangelist Jules S. Damji explores SparkSession functionality in Spark 2.0.\n\n<h2>What\u2019s Next?</h2>\n\nIn the coming weeks, we\u2019ll publish a series of posts on Spark 2.0 versions and will update this anthology. You might want to bookmark this page!"}
{"status": "publish", "description": null, "creator": "ion", "link": "https://databricks.com/blog/2016/06/07/dce-ga.html", "authors": null, "id": 7843, "categories": ["Announcements", "Company Blog", "Product"], "dates": {"publishedOn": "2016-06-07", "tz": "UTC", "createdOn": "2016-06-07"}, "title": "Databricks Community Edition is now Generally Available", "slug": "dce-ga", "content": "We are excited to <a href=\"http://www.marketwired.com/press-release/databricks-announces-general-availability-of-community-edition-2132115.htm\" target=\"_blank\">announce the General Availability</a> (GA) of <a href=\"https://databricks.com/try-databricks\">Databricks Community Edition</a> (DCE). As a free version of the Databricks service, DCE enables everyone to learn and explore <a href=\"http://spark.apache.org/\">Apache Spark</a>, by providing access to a simple, integrated development environment for data analysts, data scientists and engineers with high quality training materials and sample application notebooks.\n\nLess than four months ago, at <a href=\"https://spark-summit.org/east-2016/\">Spark Summit New York</a>, we introduced Databricks Community Edition (DCE) <em>beta</em>. Its introduction generated tremendous interest with thousands of people requesting accounts. Today, we are delighted to report that more than 8,000 users have signed on DCE, many of them using the service heavily. The top 10% active users are averaging over 6 hours per week, and are executing over 10,000 commands on average.\n\nGoing beyond these numbers, we are delighted to see DCE attracting a wide user base. According to a survey we have conducted recently, 25% of our users have never used Spark before, and 60% of the users are neither data scientists nor data engineers. This demonstrates the effectiveness of DCE to grow the open source Apache Spark user community by bringing new users into the fold, as well as its ability to train new data scientists and engineers.\n\nThe same survey also indicates that 90% of the users are using DCE for learning Apache Spark, which establishes the role of DCE as a learning platform for Spark. Indeed, since its launch, tens of universities have already used DCE for teaching, including UC Berkeley and Stanford. At UC Berkeley, over 900 students have used DCE to learn Apache Spark during the \u201c<a href=\"https://inst.eecs.berkeley.edu/~cs61a/sp16/lab/lab13/\" target=\"_blank\">Structure and Interpretation of Computer Programs</a>\u201d class in Spring, 2016.\n\nTo further aid the efforts of teaching big data with Apache Spark and to reach students worldwide, we are happy to announce that this year Databricks will offer a <a href=\"https://www.edx.org/xseries/data-science-engineering-apache-spark\">free 5-MOOC series</a> (up from two MOOCs since Databricks offered last year) on <a href=\"https://www.edx.org/\" target=\"_blank\">EdX</a>, all of which will be taught on DCE:\n<ul>\n \t<li><a href=\"https://www.edx.org/course/introduction-apache-spark-uc-berkeleyx-cs105x?gclid=CMfz_PSFkM0CFQEdaQod-VAOKQ\" target=\"_blank\">Introduction to Apache Spark</a></li>\n \t<li><a href=\"https://www.edx.org/course/distributed-machine-learning-apache-uc-berkeleyx-cs120x\" target=\"_blank\">Distributed Machine Learning with Apache Spark</a></li>\n \t<li><a href=\"https://www.edx.org/course/big-data-analysis-apache-spark-uc-berkeleyx-cs110x\" target=\"_blank\">Big Data Analysis with Apache Spark</a></li>\n \t<li><a href=\"https://www.edx.org/course/advanced-apache-spark-data-science-data-uc-berkeleyx-cs115x\" target=\"_blank\">Advanced Apache Spark for Data Science and Engineering</a></li>\n \t<li><a href=\"https://www.edx.org/course/advanced-distributed-machine-learning-uc-berkeleyx-cs125x\" target=\"_blank\">Advanced Machine Learning with Apache Spark</a></li>\n</ul>\nAlso, the GA comes with new introductory materials and sample applications. In particular, to make learning Apache Spark even easier, we have added three notebooks to provide a \u201cgentler\u201d introduction to Apache Spark. You can find these new notebooks here:\n<ul>\n \t<li><a href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/346304/2168141618055043/484361/latest.html\" target=\"_blank\">A Gentle Introduction to Apache Spark on Databricks</a></li>\n \t<li><a href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/346304/2168141618055194/484361/latest.html\" target=\"_blank\">Apache Spark on Databricks for Data Engineers</a></li>\n \t<li><a href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/346304/2168141618055109/484361/latest.html\" target=\"_blank\">Apache Spark on Databricks for Data Scientists</a></li>\n</ul>\nBy making DCE generally available, we are looking to fuel the growth of the community by introducing Apache Spark to first time users. Finally, by training a new generation of data scientists and engineers, we hope to mitigate the ever growing scarcity of data specialists.\n\n<a href=\"https://databricks.com/try-databricks\" target=\"_blank\"><img class=\"aligncenter size-full wp-image-7858\" src=\"https://databricks.com/wp-content/uploads/2016/06/blog-sign-up-for-dce-ga.png\" alt=\"Sign up for Databricks Community Edition\" width=\"653\" height=\"66\" /></a>"}
{"status": "publish", "description": "We\u2019ve put together a quick recap of the keynotes and talks for the Spark enthusiasts who could not make it to San Francisco.", "creator": "Wayne Chan", "link": "https://databricks.com/blog/2016/06/08/another-record-setting-spark-summit.html", "authors": null, "id": 7893, "categories": ["Company Blog", "Events"], "dates": {"publishedOn": "2016-06-09", "tz": "UTC", "createdOn": "2016-06-09"}, "title": "Another Record-Setting Spark Summit", "slug": "another-record-setting-spark-summit", "content": "The lure of San Francisco is indisputable as is its position as the preeminent high-tech hub. On day one of Spark Summit 2016, the largest community event dedicated to Apache Spark, drew more than 2500+ Spark enthusiasts from 720+ companies. Such a draw is a strong testament to Apache Spark\u2019s open source roots, its fast-growing community of users and contributors, and an occasion to share war stories and learn from each other\u2019s experiences.\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/06/blog-hero@1x.jpg\"><img class=\"aligncenter wp-image-7894 size-large\" src=\"https://databricks.com/wp-content/uploads/2016/06/blog-hero@1x-1024x452.jpg\" alt=\"blog-hero@1x\" width=\"1024\" height=\"452\" /></a><a href=\"https://databricks.com/wp-content/uploads/2016/06/blog-hero@1x.jpg\">\n</a>To capture some of its moments, we\u2019ve put together a quick recap of the keynotes and highlighted talks from Databricks\u2019 speakers for the Spark enthusiasts who could not make it to San Francisco.\n\n<h1>Day One: Apache Spark Takes the Spotlight</h1>\n\n<h2>Apache Spark 2.0 Improvements and the Quickest Way to Learn Spark</h2>\n\nIn front of 2500+ attendees and another 3000+ watching the live stream, Matei Zaharia - the creator of the Apache Spark project and CTO of Databricks - kicked off the event with a keynote (<a href=\"https://www.youtube.com/watch?v=pY35anClhOA\">video</a> |\u00a0<a href=\"http://www.slideshare.net/databricks/spark-summit-san-francisco-2016-matei-zaharia-keynote-apache-spark-20\">slides</a>) that unveiled the major updates to Apache Spark in the upcoming 2.0 release.\n\nHe also shared an exciting statistic with the audience: Apache Spark is now the highest-paying tech skill according to the <a href=\"https://databricks.com/blog/2016/03/22/apache-spark-trending-in-the-stack-overflow-survey.html\">2016 Stack Overflow Developer Survey</a>. In the spirit of contributing to Spark community growth, Matei announced the <a href=\"https://databricks.com/blog/2016/06/07/dce-ga.html\">general availability of Databricks Community Edition</a>, a free version of the Databricks platform, contributed to the community, for anyone to learn Apache Spark, and the launch of a series of free Massive Open Online Courses.<a href=\"https://databricks.com/wp-content/uploads/2016/06/matei-slides@1x-1.jpg\"><img class=\"alignnone size-full wp-image-7912\" src=\"https://databricks.com/wp-content/uploads/2016/06/matei-slides@1x-1.jpg\" alt=\"matei-slides@1x (1)\" width=\"2000\" height=\"882\" /></a>Michael Armbrust concluded the talk with a live demo that took the audience through an analysis of live tweets on the Databricks Community Edition platform (<a href=\"https://vimeo.com/169761851\">watch video</a> | <a href=\"https://docs.cloud.databricks.com/docs/latest/featured_notebooks/2016%20Election%20Tweets.html\">download the notebook</a>).\n\n<h2>Apache Spark\u2019s Role in Machine Learning</h2>\n\nThe next two keynotes focused on how Spark has become a key component in machine learning innovation. Both Jeff Dean (Senior Fellow at Google) and Andrew Ng (Chief Scientist at Baidu) described Spark\u2019s contributions to large-scale machine learning and enumerated several uses cases in the industry. \u00a0Noting that Spark\u2019s vast scalability is enabling machine learning to achieve higher levels of accuracy than before, Andrew succinctly summarized the future as: Spark + AI = Superpowers.\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/06/google_baidu@1x.jpg\"><img class=\"alignnone size-full wp-image-7898\" src=\"https://databricks.com/wp-content/uploads/2016/06/google_baidu@1x.jpg\" alt=\"google_baidu@1x\" width=\"2000\" height=\"882\" /></a>\n\n<h2>Putting Apache Spark to Work</h2>\n\nTo conclude the keynote sessions, Marvin Theimer, Distinguished Engineer at Amazon, shared his lessons learned from years of running production big data infrastructure. He spoke to the four main challenges of productionization\u00a0: Scalability, High Availability, Maintainability, and Evolvability. Marvin ended his talk with a brief list of must-haves to satisfy these necessities to put big data in production:\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/06/marvin-amazon@1x.jpg\"><img class=\"alignnone size-full wp-image-7899\" src=\"https://databricks.com/wp-content/uploads/2016/06/marvin-amazon@1x.jpg\" alt=\"marvin-amazon@1x\" width=\"2000\" height=\"882\" /></a>\n\n<h2>Highlights of Databricks Speakers from Day One</h2>\n\nAfter the keynotes, the attendees dispersed into several talk tracks for the rest of the day. \u00a0In the Developer Track, Michael Armbrust\u2019s (<a href=\"http://www.slideshare.net/databricks/structuring-spark-dataframes-datasets-and-streaming-62871797\">slides</a>) and Tathagata Das\u2019 (<a href=\"http://www.slideshare.net/databricks/a-deep-dive-into-structured-streaming\">slides</a>) \u00a0talks dove deeper into the new Streaming DataFrames/Datasets API\u2019s available in Spark 2.0 that optimizes execution plans and simplifies building end-to-end continuous applications.\n\nIn the Data Science Track, Xiangrui Meng - Apache Spark committers at Databricks - (<a href=\"http://www.slideshare.net/databricks/recent-developments-in-sparkr-for-advanced-analytics\">slides</a>) reviewed recent efforts by both the Spark and R communities to extend SparkR for scalable predictive analytics and demonstrated MLlib machine learning algorithms that have been ported to SparkR. \u00a0And in the Use Case &amp; Experience Track, Burak Yavuz and Yu Peng (<a href=\"http://www.slideshare.net/databricks/a-journey-into-databricks-pipelines-journey-and-lessons-learned\">slides</a>), software engineers at Databricks, gave a \u201cdog fooding\u201d presentation on how Databricks uses Spark throughout the data pipeline for use cases such as ETL, data warehousing, and real-time analysis.\n\n<h1>Day Two: Enterprise Adoption of Apache Spark</h1>\n\nDatabricks CEO and Co-founder Ali Ghodsi explained how the Databricks just-in-time data platform solves the \u201canalytics gap\u201d for today\u2019s \u201cdata reality,\u201d where data is spread, stored, and siloed in the cloud, data lakes, and data warehouses. By separating storage and compute, Databricks\u2019 just-in-time platform offers an integrated and secure workspace, can access your data securely no matter where it\u2019s stored, and manages your Spark clusters.\n\nHe also announced <a href=\"https://databricks.com/blog/2016/06/08/achieving-end-to-end-security-for-apache-spark-with-databricks.html\">Databricks Enterprise Security (DBES)</a> that\u00a0promises to provide holistic security in every aspect of the entire big data lifecycle. \u00a0With the completion of the first phase of DBES, enterprises gain the ability to control access to Spark clusters on an individual basis, manage user identity with a SAML 2.0 compatible identify management provider service, and end-to-end auditability.\n\n<a href=\"https://www.youtube.com/watch?v=YNbvE5Q6j_o\">Watch the full video here.</a>\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/06/2016-06-08_1835.png\"><img class=\"alignnone wp-image-7901\" src=\"https://databricks.com/wp-content/uploads/2016/06/2016-06-08_1835.png\" alt=\"2016-06-08_1835\" width=\"555\" height=\"552\" /></a>\n\n<h2>Talks from Leaders in the Apache Spark Ecosystem</h2>\n\nDay two also featured presentations from industry leaders who are heavily involved in driving the Spark ecosystem. Joseph Sirosh, Corporate VP of the Data Group at Microsoft and Doug Cutting, Chief Architect and Co-Founder of Apache Hadoop and Cloudera both noted the impact Spark is having across various use cases and highlighted how they are enabling organizations to quickly gain insights via advanced analytics from their data through Spark and their ecosystem of tools. \u00a0Ziya Ma, VP of Big Data at Intel discussed how they are making Spark faster and more scalable with their forward-thinking innovations in silicon technology; and Rob Thomas, VP of Product Development at IBM Analytics covered how they are using Spark for cognitive analytics to enable real-time decision making.\n\n&nbsp;\n\n<h2>Enterprise Track Features Databricks Impact</h2>\n\nDatabricks has been deployed by hundreds of organizations around the world. The enterprise track included talks from three organizations using Databricks. One of the largest video game companies in the world - Riot Games - utilized Databricks to improve the player experience for <i>League of Legends</i> (their flagship title with 67+ million active players monthly). Riot was able to improve game balance, network performance, and personalization with Databricks.\n\nDNV GL, a global classification and technical assurance company, talked about using advanced analytics to predict energy usage through the analysis of prevailing weather condition data and sensor data collected from millions of smart meters. \u00a0With Databricks, DNV GL was able to streamline data pipeline processing - allowing them to accelerate time-to-value by nearly 100 times compared to previous methods. \u00a0To learn more, <a href=\"http://go.databricks.com/case-studies/dnvgl\">read the case study</a>.\n\nAIMIA, an enterprise technology company, shared their journey with Spark from attending local meetups to learn more about Spark to partnering with Databricks as the data platform geared to accelerate application development through machine learning and rapid product prototyping.\n\n<h2>Highlight of Databricks Speakers from Day Two</h2>\n\n<h3>Apache Spark Component Deep-Dives</h3>\n\nAndrew Or and Yin Huai - Apache Spark committers at Databricks - kicked off the developer track with two deep-dive sessions. Andrew\u2019s presentation gave the audience an inside look at Spark memory management and its performance &amp; usability implications for the end user (<a href=\"http://www.slideshare.net/databricks/deep-dive-memory-management-in-apache-spark\">slides</a>). Yin provided an in-depth look at the <i>Catalyst</i> optimizer, the underpinning of all major APIs in 2.0 (<a href=\"http://www.slideshare.net/databricks/deep-dive-into-catalyst-apache-spark-20s-optimizer\">slides</a>). Joseph Bradley - a Databricks Spark committer who primarily works on MLlib- discussed three key improvements in Apache Spark 2.0: persisting models for production, customizing Pipelines, and the latest models and APIs improvements for data science (<a href=\"http://www.slideshare.net/databricks/apache-spark-mllib-20-preview-data-science-and-production\">slides</a>).\n\n<h3>Tips on Making Apache Spark Applications Faster, More Reliable, and Easier to Debug</h3>\n\nMiklos Christine - a solution architect at Databricks - gave operational tips and best practices based on his extensive experience working with Databricks customers. He shared his learnings on how system design influences performance, what he does to configure Apache Spark clusters for optimal performance, and common misconfigurations that prevent users from getting the most of Apache Spark (<a href=\"http://www.slideshare.net/databricks/operational-tips-for-deploying-apache-spark\">slides</a>).\n\n<h3>Surprising Findings on Apache Spark Usage</h3>\n\nHossein Falaki - a data scientist at Databricks - performed a quantitative study of <a href=\"https://databricks.com/blog/2016/06/07/dce-ga.html\">Databricks Community Edition</a> beta users\u2019 usage patterns and came up with some surprising conclusions (<a href=\"http://www.slideshare.net/databricks/apache-spark-usage-in-the-open-source-ecosystem\">slides</a>):\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/06/Python-packages.jpg\"><img class=\"alignnone size-full wp-image-7904\" src=\"https://databricks.com/wp-content/uploads/2016/06/Python-packages.jpg\" alt=\"Python packages\" width=\"1440\" height=\"810\" /></a>\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/06/Screen-Shot-2016-06-08-at-10.42.39-PM.png\"><img class=\"alignnone wp-image-7926 size-full\" src=\"https://databricks.com/wp-content/uploads/2016/06/Screen-Shot-2016-06-08-at-10.42.39-PM.png\" width=\"1984\" height=\"1068\" /></a>\n\n<h2>What's next?</h2>\n\nVideos and slides of all talks have been posted on the <a href=\"https://spark-summit.org/2016/schedule/\">Spark Summit website</a>.\n\n<a href=\"https://twitter.com/databricks?lang=en\">Follow Databricks on Twitter</a> or <a href=\"http://go.databricks.com/newsletter-registration\">subscribe to our newsletter</a> to stay up to date with Apache Spark and Databricks news.\n\nP.S. for Spark enthusiasts in Europe: The Summit is coming to Brussels October 25th. The call for papers is open, <a href=\"https://spark-summit.org/eu-2016/\">submit your idea today</a>!"}
{"status": "publish", "description": null, "creator": "bill", "link": "https://databricks.com/blog/2016/06/15/an-introduction-to-writing-apache-spark-applications-on-databricks.html", "authors": null, "id": 7954, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2016-06-15", "tz": "UTC", "createdOn": "2016-06-15"}, "title": "An Introduction to Writing Apache Spark Applications on Databricks", "slug": "an-introduction-to-writing-apache-spark-applications-on-databricks", "content": "[dbce_cta href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/346304/2168141618055043/484361/latest.html\"]Try this notebook in Databricks[/dbce_cta]\n\n[sidenote]This is part 1 of a 3 part series providing a gentle introduction to writing Apache Spark applications on Databricks.[/sidenote]\n\n<hr/>\n\nWhen I first started learning <a href=\"http://spark.apache.org\" target=\"_blank\">Apache Spark</a> several years ago, the biggest challenge for me was finding material that introduced me to the key concepts and the thinking that one must apply to write efficient Spark applications. I attended the UC Berkeley AMP Camps as a graduate student at UC Berkeley and Spark Summit SF 2015; while these were great learning experiences that taught me the basics, I had a hard time finding examples that took me beyond the basic word count. This changed when I started working at Databricks six months ago.\n\nDatabricks has excellent reference resources for our users in the <a href=\"https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html#00%20Welcome%20to%20Databricks.html\" target=\"_blank\">Databricks Guide</a>. However, I also observed that many users want Apache Spark tutorials that progress through an end-to-end example in more detail. I frequently get questions such as, \u201chow do I use DataFrames, SparkSQL, or Datasets?\u201d, \u201chow do I create a machine learning pipeline?\u201d or \u201chow do I create a UDF and use it in Spark SQL and on my DataFrames?\u201d So I set about writing a series of three tutorials in the form of Databricks notebooks to illustrate these answers.\n\nWith the <a href=\"https://databricks.com/blog/2016/06/07/dce-ga.html\">general availability of Databricks Community Edition</a> (DCE), a free version of the Databricks platform, anyone can sign-up and run these notebooks to learn hands-on. If you do not have access to DCE yet, <a href=\"http://databricks.com/try-databricks\">sign up now</a> to work through the examples in the notebooks.\n\n<h2>What\u2019s in the First Tutorial</h2>\n\nThe first part of the series is intended for the most general audience - anyone who is new to Apache Spark or Databricks. This notebook will demonstrate the tools in Databricks that make working with your data easier, showing you how to do things like creating Apache Spark clusters, visualizing data, and simplifying access to the Spark UI. Beyond Databricks itself, you\u2019ll also get an architectural overview of Apache Spark to give you a sense of what is going on during the execution of a Spark job. This notebook is also a good review for those who want to make sure they\u2019ve got a solid understanding of the fundamentals of Apache Spark. \n\nThe major concepts covered in this tutorial include:\n\n<ul>\n<li>Introducing Apache Spark and Databricks terminology.</li>\n<li>The different contexts and environments in Apache Spark including 2.0\u2019s SparkSession Context.</li>\n<li>The fundamental data interfaces like DataFrames and Datasets.</li>\n<li>An overview of how Apache Spark takes code and executes it on your Spark cluster in Databricks Community Edition.</li>\n</ul>\n\n<img src=\"https://databricks.com/wp-content/uploads/2016/06/spark-cluster-diagram.png\" alt=\"Diagram showing how Apache Spark executes a Databricks Notebook in Databricks Community Edition\" width=\"570\" height=\"309\" class=\"aligncenter size-full wp-image-7957\" />\n\n<ul>\n<li>The difference between transformations and actions.</li>\n<li>A walkthrough of the below directed acyclic graph (DAG), to see how Spark uses transformations and actions to take your raw data and convert it into its final form. Since Databricks already includes the preview version of Apache Spark 2.0, we\u2019ll be able to compare the ways that different Spark versions generate their query plans. Here\u2019s an extract from a 2.0 query plan:</li>\n</ul>\n\n<img src=\"https://databricks.com/wp-content/uploads/2016/06/spark-dag-screenshot.png\" alt=\"Screenshot of a DAG\" width=\"400\" height=\"203\" class=\"aligncenter size-full wp-image-7956\" />\n\n<a href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/346304/2168141618055043/484361/latest.html\" target=\"_blank\">You can download the first notebook here.</a>\n\n<h2>What\u2019s Next</h2>\n\nWhile this first notebook introduces each concept at a high level, notebooks two and three in the series take a much deeper dive into the material. The second part of this series will show how a data scientist should go about using Spark and Databricks together. We\u2019ll do this by attempting to predict the number of farmers\u2019 markets in a given zipcode based on the individual and business taxes paid in the area. The final notebook in the series will take the reader through an ETL pipeline end-to-end. This will include parsing raw log files, creating UDFs, handling messy datetimes, and combining that with another dataset \u2014 all the while taking advantage of Spark\u2019s data-connectors to integrate with multiple data sources. These notebooks have plenty of code and explanations so be sure to stay tuned for the next notebooks in the series!\n\nIf you have not done so, <a href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/346304/2168141618055043/484361/latest.html\" target=\"_blank\">download the notebook</a> and <a href=\"http://databricks.com/try-databricks\">sign-up for Databricks Community Edition</a> to get started today!"}
{"status": "publish", "description": null, "creator": "admin", "link": "https://databricks.com/blog/2016/06/17/sql-subqueries-in-apache-spark-2-0.html", "authors": null, "id": 7963, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2016-06-17", "tz": "UTC", "createdOn": "2016-06-17"}, "title": "SQL Subqueries in Apache Spark 2.0", "slug": "sql-subqueries-in-apache-spark-2-0", "content": "[dbce_cta href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2728434780191932/1483312212640900/6987336228780374/latest.html\"]Try this notebook in Databricks[/dbce_cta]\n\nIn the upcoming Apache Spark 2.0 release, we have substantially expanded the SQL standard capabilities. In this brief blog post, we will introduce subqueries in Apache Spark 2.0, including their limitations, potential pitfalls and future expansions, and through a notebook, we will explore both the scalar and predicate type of subqueries, with short examples that you can try yourself.\n\nA subquery is a query that is nested inside of another query. A subquery as a source (inside a <code>SQL FROM</code> clause) is technically also a subquery, but it is beyond the scope of this post. There are basically two kinds of subqueries: scalar and predicate subqueries. And within scalar and predicate queries, there are uncorrelated scalar and correlated scalar queries and nested predicate queries respectively.\n\nFor brevity, we will let you jump and explore the notebook, which is more an interactive experience rather than an exposition here in the blog. Click on this diagram below to view and explore the subquery notebook with <a href=\"https://spark.apache.org/news/spark-2.0.0-preview.html\">Apache Spark 2.0 preview</a> on Databricks.\n\n<a href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2728434780191932/1483312212640900/6987336228780374/latest.html\" target=\"_blank\"><img class=\"aligncenter size-full wp-image-7966\" src=\"https://databricks.com/wp-content/uploads/2016/06/sql-subqueries-databricks.png\" alt=\"Example of SQL sub-queries in a Databricks Notebook\" width=\"1928\" height=\"703\" /></a>\n<h2>What\u2019s Next</h2>\nSubquery support in Apache Spark 2.0 provides a solid solution for the most common subquery usage scenarios. However, there is room for improvement in the areas noted in detail at the end of <a href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2728434780191932/1483312212640900/6987336228780374/latest.html\" target=\"_blank\">the notebook</a>.\n\nTo try this notebook on Databricks, <a href=\"[registration_url]\">sign up now</a>."}
{"status": "publish", "description": null, "creator": "denny", "link": "https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html", "authors": null, "id": 7996, "categories": ["Ecosystem", "Engineering Blog"], "dates": {"publishedOn": "2016-06-22", "tz": "UTC", "createdOn": "2016-06-22"}, "title": "Apache Spark Key Terms, Explained", "slug": "apache-spark-key-terms-explained", "content": "[sidenote]This article was originally\u00a0<a href=\"http://www.kdnuggets.com/2016/06/spark-key-terms-explained.html\" target=\"_blank\">posted on KDnuggets</a>[/sidenote]\n\n[sidenote]The Spark Summit Europe call for presentations is open, <a href=\"https://spark-summit.org/eu-2016/\">submit your idea today</a>[/sidenote]\n\n<hr />\n\nAs observed in the Fortune article <a href=\"http://fortune.com/2015/09/25/apache-spark-survey/\" target=\"_blank\">Survey shows huge popularity spike for Apache Spark</a>:\n<blockquote><u>\"Apache Spark is the Taylor Swift of big data software.</u> The open source technology has been around and popular for a few years. But 2015 was the year Spark went from an ascendant technology to a bona fide superstar.\"</blockquote>\nOne of the reasons why Apache Spark has become so popular is because Spark provides data engineers and data scientists with a powerful, unified engine that is both fast (100x faster than Apache Hadoop for large-scale data processing) and easy to use. This allows data practitioners to solve their machine learning, graph computation, streaming, and real-time interactive query processing problems interactively and at much greater scale.\n\n<img class=\"aligncenter size-full wp-image-8000\" src=\"https://databricks.com/wp-content/uploads/2016/06/Apache-Spark-Components-Diagram.gif\" alt=\"Apache Spark components diagram\" width=\"800\" height=\"450\" />\n\nIn this blog post, we will discuss some of the key terms one encounters when working with Apache Spark.\n<h2>1. Apache Spark</h2>\nApache Spark is a powerful open-source processing engine built around speed, ease of use, and sophisticated analytics, with APIs in Java, Scala, Python, R, and SQL. Spark runs programs up to 100x faster than Hadoop MapReduce in memory, or 10x faster on disk. It can be used to build data applications as a library, or to perform ad-hoc data analysis interactively. Spark powers a stack of libraries including SQL, DataFrames, and Datasets, MLlib for machine learning, GraphX for graph processing, and Spark Streaming. You can combine these libraries seamlessly in the same application. As well, Spark runs on a laptop, Hadoop, Apache Mesos, standalone, or in the cloud. It can access diverse data sources including HDFS, Apache Cassandra, Apache HBase, and S3.\n\nIt was originally developed at UC Berkeley in 2009. (Note that Spark\u2019s creator Matei Zaharia has since become CTO at Databricks and faculty member at MIT.) Since its release, Spark has seen rapid adoption by enterprises across a wide range of industries. Internet powerhouses such as Netflix, Yahoo, and Tencent have eagerly deployed Spark at massive scale, collectively processing multiple petabytes of data on clusters of over 8,000 nodes. It has quickly become the largest open source community in big data, with over 1000 code contributors and with over 187,000 members in 420 <a href=\"http://www.meetup.com/topics/apache-spark/\" target=\"_blank\">Apache Spark Meetups</a> groups.\n<h2>2. RDD</h2>\nAt the core of Apache Spark is the notion of data abstraction as distributed collection of objects. This data abstraction, called Resilient Distributed Dataset (RDD), allows you to write programs that transform these distributed datasets.\n\nRDDs are immutable distributed collection of elements of your data that can be stored in memory or disk across a cluster of machines. The data is partitioned across machines in your cluster that can be operated in parallel with a low-level API that offers transformations and actions. RDDs are fault tolerant as they track data lineage information to rebuild lost data automatically on failure.\n\nBelow is an Apache Spark code snippet using Python and RDDs to perform a word count.\n\n[python]\n# Open textFile for Spark Context RDD\ntext_file = spark.textFile(&quot;hdfs://...&quot;)\n\n# Execute word count\ntext_file.flatMap(lambda line: line.split())\n    .map(lambda word: (word, 1))\n    .reduceByKey(lambda a, b: a+b)\n[/python]\n\n<h2>3. DataFrame</h2>\nLike an RDD, a DataFrame is an immutable distributed collection of data. Unlike an RDD, data is organized into named columns, like a table in a relational database. Designed to make large data sets processing even easier, DataFrame allows developers to impose a structure onto a distributed collection of data, allowing higher-level abstraction; it provides a domain specific language API to manipulate your distributed data; and makes Spark accessible to a wider audience, beyond specialized data engineers.\n\nBelow is an Apache Spark code snippet using SQL and DataFrames to query and join different data sources.\n\n[python]\n# Read JSON file and register temp view\ncontext.jsonFile(&quot;s3n://...&quot;).createOrReplaceTempView(&quot;json&quot;)\n\n# Execute SQL query \nresults = context.sql(&quot;&quot;&quot;SELECT * FROM people JOIN json ...&quot;&quot;&quot;)\n[/python]\n\n<h2>4. Dataset</h2>\nIntroduced in Spark 1.6, the goal of Spark Datasets is to provide an API that allows users to easily express transformations on domain objects, while also providing the performance and benefits of the robust Spark SQL execution engine.\n\nNote, starting in Spark 2.0, the DataFrame APIs will merge with <a href=\"https://databricks.com/blog/2016/01/04/introducing-spark-datasets.html\">Datasets APIs</a>, unifying data processing capabilities across all libraries. Because of unification, developers now have fewer concepts to learn or remember, and work with a single high-level and <strong>type-safe</strong> API called Dataset. Conceptually, the Spark DataFrame is an <em>alias</em> for a collection of generic objects <code>Dataset[Row]</code>, where a Row is a generic <strong>untyped</strong> JVM object. Dataset, by contrast, is a collection of <strong>strongly-typed</strong> JVM objects, dictated by a case class you define, in Scala or Java.\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/06/Screen-Shot-2016-06-21-at-18.24.16.png\"><img class=\"aligncenter wp-image-8018\" src=\"https://databricks.com/wp-content/uploads/2016/06/Screen-Shot-2016-06-21-at-18.24.16-1024x573.png\" alt=\"Screen Shot 2016-06-21 at 18.24.16\" width=\"651\" height=\"364\" /></a>\n\n[scala]\n// Define a case class that represents our type-specific Scala JVM Object\ncase class Person (email: String, iq: Long, name: String)\n\n// Read JSON file and convert to Dataset using the case class\nval ds = spark.read.json(&quot;...&quot;).as[Person]\n[/scala]\n\n<h2>5. MLlib</h2>\nApache Spark provides a general machine learning library -- MLlib -- that is designed for simplicity, scalability, and easy integration with other tools. With the scalability, language compatibility, and speed of Spark, data scientists can solve and iterate through their data problems faster.\n\nFrom the inception of the Apache Spark project, MLlib was considered foundational for Spark\u2019s success. The key benefit of MLlib is that it allows data scientists to focus on their data problems and models instead of solving the complexities surrounding distributed data (such as infrastructure, configurations, and so on). The data engineers can focus on distributed systems engineering using Spark\u2019s easy-to-use APIs, while the data scientists can leverage the scale and speed of Spark core. Just as important, Spark MLlib is a general-purpose library, providing algorithms for most use cases while at the same time allowing the community to build upon and extend it for specialized use cases. To review the key terms of machine learning, please refer to Matthew Mayo\u2019s <a href=\"http://www.kdnuggets.com/2016/05/machine-learning-key-terms-explained.html\" target=\"_blank\">Machine Learning Key Terms, Explained</a>.\n<h2>6. ML Pipelines</h2>\nTypically when running machine learning algorithms, it involves a sequence of tasks including pre-processing, feature extraction, model fitting, and validation stages. For example, when classifying text documents might involve text segmentation and cleaning, extracting features, and training a classification model with cross-validation. Though there are many libraries we can use for each stage, connecting the dots is not as easy as it may look, especially with large-scale datasets. Most ML libraries are not designed for distributed computation or they do not provide native support for pipeline creation and tuning.\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/06/ML-pipelines-diagram.png\"><img class=\"aligncenter size-full wp-image-8001\" src=\"https://databricks.com/wp-content/uploads/2016/06/ML-pipelines-diagram.png\" alt=\"Diagram of ML pipelines\" width=\"650\" height=\"191\" /></a>\n\nThe ML Pipelines is a High-Level API for MLlib that lives under the \u201cspark.ml\u201d package. A pipeline consists of a sequence of stages. There are two basic types of pipeline stages: Transformer and Estimator. A Transformer takes a dataset as input and produces an augmented dataset as output. E.g., a tokenizer is a Transformer that transforms a dataset with text into an dataset with tokenized words. An Estimator must be first fit on the input dataset to produce a model, which is a Transformer that transforms the input dataset. E.g., logistic regression is an Estimator that trains on a dataset with labels and features and produces a logistic regression model.\n<h2>7. GraphX</h2>\nGraphX is the component in Apache Spark for graphs and graph-parallel computation. At a high level, GraphX extends the Spark RDD via a Graph abstraction: a directed multigraph with properties attached to each vertex and edge. To support graph computation, GraphX exposes a set of fundamental operators (e.g., subgraph, joinVertices, and aggregateMessages) as well as an optimized variant of the Pregel API. In addition, GraphX includes a growing collection of graph algorithms and builders to simplify graph analytics tasks.\n<h2>8. Spark Streaming</h2>\nSpark Streaming is an extension of the core Spark API that allows data engineers and data scientists to process real-time data from various sources including (but not limited to) Kafka, Flume, and Amazon Kinesis. This processed data can be pushed out to filesystems, databases, and live dashboards. Its key abstraction is a Discretized Stream or, in short, a DStream, which represents a stream of data divided into small batches. DStreams are built on RDDs, Spark\u2019s core data abstraction. This allows Spark Streaming to seamlessly integrate with any other Spark components like MLlib and Spark SQL.\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/06/Apache-Spark-Streaming-ecosystem-diagram.png\"><img class=\"aligncenter size-full wp-image-8003\" src=\"https://databricks.com/wp-content/uploads/2016/06/Apache-Spark-Streaming-ecosystem-diagram.png\" alt=\"Spark Streaming example diagram\" width=\"650\" height=\"366\" /></a>\n\nThis unification of disparate data processing capabilities is the key reason behind Spark Streaming\u2019s rapid adoption. It makes it very easy for developers to use a single framework to satisfy all their processing needs.\n<h2>9. Structured Streaming</h2>\nIntroduced as part of Apache Spark 2.0, structured streaming is a high-level streaming built on top of the Spark SQL engine. It is a declarative API that extends DataFrames and Datasets to support batch, interactive, and streaming queries. The advantage of this approach is that it allows programmers to apply their experience working with static data sets (i.e. batch) and easily apply this to infinite data sets (i.e. streaming).\n<h2>10. <a href=\"http://spark-packages.org\" target=\"_blank\">spark-packages.org</a></h2>\n<a href=\"http://spark-packages.org\" target=\"_blank\">spark-packages.org</a> is a community package index to track the growing number of open source packages and libraries that work with Apache Spark. Spark Packages makes it easy for users to find, discuss, rate, and install packages for any version of Spark and makes it easy for developers to contribute packages.\n\nSpark Packages features integrations with various data sources, management tools, higher level domain-specific libraries, machine learning algorithms, code samples, and other Spark content. Examples packages include Spark-CSV (which is now included in Spark 2.0) and Spark ML integration packages including GraphFrames and TensorFrames.\n<h2>11. Catalyst Optimizer</h2>\nSpark SQL is one of the most technically involved components of Apache Spark. It powers both SQL queries and the <a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\">DataFrame API</a>. At the core of Spark SQL is the Catalyst optimizer, which leverages advanced programming language features (e.g. Scala\u2019s <a href=\"http://docs.scala-lang.org/tutorials/tour/pattern-matching.html\" target=\"_blank\">pattern matching</a> and <a href=\"http://docs.scala-lang.org/overviews/quasiquotes/intro.html\" target=\"_blank\">quasiquotes</a>) in a novel way to build an extensible query optimizer.\n\nCatalyst is based on functional programming constructs in Scala and designed with these key two purposes:\n<ul>\n \t<li>Easily add new optimization techniques and features to Spark SQL</li>\n \t<li>Enable external developers to extend the optimizer (e.g. adding data source specific rules, support for new data types, etc.)</li>\n</ul>\nAs well, Catalyst supports both rule-based and cost-based optimization.\n\n<img class=\"aligncenter size-full wp-image-8004\" src=\"https://databricks.com/wp-content/uploads/2016/06/Catalyst-Optimizer-diagram.png\" alt=\"Diagram of the Catalyst Optimizer\" width=\"650\" height=\"149\" />\n\nFor more information, please refer to <a href=\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\">Deep Dive into Spark SQL\u2019s Catalyst Optimizer</a> and the webinar <a href=\"http://go.databricks.com/databricks-webinar-spark-dataframes-simple-and-fast-analysis-of-structured-data-0\" target=\"_blank\">Apache Spark DataFrames: Simple and Fast Analysis of Structured Data</a>.\n<h2>12. Tungsten</h2>\nTungsten is the codename for the umbrella project to make changes to Apache Spark\u2019s execution engine that focuses on substantially improving the efficiency of memory and CPU for Spark applications, to push performance closer to the limits of modern hardware. This effort includes the following initiatives:\n<ul>\n \t<li><em>Memory Management and Binary Processing:</em> leveraging application semantics to manage memory explicitly and eliminate the overhead of JVM object model and garbage collection</li>\n \t<li><em>Cache-aware computation:</em> algorithms and data structures to exploit memory hierarchy</li>\n \t<li><em>Code generation:</em> using code generation to exploit modern compilers and CPUs</li>\n \t<li><em>No virtual function dispatches:</em> this reduces multiple CPU calls which can have a profound impact on performance when dispatching billions of times.</li>\n \t<li><em>Intermediate data in memory vs CPU registers:</em> Tungsten Phase 2 places intermediate data into CPU registers. This is an order of magnitudes reduction in the number of cycles to obtain data from the CPU registers instead of from memory</li>\n \t<li><em>Loop unrolling and SIMD:</em> Optimize Apache Spark\u2019s execution engine to take advantage of modern compilers and CPUs\u2019 ability to efficiently compile and execute simple for loops (as opposed to complex function call graphs).</li>\n</ul>\nFor more information, please reference <a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\">Project Tungsten: Bringing Apache Spark Closer to Bare Metal</a>, <a href=\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\">Deep Dive into Spark SQL\u2019s Catalyst Optimizer</a>, and <a href=\"https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html\">Apache Spark as a Compiler: Joining a Billion Rows per Second on a Laptop</a>.\n<h2>13. Continuous Applications</h2>\nIn Apache Spark 2.0, adding structure to Spark, through use of high-level DataFrames and Datasets APIs, accommodates a novel approach to look at real-time streaming. That is, look at streaming not as streaming but as either a static table of data (where you know all the data) or a continuous table of data (where new data is continuously arriving).\n\nAs such you can build end-to-end <strong>continuous applications</strong>, in which you can issue the same queries to batch as to real-time data, perform ETL, generate reports, update or track specific data in the stream. This combined batch &amp; real-time query-capabilities to a structured stream is a unique offering\u2014not many streaming engines offer it yet.\n\n<img class=\"aligncenter size-full wp-image-8005\" src=\"https://databricks.com/wp-content/uploads/2016/06/example-apache-spark-continuous-application.png\" alt=\"An example of an Apache Spark Continuous Application\" width=\"643\" height=\"368\" />"}
{"status": "publish", "description": null, "creator": "jules_damji", "link": "https://databricks.com/blog/2016/06/23/share-your-thoughts-in-our-apache-spark-survey-today.html", "authors": null, "id": 8023, "categories": ["Announcements", "Company Blog"], "dates": {"publishedOn": "2016-06-23", "tz": "UTC", "createdOn": "2016-06-23"}, "title": "Share your Thoughts in our Apache Spark Survey Today", "slug": "share-your-thoughts-in-our-apache-spark-survey-today", "content": "[sidenote]The Spark Summit Europe call for presentations is open, <a href=\"https://spark-summit.org/eu-2016/\" target=\"_blank\">submit your idea today</a>.[/sidenote]\n\n<hr/>\n\nSince our <a href=\"https://databricks.com/blog/2015/09/24/spark-survey-2015-results-are-now-available.html\">survey in 2015</a>, Apache Spark has seen tremendous growth with 4 releases in a single year, each adding hundreds of improvements. \n\nWe are running a short survey to understand users\u2019 needs and usage of Apache Spark. Recent examples of how community input helped influence Spark include:\n<ul>\n<li>APIs for data science including DataFrames, Datasets, Machine Learning Pipelines, and R support</li>\n<li>Platform APIs</li>\n<li>Project Tungsten and Performance Improvements</li>\n<li>Spark Streaming and Structured Streaming</li>\n</ul>\n\nTo understand your usage of Apache Spark and associated technologies, we have designed a short survey, in which all your responses will remain confidential and only the aggregate statistics will be shared. \n\nAt Databricks we value community feedback, and this survey will help us both to understand usage of Spark and to direct our future contributions to it. In appreciation, you will be entered to win one of THREE prizes once you complete the survey: $200 Visa card, iPad Mini, or Beats headphones.\n\n[btn href=\"https://www.surveymonkey.com/r/spark_survey2016\" target=\"_blank\" size=\"lg\"]Take the Survey[/btn]\n\n&nbsp;"}
{"status": "publish", "description": null, "creator": "bill", "link": "https://databricks.com/blog/2016/06/28/building-data-science-applications-on-databricks.html", "authors": null, "id": 8037, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2016-06-28", "tz": "UTC", "createdOn": "2016-06-28"}, "title": "Building Data Science Applications on Databricks", "slug": "building-data-science-applications-on-databricks", "content": "[dbce_cta href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/346304/2168141618055194/484361/latest.html\"]Try this notebook in Databricks[/dbce_cta]\n\n[sidenote]This is part 2 of a 3 part series providing a gentle introduction to writing Apache Spark applications on Databricks. This post focuses on the tools and features that are helpful for data scientists to solve business problems instead of managing infrastructure.[/sidenote]\n\n<hr />\n\nThe big challenge for data scientists is to take a model from the prototyping all the way into production. This process is often littered with a variety of different environments, samples of data that do not reflect production data quality, and a litany of infrastructure challenges. These problems become even worse when multiple teams are restricted to sharing one cluster for all of their work.\n\nIn these challenges lie the reason that, as a Solutions Architect, I\u2019ve seen many data science teams become successful with Databricks. <a href=\"https://www.youtube.com/watch?v=IptUCv38wgE\" target=\"_blank\">Jason Scheller described the secret to success well in his interview with SiliconAngle</a>: <em>\u201cbecause of the notebook product that Databricks provides, there is no need for us to build an additional stack on top of that. Whatever notebook the analyst comes up with, that notebook can be inserted directly into the primary production system.\u201d</em>\u00a0Jason also quantified the productivity gains achieved by the Eyeview data science team:\u00a0<em>\u201cprevious to [Apache] Spark it took us about 24 hours to model one day worth of data to set up optimization for a particular campaign. With Spark we can model six months of data in about ten minutes.\u201d</em>\n\nMy personal experience reflects what Jason said. Running Apache Spark before Databricks was riddled with difficulty in taking prototypes to production. Additionally, waiting for infrastructure teams to provide the environment for me was frustrating. When I started at Databricks I saw how easy it was for us to run all our production workloads with our own product and how simple it was for our customers to drastically reduce their prototype to production time.\n\nI\u2019ve written this guide as a demonstration of the process to build a data product with Databricks. One of the most powerful aspects of Databricks is the simplicity by which users can prototype applications and then take them directly to production.\n<h2>What is in this Guide</h2>\nIn the previous guide in this series, we provided an <a href=\"https://databricks.com/blog/2016/06/15/an-introduction-to-writing-apache-spark-applications-on-databricks.html\">introduction to writing Apache Spark applications on Databricks</a>. Be sure to check it out if you have not already! The second guide follows the same spirit but is geared towards the workflow of a data scientist. To do this, the guide starts with testing a simple idea and goes through the process of iterative data analytics. We will start with two sets of data provided by the US Department of Agriculture and the Internal Revenue Service.\n\n<img class=\"alignnone size-full wp-image-8043\" style=\"margin-bottom: .5em;\" src=\"https://databricks.com/wp-content/uploads/2016/06/blog-USDA-logo.png\" alt=\"United States Department of Agriculture\" width=\"80\" height=\"55\" />\n<a href=\"http://catalog.data.gov/dataset/farmers-markets-geographic-data/resource/cca1cc8a-9670-4a27-a8c7-0c0180459bef\" target=\"_blank\">The USDA data contains data about the number of farmer\u2019s markets in zip codes across the United States.</a>\n\n<img class=\"alignnone size-full wp-image-8042\" style=\"margin-bottom: .5em;\" src=\"https://databricks.com/wp-content/uploads/2016/06/blog-IRS-logo.png\" alt=\"United States Internal Revenue Service\" width=\"120\" height=\"42\" />\n<a href=\"http://catalog.data.gov/dataset/zip-code-data\" target=\"_blank\">The IRS has information about the taxes paid in different zip codes.</a>\n\nWith these two datasets, we\u2019re hoping to predict the number of farmers markets in a zip code by the amount of taxes paid in the area. The hypothesis is that zip codes that pay higher taxes (both personal and business) will have more farmers markets because the citizens in those zip codes have more income to pay a higher premium for locally sourced fruits and vegetables! Now this hypothesis makes a fair number of assumptions but will provide an excellent foundation for a worked example for data scientists.\n\nWhile these are not large datasets by any means, we\u2019ll learn a lot about processing data in Databricks with Apache Spark by creating powerful visualizations that display the key bits of information, cleaning the data to focus our analysis, and building a machine learning pipeline. This process makes for an excellent example for those that are just getting started with machine learning as well as those that are already familiar with other machine learning libraries like scikit-learn but aren\u2019t sure how to build out similar pipelines in Apache Spark.\n\nWhile building pipeline we will some of the convenient features of Databricks. As Jason mentioned in <a href=\"https://www.youtube.com/watch?v=IptUCv38wgE\" target=\"_blank\">his interview</a>, <em>\u201cA notebook in Databricks can be SQL queries, scala code, python\u2026 and the visualizations are built right in\u2026\u201d</em>. For example the image below was created very quickly using Databricks built-in visualization capabilities right on top of a SQL query to quickly get a visual sense for which zip codes pay the highest taxes and what those taxes are made up of.\n\n<a href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/346304/2168141618055194/484361/latest.html\"><img class=\"size-full wp-image-8046\" src=\"https://databricks.com/wp-content/uploads/2016/06/zip-code-analysis-in-databricks.png\" alt=\"Zip code analysis in Databricks\" width=\"810\" height=\"486\" /></a>\n<p style=\"text-align: center;\"><em>Built-in visualization in Databricks, zip codes truncated to simplify analysis.</em></p>\nThese visualizations provide the insights that we will leverage along our path to building two machine learning models: First we will utilize linear regression to demonstrate the general process of setting up a machine learning model outside of a pipeline. It will also set a nice baseline for us to understand the predictive power of a simple linear approach in this context.\n\nAfter exploring a simple linear model, we\u2019ll take a step deeper and show how you can create a pipeline that will try a variety of different versions of the same model - a process called hyperparameter tuning - to yield better results. We\u2019ll use this technique to tune a random forest regression model in an automated fashion. We\u2019ll then compare the two models to see how they perform against one another.\n<h2>What\u2019s Next</h2>\nYou can work through the examples in this guide with the Databricks platform (<a href=\"https://databricks.com/try-databricks\">Sign-up</a> to try for free). The next guide in the series will tour the workflow that a data engineer will commonly leverage while using the Databricks platform. We\u2019ll walk through building an ETL pipeline by connecting to a variety of data sources. We will also go through the process of creating UDFs to handle messy date time formats and combining with another dataset. The guide will have plenty of code and explanations, so stay tuned by <a href=\"https://twitter.com/databricks\" target=\"_blank\">following us on Twitter</a> or <a href=\"http://go.databricks.com/newsletter-registration\">subscribing to our newsletter</a>!"}
{"status": "publish", "description": "This guide helps you get started with Apache Spark and Databricks in six easy steps.", "creator": "denny", "link": "https://databricks.com/blog/2016/06/30/introducing-getting-started-with-apache-spark-on-databricks.html", "authors": null, "id": 8072, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2016-06-30", "tz": "UTC", "createdOn": "2016-06-30"}, "title": "Introducing Getting Started with Apache Spark on Databricks", "slug": "introducing-getting-started-with-apache-spark-on-databricks", "content": "We are proud to introduce the <a href=\"https://databricks.com/product/getting-started-with-apache-spark-on-databricks\">Getting Started with Apache Spark on Databricks Guide</a>.  This step-by-step guide illustrates how to leverage the Databricks\u2019 platform to work with Apache Spark.  Our just-in-time data platform simplifies common challenges when working with Spark: data integration, real-time experimentation, and robust deployment of production applications.\n\nDatabricks provides a simple, just-in-time data platform designed for data analysts, data scientists, and engineers.  Using Databricks, this step-by-step guide helps you solve real-world Data Sciences and Data Engineering scenarios with Apache Spark. It will help you familiarize yourself with the Spark UI, learn how to create Spark jobs, load data and work with Datasets, get familiar with Spark\u2019s DataFrames and Datasets API, run machine learning algorithms, and understand the basic concepts behind Spark Streaming. \n\nInstead of worrying about spinning up clusters, maintaining clusters, tracking code history, or upgrading to new Spark versions, you can start writing Spark queries instantly and focus on your data problems.\n\n<a href=\"https://databricks.com/product/getting-started-with-apache-spark-on-databricks\"><img src=\"https://databricks.com/wp-content/uploads/2016/06/Getting-Started-with-Apache-Spark-on-Databricks-thumbnail.jpeg\" alt=\"Visit the Getting Started with Apache Spark on Databricks Guide\" width=\"800\" height=\"280\" class=\"aligncenter size-full wp-image-8073\" /></a>\n\nThe guide helps you get started with Apache Spark and Databricks in six easy steps.  It will first provide a quick start on how to use open source Apache Spark and then leverage this knowledge to learn how to use Spark DataFrames with Spark SQL. In time for Spark 2.0, we also will discuss how to use Datasets and how DataFrames and Datasets are now unified. The guide also has quick starts for Machine Learning and Streaming so you can easily apply them to your data problems. Each of these modules refers to <strong>standalone notebooks</strong> and <strong>datasets</strong> so you can jump ahead if you feel comfortable:\n\n<ul>\n<li>Quick Start: <a href=\"https://databricks.com/product/getting-started-with-apache-spark-on-databricks/quick-start\">Quick Start into Apache Spark using Python or Scala</a></li>\n<li>Datasets: <a href=\"https://databricks.com/product/getting-started-with-apache-spark-on-databricks/datasets\">Examining IoT Device Using Datasets</a></li>\n<li>DataFrames: <a href=\"https://databricks.com/product/getting-started-with-apache-spark-on-databricks/dataframes\">Analyzing City Population vs. Median Home Sale Price using DataFrames</a></li>\n<li>Machine Learning: <a href=\"https://databricks.com/product/getting-started-with-apache-spark-on-databricks/machine-learning\">Performing Linear Regression on City Population vs. Median Home Sale Price</a></li>\n<li>Streaming: <a href=\"https://databricks.com/product/getting-started-with-apache-spark-on-databricks/streaming\">Jump Start into Spark Streaming Performing a Streaming Wordcount</a></li>\n<li>What\u2019s Next: <a href=\"https://databricks.com/product/getting-started-with-apache-spark-on-databricks/whats-next\">Additional resources to learn more about Apache Spark</a></li>\n</ul>\n\nWe hope you enjoy the <a href=\"https://databricks.com/product/getting-started-with-apache-spark-on-databricks\">Getting Started with Apache Spark on Databricks Guide</a> and we will continue updating it with new notebooks and samples as Apache Spark grows."}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2016/07/06/new-ebook-released-lessons-for-large-scale-machine-learning-deployments-on-apache-spark.html", "authors": null, "id": 8129, "categories": ["Announcements", "Company Blog"], "dates": {"publishedOn": "2016-07-06", "tz": "UTC", "createdOn": "2016-07-06"}, "title": "New eBook Released: Lessons for Large-Scale Machine Learning Deployments on Apache Spark", "slug": "new-ebook-released-lessons-for-large-scale-machine-learning-deployments-on-apache-spark", "content": "We are excited to announce that the third eBook in our technical blog book series, <em>Lessons for Large-Scale Machine Learning Deployments on Apache Spark</em>, has been released today!\n\n<a href=\"http://go.databricks.com/large-scale-machine-learning-deployments-spark-databricks\"><img src=\"https://databricks.com/wp-content/uploads/2016/06/Lessons-for-Large-Scale-Machine-Learning-Deployments-on-Apache-Spark-cover.jpg\" alt=\"Cover of Lessons for Large-Scale Machine Learning Deployments for Apache Spark\" width=\"700\" height=\"541\" class=\"aligncenter size-full wp-image-8131\" /></a>\n\n<a href=\"http://go.databricks.com/large-scale-machine-learning-deployments-spark-databricks\" target=\"_blank\">You can download the eBook here.</a>\n\nThis eBook, the third of a series, picks up where the <a href=\"http://go.databricks.com/mastering-advanced-analytics-apache-spark\" target=\"_blank\">second book</a> left off on the topic of advanced analytics, and jumps straight into practical tips for performance tuning and powerful integrations with other machine learning tools - including the popular deep learning framework TensorFlow and the python library scikit-learn. The second section of the book is devoted to addressing the roadblocks in developing machine learning algorithms on Apache Spark - from simple visualizations to modeling audiences with Apache Spark machine learning pipelines. Finally, the eBook showcases a selection of Spark machine learning use cases from ad tech, retail, financial services, and many other industries.\n\n<a href=\"http://go.databricks.com/large-scale-machine-learning-deployments-spark-databricks\"><img src=\"https://databricks.com/wp-content/uploads/2016/06/Lessons-for-Large-Scale-Machine-Learning-Deployments-on-Apache-Spark-eBook-sample.jpg\" alt=\"Sample of Lessons for Large-Scale Machine Learning Deployments on Apache Spark\" width=\"914\" height=\"708\" class=\"aligncenter size-full wp-image-8132\" /></a>\n\nAs with the past eBooks, we\u2019ve augmented the blogs with code examples in Databricks notebooks, which are complimentary with the eBook download. A sample of these notebooks include:\n\n<ul>\n<li><a href=\"http://go.databricks.com/hubfs/notebooks/Samples/Miscellaneous/blog_post_cv.html\" target=\"_blank\">Distributed cross-validation when training a classifier using Apache Spark and scikit-learn</a></li>\n<li><a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/Samples/Miscellaneous/On-Time_Flight_Performance.html?t=1458158073016\" target=\"_blank\">On-Time Flight Performance with GraphFrames for Apache Spark</a></li>\n<li><a href=\"https://docs.cloud.databricks.com/docs/latest/sample_applications/04%20Apache%20Spark%202.0%20Examples/05%20Approximate%20Quantile.html\" target=\"_blank\">Approximate Algorithms in Apache Spark: HyperLogLog and Quantiles</a></li>\n</ul>\n\nDownload the eBook to get started on your next advanced analytics project today. To try out the code examples, <a href=\"https://databricks.com/try-databricks\" target=\"_blank\">sign-up for Databricks</a> and import the notebooks. If you have not read the <a href=\"https://databricks.com/resources/ebooks\">previous eBooks</a> in the series, check them out to get a solid foundation!"}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2016/07/05/edmunds-com-leverages-databricks-to-improve-vehicle-data-quality-and-customer-experience.html", "authors": null, "id": 8135, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2016-07-05", "tz": "UTC", "createdOn": "2016-07-05"}, "title": "Edmunds.com Leverages Databricks to Improve Vehicle Data Quality and Customer Experience", "slug": "edmunds-com-leverages-databricks-to-improve-vehicle-data-quality-and-customer-experience", "content": "<img class=\"aligncenter size-full wp-image-8136\" style=\"border: 1px solid #c2c2c2;\" src=\"https://databricks.com/wp-content/uploads/2016/07/Edmunds_Databricks.png\" alt=\"Edmunds_Databricks\" width=\"654\" height=\"345\" />\n\nWe are happy to announce that <a href=\"http://www.edmunds.com/\" target=\"_blank\">Edmunds.com</a> has deployed Databricks to simplify the management of their Apache Spark clusters and perform ad-hoc analysis to improve vehicle data integrity and improve the overall customer experience of their website.\n\n<a href=\"http://www.marketwired.com/press-release/edmundscom-leverages-databricks-improve-vehicle-data-quality-customer-experience-2139704.htm\" target=\"_blank\">You can read the press release here.</a>\n\nEdmunds.com, a leading car information and shopping network that serves nearly 20 million visitors each month, allows shoppers to browse dealer inventory, vehicle reviews, shopping tips, photos, videos, and feature stories.\n\nTo ensure shopper satisfaction, accurate vehicle data is of utmost importance. Edmunds.com solves their data quality issues on vehicle listing pages by matching a car\u2019s VIN (vehicle identification number) against OEM (original equipment manufacturer) and Edmunds codes to identify critical information about the vehicle, such as the country it was built, vehicle year, and more. If done accurately, providing this kind of detailed vehicle information makes Edmunds.com extremely valuable in a shopper\u2019s vehicle buying process.\n\nOver the past couple years, Edmunds.com\u2019s data volumes have grown tenfold from 10\u2019s to 100\u2019s of TB, making it increasingly difficult to accurately decode each VINs and match them to the right vehicle feature codes \u2014 resulting in missing or inaccurate details which impacted the customer experience. For example, determining what percentage of Subarus are missing the options details or how many of their Hondas do not have their exterior color described are some of the problems that the Edmunds.com engineering team was trying to fix.\n\nTo solve this data integrity problem, Edmunds.com looked to Apache Spark for processing speed at scale. However, they realized that in order for their analysts and data professionals to focus on the data and the business simultaneously, they needed a comprehensive data platform that provided managed services to simplify their Spark deployment and increase their productivity.\n\nWith the implementation of Databricks, Edmunds.com was able to democratize data access across their organization, allowing its data engineering, data science, and business analyst teams to work collaboratively on the data at scale. Edmunds.com also achieved the following quantitative results:\n<ul>\n \t<li>Accelerated ad hoc data exploration and analysis by six-fold allowing them to answer data integrity questions faster;</li>\n \t<li>Improved reporting speed by reducing processing time by 60 percent, or an average of 3-5 hours per week for the engineering team;</li>\n \t<li>Improved vehicle data quality metrics across their website by 35 percent.</li>\n</ul>\nDownload <a href=\"http://go.databricks.com/case-studies/edmunds\">this case study</a> to learn more about how Edmunds.com is using Databricks.\n\nTo try out Databricks for yourself, <a href=\"[registration_url]\">sign up today</a>!"}
{"status": "publish", "description": null, "creator": "admin", "link": "https://databricks.com/blog/2016/07/07/sparkr-tutorial-at-user-2016.html", "authors": null, "id": 8150, "categories": ["Ecosystem", "Engineering Blog"], "dates": {"publishedOn": "2016-07-07", "tz": "UTC", "createdOn": "2016-07-07"}, "title": "SparkR Tutorial at useR 2016", "slug": "sparkr-tutorial-at-user-2016", "content": "AMPLab and Databricks gave a <a href=\"http://user2016.org/tutorials/11.html\" target=\"_blank\">tutorial on SparkR</a> at the useR conference. The conference was held from June 27 - June 30 at Stanford. In this blog post, we provide high-level introductions along with pointers to the training material and some findings from a survey we conducted during the tutorial.\n\n<h2>Part I: Data Exploration</h2>\n\nThe first part of the tutorial was about big data exploration with SparkR. We started the tutorial with a <a href=\"http://www.slideshare.net/databricks/use-r-tutorial-part1-introduction-to-sparkr\" target=\"_blank\">presentation introducing SparkR</a>. This included an overview of SparkR architecture and introduced three types of machine learning that is possible with SparkR:\n\n<ul>\n<li>Big Data, Small Learning</li>\n<li>Partition, Aggregate</li>\n<li>Large Scale Machine Learning</li>\n</ul>\n\nThe hands-on exercise started with a brief overview of Databricks Workspace. We used R Notebooks in <a href=\"[community_edition_signup_url]\">Databricks Community Edition</a> to run R and SparkR commands. It is a free service that supports running Spark in Scala/Python and R.\n\nParticipants started by importing the <a href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/4445213449192764/3183285071251547/936056/latest.html\">first notebook</a> into their workspace. As you can see in this notebook, we started by reading the one million songs dataset as a Apache Spark DataFrame and visually explored it with two techniques:\n\n<ul>\n<li>Summarizing and visualizing</li>\n<li>Sampling and visualizing</li>\n</ul>\n\nThe notebook introduces both techniques with practical examples and ends with a few exercises.\n\n<h2>Part II: Advanced Analytics</h2>\n\nIn the <a href=\"https://docs.google.com/presentation/d/1parLAcwxT9Qsbxl-VAdBz0g9ATt3Kmyvor38wartxsc/edit?usp=sharing\" target=\"_blank\">second part of the tutorial</a> we introduced machine learning algorithms that are available in SparkR. These include the SparkML algorithms that are exposed to R users through a natural R interface. For example, SparkR users can take advantage of a distributed GLM implementation just the same way they would use existing glmnet package. We also introduced two new powerful API that have been added to SparkR in Apache Spark 2.0.\n\n<ul>\n<li>dapply used for applying an R function on all partitions of Spark DataFrame in parallel</li>\n<li>spark.lapply used for parallelizing R functions in multiple machines/workers</li>\n</ul>\n\nThe <a href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/4445213449192764/306258535914851/936056/latest.html\">second notebook</a> again used the Million Songs dataset to do K-Means clustering and also built a predictive model using GLM. Like the first part, it ends with a few exercises for further practice.\n\n<h2>Survey Results</h2>\n\n<img src=\"https://databricks.com/wp-content/uploads/2016/07/sparkr-tutorial-participants.png\" alt=\"A chart showing the distribution of the number of survey participants by job title.\" width=\"960\" height=\"400\" class=\"aligncenter size-full wp-image-8152\" />\n\nHere is a short summary of survey responses. More than half of the attendees were data scientists, and about 20% were students. When asked about their use cases of R, every one listed \u201cdata cleaning and wrangling\u201d as a use case. The majority (~80%) also included \u201cdata exploration\u201d and \u201cpredictive analytics\u201d as their uses for R. A large majority of participants indicated that they load their data into R, from local filesystem. Loading from RDBMS systems was second in popularity with 60%.\n\nMajority of participants were dplyr users, and about 60% indicated that they prefer hadleyverse for data cleaning and wrangling. When asked about how they communicate their findings, the most popular method is publishing R plots in slides/documents and closely after is sharing rMarkdown files.\n\n<img src=\"https://databricks.com/wp-content/uploads/2016/07/familiarity-with-sparkr.png\" alt=\"Survey results outlining how familiar users were with SparkR.\" width=\"960\" height=\"400\" class=\"aligncenter size-full wp-image-8153\" />\n\nMore than half of the attendees had never used SparkR or MLLib and about 25% were actively considering both. We hope this tutorial was helpful to the attendees.\n\n<h2>What\u2019s Next?</h2>\n\nIf you want to try these notebooks do the following:\n\n<ol>\n<li>Sign up for the <a href=\"[community_edition_signup_url]\">Databricks Community Edition</a></li>\n<li>Import SparkR tutorials <a href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/4445213449192764/3183285071251547/936056/latest.html\" target=\"_blank\">part-1</a> and <a href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/4445213449192764/306258535914851/936056/latest.html\" target=\"_blank\">part-2</a> into Databricks Community Edition</li>\n</ol>"}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2016/07/12/sparkr-on-demand-webinar-and-faq.html", "authors": null, "id": 8168, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2016-07-12", "tz": "UTC", "createdOn": "2016-07-12"}, "title": "Apache SparkR On-Demand Webinar and FAQ", "slug": "sparkr-on-demand-webinar-and-faq", "content": "Two months ago we held a live webinar \u2013 <a href=\"http://go.databricks.com/enabling-exploratory-analysis-of-large-data-with-r-and-spark\">Enabling Exploratory Analysis of Large Data with Apache Spark and R</a> \u2013 to demonstrate one of the most important use cases of SparkR: the exploratory analysis of very large data. The webinar shows how Spark\u2019s features and capabilities, such as caching distributed data and integrated SQL execution, complement R\u2019s great tools such as visualization and diverse packages in a real world data analysis project with big data.\n\nThe webinar is <a href=\"http://go.databricks.com/enabling-exploratory-analysis-of-large-data-with-r-and-spark\">accessible on-demand</a>.\u00a0Its slides and sample notebooks are also downloadable as attachments to the webinar. Try out the notebooks with\u00a0<a href=\"http://www.databricks.com/try\">free access to Databricks</a>.\n\nWe have answered the common questions raised by webinar viewers below. If you have additional questions, please check out the <a href=\"https://forums.databricks.com/\">Databricks Forum</a>.\n\n<strong>Common webinar questions and answers</strong>\n\nClick on the question to see answer:\n<ul>\n \t<li><a href=\"https://forums.databricks.com/questions/8279/r-can-reside-on-my-local-pc-and-collect-from-spark.html#answer-8280\">R can reside on my local PC, and collect from Spark in AWS, correct?</a></li>\n \t<li><a href=\"https://databricks.com/blog/2015/09/16/apache-spark-1-5-dataframe-api-highlights.html\">Is it possible to work with RDD in R? For example, I want to do the reduceByKey on a huge dataset.</a></li>\n \t<li><a href=\"https://forums.databricks.com/questions/8283/any-new-mllib-functions-being-exposed-to-sparkr-in.html#answer-8284\">Any new MLlib functions being exposed to SparkR in 2.0? or just kmeans (already have glm).</a></li>\n \t<li><a href=\"https://forums.databricks.com/questions/8286/how-do-i-access-the-temp-table-created-in-r-via-sc.html#answer-8287\">How do I access the temp table created in R via Scala/Spark-Shell? I mean how do I get same Spark Context in R and Scala?</a></li>\n \t<li><a href=\"https://forums.databricks.com/questions/8309/how-to-find-r-etl-example-notebook.html#answer-8310\">The attached notebook references the ETL notebook for instructions on how to get the songsTable. Is there a link somewhere to the ETL notebook?</a></li>\n</ul>"}
{"status": "publish", "description": null, "creator": "jules_damji", "link": "https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html", "authors": null, "id": 8179, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2016-07-14", "tz": "UTC", "createdOn": "2016-07-14"}, "title": "A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets", "slug": "a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets", "content": "[dbce_cta href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/IoTDeviceGeoIPDS2.0.html\"]Try this notebook in Databricks[/dbce_cta]\n\nOf all the developers\u2019 delight, what constitutes as most attractive one is a set of APIs that make developers productive, that is easy to use, and that is intuitive and expressive. One of Apache Spark\u2019s appeal to developers has been its easy-to-use APIs, for operating on large datasets, across languages: Scala, Java, Python, and R.\n\nIn this blog, I explore three sets of APIs\u2014RDDs, DataFrames, and Datasets\u2014available in a pre-release <a href=\"https://spark.apache.org/news/spark-2.0.0-preview.html\" target=\"_blank\">preview of Apache Spark 2.0</a>; why and when you should use each set; outline their performance and optimization benefits; and enumerate scenarios when to use DataFrames and Datasets instead of RDDs. Mostly, I will focus on DataFrames and Datasets, because in <a href=\"http://go.databricks.com/videos/apache-spark-2\" target=\"_blank\">Apache Spark 2.0</a>, these two APIs are unified.\n\nOur primary motivation behind this unification is our quest to simplify Spark by limiting the number of concepts that you have to learn and by offering ways to process structured data. And through structure, Spark can offer higher-level abstraction and APIs as domain specific language constructs.\n<h2>Resilient Distributed Dataset (RDD)</h2>\nRDD was the primary user-facing API in Spark since its inception. At the core, an RDD is an immutable distributed collection of elements of your data, partitioned across nodes in your cluster that can be operated in parallel with a low-level API that offers <em>transformations</em> and <em>actions</em>.\n<h3>When to use RDDs?</h3>\nConsider these scenarios or common use cases for using RDDs when:\n<ul>\n \t<li>you want low-level transformation and actions and control on your dataset;</li>\n \t<li>your data is unstructured, such as media streams or streams of text;</li>\n \t<li>you want to manipulate your data with functional programming constructs than domain specific expressions;</li>\n \t<li>you don\u2019t care about imposing a schema, such as columnar format, while processing or accessing data attributes by name or column; and</li>\n \t<li>you can forgo some optimization and performance benefits available with DataFrames and Datasets for structured and semi-structured data.</li>\n</ul>\n<h3>What happens to RDDs in Apache Spark 2.0?</h3>\nYou may ask: Are RDDs being relegated as second class citizens? Are they being deprecated?\n\nThe answer is a resounding <strong>NO!</strong>\n\nWhat\u2019s more, as you will note below, you can seamlessly move between DataFrame or Dataset and RDDs at will\u2014by simple API method calls\u2014and DataFrames and Datasets are built on top of RDDs.\n<h2>DataFrames</h2>\nLike an RDD, a <a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\">DataFrame</a> is an immutable distributed collection of data. Unlike an RDD, data is organized into named columns, like a table in a relational database. Designed to make large data sets processing even easier, DataFrame allows developers to impose a structure onto a distributed collection of data, allowing higher-level abstraction; it provides a domain specific language API to manipulate your distributed data; and makes Spark accessible to a wider audience, beyond specialized data engineers.\n\nIn our preview of <a href=\"http://go.databricks.com/apache-spark-2.0-presented-by-databricks-co-founder-reynold-xin\" target=\"_blank\">Apache Spark 2.0 webinar</a> and <a href=\"https://databricks.com/blog/2016/05/11/apache-spark-2-0-technical-preview-easier-faster-and-smarter.html\">subsequent blog</a>, we mentioned that in Spark 2.0, DataFrame APIs will merge with <a href=\"https://databricks.com/blog/2016/01/04/introducing-spark-datasets.html\">Datasets</a> APIs, unifying data processing capabilities across libraries. Because of this unification, developers now have fewer concepts to learn or remember, and work with a single high-level and type-safe API called Dataset.\n\n<img class=\"aligncenter size-full wp-image-8114\" src=\"https://databricks.com/wp-content/uploads/2016/06/Unified-Apache-Spark-2.0-API-1.png\" alt=\"Diagram of the Unified Dataset API in Apache Spark 2.0\" width=\"1024\" height=\"573\" />\n<h2>Datasets</h2>\nStarting in Spark 2.0, Dataset takes on two distinct APIs characteristics: a <strong><em>strongly-typed</em></strong> API and an <strong><em>untyped</em></strong> API, as shown in the table below. Conceptually, consider DataFrame as an <em>alias</em> for a collection of generic objects <em>Dataset[Row]</em>, where a <em>Row</em> is a generic <em><strong>untyped</strong></em> JVM object. Dataset, by contrast, is a collection of <strong><em>strongly-typed</em></strong> JVM objects, dictated by a case class you define in Scala or a class in Java.\n<h3>Typed and Un-typed APIs</h3>\n<table class=\"table\">\n<thead>\n<tr>\n<th>Language</th>\n<th>Main Abstraction</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Scala</td>\n<td>Dataset[T] &amp; DataFrame (alias for Dataset[Row])</td>\n</tr>\n<tr>\n<td>Java</td>\n<td>Dataset[T]</td>\n</tr>\n<tr>\n<td>Python*</td>\n<td>DataFrame</td>\n</tr>\n<tr>\n<td>R*</td>\n<td>DataFrame</td>\n</tr>\n</tbody>\n</table>\n<p style=\"text-align: center;\"><em><strong>Note:</strong> Since Python and R have no compile-time type-safety, we only have untyped APIs, namely\u00a0DataFrames.</em></p>\n\n<h2>Benefits of Dataset APIs</h2>\nAs a Spark developer, you benefit with the DataFrame and Dataset unified APIs in Spark 2.0 in a number of ways.\n<h3>1. Static-typing and runtime type-safety</h3>\nConsider static-typing and runtime safety as a spectrum, with SQL least restrictive to Dataset most restrictive. For instance, in your Spark SQL string queries, you won\u2019t know a syntax error until runtime (which could be costly), whereas in DataFrames and Datasets you can catch errors at compile time (which saves developer-time and costs). That is, if you invoke a function in DataFrame that is not part of the API, the compiler will catch it. However, it won\u2019t detect a non-existing column name until runtime.\n\nAt the far end of the spectrum is Dataset, most restrictive. Since Dataset APIs are all expressed as lambda functions and JVM typed objects, any mismatch of typed-parameters will be detected at compile time. Also, your analysis error can be detected at compile time too, when using Datasets, hence saving developer-time and costs.\n\nAll this translates to is a spectrum of type-safety along syntax and analysis error in your Spark code, with Datasets as most restrictive yet productive for a developer.\n\n<img class=\"aligncenter size-full wp-image-8182\" src=\"https://databricks.com/wp-content/uploads/2016/07/sql-vs-dataframes-vs-datasets-type-safety-spectrum.png\" alt=\"Type-safety spectrum between SQL, DataFrames and Datasets\" width=\"902\" height=\"378\" />\n<h3>2. High-level abstraction and custom view into structured and semi-structured data</h3>\nDataFrames as a collection of <em>Datasets[Row]</em> render a structured custom view into your semi-structured data. For instance, let\u2019s say, you have a huge IoT device event dataset, expressed as JSON. Since JSON is a semi-structured format, it lends itself well to employing Dataset as a collection of strongly typed-specific <em>Dataset[DeviceIoTData]</em>.\n<pre>{\"device_id\": 198164, \"device_name\": \"sensor-pad-198164owomcJZ\", \"ip\": \"80.55.20.25\", \"cca2\": \"PL\", \"cca3\": \"POL\", \"cn\": \"Poland\", \"latitude\": 53.080000, \"longitude\": 18.620000, \"scale\": \"Celsius\", \"temp\": 21, \"humidity\": 65, \"battery_level\": 8, \"c02_level\": 1408, \"lcd\": \"red\", \"timestamp\" :1458081226051}</pre>\nYou could express each JSON entry as <em>DeviceIoTData</em>, a custom object, with a Scala case class.\n<pre>case class DeviceIoTData (battery_level: Long, c02_level: Long, cca2: String, cca3: String, cn: String, device_id: Long, device_name: String, humidity: Long, ip: String, latitude: Double, lcd: String, longitude: Double, scale:String, temp: Long, timestamp: Long)</pre>\nNext, we can read the data from a JSON file.\n<pre>// read the json file and create the dataset from the \n// case class DeviceIoTData\n// ds is now a collection of JVM Scala objects DeviceIoTData\nval ds = spark.read.json(\u201c/databricks-public-datasets/data/iot/iot_devices.json\u201d).as[DeviceIoTData]</pre>\nThree things happen here under the hood in the code above:\n<ol>\n \t<li>Spark reads the JSON, infers the schema, and creates a collection of DataFrames.</li>\n \t<li>At this point, Spark converts your data into <em>DataFrame = Dataset[Row]</em>, a collection of generic Row object, since it does not know the exact type.</li>\n \t<li>Now, Spark converts the <em>Dataset[Row] -&gt; Dataset[DeviceIoTData]</em> <em><strong>type-specific</strong></em> Scala JVM object, as dictated by the <strong>class</strong> <em>DeviceIoTData</em>.</li>\n</ol>\nMost of us have who work with structured data are accustomed to viewing and processing data in either columnar manner or accessing specific attributes within an object. With Dataset as a collection of <em>Dataset[ElementType] typed objects</em>, you seamlessly get both compile-time safety and custom view for strongly-typed JVM objects. And your resulting <strong><em>strongly-typed</em></strong> <em>Dataset[T]</em> from above code can be easily displayed or processed with high-level methods.\n\n<img class=\"aligncenter size-full wp-image-8183\" src=\"https://databricks.com/wp-content/uploads/2016/07/displaying-a-dataset.png\" alt=\"Screenshot of a table visualization of a Dataset in Databricks\" width=\"800\" height=\"297\" />\n<h3>3. Ease-of-use of APIs with structure</h3>\nAlthough structure may limit control in what your Spark program can do with data, it introduces rich semantics and an easy set of domain specific operations that can be expressed as high-level constructs. Most computations, however, can be accomplished with Dataset\u2019s high-level APIs. For example, it\u2019s much simpler to perform <code>agg</code>, <code>select</code>, <code>sum</code>, <code>avg</code>, <code>map</code>, <code>filter</code>, or <code>groupBy</code> operations by accessing a Dataset typed object\u2019s <em>DeviceIoTData</em> than using RDD rows\u2019 data fields.\n\nExpressing your computation in a domain specific API is far simpler and easier than with relation algebra type expressions (in RDDs). For instance, the code below will <code>filter() and\u00a0</code>\u00a0<code>map()</code>\u00a0create another immutable Dataset.\n<pre>// Use filter(), map(), groupBy() country, and compute avg() \n// for temperatures and humidity. This operation results in \n// another immutable Dataset. The query is simpler to read, \n// and expressive\n\nval dsAvgTmp = ds.filter(d =&gt; {d.temp &gt; 25}).map(d =&gt; (d.temp, d.humidity, d.cca3)).groupBy($\"_3\").avg()\n\n//display the resulting dataset\ndisplay(dsAvgTmp)</pre>\n<img class=\"aligncenter size-full wp-image-8189\" src=\"https://databricks.com/wp-content/uploads/2016/07/displaying-a-dataset-in-databricks.gif\" alt=\"Example of visualizing a Dataset in Databricks\" width=\"1175\" height=\"642\" />\n<h3>4. Performance and Optimization</h3>\nAlong with all the above benefits, you cannot overlook the space efficiency and performance gains in using DataFrames and Dataset APIs for two reasons.\n\nFirst, because DataFrame and Dataset APIs are built on top of the Spark SQL engine, it uses Catalyst to generate an optimized logical and physical query plan. Across R, Java, Scala, or Python DataFrame/Dataset APIs, all relation type queries undergo the same code optimizer, providing the space and speed efficiency. Whereas the <em>Dataset[T]</em> typed API is optimized for data engineering tasks, the untyped <em>Dataset[Row]</em> (an alias of DataFrame) is even faster and suitable for interactive analysis.\n\n<img class=\"aligncenter size-full wp-image-8187\" src=\"https://databricks.com/wp-content/uploads/2016/07/memory-usage-when-caching-datasets-vs-rdds.png\" alt=\"Datasets are much more memory efficient than RDDs\" width=\"800\" height=\"460\" />\n\nSecond, since <a href=\"https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html\">Spark as a compiler</a> understands your Dataset type JVM object, it maps your type-specific JVM object to Tungsten\u2019s internal memory representation using <a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\">Encoders</a>. As a result, Tungsten Encoders can efficiently serialize/deserialize JVM objects as well as generate compact bytecode that can execute at superior speeds.\n<h3>When should I use DataFrames or Datasets?</h3>\n<ul>\n \t<li>If you want rich semantics, high-level abstractions, and domain specific APIs, use DataFrame or Dataset.</li>\n \t<li>If your processing demands high-level expressions, filters, maps, aggregation, averages, sum, SQL queries, columnar access and use of lambda functions on semi-structured data, use DataFrame or Dataset.</li>\n \t<li>If you want higher degree of type-safety at compile time, want typed JVM objects, take advantage of Catalyst optimization, and benefit from Tungsten\u2019s efficient code generation, use Dataset.</li>\n \t<li>If you want unification and simplification of APIs across Spark Libraries, use DataFrame or Dataset.</li>\n \t<li>If you are a R user, use DataFrames.</li>\n \t<li>If you are a Python user, use DataFrames and resort back to RDDs if you need more control.</li>\n</ul>\nNote that you can always seamlessly interoperate or convert from DataFrame and/or Dataset to an RDD, by simple method call <code>.rdd</code>. For instance,\n<pre>// select specific fields from the Dataset, apply a predicate\n// using the where() method, convert to an RDD, and show first 10\n// RDD rows\nval deviceEventsDS = ds.select($\"device_name\", $\"cca3\", $\"c02_level\").where($\"c02_level\" &gt; 1300)\n// convert to RDDs and take the first 10 rows\nval eventsRDD = deviceEventsDS.rdd.take(10)</pre>\n<img class=\"aligncenter size-full wp-image-8190\" src=\"https://databricks.com/wp-content/uploads/2016/07/convert-dataset-to-rdd-on-Databricks.png\" alt=\"Screenshot of Spark converting a Dataset to an RDD on Databricks\" width=\"1999\" height=\"427\" />\n<h2>Bringing It All Together</h2>\nIn summation, the choice of when to use RDD or DataFrame and/or Dataset seems obvious. While the former offers you low-level functionality and control, the latter allows custom view and structure, offers high-level and domain specific operations, saves space, and executes at superior speeds.\n\nAs we examined the lessons we learned from early releases of Spark\u2014how to simplify Spark for developers, how to optimize and make it performant\u2014we decided to elevate the low-level RDD APIs to a high-level abstraction as DataFrame and Dataset and to build this unified data abstraction across \u00a0libraries atop Catalyst optimizer and Tungsten.\n\nPick one\u2014DataFrames and/or Dataset or RDDs APIs\u2014that meets your needs and use-case, but I would not be surprised if you fall into the camp of most developers who work with structure and semi-structured data.\n<h2>What\u2019s Next?</h2>\nYou can <a href=\"https://databricks.com/blog/2016/05/11/apache-spark-2-0-technical-preview-easier-faster-and-smarter.html\">try the pre-release preview of Apache Spark 2.0 on Databricks</a> and <a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/IoTDeviceGeoIPDS2.0.html\" target=\"_blank\">run this accompanying notebook</a>. If you haven\u2019t signed up yet, <a href=\"https://databricks.com/try-databricks\">try Databricks now</a>.\n\nIn the coming weeks, we\u2019ll have a series of blogs on Structured Streaming. Stay tuned."}
{"status": "publish", "description": "A recap of what\u2019s transpired over the last two weeks with Apache Spark from Databricks.", "creator": "jules_damji", "link": "https://databricks.com/blog/2016/07/18/databricks-bi-weekly-digest-71816.html", "authors": null, "id": 8227, "categories": ["Apache Spark", "Ecosystem", "Engineering Blog"], "dates": {"publishedOn": "2016-07-18", "tz": "UTC", "createdOn": "2016-07-18"}, "title": "Databricks Bi-Weekly Digest: 7/18/16", "slug": "databricks-bi-weekly-digest-71816", "content": "Today, we're kicking off a new series: the Databricks Bi-Weekly Digest. Our goal with this digest is to summarize Spark related content, compiled by Databricks, for the community. It will cover Spark technical content: blog posts, meetups tech-talks, conference talks, and noteworthy news articles pertaining to Apache Spark.\n\nHere\u2019s what\u2019s happened in the last two weeks:\n\n<ul>\n<li>A hands on Tutorial how to use SparkR presented at the useR 2016 conference: <a href=\"https://databricks.com/blog/2016/07/07/sparkr-tutorial-at-user-2016.html\">SparkR Tutorials at useR 2016</a>. Peruse through presentations and try the Notebooks in Databricks.</li>\n<li>Vote for <a href=\"http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-2-0-0-RC4-td18317.html\">Apache Spark 2.0 RC4 Update</a> from Reynold Xin is underway.</li>\n<li>JIRAs closed:<ul>\n<li><a href=\"https://issues.apache.org/jira/browse/SPARK-12177\">Experimental support for Kafka 0.10 in Spark 2.0</a></li></ul></li>\n<li>Want to be heard, want to share your thoughts? Please take our <a href=\"https://www.surveymonkey.com/r/spark_survey2016\">Databricks 2016 Apache Spark Survey</a> and make a difference.</li>\n<li>Tim Hunter presented <a href=\"http://www.slideshare.net/databricks/combining-machine-learning-frameworks-with-apache-spark\">Combining Machine Learning Frameworks with Apache Spark</a> at the Hadoop Summit.</li>\n<li>Databricks released <a href=\"https://spark-packages.org/package/databricks/spark-corenlp\">Stanford CoreNLP wrapper for Apache Spark</a> with an <a href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/1233855/3233914658600709/588180/latest.html\">example notebook</a>. Try it on Databricks.</li>\n<li>Joseph Bradley spoke at the NYC Spark Meetup: <a href=\"http://www.slideshare.net/databricks/distributed-ml-in-apache-spark\">Distributed ML in Apache Spark Meetup Tech Talk</a>. Learn about DataFrames in MLlib.\n<li>A blog explained <a href=\"https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html\">A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets</a>. Find out when to use which API and why.</li>\n<li>Three Apache Spark related tech-talks presented at <a href=\"http://www.meetup.com/spark-users/events/231574440/comments/467682517/\">Bay Area Apache Spark Meetup Tech-Talks @ SAP</a>. You can <a href=\"https://www.youtube.com/watch?v=VGzPTRCSVh8\">watch the video</a> on YouTube.</li>\n</ul>\n\n<h2>What\u2019s Next?</h2>\nTo stay abreast with what\u2019s happening with Apache Spark follow us on Twitter <a href=\"https://twitter.com/databricks\">@databricks</a> and visit <a href=\"https://sparkhub.databricks.com\">sparkhub.databricks.com</a>."}
{"status": "publish", "description": null, "creator": "jules_damji", "link": "https://databricks.com/blog/2016/07/20/code-4-san-francisco-hack-nite-highlights.html", "authors": null, "id": 8275, "categories": ["Company Blog", "Events"], "dates": {"publishedOn": "2016-07-20", "tz": "UTC", "createdOn": "2016-07-20"}, "title": "Code 4 San Francisco Hack Nite Highlights", "slug": "code-4-san-francisco-hack-nite-highlights", "content": "[dbce_cta href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/4332292154849829/3601578643761083/latest.html\"]Try this notebook in Databricks[/dbce_cta]\n\nFor a speechwriter, JFK\u2019s words \u201cAsk not what your country can do for you. Ask what you can do for your country\u201d serve as a rhetorical device; for a community organizer, they serve as a spark to ignite change. These immortal words, to some extent, have engendered enduring movements like <a href=\"https://www.peacecorps.gov/\" target=\"_blank\">Peace Corps</a>, <a href=\"https://www.teachforamerica.org/\" target=\"_blank\">Teach for America</a>, and <a href=\"https://www.codeforamerica.org/\" target=\"_blank\">Code for America</a>, with <a href=\"https://www.codeforamerica.org/brigade/\" target=\"_blank\">local brigades</a>.\n\nOne local brigade, <a href=\"http://www.meetup.com/Code-for-San-Francisco-Civic-Hack-Night/events/231988901/\" target=\"_blank\">Code 4 San Francisco</a> (C4SF), meets each week where local community of developers work on civic projects, using datasets from SF Open Data, to make local government agencies efficient.\n\nA couple of weeks ago, C4SF had Hack Nite hosted at <a href=\"http://microsoftreactor.com/\" target=\"_blank\">Microsoft Reactor</a>. And Databricks participated, where over 100 local data scientists gathered to analyze 16 years of San Francisco\u2019s emergency call records during a July 4th weekend, through a hands-on workshop using <a href=\"http://databricks.com/try-databricks\">Databricks Community Edition</a>. SF Open Data Program Manager Jason Lally expressed his thoughts in <a href=\"https://twitter.com/synchronouscity/status/755230369767497728\" target=\"_blank\">this tweet</a> about the hack nite.\n\n<blockquote class=\"twitter-tweet\" data-lang=\"en\">\n<p dir=\"ltr\" lang=\"en\">.<a href=\"https://twitter.com/blueplastic\">@blueplastic</a> of <a href=\"https://twitter.com/databricks\">@databricks</a> gives excellent demo of <a href=\"https://twitter.com/ApacheSpark\">@ApacheSpark</a> using <a href=\"https://twitter.com/hashtag/SF?src=hash\">#SF</a> <a href=\"https://twitter.com/hashtag/opendata?src=hash\">#opendata</a> from the Fire Dept via <a href=\"https://twitter.com/DataSF\">@DataSF</a> <a href=\"https://t.co/OMGobM0rz6\">https://t.co/OMGobM0rz6</a></p>\n\u2014 Jason Lally (@synchronouscity) <a href=\"https://twitter.com/synchronouscity/status/755230369767497728\">July 19, 2016</a></blockquote>\n\n<script src=\"//platform.twitter.com/widgets.js\" async=\"\" charset=\"utf-8\"></script>\n\nLed by Sameer Faroqui, Databricks\u2019 senior instructor and client solution engineer, the data science workshop covered core concepts, through interactive sessions using the Databricks Community Edition, followed by step-by-step exploration of a public data set to answer a question: <em>How did the 4th of July holiday affect demand for Firefighters?</em>\n\nAttendees signed up for Databricks Community edition on spot and uploaded 1.6 GB of fire emergency calls data with over 4 million records from <a href=\"https://data.sfgov.org/Public-Safety/Fire-Department-Calls-for-Service/nuek-vuh3\" target=\"_blank\">SF OpenData</a>. Following a set of guided queries in their individual Python notebooks, and coding with Apache Spark 2.0\u2019s DataFrame APIs, they extracted, explored and examined fire emergency calls data by executing each query on their provisioned Spark cluster. The exploratory workshop was an exercise in extract, transform, and load (ETL), a typical use case workload, and how to use Apache Spark 2.0 DataFrame\u2019s API in Python.\n\n<h2>Exploring Fire Emergency Calls</h2>\n\nLet\u2019s consider a few queries we explored during our workshop, revealing unexpected insights on the 4th of July calls.\n\n<h4>Q: How many different and distinct types of calls were made to the Fire Department?</h4>\n\n<img class=\"aligncenter size-full wp-image-8277\" src=\"https://databricks.com/wp-content/uploads/2016/07/different-type-of-fire-department-calls.png\" alt=\"Output of the different type of fire department calls.\" width=\"1000\" height=\"462\" />\n\n<h4>Q: How many incidents of each call type were there?</h4>\n\n<img class=\"aligncenter size-full wp-image-8278\" src=\"https://databricks.com/wp-content/uploads/2016/07/total-per-incident-type-fire-department-calls.png\" alt=\"Output of the total number of San Francisco fire department calls grouped by incident type.\" width=\"1000\" height=\"420\" />\n\n<h4>Q: How many service calls were logged in the past 7 days?</h4>\n\n<img class=\"aligncenter size-full wp-image-8279\" src=\"https://databricks.com/wp-content/uploads/2016/07/fire-department-calls-last-7-days.gif\" alt=\"Number of fire department calls in the past 7 days\" width=\"2000\" height=\"416\" />\n\nWith elaboration on how Spark executes these queries on clusters, the workshop led the participants through further transformation of data, to gain insightful queries using time analysis and visualization.\n\nData scientists in the crowd learned not only how-to use but also when-to use DataFrames as well as under the hood tips and tricks in performance and optimizing queries. For instance, reading large dataset of parquet files (columnar formatted) is significantly faster than reading CVS or JSON files, and caching DataFrames, once read as parquet files from disk, significantly accelerates the query times.\n\nSwitching seamlessly between DataFrames and SQL queries in their notebooks, the attendees explored SF\u2019s Fire calls\u2014and a few notable insights emerged.\n\n<h4>Q: Which neighborhood in SF generated the most calls last year?</h4>\n\n<img class=\"aligncenter size-full wp-image-8280\" src=\"https://databricks.com/wp-content/uploads/2016/07/fire-department-calls-grouped-by-neighborhood.png\" alt=\"Number of Fire Department calls grouped by neighborhood.\" width=\"1000\" height=\"462\" />\n\nBy joining DataFrames from two distinct datasets, we explored how neighborhood type of <a href=\"https://data.sfgov.org/Public-Safety/Fire-Incidents/wr8u-xric\" target=\"_blank\">incidents data</a> and fire calls data were related.\n\n<h4>Q: What was the primary non-medical reason most people called the fire department from the Tenderloin and Russian Hill last year?</h4>\n\n<img class=\"aligncenter size-full wp-image-8283\" src=\"https://databricks.com/wp-content/uploads/2016/07/tenderloin-fire-department-calls-grouped-by-reason.png\" alt=\"List of fire department calls from the Russian Hill neighborhood in San Francisco grouped by reason.\" width=\"1000\" height=\"468\" />\n\nAs for the Russian Hill, a good number, as you can observe, were false alarms too, with a fourth less calls than Tenderloin.\n\n<img class=\"aligncenter size-full wp-image-8282\" src=\"https://databricks.com/wp-content/uploads/2016/07/russian-hill-fire-department-calls-grouped-by-reason.png\" alt=\"List of fire department calls from the Russian Hill neighborhood in San Francisco grouped by reason.\" width=\"1000\" height=\"465\" />\n\n<h2>Local Projects Making A Difference</h2>\n\nBeyond offering workshop about Apache Spark on Databricks and exploring local emergency calls, the camaraderie in sharing with the members of the local SF brigade was rewarding, too. One member commented on the C4SF meetup page: \u201cGreat session to get you started with Databrick Community Edition and sources of open data in SF.\u201d\n\nOne project, according to C4SF, that emerged from weekly hack nights for the local community is <a href=\"https://adoptadrain.sfwater.org/\" target=\"_blank\">Adopt a Drain</a>, which helps the city avoid flooding by clearing the drains of debris. Pleased with the outcome of the civic project, Jean Walsh of California Public Utilities recognized its merits with <a href=\"https://twitter.com/SFWater/status/739854808895475713\" target=\"_blank\">this tweet</a>.\n\n<blockquote class=\"twitter-tweet\" data-lang=\"en\">\nJean Walsh of <a href=\"https://twitter.com/SFWater\">@SFwater</a> presents certificate of appreciation to <a href=\"https://twitter.com/SFbrigade\">@sfbrigade</a> for Adopt-a-Drain: <a href=\"https://t.co/xkmUZMfFJd\">https://t.co/xkmUZMfFJd</a>\n\n\u2014 CivicMakers (@CivicMakers) <a href=\"https://twitter.com/CivicMakers/status/739510321270788096\">June 5, 2016</a>\n</blockquote>\n\n<script src=\"//platform.twitter.com/widgets.js\" async=\"\" charset=\"utf-8\"></script>\n\nPerusing other <a href=\"http://codeforsanfrancisco.org/projects/\" target=\"_blank\">local civic projects underway</a> that benefit citizenry reassures me that JFK\u2019s immortal words uttered decades ago still inspire many local brigades\u2019 captains, in their purpose and passion to make a difference in their local community\u2014and by extension in their country.\n\nWe hope that people who gathered at this meetup who learned Apache Spark using Databricks to explore <a href=\"https://datasf.org/\" target=\"_blank\">SF OpenData</a> will employ the knowledge acquired in their civic projects.\n\nWe want to thank our co-sponsors Microsoft for hosting us and C4SF\u2019s local brigades for inviting us to be part of this rewarding Hack Nite.\n\n<a href=\"http://www.meetup.com/Code-for-San-Francisco-Civic-Hack-Night/\" target=\"_blank\">Join the C4SF brigade for their next Hack Nite!</a>\n\n<h2>What\u2019s Next?</h2>\n\nIf you missed the Hack Nite, you can still partake in exploring the City of San Francisco open data with Apache Spark 2.0. Here are the steps:\n\n<ol>\n    <li><a href=\"http://databricks.com/try-databricks\">Sign up for Databricks Community Edition</a> (DCE)</li>\n    <li>Import <a href=\"http://bitly.com/sfopenlabs\">this Notebook folder</a> into DCE\n<ol>\n    <li>Run the dataset_mounts notebook</li>\n    <li>Run the File Incident Exploration notebook</li>\n</ol>\n</li>\n    <li>If you want training, visit <a href=\"https://databricks.com/spark/training\">Databricks Training</a> or our training partners, <a href=\"https://newcircle.com/instructor-led-training/apache-spark-development-bootcamp\">Newcircle</a>.</li>\n</ol>"}
{"status": "publish", "description": null, "creator": "admin", "link": "https://databricks.com/blog/2016/07/21/the-new-mongodb-connector-for-apache-spark-in-action-building-a-movie-recommendation-engine.html", "authors": null, "id": 8294, "categories": ["Company Blog", "Partners", "Product"], "dates": {"publishedOn": "2016-07-21", "tz": "UTC", "createdOn": "2016-07-21"}, "title": "The New MongoDB Connector for Apache Spark In Action: Building a Movie Recommendation Engine", "slug": "the-new-mongodb-connector-for-apache-spark-in-action-building-a-movie-recommendation-engine", "content": "[dbce_cta href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/MongoDB_guest_blog/Using_MongoDB_Connector_for_Spark.html\"]Try this notebook in Databricks[/dbce_cta]\n\n<em>This is a repost of a blog from our friends at <a href=\"https://www.mongodb.com\" target=\"_blank\">MongoDB</a>. Sam is the Product Manager for Developer Experience at MongoDB based in New York. </em>\n\n<em>We've added an example of the connector in the Databricks environment <a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/MongoDB_guest_blog/Using_MongoDB_Connector_for_Spark.html\" target=\"_blank\">as a notebook</a>.</em>\n\n<hr />\n\nWe are delighted to announce general availability of the new, native <a href=\"https://github.com/mongodb/mongo-spark\" target=\"_blank\">MongoDB Connector for Apache Spark</a>. It provides higher performance, greater ease of use, and access to more advanced Spark functionality than other connectors. With certification from Databricks, the company founded by the creators of Apache Spark project, developers can focus on building modern, data driven applications, knowing that the connector provides seamless integration and complete API compatibility between Spark processes and MongoDB.\n\nWritten in Scala, Apache Spark\u2019s native language, the Connector provides a more natural development experience for Spark users. The connector exposes all of Spark\u2019s libraries, enabling MongoDB data to be materialized as DataFrames and Datasets for analysis with machine learning, graph, streaming and SQL APIs, further benefiting from automatic schema inference.\n\nThe Connector also takes advantage of MongoDB\u2019s <a href=\"https://docs.mongodb.com/manual/core/aggregation-pipeline/\" target=\"_blank\">aggregation pipeline</a> and <a href=\"https://docs.mongodb.com/manual/indexes/\" target=\"_blank\">rich secondary indexes</a> to extract, filter, and process only the range of data it needs \u2013 for example, analyzing all customers located in a specific geography. This is very different from simple NoSQL data stores that do not offer either secondary indexes or in-database aggregations. In these cases, Apache Spark would need to extract all data based on a simple primary key, even if only a subset of that data is required for the Spark process. This means more processing overhead, more hardware, and longer time-to-insight for the analyst.\n\nTo maximize performance across large, distributed data sets, the Spark connector is aware of data locality in a MongoDB cluster. RDDs are automatically processed on workers co-located with the associated MongoDB shard to minimize data movement across the cluster. The <a href=\"https://docs.mongodb.com/manual/reference/read-preference/\" target=\"_blank\">nearest read preference</a> can be used to route Spark queries to the closest physical node in a MongoDB replica set, thus reducing latency.\n<blockquote>\u201cUsers are already combining Apache Spark and MongoDB to build sophisticated analytics applications. The new native MongoDB Connector for Apache Spark provides higher performance, greater ease of use, and access to more advanced Apache Spark functionality than any MongoDB connector available today.\u201d <small>Reynold Xin, co-founder and chief architect of Databricks</small></blockquote>\nTo demonstrate how to use the connector, we\u2019ve created a tutorial that uses MongoDB together with Apache Spark\u2019s machine learning libraries to build a movie recommendation system. This example presumes you have familiarity with Spark. If you are new to Spark but would like to learn the basics of using Spark and MongoDB together, we encourage you to check out our <a href=\"https://university.mongodb.com/courses/M233/about\" target=\"_blank\">new MongoDB University Course</a>.\n\nYou can explore the tutorial in a Databricks notebook <a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/MongoDB_guest_blog/Using_MongoDB_Connector_for_Spark.html\">here</a>.\n<h2>What's next</h2>\n<ul>\n \t<li><a href=\"http://www.databricks.com/try\">Try Apache Spark on Databricks</a></li>\n \t<li><a href=\"http://www.mongodb.com/downloads\">Download MongoDB from mongodb.com</a></li>\n \t<li><a href=\"http://docs.mongodb.com/spark-connector?_ga=1.257494478.456139522.1467395529\">Read the MongoDB-Spark connector documentation</a></li>\n</ul>"}
{"status": "publish", "description": "We announce the general availability of Spark 2.0 on Databricks. This release builds on what the community has learned in the past two years.", "creator": "rxin", "link": "https://databricks.com/blog/2016/07/26/introducing-apache-spark-2-0.html", "authors": null, "id": 8319, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2016-07-26", "tz": "UTC", "createdOn": "2016-07-26"}, "title": "Introducing Apache Spark 2.0", "slug": "introducing-apache-spark-2-0", "content": "Today, we're excited to announce the general availability of <a href=\"https://spark.apache.org/releases/spark-release-2-0-0.html\">Apache Spark 2.0</a> on Databricks. This release builds on what the community has learned in the past two years, doubling down on what users love and fixing\u00a0the pain points. This post\u00a0summarizes the three major themes\u2014easier, faster, and smarter\u2014that comprise Spark 2.0. We also explore many of them in more detail in our\u00a0<a href=\"https://databricks.com/blog/2016/06/01/preview-apache-spark-2-0-an-anthology-of-technical-assets.html\">anthology of Spark 2.0 content</a>.\n\nTwo months ago, we\u00a0launched a\u00a0preview release of Apache Spark 2.0 on Databricks. As you can see in the chart below,\u00a010% of our clusters are already using this release, as customers experiment with the new features and give us feedback. Thanks to this experience, we are\u00a0excited to be the first commercial vendor to support Spark 2.0.\n\n[caption id=\"attachment_8330\" align=\"aligncenter\" width=\"650\"]<a href=\"https://databricks.com/wp-content/uploads/2016/07/image00.png\"><img src=\"https://databricks.com/wp-content/uploads/2016/07/image00.png\" alt=\"Spark Usage over Time by Release Versions\" width=\"650\" /></a> Apache Spark Usage over Time by Version[/caption]\n\n&nbsp;\n\nNow, let\u2019s dive into what's new in Apache Spark 2.0.\n<h3>Easier: ANSI SQL and Streamlined APIs</h3>\nOne thing we are proud of in Spark is APIs that are simple, intuitive, and expressive. Spark 2.0 continues this tradition, focusing\u00a0on two areas: (1) standard SQL support and (2) unifying DataFrame/Dataset API.\n\nOn the SQL side, we have significantly expanded Spark's SQL support, with the introduction of a new ANSI SQL parser and\u00a0<a href=\"https://databricks.com/blog/2016/06/17/sql-subqueries-in-apache-spark-2-0.html\">subqueries</a>. <strong>Spark 2.0 can run all the 99 TPC-DS queries, which require many of the SQL:2003 features.</strong> Because SQL has been one of the primary interfaces to Spark, these\u00a0extended capabilities drastically reduce the effort of porting legacy applications.\n\nOn the programmatic\u00a0API side, we have streamlined Spark's\u00a0APIs:\n<ul>\n \t<li><strong>Unifying DataFrames and Datasets in Scala/Java:</strong> Starting in Spark 2.0, DataFrame is just a type alias for Dataset of Row. Both the typed methods (e.g. <code>map</code>, <code>filter</code>, <code>groupByKey</code>) and the untyped methods (e.g. <code>select</code>, <code>groupBy</code>) are available on the Dataset class. Also, this new combined Dataset interface is the abstraction used for Structured Streaming. Since compile-time type-safety\u00a0is not a feature in Python and R, the concept of Dataset does not apply to these language APIs. Instead, DataFrame remains the primary interface\u00a0there, and\u00a0is analogous to the single-node data frame notion in these languages. Get a peek from <a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/Dataset.html\" target=\"_blank\">this notebook</a> and <a href=\"https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html\">this blog</a> for the stories behind these APIs.</li>\n \t<li><strong>SparkSession:</strong> a new entry point that supersedes\u00a0SQLContext and HiveContext. For users of the DataFrame API, a common source of confusion for Spark is which \u201ccontext\u201d to use. Now you can use SparkSession, which subsumes both, as a single entry point, as <a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/SparkSession.html\" target=\"_blank\">demonstrated in this notebook</a>. Note that the old SQLContext and HiveContext classes are still kept for backward compatibility.</li>\n \t<li><strong>Simpler, more performant Accumulator API:</strong> We have designed a <a href=\"http://spark.apache.org/docs/2.0.0/api/scala/index.html#org.apache.spark.util.AccumulatorV2\">new Accumulator API</a> that has a simpler type hierarchy and support specialization for primitive types. The old Accumulator API has been deprecated but retained for backward compatibility</li>\n \t<li><strong>DataFrame-based Machine Learning API emerges as the primary ML API:</strong> With Spark 2.0, the <a href=\"http://spark.apache.org/docs/2.0.0/api/scala/index.html#org.apache.spark.ml.package\">spark.ml</a> package, with its \u201cpipeline\u201d APIs, will emerge as the primary machine learning API. While the original spark.mllib package is preserved, future development will focus on the DataFrame-based API.</li>\n \t<li><strong>Machine learning pipeline persistence:</strong> Users can now save and load machine learning pipelines and models across all programming languages supported by Spark. See <a href=\"https://databricks.com/blog/2016/05/31/apache-spark-2-0-preview-machine-learning-model-persistence.html\">this blog post</a> for more details and <a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/ML%20persistence%20in%202.0.html\">this notebook</a> for examples.</li>\n \t<li><strong>Distributed algorithms in R:</strong> Added support for Generalized Linear Models (GLM), Naive Bayes, Survival Regression, and K-Means in R.</li>\n \t<li><strong>User-defined functions (UDFs) in R</strong>: Added support for running partition level UDFs (dapply and gapply) and hyper-parameter tuning (lapply).</li>\n</ul>\n<h3>Faster: Apache Spark as a Compiler</h3>\nAccording to our <a href=\"https://databricks.com/blog/2015/09/24/spark-survey-results-2015-are-now-available.html\" target=\"_blank\">2015 Spark Survey</a>, 91% of users consider performance as the most important aspect of Apache Spark. As a result, performance optimizations have always been a focus in our Spark development. Before we started planning our contributions to Spark 2.0, we asked ourselves a question: <strong>Spark is already pretty fast, but can we push the boundary and make Spark 10X faster?</strong>\n\nThis question led us to fundamentally rethink the way we build Spark\u2019s physical execution layer. When you look into a modern data engine (e.g. Spark or other MPP databases), majority of the CPU cycles are spent in useless work, such as making virtual function calls or reading/writing intermediate data to CPU cache or memory. Optimizing performance by reducing the amount of CPU cycles wasted in these useless work has been a long time focus of modern compilers.\n\nSpark 2.0 ships with the second generation <a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\">Tungsten</a> engine. <strong>This engine builds upon ideas from modern compilers and MPP databases and applies them to Spark workloads.</strong> The main idea is to emit optimized code at runtime that collapses the entire query into a single function, eliminating virtual function calls and leveraging CPU registers for intermediate data. We call this technique \u201c<a href=\"https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html\">whole-stage code generation</a>.\u201d\n\nTo give you a teaser, we have measured the time (in nanoseconds) it takes to process a row on one core for some of the operators in Spark 1.6 vs. Spark 2.0. The table below shows the improvements\u00a0in Spark 2.0. Spark 1.6 also included an expression code generation technique that is used\u00a0in some state-of-the-art commercial databases, but as you can see,\u00a0many operators became an order of magnitude faster with whole-stage code generation.\n\nYou can see the power of whole-stage code generation in action in <a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/Whole-stage%20code%20generation.html\n\" target=\"_blank\">this notebook</a>, in which we perform aggregations and joins on 1 billion records on a single machine.\n\n<figure><figcaption style=\"text-align: center; font-style: italic; margin-bottom: .5em;\">Cost per Row (single thread)</figcaption>\n<table class=\"table\">\n<thead>\n<tr>\n<th>primitive</th>\n<th>Spark 1.6</th>\n<th>Spark 2.0</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>filter</td>\n<td>15ns</td>\n<td>1.1ns</td>\n</tr>\n<tr>\n<td>sum w/o group</td>\n<td>14ns</td>\n<td>0.9ns</td>\n</tr>\n<tr>\n<td>sum w/ group</td>\n<td>79ns</td>\n<td>10.7ns</td>\n</tr>\n<tr>\n<td>hash join</td>\n<td>115ns</td>\n<td>4.0ns</td>\n</tr>\n<tr>\n<td>sort (8-bit entropy)</td>\n<td>620ns</td>\n<td>5.3ns</td>\n</tr>\n<tr>\n<td>sort (64-bit entropy)</td>\n<td>620ns</td>\n<td>40ns</td>\n</tr>\n<tr>\n<td>sort-merge join</td>\n<td>750ns</td>\n<td>700ns</td>\n</tr>\n</tbody>\n</table>\n</figure>How does this new engine work on end-to-end queries? We did some preliminary analysis using TPC-DS queries to compare Spark 1.6 and Spark 2.0:\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/05/preliminary-tpc-ds-spark-2-0-vs-1-6.png\">\n<img class=\"aligncenter size-full wp-image-7218\" src=\"https://databricks.com/wp-content/uploads/2016/05/preliminary-tpc-ds-spark-2-0-vs-1-6.png\" alt=\"Preliminary TPC-DS Spark 2.0 vs 1.6\" width=\"703\" height=\"380\" /></a>\n\nBeyond whole-stage code generation to improve performance, a lot of work has also gone into improving the Catalyst optimizer for general query optimizations such as nullability propagation, as well as a new vectorized Parquet decoder that improved Parquet scan throughput by 3X.\u00a0<a href=\"https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html\">Read this blog post</a>\u00a0for more detail on the optimizations\u00a0in Spark 2.0.\n<h3>Smarter: Structured Streaming</h3>\nSpark Streaming has long led the big data space as one of the first systems\u00a0unifying batch and streaming computation. When its streaming API, called DStreams, was\u00a0introduced in Spark 0.7, it offered developers with several powerful properties: exactly-once semantics, fault-tolerance at scale, strong consistency guarantees and high throughput.\n\nHowever, after working with hundreds of real-world deployments of Spark Streaming, we found that applications that need to make decisions in real-time often require <strong>more than just a streaming engine</strong>. They require deep integration of the batch stack and the streaming stack, interaction\u00a0with external storage systems, as well as the ability to cope with changes in business logic. As a result, enterprises want more than just a streaming engine; instead they need a full stack that enables them to develop end-to-end <strong>\u201ccontinuous applications.\u201d</strong>\n\nSpark 2.0\u00a0tackles these use cases through a new API called Structured Streaming. Compared to existing streaming systems, Structured Streaming makes three key improvements:\n<ol>\n \t<li><strong>Integrated API with batch jobs.</strong>\u00a0To run a streaming computation, developers simply write a batch computation against the DataFrame / Dataset API, and Spark automatically <em>incrementalizes</em>\u00a0the computation to run it in a streaming fashion (i.e. update the result as data comes in). This powerful design\u00a0means that developers don't have to manually manage state, failures, or keeping the application in sync with batch jobs.\u00a0Instead, the\u00a0streaming job\u00a0always gives the same answer as a batch job on the same data.</li>\n \t<li><strong>Transactional interaction\u00a0with storage systems.</strong> Structured Streaming handles fault tolerance and consistency holistically across the engine and storage systems, making it easy to write applications that update a live database used for serving, join in\u00a0static data, or move data reliably\u00a0between storage systems.</li>\n \t<li><strong>Rich integration with the rest of Spark.</strong> Structured Streaming supports interactive queries on streaming data through Spark SQL, joins against static data, and many libraries that already use DataFrames, letting developers build complete applications instead of just streaming pipelines. In the future, expect more integrations with MLlib and other libraries.</li>\n</ol>\nSpark 2.0 ships with an initial, alpha version of Structured Streaming, as a (surprisingly small!) extension to the DataFrame/Dataset API. This\u00a0makes\u00a0it\u00a0easy to adopt for existing Spark users\u00a0that want to answer new questions in real-time. Other key features include support for event-time based processing, out-of-order/delayed data, interactive queries, and interaction with non-streaming data sources and sinks.\n\nWe also updated the Databricks workspace to\u00a0support Structured Streaming. For example, when launching a streaming query, the notebook UI\u00a0will automatically display its status.<a href=\"https://databricks.com/wp-content/uploads/2016/07/image01.png\"><img class=\"alignnone size-full wp-image-8332\" src=\"https://databricks.com/wp-content/uploads/2016/07/image01.png\" alt=\"image01\" width=\"1978\" height=\"834\" /></a>\n\nStreaming is clearly a broad topic, so stay tuned for\u00a0a series of blog posts with\u00a0more details on Structured Streaming in Apache Spark 2.0.\n<h3>Conclusion</h3>\nSpark users initially came to Apache Spark for its ease-of-use and performance. Spark 2.0 doubles down on these while extending it to support an even wider range of workloads. Enjoy the new release on Databricks.\n<h3>Read More</h3>\nYou can also import the following notebooks and try them on\u00a0<a href=\"https://databricks.com/try-databricks\">Databricks Community Edition</a> with Spark 2.0.\n<ul>\n \t<li><a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/SparkSession.html\" target=\"_blank\">SparkSession: A new entry point</a></li>\n \t<li><a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/Dataset.html\" target=\"_blank\">Datasets: A more streamlined API</a></li>\n \t<li><a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/Whole-stage%20code%20generation.html\" target=\"_blank\">Performance of whole-stage code generation</a></li>\n \t<li><a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/ML%20persistence%20in%202.0.html\" target=\"_blank\">Machine learning pipeline persistence</a></li>\n</ul>"}
{"status": "publish", "description": null, "creator": "matei", "link": "https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html", "authors": null, "id": 8406, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2016-07-28", "tz": "UTC", "createdOn": "2016-07-28"}, "title": "Continuous Applications: Evolving Streaming in Apache Spark 2.0", "slug": "continuous-applications-evolving-streaming-in-apache-spark-2-0", "content": "Since its release, Spark Streaming has become <a href=\"http://www.datanami.com/2016/07/07/investments-fast-data-analytics-surge/\">one of the most widely used</a>\u00a0distributed streaming engines, thanks to its high-level API and exactly-once semantics. Nonetheless, as these types of engines became common, we\u2019ve noticed that developers often need more than just a streaming programming model to build real-time applications. At Databricks, we\u2019ve worked with thousands of users to\u00a0understand\u00a0how to simplify real-time applications. In this post, we present the resulting idea, <em>continuous applications</em>, which we have started to implement\u00a0through the <a href=\"https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html\">Structured Streaming API</a> in <a href=\"https://databricks.com/blog/2016/07/26/introducing-apache-spark-2-0.html\">Apache Spark 2.0.</a>\n\nMost streaming engines focus on performing computations on a stream: for example, one can map a stream to run a function on each record, reduce it to aggregate events by time, etc. However, as we worked with users, we found\u00a0that <b>virtually no use case of streaming engines only involved performing computations on a stream</b>. Instead, stream processing happens as part of a larger application, which we\u2019ll call a <i>continuous application</i>. Here are some examples:\n<ol>\n \t<li><b>Updating data that will be served in real-time</b>. For instance, developers might want to update a summary table that users will query through a web application. In this case, much of the complexity is in the interaction between the streaming engine and the serving system: for example, can you run queries on the table while the streaming engine is updating it? The \u201ccomplete\u201d application is a real-time serving system, not a map or reduce on a stream.</li>\n \t<li><b>Extract, transform and load (ETL)</b>. One common use case is continuously moving and transforming data from one storage system to another (e.g. JSON logs\u00a0to an Apache Hive table). This requires careful interaction with both storage systems to ensure no data is duplicated or lost -- much of the logic is in this coordination work.</li>\n \t<li><b>Creating a real-time version of an existing batch job.</b> This is hard\u00a0because many streaming systems don\u2019t guarantee their result will match a batch job. For example, we've seen companies that built live dashboards using a streaming engine and daily reporting using batch jobs, only to have customers complain that their daily report (or worse, their bill!)\u00a0did not match the live metrics.</li>\n \t<li><b>Online machine learning</b>. These continuous applications often combine large static datasets, processed using batch jobs, with real-time data and live prediction serving.</li>\n</ol>\nThese examples show that streaming computations are part of larger applications that include serving, storage, or batch jobs. Unfortunately, in current systems, streaming computations run on their own, in an engine focused just on streaming. This leaves developers responsible for the complex tasks of interacting with external systems (e.g. managing transactions) and making their result consistent with the the rest of the application (e.g., batch jobs). This is what we'd like to solve with continuous applications.\n<h2>Continuous Applications</h2>\nWe define a continuous application as <i>an end-to-end application that reacts to data in real-time</i>. In particular, we\u2019d like developers to use a <i>single programming interface</i> to support the facets of continuous applications that are currently handled in separate systems, such as query serving or interaction with batch jobs. For example, here is how we would handle the use cases above:\n<ol>\n \t<li><b>Updating data that will be served in real time</b>. The developer would write a single Spark application that handles both updates and serving (e.g. through Spark\u2019s <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#running-the-thrift-jdbcodbc-server\">JDBC server</a>), or would use an API that automatically performs transactional updates on a serving system like MySQL, Redis or Apache Cassandra.</li>\n \t<li><b>Extract, transform and load (ETL)</b>. The developer would simply list the transformations required as in a batch job, and the streaming system would handle coordination with both storage systems to ensure exactly-once processing.</li>\n \t<li><b>Creating a real-time version of an existing batch job</b>. The streaming system would guarantee results are always consistent with a batch job on the same data.</li>\n \t<li><b>Online machine learning</b>. The machine learning library would be designed to combine real-time training, periodic batch training, and prediction serving behind the same API.</li>\n</ol>\nThe figure below shows which concerns are usually handled in streaming engines, and which would be needed in continuous applications:\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/07/continuous-apps.png\"><img class=\"alignnone wp-image-8546 size-large\" src=\"https://databricks.com/wp-content/uploads/2016/07/continuous-apps-1024x366.png\" width=\"1024\" height=\"366\" /></a>\n<h2>Structured Streaming</h2>\n<a href=\"https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html\">Structured Streaming</a> is a new high-level API we have contributed to Apache Spark 2.0 to support continuous applications. It is, first, a higher-level API than Spark Streaming, bringing in ideas from the other structured APIs in Spark (DataFrames and Datasets)\u2014most notably, a way to perform database-like query optimizations. More importantly, however, Structured Streaming also incorporates the idea of continuous applications to provide a number of features that no other streaming engines offer.\n<ol>\n \t<li><b>\u00a0Strong guarantees about consistency with batch jobs</b>. Users <a href=\"https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html\">specify a streaming computation by writing a batch computation</a> (using Spark\u2019s DataFrame/Dataset API), and the engine automatically <i>incrementalizes</i> this computation (runs it\u00a0continuously). At any point, the output of the Structured Streaming job is <a href=\"https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html\">the same</a> as running the batch job on a prefix of the input data. Most current streaming systems (e.g. Apache Storm, Kafka Streams, Google Dataflow and Apache Flink) <b>do not</b> provide this \"prefix integrity\" property.</li>\n \t<li><b>Transactional integration with storage systems</b>. We have taken\u00a0care in the internal design to process data exactly once and update output sinks transactionally, so that serving applications always see a consistent snapshot of the data. While the Spark 2.0 release only supports a few data sources (HDFS and S3), we plan to add more in future versions. Transactional updates were one of the top pain points for users of Spark and other streaming systems, requiring <a href=\"http://spark.apache.org/docs/latest/streaming-programming-guide.html#semantics-of-output-operations\">manual work</a>, so we are excited to make these part of the core API.</li>\n \t<li><b>\u00a0Tight integration with the rest of Spark</b>. Structured Streaming supports serving interactive queries on streaming state with <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#running-the-thrift-jdbcodbc-server\">Spark SQL and JDBC</a>, and integrates with <a href=\"http://spark.apache.org/mllib/\">MLlib</a>. These integrations are only beginning in Spark 2.0, but will grow in future releases. Because Structured Streaming builds on DataFrames, many other libraries of Spark will naturally run over it (e.g., all feature transformations in MLlib are written against DataFrames).</li>\n</ol>\nApart from these unique characteristics, Structured Streaming has other new features to simplify streaming, such as explicit support for \u201cevent time\u201d to aggregate out of order data, and richer support for windowing and sessions. Achieving its consistency semantics in a fault-tolerant manner is also not easy\u2014see our <a href=\"https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html\">sister blog post</a> about the API and execution model.\n\nStructured Streaming is still in alpha in Spark 2.0, but we hope you try it out and send feedback. Our team and many other community members will be expanding it in the next few releases.\n<h2>An Example</h2>\nAs a simple example of Structured Streaming, the code below shows an Extract, Transform and Load (ETL) job that converts data from JSON into Apache Parquet. Note how Structured Streaming simply uses the <a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\">DataFrame API</a>, so the code is nearly identical to a batch version.\n\n[row grid=\"yes\"]\n[col xs=\"12\" md=\"6\"]\n\n<center><strong>Streaming Version</strong></center>\n<pre style=\"font-size: 8pt;\">// Read JSON continuously from S3\nlogsDF = spark.readStream.json(\"s3://logs\")\n\n// Transform with DataFrame API and save\nlogsDF.select(\"user\", \"url\", \"date\")\n      .writeStream.parquet(\"s3://out\")\n      .start()</pre>\n[/col]\n[col xs=\"12\" md=\"6\"]\n\n<center><strong>Batch Version</strong></center>\n<pre style=\"font-size: 8pt;\">// Read JSON once from S3\nlogsDF = spark.read.json(\"s3://logs\")\n\n// Transform with DataFrame API and save\nlogsDF.select(\"user\", \"url\", \"date\")\n      .write.parquet(\"s3://out\")\n \n</pre>\n[/col]\n[/row]\n\n&nbsp;\n\nWhile the\u00a0code\u00a0looks deceptively simple, Spark does a lot of work under the hood, such as grouping the data into Parquet partitions, ensuring each record appears in the output exactly once, and recovering the job\u2019s state if you restart it. Finally, to serve this data <i>interactively</i> instead of writing it to Parquet, we could just change writeStream to use the\u00a0(currently alpha) <a href=\"https://spark.apache.org/docs/2.0.0/structured-streaming-programming-guide.html#output-sinks\">in-memory sink</a> and connect a JDBC client to Spark to query it.\n<h2>Long-Term Vision</h2>\nOur long-term vision\u00a0for streaming in Spark is ambitious: <i>we want every library in Spark to work in an incremental fashion on Structured Streaming</i>. Although this is a big goal, Apache Spark is well positioned to achieve it. Its libraries are already built on common, narrow APIs (RDDs and DataFrames), and Structured Streaming is designed explicitly to give results consistent with these unified interfaces.\n\nThe biggest insight in Spark since its beginning is that developers need <strong>unified interfaces</strong><i>.</i> For example, batch computation on clusters used to require many disjoint systems (MapReduce for ETL, Hive for SQL, Giraph for graphs, etc), complicating both development and\u00a0operations. Spark unified these workloads on one engine, greatly simplifying both tasks. The same insight applies to streaming. Because streaming workloads are usually\u00a0part of a much larger continuous application, which may include serving, storage, and batch jobs, we want to offer a unified API and system for building end-to-end continuous applications.\n<h2>Read More</h2>\nOur <a href=\"https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html\">Structured Streaming model</a> blog post explores the streaming API and execution model in more detail. We recommend you read this post to get started with Structured Streaming.\n\nIn addition, the following resources\u00a0cover Structured Streaming:\n<ul>\n \t<li><a href=\"https://spark-summit.org/east-2016/speakers/matei-zaharia/\">Spark 2.0 and Structured Streaming</a></li>\n \t<li><a href=\"https://spark-summit.org/east-2016/events/keynote-day-3/\">Future of Real-time Spark</a></li>\n \t<li><a href=\"https://spark-summit.org/2016/events/structuring-spark-dataframes-datasets-and-streaming/\">Structuring Spark: DataFrames, Datasets and Streaming</a></li>\n \t<li><a href=\"https://spark-summit.org/2016/events/a-deep-dive-into-structured-streaming/\">A Deep Dive Into Structured Streaming</a></li>\n \t<li><a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\">Structured Streaming Programming Guide</a></li>\n</ul>\n&nbsp;"}
{"status": "publish", "description": null, "creator": "matei", "link": "https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html", "authors": null, "id": 8547, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2016-07-28", "tz": "UTC", "createdOn": "2016-07-28"}, "title": "Structured Streaming In Apache Spark", "slug": "structured-streaming-in-apache-spark", "content": "Apache Spark 2.0 adds the first version of a new higher-level API, Structured Streaming, for building <a href=\"https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html\" target=\"_blank\">continuous applications</a>. The main goal is to make it easier to build <i>end-to-end</i> streaming applications, which integrate with storage, serving systems, and batch jobs in a consistent and fault-tolerant way. In this post, we explain why this is hard to do with current distributed streaming engines, and introduce Structured Streaming.\n\n<h2>Why Streaming is Difficult</h2>\n\nAt first glance, building a distributed streaming engine might seem as simple as launching a set\u00a0of servers and pushing data between them. Unfortunately, distributed stream processing runs into multiple complications that don\u2019t affect simpler computations like batch jobs.\n\nTo start, consider a simple application: we receive (phone_id, time, action) events from a mobile app, and want to count how many actions of each type happened each hour, then store the result in MySQL. If we were running this application as a batch job and had a table with all the input events, we could express it as the following SQL query:\n\n<pre>SELECT action, WINDOW(time, \"1 hour\"), COUNT(*)\nFROM events\nGROUP BY action, WINDOW(time, \"1 hour\")\n</pre>\n\nIn a distributed streaming engine, we might set up nodes to process the data in a \"map-reduce\" pattern, as shown below. Each node in the first layer reads a partition of the input data (say, the stream from one set of phones), then hashes the events by (action, hour) to send them to a reducer node, which tracks that group\u2019s count and periodically updates MySQL.\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/07/image00-2.png\"><img class=\"alignnone size-full wp-image-8560\" src=\"https://databricks.com/wp-content/uploads/2016/07/image00-2.png\" alt=\"image00\" width=\"1281\" height=\"672\" /></a>\n\nUnfortunately, this type of design can introduce quite a few challenges:\n\n<ol>\n    <li><strong>Consistency</strong>: This distributed design can cause records to be processed in one part of the system before they\u2019re processed in another, leading to nonsensical results. For example, suppose our app sends an \"open\" event when users open it, and a \"close\" event when closed. If the reducer node responsible for \"open\" is slower than the one for \"close\", we might see a <em>higher total count of \"closes\" than \"opens\" in MySQL</em>, which would not make sense. The image above actually shows one such example.</li>\n    <li><strong>Fault tolerance</strong>: What happens if one of the mappers or reducers fails? A reducer should not count an action in MySQL twice, but should somehow know how to request old data from the mappers when it comes up. Streaming engines go through a great deal of trouble to provide strong semantics here, at least <em>within</em> the engine. In many engines, however, keeping the result consistent in external storage is left to the user.</li>\n    <li><strong>Out-of-order data</strong>: In the real world, data from different sources can come out of order: for example, a phone might upload its data\u00a0hours late if it's\u00a0out of coverage. Just writing the reducer operators to assume data arrives in order of time fields will not work---they need to be prepared to receive out-of-order data, and to update the results in MySQL accordingly.</li>\n</ol>\n\nIn most current streaming systems, some or all of these concerns are left to the user. This is unfortunate because these issues---how the application interacts with the outside world---are some of the hardest to reason about and get right. In particular, there is no easy way to get semantics as simple as the SQL query above.\n\n<h2>Structured Streaming Model</h2>\n\nIn Structured Streaming, we tackle the issue of semantics head-on by making a strong guarantee about the system: <em>at any time, the output of the application is equivalent to executing a batch job on a prefix of the data</em>. For example, in our monitoring application, the result table in MySQL will always be equivalent to taking a prefix of each phone\u2019s update stream (whatever data made it to the system so far) and running the SQL query we showed above. There will never be \"open\" events counted faster than \"close\" events, duplicate updates on failure, etc. Structured Streaming automatically handles consistency and reliability both within the engine and in interactions with external systems (e.g. updating MySQL transactionally).\n\nThis <em>prefix integrity</em> guarantee makes it easy to reason about the three challenges we identified. In particular:\n\n<ol>\n    <li>Output tables are <strong>always consistent</strong> with all the records in a prefix of the data. For example, as long as each phone uploads its data as a sequential stream (e.g., to the same\u00a0partition in Apache Kafka), we will always process and count its events in order.</li>\n    <li><strong>Fault tolerance</strong> is handled holistically by Structured Streaming, including in interactions with output sinks. This was a major goal in supporting <a href=\"https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html\">continuous applications</a>.</li>\n    <li>The effect of <strong>out-of-order data</strong> is clear. We know that the job outputs counts grouped by action and time for a prefix of the stream. If we later receive more data, we might see a time field for an hour in the past, and we will simply update its respective row in MySQL. Structured Streaming also supports\u00a0APIs for filtering out overly old data if the user wants. But fundamentally, out-of-order data is not a \"special case\": the query says to group by time field, and seeing an old time is no different than seeing a repeated action.</li>\n</ol>\n\nThe last\u00a0benefit of Structured Streaming is that the\u00a0API is very easy to use: it is simply Spark's <a href=\"https://databricks.com/blog/2016/01/04/introducing-apache-spark-datasets.html\">DataFrame and Dataset</a>\u00a0API. Users just describe the query they want to run, the input and output locations, and optionally a few more details. The system then runs their query incrementally, maintaining enough state to recover from failure, keep the results consistent in external storage, etc. For example, here is how to write our streaming monitoring application:\n\n<pre>// Read data continuously from an S3 location\nval inputDF = spark.readStream.json(\"s3://logs\")\n \n// Do operations using the standard DataFrame API and write to MySQL\ninputDF.groupBy($\"action\", window($\"time\", \"1 hour\")).count()\n       .writeStream.format(\"jdbc\")\n       .start(\"jdbc:mysql//\u2026\")\n</pre>\n\nThis code is nearly identical to the batch version below---only the \"read\" and \"write\" changed:\n\n<pre>// Read data once from an S3 location\nval inputDF = spark.read.json(\"s3://logs\")\n \n// Do operations using the standard DataFrame API and write to MySQL\ninputDF.groupBy($\"action\", window($\"time\", \"1 hour\")).count()\n       .writeStream.format(\"jdbc\")\n       .save(\"jdbc:mysql//\u2026\")\n</pre>\n\nThe next sections explain the model in more detail, as well as the API.\n\n<h2>Model Details</h2>\n\nConceptually, Structured Streaming treats\u00a0all the data arriving as an unbounded <strong>input table</strong>. Each new item in the stream is like a row appended to the input table. We won\u2019t actually retain all the input, but our\u00a0results will be equivalent to having all of it and running a batch job.\n<a href=\"https://databricks.com/wp-content/uploads/2016/07/image01-1.png\"><img class=\"alignnone size-full wp-image-8572\" src=\"https://databricks.com/wp-content/uploads/2016/07/image01-1.png\" alt=\"image01\" width=\"1472\" height=\"792\" /></a>\nThe developer then defines a <strong>query</strong> on this input table, as if it were a static table, to compute a final <strong>result table</strong> that will be written to an output sink. Spark automatically converts this batch-like query to a streaming execution plan. This is called <em>incrementalization:</em> Spark figures out what state needs to be maintained to update the result each time a record arrives. Finally, developers specify <strong>triggers</strong> to control when to update the results. Each time a trigger fires, Spark checks for new data (new\u00a0row\u00a0in the input table), and incrementally updates the result.\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/07/structured-model-1.png\"><img class=\"alignnone size-full wp-image-8641\" src=\"https://databricks.com/wp-content/uploads/2016/07/structured-model-1.png\" alt=\"structured-model\" width=\"1470\" height=\"1076\" /></a>\n\nThe last part of the model is <strong>output modes</strong>. Each time the result table is updated, the developer wants to write the changes to an external system, such as S3, HDFS, or a database. We usually want to write output incrementally. For this purpose, Structured Streaming provides three output modes:\n\n<ul>\n    <li><strong>Append:</strong> Only the new rows appended to the result table since the last trigger will be written to the external storage. This is applicable only on queries where existing rows in the result table cannot change (e.g. a map on an input stream).</li>\n    <li><strong>Complete</strong>: The entire updated result table will be written to external storage.</li>\n    <li><strong>Update</strong>: Only the rows that were updated in the result table since the last trigger will be changed in the external storage. This mode works for output sinks that can be updated in place, such as a MySQL table.</li>\n</ul>\n\nLet\u2019s see how we can run our mobile monitoring application in this model. Our batch query is to compute a count of actions grouped by (action, hour). To run this query incrementally, Spark will maintain some state with the counts for each pair so far, and update when new records arrive. For each record changed, it will then output data according to its output mode. The figure below shows this execution using the Update output mode:\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/07/stream-example1-updated.png\"><img src=\"https://databricks.com/wp-content/uploads/2016/07/stream-example1-updated.png\" alt=\"\" width=\"1104\" height=\"820\" class=\"aligncenter size-full wp-image-10749\" /></a>\n\nAt every trigger point, we take the previous grouped counts and update them with new data that arrived since the last trigger to get a new result table. We then emit only the changes required by our output mode to the sink---here, we update the records for (action, hour) pairs that changed during that trigger in MySQL (shown in red).\n\nNote that the system also automatically handles late data. In the figure above, the \"open\" event for phone3, which happened at 1:58 on the phone, only gets to the system at 2:02. Nonetheless, even though it\u2019s past 2:00, we update the record for 1:00 in MySQL. However, the prefix integrity guarantee in Structured Streaming ensures that we process the records from each source <em>in the order they arrive</em>. For example, because phone1\u2019s \"close\" event arrives after its \"open\" event, we will always update the \"open\" count before we update the \"close\" count.\n\n<h2>Fault Recovery and Storage System Requirements</h2>\n\nStructured Streaming keeps its results valid even if machines fail. To do this, it places two requirements on the input sources and output sinks:\n\n<ol>\n    <li>Input sources must be <em>replayable</em>, so that recent data can be re-read if the job crashes. For example, message buses like Amazon Kinesis and Apache Kafka are replayable, as is the file system input source. Only a few minutes\u2019 worth of data needs to be retained; Structured Streaming will maintain its own internal state after that.</li>\n    <li>Output sinks must support <em>transactional updates</em>, so that the system can make a set of records appear atomically. The current version of Structured Streaming implements this for file sinks, and we also plan to add it for common databases and key-value stores.</li>\n</ol>\n\nWe found that most Spark applications already use sinks and sources with these properties, because users want their jobs to be reliable.\n\nApart from these requirements, Structured Streaming will manage its internal state in a reliable storage system, such as S3 or HDFS, to store data such as the running counts in our example. Given these properties, Structured Streaming will enforce prefix integrity end-to-end.\n\n<h2>Structured Streaming API</h2>\n\nStructured Streaming is integrated into Spark\u2019s <a href=\"https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html\" target=\"_blank\">Dataset and DataFrame APIs</a>; in most cases, you only need to add a few method calls to run a streaming computation. It also adds new operators for windowed aggregation and for setting parameters of the execution model (e.g. output modes). In <a href=\"https://databricks.com/blog/2016/07/26/introducing-apache-spark-2-0.html\" target=\"_blank\">Apache Spark 2.0</a>, we\u2019ve built an alpha version of the system with the core APIs. More operators, such as sessionization, will come in future releases.\n\n<h3>API Basics</h3>\n\nStreams in Structured Streaming are represented as DataFrames or Datasets with the isStreaming property set to true. You can create them using special read methods from various sources. For example, suppose we wanted to read data in our monitoring application from JSON files uploaded to Amazon S3. The code below shows how to do this in Scala:\n\n<pre>val inputDF = spark.readStream.json(\"s3://logs\")\n</pre>\n\nOur resulting DataFrame, inputDF, is our input table, which will be continuously extended with new rows as new files are added to the directory. The table has two columns---time and action. Now you can use the usual DataFrame/Dataset operations to transform the data. In our example, we want to count action types each hour. To do that we have to group the data by action and 1 hours windows of time.\n\n<pre>val countsDF = inputDF.groupBy($\"action\", window($\"time\", \"1 hour\"))\n                      .count()\n</pre>\n\nThe new DataFrame countsDF is our result table, which has the columns action, window, and count, and will be continuously updated when the query is started. Note that this transformation would give hourly counts even if inputDF was a static table. This allows developers to test their business logic on static datasets and seamless apply them on streaming data without changing the logic.\n\nFinally, we tell the engine to write this table to a sink and start the streaming computation.\n\n<pre>val query = countsDF.writeStream.format(\"jdbc\").start(\"jdbc://...\")\n</pre>\n\nThe returned <code>query</code> is a StreamingQuery, a handle to the active streaming execution and can be used to manage and monitor the execution. You can run this complete example by importing the following notebooks into <a href=\"https://databricks.com/try-databricks\">Databricks Community edition</a>.\n\n<ul>\n    <li><a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/Structured%20Streaming%20using%20Scala%20DataFrames%20API.html\">Scala Notebook</a></li>\n    <li><a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/Structured%20Streaming%20using%20Python%20DataFrames%20API.html\">Python Notebook</a></li>\n</ul>\n\nBeyond these basics, there are many more operations that can be done in Structured Streaming.\n\n<h3>Mapping, Filtering and Running Aggregations</h3>\n\nStructured Streaming programs can use DataFrame and Dataset\u2019s existing methods to transform data, including map, filter, select, and others. In addition, running (or infinite) aggregations, such as a <code>count</code> from the beginning of time, are available through the existing APIs. This is what we used in our monitoring application above.\n\n<h3>Windowed Aggregations on Event Time</h3>\n\nStreaming applications often need to compute data on various types of <em>windows</em>, including <em>sliding windows</em>, which overlap with each other (e.g. a 1-hour window that advances every 5 minutes), and tumbling windows, which do not (e.g. just every hour). In Structured Streaming, <em>windowing is simply represented as a group-by</em>. Each input event can be mapped to one or more windows, and simply results in updating one or more result table rows.\n\nWindows can be specified using the window function in DataFrames. For example, we could change our monitoring job to count actions by sliding windows as follows:\n\n<pre>inputDF.groupBy($\"action\", window($\"time\", \"1 hour\", \"5 minutes\"))\n       .count()\n</pre>\n\nWhereas our previous application outputted results of the form (hour, action, count), this new one will output results of the form (window, action, count), such as (\"1:10-2:10\", \"open\", 17). If a late record arrives, we will update all the corresponding windows in MySQL. And unlike in many other systems, windowing is not just a special operator for streaming computations; we can run the same code in a batch job to group data in the same way.\n\nWindowed aggregation is one area where we will continue to expand Structured Streaming. In particular, in Spark 2.1, we plan to add <em>watermarks</em>, a feature for dropping overly old data when sufficient time has passed. Without this type of feature, the system might have to track state for all old windows, which would not scale as the application runs. In addition, we plan to add support for <em>session-based windows</em>, i.e. grouping the events from one source into variable-length sessions according to business logic.\n\n<h3>Joining Streams with Static Data</h3>\n\nBecause Structured Streaming simply uses the DataFrame API, it is straightforward to join a stream against a static DataFrame, such as an Apache Hive table:\n\n<pre>// Bring in data about each customer from a static \"customers\" table,\n// then join it with a streaming DataFrame\nval customersDF = spark.table(\"customers\")\ninputDF.join(customersDF, \"customer_id\")\n       .groupBy($\"customer_name\", hour($\"time\"))\n       .count()\n</pre>\n\nMoreover, the static DataFrame could itself be computed using a Spark query, allowing us to mix batch and streaming computations.\n\n<h3>Interactive Queries</h3>\n\nStructured Streaming can expose results directly to interactive queries through Spark\u2019s JDBC server. In Spark 2.0, there is a rudimentary \"memory\" output sink for this purpose that is not designed for large data volumes. However, in future releases, this will let you write query results to an in-memory Spark SQL table, and run queries directly against it.\n\n<pre>// Save our previous counts query to an in-memory table\ncountsDF.writeStream.format(\"memory\")\n  .queryName(\"counts\")\n  .outputMode(\"complete\")\n  .start()\n\n// Then any thread can query the table using SQL\nsql(\"select sum(count) from counts where action=\u2019login\u2019\")\n</pre>\n\n<h2>Comparison With Other Engines</h2>\n\nTo show what\u2019s unique about Structured Streaming, the next table compares it with several other systems. As we discussed, Structured Streaming\u2019s strong guarantee of prefix integrity makes it equivalent to batch jobs and easy to integrate into larger applications. Moreover, building on Spark enables integration with batch and interactive queries.\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/07/streaming-engine-comparison-2.png\"><img class=\"alignnone size-full wp-image-8666\" src=\"https://databricks.com/wp-content/uploads/2016/07/streaming-engine-comparison-2.png\" alt=\"streaming-engine-comparison\" width=\"1174\" height=\"555\" /></a>\n\n<h2>Conclusion</h2>\n\nStructured Streaming promises to be a much simpler model for building end-to-end\u00a0real-time applications, built on the features that work best in Spark Streaming. Although Structured Streaming is in alpha for\u00a0Apache Spark 2.0, we hope this post encourages\u00a0you to try it out.\n\nLong-term, much like the DataFrame API, we expect Structured Streaming to complement Spark Streaming by providing a more restricted but higher-level interface. If you are running Spark Streaming today, don\u2019t worry---it will continue to be supported. But we believe that Structured Streaming can open up real-time computation to many more users.\n\nStructured Streaming is also fully supported on Databricks, including in the free<a href=\"http://databricks.com/try\" target=\"_blank\"> Databricks Community Edition</a>. Try out any of our sample notebooks to see it in action:\n\n<ul>\n    <li><a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/Structured%20Streaming%20using%20Scala%20DataFrames%20API.html\">Scala notebook for monitoring app</a></li>\n    <li><a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/Structured%20Streaming%20using%20Python%20DataFrames%20API.html\">Python\u00a0notebook for monitoring app</a></li>\n</ul>\n\n<h2>Read More</h2>\n\nIn addition, the following resources\u00a0cover Structured Streaming:\n\n<ul>\n    <li><a href=\"https://spark-summit.org/2016/events/structuring-spark-dataframes-datasets-and-streaming/\">Structuring Spark: DataFrames, Datasets and Streaming</a></li>\n    <li><a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\">Structured Streaming Programming Guide</a></li>\n    <li><a href=\"https://spark-summit.org/east-2016/speakers/matei-zaharia/\">Spark 2.0 and Structured Streaming</a></li>\n    <li><a href=\"https://spark-summit.org/2016/events/a-deep-dive-into-structured-streaming/\">A Deep Dive Into Structured Streaming</a></li>\n</ul>"}
{"status": "publish", "description": "Learn how to write Apache Spark apps in .NET using Mobius SDK, the open source project.", "creator": "jules_damji", "link": "https://databricks.com/blog/2016/08/03/developing-apache-spark-applications-in-net-using-mobius.html", "authors": null, "id": 8671, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2016-08-03", "tz": "UTC", "createdOn": "2016-08-03"}, "title": "Developing Apache Spark Applications in .NET using Mobius", "slug": "developing-apache-spark-applications-in-net-using-mobius", "content": "To address the gap between Spark and .NET, <a href=\"http://www.microsoft.com\">Microsoft</a> created Mobius, an open source project, with guidance from Databricks. By adding the C# language API to Spark, it extends and enables .NET framework developers to build Apache Spark Applications. This guest blog provides an overview of this C# API.\n\nApache Spark has transformed the big data processing and analytics space over the last few years. It provides high-level APIs in Scala, Java, Python and R and dramatically reduced the cost and complexity of building a wide variety of big data workloads. The results of <a href=\"https://databricks.com/blog/2015/09/24/spark-survey-2015-results-are-now-available.html\">Spark Survey 2015</a> indicate that the ease of programming is one of the most important aspects of Spark. So it is apparent that having APIs in multiple languages appealed to various developer persona and contributed to the rapid adoption of Spark.\n\nHowever, Spark had been out of reach for the .NET developer community. The results of Spark Survey 2015 also indicated that there was huge spike in the Spark usage in Windows, and there is a high likelihood that a good portion of the developers using Spark in Windows are .NET professionals. To address the gap between Spark and .NET, Microsoft created <a href=\"https://github.com/Microsoft/Mobius\">Mobius</a> as an open source project with the goal of adding a C# language API to Spark enabling the usage of any .NET Framework language in building Apache Spark applications. With Mobius, organizations deeply invested in .NET can reuse their existing .NET libraries in their Spark applications.\n<h2>Spark Applications in .NET</h2>\nThe C# language binding to Spark is similar to the Python and R bindings. In fact, Mobius follows the same design pattern and leverages the existing implementation of language binding components in Spark where applicable for consistency and reuse. The following picture shows the dependency between the .NET application and the C# API in Mobius, which internally depends on Spark\u2019s public API in Scala and Java and extends PythonRDD from PySpark to implement CSharpRDD.\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/08/image00.png\"><img class=\"aligncenter size-full wp-image-8673\" src=\"https://databricks.com/wp-content/uploads/2016/08/image00.png\" alt=\"image00\" width=\"981\" height=\"599\" /></a>\n\nAs shown above, the driver programs are written entirely in a .NET programming language like C# or F# using the C# API in Mobius. Mobius applications can be used with Spark deployed on premises or in the cloud. Mobius is supported on Windows and Linux. In Linux, Mobius uses <a href=\"http://www.mono-project.com/\">Mono</a>, an open source implementation of the .NET framework.\n<h2>Developing &amp; Submitting Mobius Applications</h2>\nMobius driver applications can be developed in an IDE (like Visual Studio) that supports .NET development. Mobius API and the worker implementation (used to execute user defined functionality in C# code in Spark worker nodes) are released to <a href=\"https://www.nuget.org/packages/Microsoft.SparkCLR\">NuGet</a>. Once these Mobius binaries and any other .NET library dependencies are added to the Mobius driver project in the IDE, the driver application code can be developed, <a href=\"https://github.com/Microsoft/Mobius/blob/master/notes/running-mobius-app.md#debug-mode\">debugged</a> and tested like any other .NET program within the IDE.\n\nMobius driver applications in .NET are compiled into an executable (.exe file), which is copied along with its dependencies to the client machine from which Spark job needs to be submitted. A supported version of <a href=\"https://github.com/Microsoft/Mobius/releases\">Mobius release</a> is also needed on the client machine on which Mobius job submission script (<i>sparkclr-submit.cmd or sparkclr-submit.sh</i>) is used to submit Mobius-based application to a Spark cluster. A Mobius job submission script accepts the same parameters as a <a href=\"http://spark.apache.org/docs/latest/submitting-applications.html\">spark-submit script</a>, but it also needs an additional parameter for specifying the Mobius driver executable name and its path. As shown above, the driver programs are written entirely in a .NET programming language like C# or F# using the C# API in Mobius.\n\nMore information on running a Mobius application is available at <a href=\"https://github.com/Microsoft/Mobius/blob/master/notes/running-mobius-app.md\">on GitHub</a>.\n\nThe Mobius API has the same method names and signatures with similar data types as the Scala API for Spark. As a result, the driver programs implemented using Mobius look similar to those implemented in Scala or Java. Here is a code example for implementing Spark\u2019s \u201cWord Count\u201d example in C# using Mobius API.\n<pre>var textFile = sparkContext.TextFile(@\"hdfs://...\");\nvar counts = textFile\n             .FlatMap(x =&gt; x.Split(' '))\n             .Map(word =&gt; new KeyValuePair&lt;string, int&gt;(word, 1))\n             .ReduceByKey((x, y) =&gt; x + y)\n             .Map(wordCount =&gt; $\"{wordCount.Key},{wordCount.Value}\");\ncounts.SaveAsTextFile(@\"hdfs://...\");</pre>\nThe code snippet below is in F# and shows how to query the data in JSON format and use the DataFrame API to look for rows with <code>State = \u2018California\u2019</code> and also register those rows as a temp table and use Spark SQL to query for all rows with <code>name = \u2018Bill\u2019</code>.\n<pre>let peopleDataFrame = sqlContext.Read().Json(\"hdfs://...\")\nlet filteredDf = peopleDataFrame.Select(\"name\", \"address.state\")\n                 .Where(\"state = 'California'\")\nfilteredDf.Show()\nfilteredDf.RegisterTempTable \"filteredDfAsTempTable\"\nlet countAsDf = sqlContext.Sql \"SELECT * FROM filteredDfAsTempTable where name='Bill'\"\nlet countOfRows = countAsDf.Count()\nprintf \"Count of rows with name='Bill' and State='California' = %d\" countOfRows</pre>\nMore examples for RDD, DataFrame and DStream API are available <a href=\"https://github.com/Microsoft/Mobius/tree/master/examples\">here</a>. These examples also cover HDFS, Cassandra, Event Hubs, Kafka, Hive and JDBC sources in Mobius applications.\n<h2>More Resources</h2>\nYou can peruse our <a href=\"https://github.com/Microsoft/Mobius\">GitHub repository</a>, and we welcome your contributions. Additional information on Mobius is available in the <a href=\"http://www.slideshare.net/SparkSummit/mobius-c-language-binding-for-spark\">slides</a> and <a href=\"https://youtu.be/biYFeMyWqxg\">video</a> from the <a href=\"https://spark-summit.org/2016/events/mobius-c-language-binding-for-spark/\">talk on Mobius</a> presented at \u00a0Spark Summit 2016. Finally, Mobius powers several .NET-based Spark workloads in Microsoft. For example, the Spark Summit 2016 <a href=\"https://spark-summit.org/2016/events/five-lessons-learned-in-building-streaming-applications-at-microsoft-bing-scale/\">talk</a> (<a href=\"http://www.slideshare.net/JenAman/top-5-lessons-learned-in-building-streaming-applications-at-microsoft-bing-scale\">slides</a>, <a href=\"https://youtu.be/x8QVE2ogNg0\">video</a>) covers the lessons learned using Spark in a Bing-scale workload.\n<h3>About the Author</h3>\n<a href=\"https://www.linkedin.com/in/kaarthik\">Kaarthik Sivashanmugam</a> is a Principal Software Engineer in the Shared Data platform team at Microsoft. Kaarthik is the tech lead for the <a href=\"https://github.com/Microsoft/Mobius\">Mobius project</a>. Prior to joining the Shared Data team, he was in the Bing Ads team, where he built a near real-time analytics platform using Kafka, Storm and Elasticsearch, and used it to implement data processing pipelines. Previously, at Microsoft, he was involved in the development of Data Quality Service in Azure and also contributed to multiple releases of SQL Server Integration Services.\n\nTwitter: <a href=\"https://twitter.com/kaarthikss\">@kaarthikss</a>"}
{"status": "publish", "description": "Join us in October at the Square in Brussels to learn more about data engineering and data science at scale, spend time with other members of the Spark community, and more.", "creator": "scott", "link": "https://databricks.com/blog/2016/08/04/agenda-announced-for-sparksummit-europe-2016-in-brussels.html", "authors": null, "id": 8848, "categories": ["Announcements", "Company Blog", "Events"], "dates": {"publishedOn": "2016-08-04", "tz": "UTC", "createdOn": "2016-08-04"}, "title": "Agenda Announced for #SparkSummit Europe 2016 in Brussels", "slug": "agenda-announced-for-sparksummit-europe-2016-in-brussels", "content": "Brussels, the <em>de facto</em> capital of the European Union, has its draw not only to politicians and tourists but technologists and entrepreneurs.\n\n<img src=\"https://databricks.com/wp-content/uploads/2016/08/spark-summit-brussels.jpg\" alt=\"spark-summit-brussels\" width=\"700\" height=\"429\" class=\"aligncenter size-full wp-image-8850\" />\n\nSo, get ready for Europe's largest big data community gathering dedicated to Apache Spark!\n\nRegistration is <a href=\"https://spark-summit.org/eu-2016/register\">open now</a>, and you can save 15% when you use \"DatabricksEU16\" during registration.\n\n<a href=\"https://spark-summit.org/eu-2016/\">Spark Summit Europe 2016</a> will be held from October 25-27 at the <a href=\"http://www.squarebrussels.com/\">Square</a> in Brussels, and the <a href=\"https://spark-summit.org/eu-2016/schedule/\">recently released agenda</a> features a stellar lineup of community talks led by top engineers, architects,  data scientists, researchers, entrepreneurs and analysts from Bloomberg, Databricks, Microsoft, Ericsson, IBM, ING, Cloudera, MongoDB and Swisscom. There is also a full day of hands-on Spark training, with courses for both beginners and advanced users.\n\nAs the excitement around Spark continues to grow, and the rapid adoption rate shows no signs of slowing down, Spark Summit Europe is growing, too. More than 1,200 participants are expected at the Brussels conference.\n\nJoin us in October to learn more about data engineering and data science at scale,   spend time with other members of the Spark community, attend community meetups, revel in social activities associated with the Summit, and enjoy the beautiful city!\n\n<h2>Something for everyone</h2>\n\n<h3>Developer Day: (October 26)</h3>\n\nAimed at a highly technical audience, this day will focus on topics on Spark dealing with memory management, performance, optimization, scale, and integration with the ecosystem, including dedicated tracks and sessions covering:\n\n<ul>\n<li>Keynotes focusing on what's new with Spark, where Spark is heading,  and technical trends within Big Data.</li>\n<li>Five technical tracks, including Developer, Data Science, Spark Ecosystem, Use Cases & Experiences, and Research.</li>\n<li>Office hours from the Spark project leads at the Expo Hall Theater.</li>\n</ul>\n\n<h3>Enterprise Day: (October 27)</h3>\n\nFor anyone interested in understanding how Spark is used in the enterprise, this day\u2019s lineup will include:\n\n<ul>\n<li>Keynotes from leading vendors contributing to Spark and enterprise use cases.</li>\n<li>Full day-long track of enterprise talks featuring use cases and a vendor panel.</li>\n<li>3 technical tracks for continued learning from Developer Day.</li>\n</ul>\n\nWith more than 80 sessions, you\u2019ll be able to pick and choose the topics that best suit your interests and expertise.\n\nThe <a href=\"https://spark-summit.org/eu-2016/schedule/\">full schedule is online</a>, and some of the sessions to look for include:\n\n<ul>\n<li><a href=\"https://spark-summit.org/eu-2016/events/tensorframes-deep-learning-with-tensorflow-on-apache-spark/\">TensorFrames: Deep Learning with TensorFlow on Apache Spark</a> \u2014 Tim Hunter (Databricks)</li>\n<li><a href=\"https://spark-summit.org/eu-2016/events/scalable-bayesian-inference-with-spark-sparkr-and-microsoft-r-server/\">Scalable Bayesian Inference with Spark, SparkR and Microsoft R Server</a> \u2014 Ali Zaidi (Microsoft)</li>\n<li><a href=\"https://spark-summit.org/eu-2016/events/scaling-factorization-machines-on-spark-using-parameter-servers/\">Scaling Factorization Machines on Spark Using Parameter Servers</a> \u2014 Nick Pentreath (IBM)</li>\n<li><a href=\"https://spark-summit.org/eu-2016/events/lambda-architecture-with-spark-in-the-iot/\">Lambda Architecture with Spark in the IoT</a> \u2014 Bas Geerdink (ING)</li>\n<li><a href=\"https://spark-summit.org/eu-2016/events/just-enough-scala-for-spark/\">Just Enough Scala for Spark</a> \u2014 Dean Wampler (Lightbend)</li>\n<li><a href=\"https://spark-summit.org/eu-2016/events/prediction-as-a-service-with-ensemble-model-trained-in-sparkml-on-1-billion-observed-flight-prices-daily/\">Prediction as a Service with Ensemble Model Trained in SparkML on 1 billion Observed Flight Prices Daily</a> \u2014 Josef Habdank (Infare Solutions)</li>\n</ul>\n\nDon\u2019t forget the Spark Training workshops on October 25. There will be three hands-on courses with labs hosted in Databricks:\n\n<ul>\n<li><a href=\"https://spark-summit.org/eu-2016/spark-training/#apache-spark-essentials\">Spark Essentials</a> for those getting started.</li>\n<li><a href=\"https://spark-summit.org/eu-2016/spark-training/#explore-wikipedia-with-spark\">Exploring Wikipedia with Spark</a> for advanced users who want to take a deeper dive.</li>\n<li><a href=\"https://spark-summit.org/eu-2016/spark-training/#data-science\">Data Science with Spark</a> for software developers, analysts, engineers and data scientists.</li>\n</ul>\n\n<h2>Get tickets online</h2>\n\nRegistration is <a href=\"https://spark-summit.org/eu-2016/register\">open now</a>, and you can save 15% when you use \"DatabricksEU16\" during registration. We hope to see you at Spark Summit Europe 2016 in Brussels. \n\nFollow <a href=\"https://twitter.com/spark_summit\">@spark_summit</a> and <a href=\"https://twitter.com/search?q=%23SparkSummit\">#SparkSummit</a> for updates."}
{"status": "publish", "description": "Continuing with our bi-weekly digest series, here\u2019s our recap of what\u2019s transpired over the last two weeks with Apache Spark.", "creator": "jules_damji", "link": "https://databricks.com/blog/2016/08/08/databricks-bi-weekly-digest-8816.html", "authors": null, "id": 8863, "categories": ["Apache Spark", "Engineering Blog", "Machine Learning", "Streaming"], "dates": {"publishedOn": "2016-08-08", "tz": "UTC", "createdOn": "2016-08-08"}, "title": "Databricks Bi-Weekly Digest: 8/8/16", "slug": "databricks-bi-weekly-digest-8816", "content": "Continuing with our bi-weekly digest series, here\u2019s our recap of what\u2019s transpired over the last two weeks with Apache Spark since  our <a href=\"https://databricks.com/blog/2016/07/18/databricks-bi-weekly-digest-71816.html\" target=\"_blank\">previous digest.</a>\n\n<ul>\n\t<li>Databricks announced <a href=\"https://databricks.com/blog/2016/07/26/introducing-apache-spark-2-0.html\" target=\"_blank\">general availability of Apache Spark 2.0</a> on its just-in-time data platform. Reynold Xin elaborated on the merits of Spark 2.0, as easier, faster, and smarter.</li>\n\t<li>Databricks\u2019 CTO and Co-founder Matei Zaharia envisaged the evolution of real-time streaming in Apache Spark 2.0 with <a href=\"https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html\" target=\"_blank\">Continuous Applications</a>.</li>\n\t<li>Messrs Matei Zaharia, Tathagata Das, Michael Armbrust, and Reynold Xin elaborated on <a href=\"https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html\" target=\"_blank\">Structured Streaming Model and its API</a> in Apache Spark 2.0: How to write end-to-end continuous applications using the DataFrame and Dataset based streaming API.</li>\n\t<li>Jax Magazine interviewed Xiangrui Meng about <a href=\"https://jaxenter.com/apache-mllib-making-practical-machine-learning-easy-and-scalable-128037.html\" target=\"_blank\">Apache MLlib - Making practical machine learning easier and scalable</a>.</li>\n\t<li>Xiangrui Meng talked about Apache SparkR at the Walmart Meetup: Peruse the <a href=\"http://files.meetup.com/4439192/Recent%20Development%20in%20SparkR%20for%20Advanced%20Analytics.pdf\" target=\"_blank\">tech-talk slides here</a>.</li>\n\t<li>The <a href=\"https://spark-packages.org/package/mongodb/mongo-spark\" target=\"_blank\">MongoDB Connector for Apache Spark</a> released as a Spark package, and <a href=\"https://databricks.com/blog/2016/07/21/the-new-mongodb-connector-for-apache-spark-in-action-building-a-movie-recommendation-engine.html\" target=\"_blank\">Sam Weaver\u2019s guest blog</a> demonstrated the connector using a <a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/MongoDB_guest_blog/Using_MongoDB_Connector_for_Spark.html\" target=\"_blank\">Databricks notebook</a>.</li>\n\t<li>Databricks released a new version of the <a href=\"https://spark-packages.org/package/databricks/spark-redshift\" target=\"_blank\">spark-redshift package</a> that works with Spark 2.0.</li>\n\t<li>Kaarthik Sivashanmugam, tech lead for the Mobius Project at Microsoft, explained in a guest blog how <a href=\"https://databricks.com/blog/2016/08/03/developing-apache-spark-applications-in-net-using-mobius.html\" target=\"_blank\">Mobius\u2019 C# API extends and enables .NET developers</a> to write Apache Spark applications.</li>\n\t<li>KDD rated <a href=\"http://www.kdd.org/kdd2016/subtopic/view/matrix-computations-and-optimization-in-apache-spark\" target=\"_blank\">Matrix Computations and Optimization in Apache Spark</a> as Best Runner up Paper in Applied Data Science Track for 2016.</li>\n\t<li>Daniel Pape opined on <a href=\"https://blog.codecentric.de/en/2016/07/spark-2-0-datasets-case-classes/\" target=\"_blank\">Spark 2.0: Datasets and case classes</a> in a community blog.</li>\n\t<li>Databricks <a href=\"https://databricks.com/blog/2016/08/04/agenda-announced-for-sparksummit-europe-2016-in-brussels.html\" target=\"_blank\">announced the agenda</a> for <a href=\"https://spark-summit.org/eu-2016\">Spark Summit Europe 2016</a> in Brussels, Belgium. <a href=\"https://www.prevalentdesignevents.com/sparksummit2016/europe/registration.aspx\" target=\"_blank\">Register today!</a></li>\n</ul>\n\n<h2>What\u2019s Next?</h2>\nTo stay abreast with what\u2019s happening with Apache Spark, follow us on Twitter <a href=\"https://twitter.com/databricks\" target=\"_blank\">@databricks</a> and visit <a href=\"http://sparkhub.databricks.com\" target=\"_blank\">SparkHub</a>."}
{"status": "publish", "description": "Riak Spark Connector \u2013 an open source library, which bridges the gap between Apache Spark and Riak NoSQL database. It brings the full power of Apache Spark to the operational data, managed in Riak distributed clusters.", "creator": "admin", "link": "https://databricks.com/blog/2016/08/11/the-quest-for-hidden-treasure-an-apache-spark-connector-for-the-riak-nosql-database.html", "authors": null, "id": 8872, "categories": ["Company Blog", "Partners"], "dates": {"publishedOn": "2016-08-11", "tz": "UTC", "createdOn": "2016-08-11"}, "title": "The Quest for Hidden Treasure: An Apache Spark Connector for the Riak NoSQL database", "slug": "the-quest-for-hidden-treasure-an-apache-spark-connector-for-the-riak-nosql-database", "content": "[dbce_cta href=\"https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html#03%20Data%20Sources/5%20Databases%20%26%20Other%20Data%20Sources/7%20RiakTS%20Tutorial.html\"]View this notebook in Databricks[/dbce_cta]\n\n[sidenote]This is a guest blog from our friends at Basho. Pavel Hardak is a director of product management at Basho.[/sidenote]\n\n<hr />\n\nThis article introduces <a href=\"http://basho.com/products/riak-ts/apache-spark-connector/\" target=\"_blank\">Riak Spark Connector</a>, an open source library which bridges the gap between Apache Spark and Riak NoSQL database. It brings the full power of Apache Spark to the operational data, managed in Riak distributed clusters. With the Riak Apache Spark Connector, Riak users now have an integrated, scalable solution for Big Data analytics and Spark users now have a resilient, highly available datastore.\n<h2>About Riak</h2>\nRiak is open source, distributed NoSQL database, which is developed and supported by <a href=\"http://basho.com/\" target=\"_blank\">Basho Technologies</a>. Basho offers two major products: Riak KV (Key Value) and Riak TS (Time Series). Both products share the same core codebase, but are tuned for different use cases. <a href=\"http://docs.basho.com/riak/kv/latest/\" target=\"_blank\">Riak KV</a> is a highly resilient, scalable, key-value store. Riak KV is known for its ability to scale up and down in a linear fashion, handle huge amounts of reads, updates and writes with low latency while being extremely reliable and fault tolerant. More recently, <a href=\"http://basho.com/products/riak-ts/\" target=\"_blank\">Riak TS</a> was introduced, specifically optimized for time series data. It adds very fast bulk writes, very efficient \u201ctime slice\u201d read queries and supports a\u00a0subset of the SQL language over Riak TS tables.\n<h2>Introducing the Riak Connector for Apache Spark</h2>\nWe have found that many leading organizations use a mix of NoSQL and SQL database products in their infrastructure as each one has specific advantages depending on the use case. In the past, some databases were used more for analytical workloads while others were used for operational ones. As modern NoSQL databases, like <a href=\"http://basho.com/products/#riak\" target=\"_blank\">Riak</a>, are gaining new capabilities, they are being adopted for additional use cases, like <a href=\"http://basho.com/use-cases/iot-sensor-device-data/\" target=\"_blank\">IoT</a>, <a href=\"http://basho.com/use-cases/metrics-log-analytics/\" target=\"_blank\">metrics</a>, and <a href=\"http://basho.com/use-cases/iot-sensor-device-data/\" target=\"_blank\">edge-device analytics</a>. To make it easier to perform such tasks, Basho has created a <a href=\"http://basho.com/products/riak-ts/apache-spark-connector/\" target=\"_blank\">Riak Spark Connector</a>, as we believe that Apache Spark is currently the best technology choice to use alongside Riak. Basho selected Spark for this development effort not only due to customer and market demand but also due to the fact that Spark and Riak share major design principles: high performance, scalability, resiliency and operational simplicity.\n<h2>Implementing the Apache Spark Connector for Riak</h2>\nModeled using principles from the <a href=\"http://docs.basho.com/riak/kv/latest/learn/dynamo/\" target=\"_blank\">\u201cAWS Dynamo\u201d paper</a>, Riak KV buckets are good for scenarios which require frequent, small data-sized operations in near real-time, especially workloads with reads, writes, and updates \u2014 something which might cause data corruption in some distributed databases or bring them to \u201ccrawl\u201d under bigger workloads. In Riak, each data item is <a href=\"http://docs.basho.com/riak/kv/latest/learn/concepts/replication/\" target=\"_blank\">replicated</a> on several nodes, which allows the database to process a huge number of operations with very low latency\u00a0while having unique <a href=\"http://docs.basho.com/riak/kv/2.1.4/learn/concepts/active-anti-entropy/\" target=\"_blank\">anti-corruption</a> and <a href=\"http://docs.basho.com/riak/kv/latest/developing/usage/conflict-resolution/\" target=\"_blank\">conflict-resolution mechanisms</a>. However, integration with Apache Spark requires a very different mode of operation \u2014 extracting large amounts of data in bulk, so that Spark can do its \u201cmagic\u201d in memory over the whole data set. One approach to solve this challenge is to create a myriad of Spark workers, each asking for several data items. This approach works well with Riak, but it creates unacceptable overhead on the Spark side.\n\n<img class=\"aligncenter size-full wp-image-8877\" src=\"https://databricks.com/wp-content/uploads/2016/08/Riak-Spark-Connector-Diagram-1-of-3.png\" alt=\"Diagram of how we can use Riak to create a myriad of Spark workers.\" width=\"425\" height=\"374\" />\n\nAnother option is using Riak\u2019s built-in <a href=\"http://docs.basho.com/riak/kv/latest/developing/usage/secondary-indexes/\" target=\"_blank\">secondary index query</a> (2i). In this approach, the user\u2019s application contacts any Riak node with a query, then this Riak node, becoming a \u201ccoordinating node\u201d, queries all other relevant Riak nodes, collects required keys and streams it back to the user application. Then the user app will loop over the keys to retrieve the values. Alas, it was found that queries with a bigger result set could possibly overload the coordinating node. Again, not a good result, so we had to teach Riak new tricks.\n\n<img class=\"aligncenter size-full wp-image-8878\" src=\"https://databricks.com/wp-content/uploads/2016/08/Riak-Spark-Connector-Diagram-2-of-3.png\" alt=\"Diagram of Riak's secondary query index strategy on Spark.\" width=\"425\" height=\"412\" />\n\nThe solution was found in enhancing the 2i query with a smart Coverage Plan and Parallel Extract APIs. In the new approach, the user application contacts the coordinating node, but this time instead of doing all the work, this node returns the locations of the data using cluster replication and availability information. Then \u201cN\u201d Spark workers open \u201cN\u201d parallel connections to different nodes, which allow the application to retrieve the desired dataset \u201cN\u201d times faster, without generating \u201chot spots\u201d. To make it even faster, we implemented a special type of bulk query, called a \u201cfull bucket read\u201d, which extracts the whole logical bucket without the need for a query condition. Also, it returns both keys and values, saving another round-trip to the server.\n\n<img class=\"aligncenter size-full wp-image-8879\" src=\"https://databricks.com/wp-content/uploads/2016/08/Riak-Spark-Connector-Diagram-3-of-3.png\" alt=\"Riak integrates with Spark by using a variation of our 2i query with a smart Coverage Plan and Parallel Extract APIs.\" width=\"425\" height=\"394\" />\n\nThe strength of a <a href=\"http://docs.basho.com/riak/kv/2.1.4/learn/concepts/buckets/\" target=\"_blank\">Riak KV bucket</a> is its ability to store unstructured data in a schema-less architecture with the \u201cvalues\u201d being opaque. But for many Spark use cases, data must be mapped into a record with Scala or Java types. Fortunately, many Riak applications use JSON, which allows Spark developers to easily\u00a0<a href=\"https://github.com/basho/spark-riak-connector/blob/master/docs/using-connector.md#spark-dataframes-with-kv-bucket\" target=\"_blank\">convert it into a Spark DataFrame</a> by providing a\u00a0user-defined schema. The conversion happens \u201con the fly\u201d and makes it easier for a Spark programmer to work with the retrieved data.\n<pre>import sqlContext.implicits._\nval sqlContext = new org.apache.spark.sql.SQLContext(sc)\ncase class UserData(user_id: String, name: String, age: Int, category: String)\nval kv_bucket_name = new Namespace(\"test-data\")\nval riakRdd = sc.riakBucket[UserData](kv_bucket_name).queryAll()\nval df = riakRdd.toDF()\ndf.where(df(\"age\") &gt;= 50).select(\"id\", \"name\")\ndf.groupBy(\"category\").count</pre>\n<h2>Riak TS meets Spark SQL</h2>\nBeing a distributed, master-less, highly available and linearly scalable NoSQL datastore, Riak TS adds a number of SQL-like capabilities. It includes a DDL for tables (yeah, <a href=\"http://docs.basho.com/riak/ts/latest/using/creating-activating/\" target=\"_blank\">CREATE TABLE</a> ...) with named attributes and data types, primary keys (used both for local indexing and clustering), a subset of <a href=\"http://docs.basho.com/riak/ts/1.3.0/using/querying/\" target=\"_blank\">SQL query language</a> with filters and <a href=\"http://docs.basho.com/riak/ts/latest/using/aggregate-functions/\" target=\"_blank\">aggregations</a> and more (additional SQL commands are being added as we speak).\n\nAdding SQL support for a NoSQL database is not a trivial endeavor by itself and we were happy to leverage SQL capabilities, mapping them to well-known Spark constructs, such as <a href=\"http://docs.basho.com/riak/ts/latest/add-ons/spark-riak-connector/usage/dataframes/\" target=\"_blank\">DataFrames</a> and <a href=\"https://github.com/basho/spark-riak-connector/blob/master/docs/using-connector.md#reading-data-from-ts-table\" target=\"_blank\">Spark SQL</a>. Riak Spark Connector <a href=\"https://github.com/basho/spark-riak-connector/blob/master/docs/using-connector.md#ts-table-range-query-partitioning\" target=\"_blank\">automatically partitions SQL queries</a> between Spark workers. Riak TS also supports key/value functionality, which does not have a schema, so <a href=\"https://github.com/basho/spark-riak-connector/blob/master/docs/using-connector.md#reading-data-from-kv-bucket\" target=\"_blank\">we used Spark RDDs to integrate with key/value (KV) buckets</a>. It is convenient and more efficient to store device metadata, configuration information, and profiles in key/value buckets.\n<pre>val ts_table_name = \"input-table\"\nval df = sc.riakTSTable(ts_table_name)\n\t.sql(s\"SELECT * FROM $ts_table_name WHERE time &gt;= $from AND time &lt;= $to\")\n// ...\nval output_ts_table = \"result-table\"\ndf.saveToRiakTS(output_ts_table);</pre>\n<a href=\"https://github.com/basho/spark-riak-connector\" target=\"_blank\">Riak Spark Connector</a> is implemented in Scala, supporting both <a href=\"https://github.com/basho/spark-riak-connector/blob/master/docs/getting-connector.md#java\" target=\"_blank\">Java</a> and <a href=\"https://github.com/basho/spark-riak-connector/blob/master/docs/getting-connector.md#scala\" target=\"_blank\">Scala</a>, but its <a href=\"http://docs.basho.com/riak/ts/latest/add-ons/spark-riak-connector/usage/dataframes/\" target=\"_blank\">DataFrames</a> functionality gave us an easy way to also support <a href=\"https://github.com/basho/spark-riak-connector/blob/master/docs/getting-connector.md#python\" target=\"_blank\">Python</a> to query and write into Riak TS tables. Python support for KV buckets will arrive soon, for now developers can use either Java or Scala with KV buckets.\n<pre>df = sqlContext.read \\\n    .option(\"spark.riak.connection.hosts\",\"riak_host_ip:10017\") \\\n    .format(\"org.apache.spark.sql.riak\") \\\n    .load(ts_table_name) \\\n    .select(\"time\", \"col1\", \"col2\") \\\n    .filter(s\"time &gt;= CAST($from AS TIMESTAMP) AND time &lt;= CAST($to AS TIMESTAMP) AND  col1= $value1\")</pre>\n<a href=\"http://docs.basho.com/riak/ts/latest/add-ons/spark-riak-connector/usage/streaming-example/\" target=\"_blank\">Spark Streaming</a> is also supported for writing into Riak TS. In a typical pipeline, a Spark Streaming job would read (a.k.a. \u201cconsume\u201d) the data from a Kafka topic and <a href=\"http://docs.basho.com/riak/ts/latest/add-ons/spark-riak-connector/usage/writing-data/\" target=\"_blank\">efficiently write (stream) it into Riak TS</a> at high speed, automatically spreading the load across multiple nodes. Then <a href=\"http://docs.basho.com/riak/ts/latest/using/querying/\" target=\"_blank\">users can query Riak TS</a> using familiar SQL commands or use the Spark Connector to <a href=\"https://github.com/basho/spark-riak-connector/blob/master/docs/using-connector.md#reading-data-from-ts-table\" target=\"_blank\">extract the data</a> for processing in the Spark Cluster, optionally <a href=\"https://github.com/basho/spark-riak-connector/blob/master/docs/using-connector.md#ts-bulk-write\" target=\"_blank\">writing the results back</a> into Riak TS.\n<pre>df.write \\\n   .option(\"spark.riak.connection.hosts\",\"riak_host_ip:10017\") \\\n   .format(\"org.apache.spark.sql.riak\") \\\n   .mode(SaveMode.Append) \\\n   .save(ts_table_name)</pre>\n<h2>Conclusion</h2>\nAs mentioned earlier in this post, there is no \u201cbest\u201d NoSQL database for every possible use case. Riak KV and Riak TS provide an excellent solution for a number of popular use cases, as evidenced by the fact that Riak is the underlying database supporting many of the world\u2019s most highly trafficked applications. The Riak Spark Connector gives users the ability to efficiently analyze the data in Riak utilizing the power of Apache Spark. This makes the quest for discovering incredible insights hidden in the enormous volumes of data being driven by modern applications, a lot easier. Riak Spark Connector fully supports Riak TS 1.3 for both time-series tables and key/value buckets<sup><a href=\"#footnote-1\">1</a></sup>.\n\nWorking closely with the Databricks team, <a href=\"https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html#03%20Data%20Sources/5%20Databases%20%26%20Other%20Data%20Sources/7%20RiakTS%20Tutorial.html\" target=\"_blank\">we created a notebook</a>, showing Spark and Riak TS integration, which can be found in Databricks at <strong>\u2018Data Sources / Databases &amp; Other Data Sources / RiakTS Tutorial\u2019</strong>.\n\n<a href=\"https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html#03%20Data%20Sources/5%20Databases%20%26%20Other%20Data%20Sources/7%20RiakTS%20Tutorial.html\" target=\"_blank\"><img class=\"aligncenter size-full wp-image-8880\" style=\"border: 1px solid #c2c2c2;\" src=\"https://databricks.com/wp-content/uploads/2016/08/RiakTS-Tutorial-on-Databricks-Screenshot.png\" alt=\"Screenshot of the RiakTS tutorial on Databricks\" width=\"557\" height=\"325\" /></a>\n\nAdditionally, there are a couple of nice tutorials/demonstrations on Basho.com. The first <a href=\"http://basho.com/products/riak-ts/\" target=\"_blank\">gives a demonstration of Riak TS</a> using a publicly available data set and the second builds on this example by <a href=\"http://basho.com/products/riak-ts/apache-spark-connector/\" target=\"_blank\">using the same data set and analyzing it using the Apache Spark Connector</a>.\n\n<hr />\n\n[breadcrumb slug=\"footnote-1\"]\n<sup>1</sup> Riak KV will gain Spark support soon, <a href=\"http://info.basho.com/2016-08_WR_SparkConnectorForKV_StayUpToDate_Landing-Page.html\">contact me</a> if you are interested to check it out once it is available."}
{"status": "publish", "description": "Learn how Databricks built its log ETL pipeline with Apache Spark.", "creator": "dave_wang", "link": "https://databricks.com/blog/2016/08/16/databricks-data-pipeline-on-demand-webinar-faq.html", "authors": null, "id": 8905, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2016-08-16", "tz": "UTC", "createdOn": "2016-08-16"}, "title": "On-demand webinar available: Databricks\u2019 Data Pipeline", "slug": "databricks-data-pipeline-on-demand-webinar-faq", "content": "Two weeks ago we held a live webinar \u2013 <a href=\"http://go.databricks.com/databricks-data-pipeline\"><i>Databricks' Data Pipeline: Journey and Lessons Learned</i></a> \u2013 to show how Databricks used Apache Spark to simplify our own log ETL pipeline. The webinar describes an architecture where you can develop your pipeline code in notebooks, create Jobs to productionize your notebooks, and utilize REST APIs to turn all of this into a continuous integration workflow.\n\n<iframe style=\"overflow: hidden;\" src=\"https://www.brighttalk.com/clients/js/embed/embed_frame.html?channelid=12891&amp;communicationid=216755&amp;player=webcast_player_widescreen&amp;theme=generic.swf&amp;width=656&amp;height=507\" width=\"656\" height=\"507\" frameborder=\"0\" scrolling=\"no\" allowfullscreen=\"allowfullscreen\">\n</iframe>\n\nThe slides and related materials are\u00a0downloadable as attachments to the webinar.\n\nWe have answered the common questions raised by webinar viewers below. If you have additional questions, please check out the <a href=\"https://forums.databricks.com/\">Databricks Forum</a>.\n<h2>Common webinar questions and answers</h2>\nClick on the question to see answer:\n<ul>\n \t<li><a href=\"https://forums.databricks.com/questions/9438/debugging-without-ssh-access.html#answer-9439\">How can I use jstack to debug what the threads are doing if I do not have SSH access to machines?</a></li>\n \t<li><a href=\"https://forums.databricks.com/questions/9440/from-webinar-databricks-data-pipelines-log-date-an.html#answer-9441\">In the recommended approach we were to reduce the number of partition by dropping column of the date of log. However, in optimizing output we were to include the date in the directory path to evenly distribute data into different partitions - are these contradictory recommendations?</a></li>\n \t<li><a href=\"https://forums.databricks.com/questions/9442/from-webinar-databricks-data-pipelines-data-persis.html#answer-9443\">What would happen regarding the persistence of the data in Parquet files when working with streaming instead of the current solution? Do you foresee issues with things like ACID when writing the parquet tables?</a></li>\n \t<li><a href=\"https://forums.databricks.com/questions/9445/from-webinar-databricks-data-pipelines-can-we-use.html#answer-9446\">Can we use Databricks and Apache Spark for an \"Operational Data Store\"? Meaning data ingested as batches, incremental when user update the previously loaded data.</a></li>\n \t<li><a href=\"https://forums.databricks.com/questions/9448/from-webinar-databricks-data-pipelines-accessing-l.html#answer-9449\">Is there a way to get the content of the logs (driver for example) as the data is being appended to the files while running job? Meaning that not waiting for the job to finish in order to see the logs?</a></li>\n</ul>"}
{"status": "publish", "description": "This is the first in the series of how-to use blog posts on new features and functionality in Apache Spark 2.0", "creator": "jules_damji", "link": "https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html", "authors": null, "id": 8913, "categories": ["Apache Spark", "Engineering Blog", "Streaming"], "dates": {"publishedOn": "2016-08-15", "tz": "UTC", "createdOn": "2016-08-15"}, "title": "How to use SparkSession in Apache Spark 2.0", "slug": "how-to-use-sparksession-in-apache-spark-2-0", "content": "[dbce_cta href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/SparkSessionZipsExample.html\"]Try this notebook in Databricks[/dbce_cta]\n\nGenerally, a session is an interaction between two or more entities. In computer parlance, its usage is prominent in the realm of networked computers on the internet. First with TCP session, then with login session, followed by HTTP and user session, so no surprise that we now have <em>SparkSession</em>, introduced in <a href=\"https://databricks.com/blog/2016/07/26/introducing-apache-spark-2-0.html\" target=\"_blank\">Apache Spark 2.0</a>.\n\nBeyond a time-bounded interaction, <em>SparkSession</em> provides a single point of entry to interact with underlying Spark functionality and allows programming Spark with DataFrame and Dataset APIs. Most importantly, it curbs the number of concepts and constructs a developer has to juggle while interacting with Spark.\n\nIn this blog and its accompanying Databricks notebook, we will explore SparkSession functionality in Spark 2.0.\n\n<h2>Exploring SparkSession\u2019s Unified Functionality</h2>\n\nFirst, we will examine a Spark application, <a href=\"https://github.com/dmatrix/examples/blob/master/spark/databricks/apps/scala/2.x/src/main/scala/zips/SparkSessionZipsExample.scala\" target=\"_blank\">SparkSessionZipsExample</a>, that reads zip codes from a JSON file and do some analytics using DataFrames APIs, followed by issuing Spark SQL queries, without accessing SparkContext, SQLContext or HiveContext.\n\n<h3>Creating a SparkSession</h3>\n\nIn previous versions of Spark, you had to create a SparkConf and SparkContext to interact with Spark, as shown here:\n\n<pre>//set up the spark configuration and create contexts\nval sparkConf = new SparkConf().setAppName(\"SparkSessionZipsExample\").setMaster(\"local\")\n// your handle to SparkContext to access other context like SQLContext\nval sc = new SparkContext(sparkConf).set(\"spark.some.config.option\", \"some-value\")\nval sqlContext = new org.apache.spark.sql.SQLContext(sc)</pre>\n\nWhereas in Spark 2.0 the same effects can be achieved through SparkSession, without expliciting creating SparkConf, SparkContext or SQLContext, as they\u2019re encapsulated within the SparkSession. Using a builder design pattern, it instantiates a SparkSession object if one does not already exist, along with its associated underlying contexts.\n\n<pre>// Create a SparkSession. No need to create SparkContext\n// You automatically get it as part of the SparkSession\nval warehouseLocation = \"file:${system:user.dir}/spark-warehouse\"\nval spark = SparkSession\n   .builder()\n   .appName(\"SparkSessionZipsExample\")\n   .config(\"spark.sql.warehouse.dir\", warehouseLocation)\n   .enableHiveSupport()\n   .getOrCreate()</pre>\n\nAt this point you can use the <em>spark</em> variable as your instance object to access its public methods and instances for the duration of your Spark job.\n\n<h3>Configuring Spark\u2019s Runtime Properties</h3>\n\nOnce the SparkSession is instantiated, you can configure Spark\u2019s runtime config properties. For example, in this code snippet, we can alter the existing runtime config options. Since <em>configMap</em> is a collection, you can use all of Scala\u2019s iterable methods to access the data.\n\n<pre>//set new runtime options\nspark.conf.set(\"spark.sql.shuffle.partitions\", 6)\nspark.conf.set(\"spark.executor.memory\", \"2g\")\n//get all settings\nval configMap:Map[String, String] = spark.conf.getAll()</pre>\n\n<h3>Accessing Catalog Metadata</h3>\n\nOften, you may want to access and peruse the underlying catalog metadata. SparkSession exposes \u201ccatalog\u201d as a public instance that contains methods that work with the metastore (i.e data catalog). Since these methods return a Dataset, you can use Dataset API to access or view data. In this snippet, we access table names and list of databases.\n\n<pre>//fetch metadata data from the catalog\nspark.catalog.listDatabases.show(false)\nspark.catalog.listTables.show(false)</pre>\n\n[caption id=\"attachment_8915\" align=\"aligncenter\" width=\"615\"]<a href=\"https://databricks.com/wp-content/uploads/2016/08/Screen-Shot-2016-08-12-at-5.38.30-PM.png\"><img src=\"https://databricks.com/wp-content/uploads/2016/08/Screen-Shot-2016-08-12-at-5.38.30-PM-1024x464.png\" alt=\"Fig 1. Datasets returned from catalog \" width=\"615\" height=\"329\" class=\"size-large wp-image-8915\" /></a> Fig 1. Datasets returned from catalog<br />[/caption]\n\n<h3>Creating Datasets and Dataframes</h3>\n\nThere are a number of ways to create DataFrames and Datasets using <a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession\" target=\"_blank\">SparkSession APIs</a>\nOne quick way to generate a <a href=\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset\" target=\"_blank\">Dataset</a> is by using the <em>spark.range</em> method. When learning to manipulate Dataset with its API, this quick method proves useful. For example,\n\n<pre>//create a Dataset using spark.range starting from 5 to 100, with increments of 5\nval numDS = spark.range(5, 100, 5)\n// reverse the order and display first 5 items\nnumDS.orderBy(desc(\"id\")).show(5)\n//compute descriptive stats and display them\nnumDs.describe().show()\n// create a DataFrame using spark.createDataFrame from a List or Seq\nval langPercentDF = spark.createDataFrame(List((\"Scala\", 35), (\"Python\", 30), (\"R\", 15), (\"Java\", 20)))\n//rename the columns\nval lpDF = langPercentDF.withColumnRenamed(\"_1\", \"language\").withColumnRenamed(\"_2\", \"percent\")\n//order the DataFrame in descending order of percentage\nlpDF.orderBy(desc(\"percent\")).show(false)</pre>\n\n[caption id=\"attachment_8920\" align=\"aligncenter\" width=\"615\"]<a href=\"https://databricks.com/wp-content/uploads/2016/08/image02.png\"><img src=\"https://databricks.com/wp-content/uploads/2016/08/image02-1024x659.png\" alt=\"Fig 2. DataFrame &amp; Dataset output\" width=\"615\" height=\"330\" class=\"size-large wp-image-8920\" /></a> Fig 2. DataFrame &amp; Dataset output[/caption]\n\n<h3>Reading JSON Data with SparkSession API</h3>\n\nLike any Scala object you can use <em>spark</em>, the SparkSession object, to access its public methods and instance fields. I can read JSON or CVS or TXT file, or I can read a parquet table. For example, in this code snippet, we will read a JSON file of zip codes, which returns a DataFrame, a collection of generic Rows.\n\n<pre>// read the json file and create the dataframe\nval jsonFile = args(0)\nval zipsDF = spark.read.json(jsonFile)\n//filter all cities whose population > 40K\nzipsDF.filter(zipsDF.col(\"pop\") > 40000).show(10)</pre>\n\n<h3>Using Spark SQL with SparkSession</h3>\n\nThrough SparkSession, you can access all of the Spark SQL functionality as you would through SQLContext. In the code sample below, we create a table against which we issue SQL queries.\n\n<pre>// Now create an SQL table and issue SQL queries against it without\n// using the sqlContext but through the SparkSession object.\n// Creates a temporary view of the DataFrame\nzipsDF.createOrReplaceTempView(\"zips_table\")\nzipsDF.cache()\nval resultsDF = spark.sql(\"SELECT city, pop, state, zip FROM zips_table\")\nresultsDF.show(10)</pre>\n\n[caption id=\"attachment_8922\" align=\"aligncenter\" width=\"615\"]<a href=\"https://databricks.com/wp-content/uploads/2016/08/image03.png\"><img src=\"https://databricks.com/wp-content/uploads/2016/08/image03-1024x886.png\" alt=\"Fig. 3 Partial output from the Spark Job run\" width=\"615\" height=\"330\" class=\"size-large wp-image-8922\" /></a> Fig. 3 Partial output from the Spark Job run[/caption]\n\n<h3>Saving and Reading from Hive table with SparkSession</h3>\n\nNext, we are going to create a Hive table and issue queries against it using SparkSession object as you would with a HiveContext.\n\n<pre>//drop the table if exists to get around existing table error\nspark.sql(\"DROP TABLE IF EXISTS zips_hive_table\")\n//save as a hive table\nspark.table(\"zips_table\").write.saveAsTable(\"zips_hive_table\")\n//make a similar query against the hive table \nval resultsHiveDF = spark.sql(\"SELECT city, pop, state, zip FROM zips_hive_table WHERE pop > 40000\")\nresultsHiveDF.show(10)</pre>\n\n[caption id=\"attachment_8944\" align=\"aligncenter\" width=\"615\"]<a href=\"https://databricks.com/wp-content/uploads/2016/08/image03-1.png\"><img src=\"https://databricks.com/wp-content/uploads/2016/08/image03-1-1024x886.png\" alt=\"Fig 4. Output from the Hive Table \" width=\"615\" height=\"330\" class=\"size-large wp-image-8944\" /></a> Fig 4. Output from the Hive Table<br />[/caption]\nAs you can observe, the results in the output runs from using the DataFrame API, Spark SQL and Hive queries are identical. You can access all sources and data, and how to run this example, from <a href=\"https://github.com/dmatrix/examples/tree/master/spark/databricks/apps/scala/2.0\" target=\"_blank\">my github repo</a>.\n\nSecond, let\u2019s turn our attention to two Spark developer environments where the SparkSession is automatically created for you.\n\n<h2>SparkSession in Spark REPL and Databricks Notebook</h2>\n\nFirst, as in previous versions of Spark, the spark-shell created a SparkContext (<em>sc</em>), so in Spark 2.0, the spark-shell creates a SparkSession (<em>spark</em>). In this spark-shell, you can see <em>spark</em> already exists, and you can view all its attributes.\n[caption id=\"attachment_8924\" align=\"aligncenter\" width=\"615\"]<a href=\"https://databricks.com/wp-content/uploads/2016/08/image07.png\"><img src=\"https://databricks.com/wp-content/uploads/2016/08/image07-1024x435.png\" alt=\"Fig 5. SparkSession in spark-shell \" width=\"1024\" height=\"330\" class=\"size-large wp-image-8924\" /></a> Fig 5. SparkSession in spark-shell<br />[/caption]\nSecond, in the Databricks notebook, when you create a cluster, the SparkSession is created for you. In both cases it\u2019s accessible through a variable called <em>spark</em>. And through this variable you can access all its public fields and methods. Rather than repeating the same functionality here, I defer you to examine the notebook, since each section explores SparkSession\u2019s functionality\u2014and more.\n\n[caption id=\"attachment_8925\" align=\"aligncenter\" width=\"615\"]<a href=\"https://databricks.com/wp-content/uploads/2016/08/image06.png\"><img src=\"https://databricks.com/wp-content/uploads/2016/08/image06-1024x459.png\" alt=\"Fig 6. SparkSession in Databricks Notebook\" width=\"615\" height=\"330\" class=\"size-large wp-image-8925\" /></a> Fig 6. SparkSession in Databricks Notebook[/caption]\n\nYou can explore an extended version of the above example in the <a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/SparkSessionZipsExample.html\" target=\"_blank\">Databricks notebook SparkSessionZipsExample</a>, by doing some basic analytics on zip code data. Unlike our above Spark application example, we don\u2019t create a SparkSession\u2014since one is created for us\u2014yet employ all its exposed Spark functionality. To try this notebook, import it in <a href=\"https://databricks.com/try-databricks\" target=\"_blank\">Databricks</a>.\n\n<h2>SparkSession Encapsulates SparkContext</h2>\n\nLastly, for historical context, let\u2019s briefly understand the SparkContext\u2019s underlying functionality.\n\n[caption id=\"attachment_8927\" align=\"aligncenter\" width=\"615\"]<a href=\"https://databricks.com/wp-content/uploads/2016/08/image04.png\"><img src=\"https://databricks.com/wp-content/uploads/2016/08/image04.png\" alt=\"Fig 7. SparkContext as it relates to Driver and Cluster Manager\" width=\"613\" height=\"330\" class=\"size-full wp-image-8927\" /></a> Fig 7. SparkContext as it relates to Driver and Cluster Manager[/caption]\n\nAs shown in the diagram, a <a href=\"https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkContext.scala\" target=\"_blank\">SparkContext</a> is a conduit to access all Spark functionality; only a single SparkContext exists per JVM. The Spark driver program uses it to connect to the cluster manager to communicate, submit Spark jobs and knows what resource manager (YARN, Mesos or Standalone) to communicate to. It allows you to configure Spark configuration parameters. And through SparkContext, the driver can access other contexts such as SQLContext, HiveContext, and StreamingContext to program Spark.\n\nHowever, with Spark 2.0, SparkSession can access all aforementioned Spark\u2019s functionality through a single-unified point of entry. As well as making it simpler to access DataFrame and Dataset APIs, it also subsumes the underlying contexts to manipulate data.\n\nIn summation, what I demonstrated in this blog is that all functionality previously available through SparkContext, SQLContext or HiveContext in early versions of Spark are now available via SparkSession. In essence, SparkSession is a single-unified entry point to manipulate data with Spark, minimizing number of concepts to remember or construct. Hence, if you have fewer programming constructs to juggle, you\u2019re more likely to make fewer mistakes and your code is likely to be less cluttered.\n\n<h2>What's Next?</h2>\n\nThis is the first in the series of how-to blog posts on new features and functionality introduced in Spark 2.0 and how you can use them on the Databricks just-time-data platform. Stay tuned for other how-to blogs in the coming weeks.\n\n<ul>\n    <li>Try the accompanying <a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/SparkSessionZipsExample.html\" target=\"_blank\">SparkSessionZipsExample Notebook</a></li>\n    <li>Try the corresponding Spark application on <a href=\"https://github.com/dmatrix/examples/tree/master/spark/databricks/apps/scala/2.x\" target=\"_blank\">my github repo</a></li>\n    <li>Try an additional <a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/spark2.0/SparkSession.html\" target=\"_blank\">SparkSession Notebook</a></li>\n    <li>Import these notebooks today in <a href=\"http://databricks.com/try\" target=\"_blank\">Databricks</a> for free</li>\n</ul>"}
{"status": "publish", "description": "Chain notebooks together using a simple API in Databricks to build production pipelines.", "creator": "dave_wang", "link": "https://databricks.com/blog/2016/08/30/notebook-workflows-the-easiest-way-to-implement-apache-spark-pipelines.html", "authors": null, "id": 8986, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2016-08-30", "tz": "UTC", "createdOn": "2016-08-30"}, "title": "Notebook Workflows: The Easiest Way to Implement Apache Spark Pipelines", "slug": "notebook-workflows-the-easiest-way-to-implement-apache-spark-pipelines", "content": "Today we are excited to announce Notebook Workflows in Databricks. Notebook Workflows is a set of APIs that allow users to chain notebooks together using the standard control structures of the source programming language \u2014 Python, Scala, or R \u2014 to build production pipelines. This functionality makes Databricks the first and only product to support building Apache Spark workflows directly from notebooks, offering data science and engineering teams a new paradigm to build production data pipelines.\n\nTraditionally, teams need to integrate many complicated tools (notebooks, Spark infrastructure, external workflow manager just to name a few) to analyze data, prototype applications, and then deploy them into production. With Databricks, everything can be done in a single environment, making the entire process much easier, faster, and more reliable.\n<h2>Simplifying Pipelines with Notebooks</h2>\nNotebooks are very helpful in building a pipeline even with compiled artifacts. Being able to visualize data and interactively experiment with transformations makes it much easier to write code in small, testable chunks. More importantly, the development of most data pipelines begins with exploration, which is the perfect use case for notebooks. As an example, <a href=\"https://databricks.com/blog/2015/11/05/using-databricks-to-transition-from-concept-to-product.html\">Yesware regularly uses Databricks Notebooks to prototype new features for their ETL pipeline</a>.\n\nOn the flip side, teams also run into problems as they use notebooks to take on more complex data processing tasks:\n<ul>\n \t<li><strong>Logic within notebooks becomes harder to organize.</strong> Exploratory notebooks start off as simple sequences of Spark commands that run in order. However, it is common to make decisions based on the result of prior steps in a production pipeline - which is often at odds with how notebooks are written during the initial exploration.</li>\n \t<li><strong>Notebooks are not modular enough.</strong> Teams need the ability to retry only a subset of a data pipeline so that a failure does not require re-running the entire pipeline.</li>\n</ul>\nThese are the common reasons that teams often re-implement notebook code for production. The re-implementation process is time-consuming, tedious, and negates the interactive properties of notebooks.\n<h2>Databricks Notebook Workflows</h2>\nWe took a fresh look at the problem and decided that a new approach is needed. Our goal is to provide a unified platform that eliminates the friction between data exploration and production applications. We started out by providing a fully managed notebook environment for ad hoc experimentation, as well as a <a href=\"https://vimeo.com/156886719#t=82s\">Job Scheduler</a> that allows users to deploy notebooks directly to production via a simple UI. By adding Notebook Workflows on top of these existing functionalities, we are providing users the fastest, easiest way to create complex workflows out of their data processing code.\n\nDatabricks Notebook Workflows are a <a href=\"https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html#01%20Databricks%20Overview/030%20Notebook%20Workflows.html\">set of APIs</a> to chain together Notebooks and run them in the Job Scheduler. Users create their workflows directly inside notebooks, using the control structures of the source programming language (Python, Scala, or R). For example, you can use if statements to check the status of a workflow step, use loops to repeat work, or even take decisions based on the value returned by a step. This approach is much simpler than external workflow tools such as Apache Airflow, Oozie, Pinball, or Luigi because users can transition from exploration to production in the same environment instead of operating another system.\n\nNotebook Workflows are supervised by the Databricks Jobs Scheduler. This means that every workflow gets the production functionality provided by Jobs, such as fault recovery and timeout mechanisms. It also takes advantage of Databricks\u2019 version control and security features \u2014\u00a0helping teams manage the evolution of complex workflows through GitHub, and securing access to production infrastructure through role-based access control.\n\n[caption id=\"attachment_8987\" align=\"aligncenter\" width=\"764\"]<img class=\"wp-image-8987 size-full\" src=\"https://databricks.com/wp-content/uploads/2016/08/databricks-notebook-workflows-diagram-e1472236191717.png\" alt=\"Databricks Notebook Workflows diagram\" width=\"764\" height=\"493\" /> Figure: Databricks Notebook Workflows is a set of APIs to chain together Databricks Notebooks and run them in the Job Scheduler. Highlighted cells in the diagram show\u00a0the API calling other notebooks.[/caption]\n<h2>How to Use Notebook Workflows</h2>\n<h3>Running a notebook as a workflow with parameters</h3>\nThe most basic action of a Notebook Workflow is to simply run a notebook with the <code>dbutils.notebook.run()</code> command. The command runs the notebook on the cluster the caller notebook is attached to, provided that you have the right permissions (<a href=\"https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html#01%20Databricks%20Overview/12%20Access%20Controls/01%20Workspace%20ACLs.html\">see our ACLs documentation</a> to learn more about notebook and cluster level permissions).\n\nThe <code>dbutils.notebook.run()</code> command also allows you to pass in arguments to the notebook, like this:\n<pre>dbutils.notebook.run(\n  \"../path/to/my/notebook\",\n  timeout_seconds = 60,\n  arguments = {\"x\": \"value1\", \"y\": \"value2\", ...})</pre>\n[caption id=\"attachment_8988\" align=\"aligncenter\" width=\"600\"]<img class=\"wp-image-8988\" src=\"https://databricks.com/wp-content/uploads/2016/08/dbutil-notebook-run-output.png\" alt=\"Example: Running a notebook in Databricks\" width=\"600\" height=\"74\" /> Example: Running a notebook in Databricks[/caption]\n<h3>Getting return values</h3>\nTo create more flexible workflows, the <code>dbutils.notebook.run()</code> command can pass back a return value, like this:\n<pre>status = dbutils.notebook.run(\"../path/to/my/notebook\", timeout_seconds = 60)</pre>\nThe <code>dbutils.notebook.exit()</code> command in the callee notebook needs to be invoked with a string as the argument, like this:\n<pre>dbutils.notebook.exit(str(resultValue))</pre>\nIt is also possible to return structured data by referencing data stored in a temporary table or write the results to DBFS (Databricks\u2019 caching layer over Amazon S3) and then return the path of the stored data.\n<h3>Control flow and exception handling</h3>\nYou can control the execution flow of your workflow and handle exceptions using the standard if/then statements and exception processing statements in either Scala or Python. For example:\n<pre>try:\n  nextStep = dbutils.notebook.run(\n    \"DataImportNotebook\", 250, {\"input_path\": importStagingDir})\n  if nextStep == \"Clean\":\n    dbutils.notebook.run(\"CleaningNotebook\", 500)\n  else:\n    dbutils.notebook.run(\u201cETLNotebook\u201d, 3600)\nexcept Exception as e:\n  print \"Error importing data.\"\n  dbutils.notebook.run(\u201cErrorNotebook\u201d, 1500)</pre>\nYou can also use workflows to perform retries and pass more complex data between notebooks. See <a href=\"https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html#01%20Databricks%20Overview/030%20Notebook%20Workflows.html\">the documentation</a> for more details.\n<h3>Running concurrent notebook workflows</h3>\nUsing built-in libraries in Python and Scala, you can launch multiple workflows in parallel. Here we show a simple example of running three ETL tasks in parallel from a Python notebook. Since workflows are integrated with the native language, it is possible to express arbitrary concurrency and retry behaviors in the user's preferred language, in contrast to other workflow engines.\n\n<img class=\"aligncenter size-full wp-image-8989\" src=\"https://databricks.com/wp-content/uploads/2016/08/concurrent-notebook-workflows-example.png\" alt=\"Example of running concurrent Notebook workflows\" width=\"672\" height=\"486\" />\n<h3>Debugging</h3>\nThe run command returns a link to a job, which you can use to deep-dive on performance and debug the workflow. Simply open the caller notebook and click on the callee notebook link as shown below and you can start drilling down with the built-in Spark History UI.\n\n<img class=\"aligncenter size-full wp-image-8990\" src=\"https://databricks.com/wp-content/uploads/2016/08/debugging-spark-jobs.png\" alt=\"Debugging spark jobs in Databricks\" width=\"600\" height=\"392\" />\n<h2>What\u2019s Next</h2>\nHave questions? Got tips you want to share with others? Visit <a href=\"https://forums.databricks.com/\">the Databricks forum</a> and participate in our user community.\n\nWe are just getting started with helping Databricks users build workflows. Stay tuned for more functionality in the near future. Try to build workflows by <a href=\"https://databricks.com/try-databricks\">signing up for a trial of Databricks</a> today. You can also find <a href=\"https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html#01%20Databricks%20Overview/030%20Notebook%20Workflows.html\">more detailed documentation here</a>, and a <a href=\"https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html#01%20Databricks%20Overview/13%20Advanced%20Features/03%20Notebook%20Workflows%20Demo/01%20Demo.html\">demo here</a>."}
{"status": "publish", "description": "A recap of what\u2019s transpired over the last two weeks with Apache Spark from Databricks", "creator": "jules_damji", "link": "https://databricks.com/blog/2016/08/31/databricks-bi-weekly-digest-83116.html", "authors": null, "id": 9072, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2016-08-31", "tz": "UTC", "createdOn": "2016-08-31"}, "title": "Databricks Bi-Weekly Digest: 8/31/16", "slug": "databricks-bi-weekly-digest-83116", "content": "Here\u2019s our recap of what\u2019s transpired with Apache Spark since our <a href=\"https://databricks.com/blog/2016/08/08/databricks-bi-weekly-digest-8816.html\" target=\"_blank\">previous digest</a>.\n\n<ul>\n\t<li>Databricks CTO and Co-founder Matei Zaharia presented \u201cUnifying big data workloads in Apache Spark\u201d at <a href=\"https://atscaleconference.com/events/main-event/\" target=\"_blank\">@Scale Conference</a>.</li>\n\t<li>Databricks CTO and Co-founder Matei Zaharia talked with SiliconAngle/theCUBE about <a href=\"http://siliconangle.com/blog/2016/08/17/streaming-batch-and-interactive-the-synergy-of-end-to-end-data-apps/\" target=\"_blank\">How Apache Spark is transforming apps with data streaming</a>.</li>\n\t<li>Tim Hunter released two Apache Spark packages: <a href=\"https://spark-packages.org/package/graphframes/graphframes\" target=\"_blank\">GraphFrames Package</a> and <a href=\"https://pypi.python.org/pypi/spark-sklearn/0.2.0\" target=\"_blank\">scikit-learn package 0.2.0</a> (with support for fitting keyed models\u2014e.g., fitting 1 model per customer).</li>\n\t<li>Tim Hunter presented at the Bay Area Spark Meetup @ Salesforce: <a href=\"http://www.meetup.com/spark-users/events/232428471/\" target=\"_blank\">TensorFrames on Google\u2019s TensorFlow and Apache Spark</a>.</li>\n\t<li>Joseph Bradley answered on SiliconAngle/theCUBE: <a href=\"http://siliconangle.com/blog/2016/08/17/can-spark-do-for-machine-learning-what-its-done-for-data/\" target=\"_blank\">Can Apache Spark do for machine learning what it\u2019s has done for data?</a></li>\n\t<li>Spark Community Evangelist Jules Damji blogged on <a href=\"https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html\" target=\"_blank\">How to use SparkSession in Apache Spark 2.0</a>.</li>\n\t<li>Pavel Hardak of Basho demonstrated the <a href=\"https://databricks.com/blog/2016/08/11/the-quest-for-hidden-treasure-an-apache-spark-connector-for-the-riak-nosql-database.html\" target=\"_blank\">Riak Integration and connector to Apache Spark 2.0</a></li>\n\t<li>Burak Yavuz showed in a webinar how Apache Spark is used to <a href=\"https://databricks.com/blog/2016/08/16/databricks-data-pipeline-on-demand-webinar-faq.html\" target=\"_blank\">simplify log ETL Pipeline</a>.</li>\n\t<li>Michael Armbrust explained aspects of declarative programming on SiliconAngle/theCUBE: <a href=\"http://siliconangle.com/blog/2016/08/17/just-do-it-declarative-programming-for-simplifying-data-queries/\" target=\"_blank\">Just do it: Declarative programming for simplifying data queries</a>.</li>\n\t<li>Databricks VP of Engineering Patrick Wendell interviewed by SiliconAngle/theCUBE: <a href=\"http://siliconangle.com/blog/2016/08/17/double-team-the-softwarehardware-sandwich-thats-taking-data-up-a-level/\" target=\"_blank\">Double-team: The software/hardware sandwich that\u2019s taking data up a level</a>.</li>\n</ul>\n<h2>What\u2019s Next</h2>\n\nTo stay abreast with what\u2019s happening with Apache Spark, follow us on Twitter <a href=\"https://twitter.com/databricks\" target=\"_blank\">@databricks</a> and visit <a href=\"https://sparkhub.databricks.com/\" target=\"_blank\">SparkHub</a>."}
{"status": "publish", "description": "This is a guest Apache Spark community blog from Facebook Engineering. In this technical blog, authors Sital Kedia et.al share their usage of Apache Spark at terabyte scale in a production use case.", "creator": "jules_damji", "link": "https://databricks.com/blog/2016/08/31/apache-spark-scale-a-60-tb-production-use-case.html", "authors": null, "id": 9081, "categories": ["Apache Spark", "Ecosystem", "Engineering Blog"], "dates": {"publishedOn": "2016-08-31", "tz": "UTC", "createdOn": "2016-08-31"}, "title": "Apache Spark @Scale: A 60 TB+ production use case from Facebook", "slug": "apache-spark-scale-a-60-tb-production-use-case", "content": "<em>This is a guest Apache Spark community blog from <a href=\"https://code.facebook.com/posts/1671373793181703\" target=\"_blank\">Facebook Engineering</a>. In this technical blog, Facebook shares their usage of Apache Spark at terabyte scale in a production use case.</em>\n\nFacebook often uses analytics for data-driven decision making. Over the past few years, user and product growth has pushed our analytics engines to operate on data sets in the tens of terabytes for a single query. Some of our batch analytics is executed through the venerable\u00a0<a href=\"https://code.facebook.com/posts/370832626374903/even-faster-data-at-the-speed-of-presto-orc/\" target=\"_blank\">Hive</a>\u00a0platform (contributed to Apache Hive by Facebook in 2009) and\u00a0<a href=\"https://www.facebook.com/notes/facebook-engineering/under-the-hood-scheduling-mapreduce-jobs-more-efficiently-with-corona/10151142560538920/\" target=\"_blank\">Corona</a>, our custom MapReduce implementation. Facebook has also continued to grow its Presto footprint for ANSI-SQL queries against several internal data stores, including Hive. We support other types of analytics such as graph processing and machine learning (<a href=\"https://code.facebook.com/posts/509727595776839/scaling-apache-giraph-to-a-trillion-edges/\" target=\"_blank\">Apache Giraph</a>) and streaming (e.g.,\u00a0<a href=\"https://research.facebook.com/publications/realtime-data-processing-at-facebook/\" target=\"_blank\">Puma</a>, <a href=\"https://research.facebook.com/publications/realtime-data-processing-at-facebook/\" target=\"_blank\">Swift</a>, and <a href=\"https://research.facebook.com/publications/realtime-data-processing-at-facebook/\" target=\"_blank\">Stylus</a>).\n\nWhile the sum of Facebook's offerings covers a broad spectrum of the analytics space, we continually interact with the open source community in order to share our experiences and also learn from others. <a href=\"http://spark.apache.org\" target=\"_blank\">Apache Spark\u00a0</a>was started by Matei Zaharia at UC-Berkeley's AMPLab in 2009 and was later contributed to Apache in 2013. It is currently one of the fastest-growing data processing platforms, due to its ability to support streaming, batch, imperative (RDD), declarative (SQL), graph, and machine learning use cases all within the same API and underlying compute engine. Spark can efficiently leverage larger amounts of memory, optimize code across entire pipelines, and reuse JVMs across tasks for better performance. Recently, we felt Spark had matured to the point where we could compare it with Hive for a number of batch-processing use cases. In the remainder of this article, we describe our experiences and lessons learned while scaling Spark to replace one of our Hive workload.\n\n<h2>Use case: Feature preparation for entity ranking</h2>\n\nReal-time entity ranking is used in a variety of ways at Facebook. For some of these online serving platforms raw feature values are generated offline with Hive and data loaded into its real-time affinity query system. The old Hive-based infrastructure built years ago was computationally resource intensive and challenging to maintain because the pipeline was sharded into hundreds of smaller Hive jobs. In order to enable fresher feature data and improve manageability, we took one of the existing pipelines and tried to migrate it to Spark.\n\n<h2>Previous Hive implementation</h2>\n\nThe Hive-based pipeline was composed of three logical stages where each stage corresponded to hundreds of smaller Hive jobs sharded by entity_id, since running large Hive jobs for each stage was less reliable and limited by the maximum number of tasks per job.\n<a href=\"https://databricks.com/wp-content/uploads/2016/08/fb_1.png\"><img src=\"https://databricks.com/wp-content/uploads/2016/08/fb_1-941x1024.png\" alt=\"fb_1\" width=\"941\" height=\"1024\" class=\"aligncenter size-large wp-image-9083\" /></a>\nThe three logical steps can be summarized as follows:\n\n<ol>\n    <li>Filter out non-production features and noise.</li>\n    <li>Aggregate on each\u00a0(<em>entity_id</em>, <em>target_id</em>)\u00a0pair.</li>\n    <li>Shard the table into N number of shards and pipe each shard through a custom binary to generate a custom index file for online querying.</li>\n</ol>\n\nThe Hive-based pipeline building the index took roughly three days to complete. It was also challenging to manage, because the pipeline contained hundreds of sharded jobs that made monitoring difficult. There was no easy way to gauge the overall progress of the pipeline or calculate an ETA. When considering the aforementioned limitations of the existing Hive pipeline, we decided to attempt to build a faster and more manageable pipeline with Spark.\n\n<h2>Spark implementation</h2>\n\nDebugging at full scale can be slow, challenging, and resource intensive. We started off by converting the most resource intensive part of the Hive-based pipeline: stage two. We started with a sample of 50 GB of compressed input, then gradually scaled up to 300 GB, 1 TB, and then 20 TB. At each size increment, we resolved performance and stability issues, but experimenting with 20 TB is where we found our largest opportunity for improvement.\n\nWhile running on 20 TB of input, we discovered that we were generating too many output files (each sized around 100 MB) due to the large number of tasks. Three out of 10 hours of job runtime were spent moving files from the staging directory to the final directory in HDFS. Initially, we considered two options: Either improve batch renaming in HDFS to support our use case, or configure Spark to generate fewer output files (difficult due to the large number of tasks \u2014 70,000 \u2014 in this stage). We stepped back from the problem and considered a third alternative. Since the tmp_table2 table we generate in step two of the pipeline is temporary and used only to store the pipeline's intermediate output, we were essentially compressing, serializing, and replicating three copies for a single read workload with terabytes of data. Instead, we went a step further: Remove the two temporary tables and combine all three Hive stages into a single Spark job that reads 60 TB of compressed data and performs a 90 TB shuffle and sort. The final Spark job is as follows:\n<a href=\"https://databricks.com/wp-content/uploads/2016/08/fb_2.png\"><img src=\"https://databricks.com/wp-content/uploads/2016/08/fb_2-1024x447.png\" alt=\"fb_2\" width=\"1024\" height=\"447\" class=\"aligncenter size-large wp-image-9085\" /></a>\n\n<h2>How did we scale Spark for this job?</h2>\n\nOf course, running a single Spark job for such a large pipeline didn't work on the first try, or even on the 10th try. As far as we know, this is the largest real-world Spark job attempted in terms of shuffle data size (<a href=\"https://databricks.com/blog/2014/10/10/spark-petabyte-sort.html\" target=\"_blank\">Databricks' Petabyte sort</a>\u00a0was on synthetic data). It took numerous improvements and optimizations to the core Spark infrastructure and our application to get this job to run. The upside of this effort is that many of these improvements are applicable to other large-scale workloads for Spark, and we were able to contribute all our work back into the open source Apache Spark project \u2014 see the JIRAs for additional details. Below, we highlight the major improvements that enabled one of the entity ranking pipelines to be deployed into production.\n\n<h2>Reliability fixes</h2>\n\n<h3>Dealing with frequent node reboots</h3>\n\nIn order to reliably execute long-running jobs, we want the system to be fault-tolerant and recover from failures (mainly due to machine reboots that can occur due to normal maintenance or software errors). Although Spark is designed to tolerate machine reboots, we found various bugs/issues that needed to be addressed before it was robust enough to handle common failures.\n\n<ul>\n    <li><strong>Make PipedRDD robust to fetch failure</strong>\u00a0(<a href=\"https://issues.apache.org/jira/browse/SPARK-13793\" target=\"_blank\">SPARK-13793</a>): The previous implementation of PipedRDD was not robust enough to fetch failures that occur due to node reboots, and the job would fail whenever there was a fetch failure. We made change in the PipedRDD to handle fetch failure gracefully so the job can recover from these types of fetch failure.</li>\n    <li><strong>Configurable max number of fetch failures</strong> (<a href=\"https://issues.apache.org/jira/browse/SPARK-13369\" target=\"_blank\">SPARK-13369</a>): With long-running jobs such as this one, probability of fetch failure due to a machine reboot increases significantly. The maximum allowed fetch failures per stage was hard-coded in Spark, and, as a result, the job used to fail when the max number was reached. We made a change to make it configurable and increased it from four to 20 for this use case, which made the job more robust against fetch failure.</li>\n    <li><strong>Less disruptive cluster restart</strong>: Long-running jobs should be able to survive a cluster restart so we don't waste all the processing completed so far. Spark's restartable shuffle service feature lets us preserve the shuffle files after node restart. On top of that, we implemented a feature in Spark driver to be able to pause scheduling of tasks so the jobs don't fail due to excessive task failure due to cluster restart.</li>\n</ul>\n\n<h3>Other reliability fixes</h3>\n\n<ul>\n    <li><strong>Unresponsive driver</strong>\u00a0(<a href=\"https://issues.apache.org/jira/browse/SPARK-13279\" target=\"_blank\">SPARK-13279</a>): Spark driver was stuck due to O(N^2) operations while adding tasks, resulting in the job being stuck and killed eventually. We fixed the issue by removing the unnecessary O(N^2) operations.</li>\n    <li><strong>Excessive driver speculation</strong>: We discovered that the Spark driver was spending a lot of time in speculation when managing a large number of tasks. In the short term, we disabled speculation for this job. We are currently working on a change in the Spark driver to reduce speculation time in the long term.</li>\n    <li><strong>TimSort issue due to integer overflow for large buffer</strong>\u00a0(<a href=\"https://issues.apache.org/jira/browse/SPARK-13850\" target=\"_blank\">SPARK-13850</a>): We found that Spark's unsafe memory operation had a bug that leads to memory corruption in TimSort. Thanks to Databricks folks for fixing this issue, which enabled us to operate on large in-memory buffer.</li>\n    <li><strong>Tune the shuffle service to handle large number of connections</strong>: During the shuffle phase, we saw many executors timing out while trying to connect to the shuffle service. Increasing the number of Netty server threads (<em>spark.shuffle.io.serverThreads</em>) and backlog (<em>spark.shuffle.io.backLog</em>) resolved the issue.</li>\n    <li><strong>Fix Spark executor OOM</strong>\u00a0(<a href=\"https://issues.apache.org/jira/browse/SPARK-13958\" target=\"_blank\">SPARK-13958</a>)\u00a0<strong>(deal maker)</strong>: It was challenging to pack more than four reduce tasks per host at first. Spark executors were running out of memory because there was a bug in the sorter that caused a pointer array to grow indefinitely. We fixed the issue by forcing the data to be spilled to disk when there is no more memory available for the pointer array to grow. As a result, now we can run 24 tasks/host without running out of memory.</li>\n</ul>\n\n<h2>Performance improvements</h2>\n\nAfter implementing the reliability improvements above, we were able to reliably run the Spark job. At this point, we shifted our efforts on performance-related projects to get the most out of Spark. We used Spark's metrics and several profilers to find some of the performance bottlenecks.\n\n<h3>Tools we used to find performance bottleneck</h3>\n\n<ul>\n    <li><strong>Spark UI Metrics</strong>: Spark UI provides great insight into where time is being spent in a particular phase. Each task's execution time is split into sub-phases that make it easier to find the bottleneck in the job.</li>\n    <li><strong>Jstack</strong>: Spark UI also provides an on-demand jstack function on an executor process that can be used to find hotspots in the code.</li>\n    <li><strong>Spark Linux Perf/Flame Graph support:</strong> Although the two tools above are very handy, they do not provide an aggregated view of CPU profiling for the job running across hundreds of machines at the same time. On a per-job basis, we added support for enabling Perf profiling (via libperfagent for Java symbols) and can customize the duration/frequency of sampling. The profiling samples are aggregated and displayed as a Flame Graph across the executors using our internal metrics collection framework.</li>\n</ul>\n\n<h3>Performance optimizations</h3>\n\n<ul>\n    <li><strong>Fix memory leak in the sorter</strong>\u00a0(<a href=\"https://issues.apache.org/jira/browse/SPARK-14363\" target=\"_blank\">SPARK-14363</a>)\u00a0<strong>(30 percent speed-up)</strong>: We found an issue when tasks were releasing all memory pages but the pointer array was not being released. As a result, large chunks of memory were unused and caused frequent spilling and executor OOMs. Our change now releases memory properly and enabled large sorts to run efficiently. We noticed a 30 percent CPU improvement after this change.</li>\n    <li><strong>Snappy optimization</strong>\u00a0(<a href=\"https://issues.apache.org/jira/browse/SPARK-14277\" target=\"_blank\">SPARK-14277</a>)\u00a0<strong>(10 percent speed-up)</strong>: A JNI method \u2014 <em>(Snappy.ArrayCopy</em>) \u2014 was being called for each row being read/written. We raised this issue, and the Snappy behavior was changed to use the non-JNI based\u00a0<em>System.ArrayCopy</em>\u00a0instead. This change alone provided around 10 percent CPU improvement.</li>\n    <li><strong>Reduce shuffle write latency\u00a0</strong>(<a href=\"https://issues.apache.org/jira/browse/SPARK-5581\" target=\"_blank\">SPARK-5581</a>)\u00a0<strong>(up to 50 percent speed-up)</strong>: On the map side, when writing shuffle data to disk, the map task was opening and closing the same file for each partition. We made a fix to avoid unnecessary open/close and observed a CPU improvement of up to 50 percent for jobs writing a very high number of shuffle partitions.</li>\n    <li><strong>Fix duplicate task run issue due to fetch failure\u00a0</strong>(<a href=\"https://issues.apache.org/jira/browse/SPARK-14649\" target=\"_blank\">SPARK-14649</a>): The Spark driver was resubmitting already running tasks when a fetch failure occurred, which led to poor performance. We fixed the issue by avoiding rerunning the running tasks, and we saw the job was more stable when fetch failures occurred.</li>\n    <li><strong>Configurable buffer size for PipedRDD</strong>\u00a0(<a href=\"https://issues.apache.org/jira/browse/SPARK-14542\" target=\"_blank\">SPARK-14542</a>)\u00a0<strong>(10 percent speed-up)</strong>: While using a PipedRDD, we found out that the default buffer size for transferring the data from the sorter to the piped process was too small and our job was spending more than 10 percent of time in copying the data. We made the buffer size configurable to avoid this bottleneck.</li>\n    <li><strong>Cache index files for shuffle fetch speed-up</strong>\u00a0(<a href=\"https://issues.apache.org/jira/browse/SPARK-15074\" target=\"_blank\">SPARK-15074</a>): We observed that the shuffle service often becomes the bottleneck, and the reducers spend 10 percent to 15 percent of time waiting to fetch map data. Digging deeper into the issue, we found out that the shuffle service is opening/closing the shuffle index file for each shuffle fetch. We made a change to cache the index information so that we can avoid file open/close and reuse the index information for subsequent fetches. This change reduced the total shuffle fetch time by 50 percent.</li>\n    <li><strong>Reduce update frequency of shuffle bytes written metrics</strong>\u00a0(<a href=\"https://issues.apache.org/jira/browse/SPARK-15569\" target=\"_blank\">SPARK-15569</a>)\u00a0<strong>(up to 20 percent speed-up)</strong>: Using the Spark Linux Perf integration, we found that around 20 percent of the CPU time was being spent probing and updating the shuffle bytes written metrics.</li>\n    <li><strong>Configurable initial buffer size for Sorter</strong>\u00a0(<a href=\"https://issues.apache.org/jira/browse/SPARK-15958\" target=\"_blank\">SPARK-15958</a>)\u00a0<strong>(up to 5 percent speed-up)</strong>: The default initial buffer size for the Sorter is too small (4 KB), and we found that it is very small for large workloads \u2014 and as a result we waste a significant amount of time expending the buffer and copying the contents. We made a change to make the buffer size configurable, and with large buffer size of 64 MB we could avoid significant data copying, making the job around 5 percent faster.</li>\n    <li><strong>Configuring number of tasks:</strong> Since our input size is 60 T and each HDFS block size is 256 M, we were spawning more than 250,000 tasks for the job. Although we were able to run the Spark job with such a high number of tasks, we found that there is significant performance degradation when the number of tasks is too high. We introduced a configuration parameter to make the map input size configurable, so we can reduce that number by 8x by setting the input split size to 2 GB.</li>\n</ul>\n\nAfter all these reliability and performance improvements, we are pleased to report that we built and deployed a faster and more manageable pipeline for one of our entity ranking systems, and we provided the ability for other similar jobs to run in Spark.\n\n<h2>Spark pipeline vs. Hive pipeline performance comparison</h2>\n\nWe used the following performance metrics to compare the Spark pipeline against the Hive pipeline. Please note that these numbers aren't a direct comparison of Spark to Hive at the query or job level, but rather a comparison of building an optimized pipeline with a flexible compute engine (e.g., Spark) instead of a compute engine that operates only at the query/job level (e.g., Hive).\n\n<strong>CPU time</strong>:\u00a0This is the CPU usage from the perspective of the OS. For example, if you have a job that is running only one process on a 32-core machine using 50 percent of all CPU for 10 seconds, then your CPU time would be 32 * 0.5 * 10 = 160 CPU seconds.\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/08/fb_3.png\"><img src=\"https://databricks.com/wp-content/uploads/2016/08/fb_3-1024x639.png\" alt=\"fb_3\" width=\"1024\" height=\"639\" class=\"aligncenter size-large wp-image-9089\" /></a>\n\n<strong>CPU reservation time:</strong>\u00a0This is the CPU reservation from the perspective of the resource management framework. For example, if we reserve a 32-core machine for 10 seconds to run the job, the CPU reservation time is 32 * 10 = 320 CPU seconds. The ratio of CPU time to CPU reservation time reflects how well are we utilizing the reserved CPU resources on the cluster. When accurate, the reservation time provides a better comparison between execution engines when running the same workloads when compared with CPU time. For example, if a process requires 1 CPU second to run but must reserve 100 CPU seconds, it is less efficient by this metric than a process that requires 10 CPU seconds but reserves only 10 CPU seconds to do the same amount of work. We also compute the memory reservation time but do not include it here, since the numbers were similar to the CPU reservation time due to running experiments on the same hardware, and that in both the Spark and Hive cases we do not cache data in memory. Spark has the ability to cache data in memory, but due to our cluster memory limitations we decided to work out-of-core, similar to Hive.\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/08/fb_4.png\"><img src=\"https://databricks.com/wp-content/uploads/2016/08/fb_4-1024x647.png\" alt=\"fb_4\" width=\"1024\" height=\"647\" class=\"aligncenter size-large wp-image-9090\" /></a>\n\n<strong>Latency</strong>:\u00a0End-to-end elapsed time of the job.\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/08/fb_5.png\"><img src=\"https://databricks.com/wp-content/uploads/2016/08/fb_5-1024x654.png\" alt=\"fb_5\" width=\"1024\" height=\"654\" class=\"aligncenter size-large wp-image-9091\" /></a>\n\n<h2>Conclusion and future work</h2>\n\nFacebook uses performant and scalable analytics to assist in product development. Apache Spark offers the unique ability to unify various analytics use cases into a single API and efficient compute engine. We challenged Spark to replace a pipeline that decomposed to hundreds of Hive jobs into a single Spark job. Through a series of performance and reliability improvements, we were able to scale Spark to handle one of our entity ranking data processing use cases in production. In this particular use case, we showed that Spark could reliably shuffle and sort 90 TB+ intermediate data and run 250,000 tasks in a single job. The Spark-based pipeline produced significant performance improvements (4.5-6x CPU, 3-4x resource reservation, and ~5x latency) compared with the old Hive-based pipeline, and it has been running in production for several months.\n\nWhile this post details our most challenging use case for Spark, a growing number of customer teams have deployed Spark workloads into production. Performance, maintainability, and flexibility are the strengths that continue to drive more use cases to Spark. Facebook is excited to be a part of the Spark open source community and will work together to develop Spark toward its full potential."}
{"status": "publish", "description": "This guide demonstrates how to transform raw data, create Apache Spark UDFs, and write the data to multiple locations using Spark\u2019s data source APIs.", "creator": "bill", "link": "https://databricks.com/blog/2016/09/06/writing-data-engineering-pipelines-in-apache-spark-on-databricks.html", "authors": null, "id": 9129, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2016-09-06", "tz": "UTC", "createdOn": "2016-09-06"}, "title": "Writing Data Engineering Pipelines in Apache Spark on Databricks", "slug": "writing-data-engineering-pipelines-in-apache-spark-on-databricks", "content": "[dbce_cta href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/346304/2168141618055109/484361/latest.html\"]Try this notebook in Databricks[/dbce_cta]\n\n[sidenote]This is part 3 of a 3 part series providing a gentle introduction to writing Apache Spark applications on Databricks.[/sidenote]\n\n<hr />\n\nThe big advantage of running Apache Spark on Databricks for data engineers is that it\u2019s an easy tool to plug and play with an entire ecosystem of databases, tools, and environments. Building robust pipelines is simple because you can work with a smaller amount of data to prototype and scale to nearly any size of data you throw at it. Beyond sheer data volume, there are many <a href=\"https://spark-packages.org/\" target=\"_blank\" rel=\"noopener\">spark packages</a> for connecting to a variety of different data sources from SQL to NoSQL and everything in between.\n\nDatabricks takes this to the next level by providing a holistic set of managed services such as <a href=\"https://databricks.com/product/databricks#notebooks\" target=\"_blank\" rel=\"noopener\">notebooks</a>, <a href=\"https://databricks.com/blog/2016/03/30/announcing-new-databricks-apis-for-faster-production-apache-spark-application-deployment.html\" target=\"_blank\" rel=\"noopener\">a sophisticated API</a>, <a href=\"https://databricks.com/product/databricks#jobs\" target=\"_blank\" rel=\"noopener\">production jobs</a> and <a href=\"https://databricks.com/blog/2016/08/30/notebook-workflows-the-easiest-way-to-implement-apache-spark-pipelines.html\" target=\"_blank\" rel=\"noopener\">workflows</a>, and <a href=\"https://databricks.com/blog/2016/04/06/continuous-integration-and-delivery-of-apache-spark-applications-at-metacog.html\" target=\"_blank\" rel=\"noopener\">continuous integration use cases</a>. Take a look at demonstrations from companies building production systems on Apache Spark and Databricks like <a href=\"https://spark-summit.org/2016/events/bulletproof-jobs-patterns-for-large-scale-spark-processing/\" target=\"_blank\" rel=\"noopener\">Swoop</a>, <a href=\"https://spark-summit.org/2016/events/video-games-at-scale-improving-the-gaming-experience-with-apache-spark/\" target=\"_blank\" rel=\"noopener\">Riot Games</a>, and\u00a0<a href=\"https://spark-summit.org/2016/events/unified-framework-for-real-time-near-real-time-and-offline-analysis-of-video-streaming-using-apache-spark-and-databricks/\" target=\"_blank\" rel=\"noopener\">Conviva</a>.\n\nIn the previous guides in this series, we provided an <a href=\"https://databricks.com/blog/2016/06/15/an-introduction-to-writing-apache-spark-applications-on-databricks.html\" target=\"_blank\" rel=\"noopener\">introduction to writing Apache Spark applications on Databricks</a>. Be sure to check it out if you have not already! The <a href=\"https://databricks.com/blog/2016/06/28/building-data-science-applications-on-databricks.html\" target=\"_blank\" rel=\"noopener\">second guide was geared towards the workflow of a data scientist</a> - showing the process of iterative data analytics towards building a predictive model using datasets from the US Department of Agriculture and the Internal Revenue Service.\n\nThis third guide is meant for the data engineer, who needs a simple, production-ready guide to show them how to take raw data, transform it through Python, SQL or Scala, create and register Apache Spark UDFs, and finally write that data to multiple locations using Apache Spark\u2019s data source APIs. While these tasks are made simpler with Spark, this example will show how Databricks makes it even easier for a data engineer to take a prototype to production.\n\n<h2>What\u2019s in this guide</h2>\n\n<a href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/346304/2168141618055109/484361/latest.html\" target=\"_blank\" rel=\"noopener\">The guide</a> illustrates how to import data and build a robust Apache Spark data pipeline on Databricks. We\u2019ll walk through building simple log pipeline from the raw logs all the way to placing this data into permanent storage. In the process, we will demonstrate common tasks data engineers have to perform in an ETL pipeline, such as getting raw data from a variety of sources like Amazon S3, converting the data to Parquet format, and putting it into a data warehouse.\n\n<img class=\"aligncenter size-full wp-image-9125\" src=\"https://databricks.com/wp-content/uploads/2016/09/gentle-intro-blog-3-diagram.jpg\" alt=\"Diagram of the guide\" width=\"552\" height=\"112\" />\n\nThis end-to-end tutorial will also shed light on common challenges in data engineering and provide solutions to them. This includes working with a variety of different languages and handling non-standard datetimes in UDFs in Scala and SparkSQL.\n\nThe guide gives you an example of a stable ETL pipeline that we\u2019ll be able to put right into production with Databricks\u2019 Job Scheduler. This guide will go through:\n\n<ol>\n    <li>We\u2019ll create a function in Python that will convert raw Apache logs sitting in an S3 bucket to a DataFrame.</li>\n    <li>Next, we\u2019ll enumerate all the ways to create a UDF in Scala. This will allow us to use it as both an SQL function as well as on Scala DataFrames.</li>\n    <li>After joining that DataFrame with another one, we\u2019ll write data out to permanent storage in Redshift as well as a backup copy in Amazon S3.</li>\n</ol>\n\n<h2>What\u2019s next</h2>\n\nYou can work through the examples in this guide with the Databricks platform (<a href=\"https://databricks.com/try-databricks\">Sign-up</a> to try for free). For more resources to help you get started with Apache Spark, check out our <a href=\"https://databricks.com/product/getting-started-with-apache-spark-on-databricks\">introduction guide</a>."}
{"status": "publish", "description": null, "creator": "jules_damji", "link": "https://databricks.com/blog/2016/09/19/apache-spark-earns-datanami-awards-for-machine-learning-real-time-analytics-and-more.html", "authors": null, "id": 9144, "categories": ["Announcements", "Company Blog"], "dates": {"publishedOn": "2016-09-19", "tz": "UTC", "createdOn": "2016-09-19"}, "title": "Apache Spark Earns Datanami Awards for Machine Learning, Real-time Analytics, and More", "slug": "apache-spark-earns-datanami-awards-for-machine-learning-real-time-analytics-and-more", "content": "Today, the <a href=\"https://www.datanami.com/this-just-in/datanami-reveals-winners-inaugural-readers-editors-choice-awards/\" target=\"_blank\">Datanami Readers\u2019 and Editors\u2019 Choice Awards</a> recognized the sweeping changes Apache Spark is bringing to the Big Data landscape with four awards:\n<ul>\n\t<li><strong>Readers' Choice \u2013 Best Big Data Product or Technology: Machine Learning</strong></li>\n\t<li><strong>Readers' Choice \u2013 Best Big Data Product or Technology: Real-Time Analytics</strong></li>\n\t<li><strong>Readers' and Editors' Choice \u2013 Top 5 Open Source Projects to Watch</strong></li>\n\t<li><strong>Readers' Choice - Best Big Data Startup: Databricks</strong></li> \n</ul>\n\nDetermined through a nomination and voting process with input from the global Big Data Community and Datanami editors, the awards highlight key trends, shine a spotlight on technical breakthroughs, and capture a critical cross section of the state of the industry.\n\nWhile at UC Berkeley, Databricks Chief Technologist Matei Zaharia created Spark to unify different types of workloads under a fast and flexible engine. He believed that solving Big Data problems needed a simple way to merge a multitude of analytical and data processing techniques under one platform.\n\nThese awards validate our recent efforts with the Spark community. We\u2019ve focused in simplifying Spark streaming by creating a unified interface across all components, including <a href=\"https://databricks.com/blog/2016/05/31/apache-spark-2-0-preview-machine-learning-model-persistence.html\" target=\"_blank\">machine learning MLlib</a>, so that users can build end-to-end <a href=\"https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html\" target=\"_blank\">continuous applications</a> in an incremental fashion. We see a need among users to combine streaming with online machine learning by combining real-time training, periodic batch training, and prediction serving behind the same unified API. \n\nOur team is working hard with the community towards the next Apache Spark release, with many more innovation in machine learning and real-time analytics planned. They will share more details about the projects they have been working on in the coming months at the Databricks blog, so stay tuned.\n\nMeanwhile, you can try <a href=\"https://databricks.com/blog/2016/07/26/introducing-apache-spark-2-0.html\" target=\"_blank\">Apache Spark 2.0</a>, which lays the foundation for <a href=\"https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html\" target=\"_blank\">Structured Streaming APIs</a> and new <a href=\"http://spark.apache.org/mllib/\" target=\"_blank\">DataFrame-based APIs</a> for model persistence and machine learning pipelines in MLlib."}
{"status": "publish", "description": "Learn how to evaluate the most popular cloud-based Apache Spark solutions and understand the differences between them.", "creator": "Wayne Chan", "link": "https://databricks.com/blog/2016/09/28/how-to-evaluate-a-cloud-based-apache-spark-platform.html", "authors": null, "id": 9157, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2016-09-28", "tz": "UTC", "createdOn": "2016-09-28"}, "title": "How to Evaluate a Cloud-based Apache Spark Platform", "slug": "how-to-evaluate-a-cloud-based-apache-spark-platform", "content": "As the most active open source project in big data, Apache Spark is taking the world by storm. This rise to prominence coupled with the adoption of the cloud is fueling the emergence of many Spark-powered data platforms in the public cloud.\n\nThere are\u00a0different types of solutions built around\u00a0Apache Spark including:\n\n<ul>\n    <li>Hadoop-based platforms that are now providing support for Spark in addition to MapReduce;</li>\n    <li>Infrastructure management services that help you run and manage your clusters;</li>\n    <li>General-purpose managed services that provide basic functionality and support for a broad range of technologies;</li>\n    <li>Fully managed platforms that offer a comprehensive set of functionality for a diverse data organization.</li>\n</ul>\n\nThere is no \u201cone size fits all\u201d approach when selecting the right platform to meet your specific requirements. With all the information on the internet from vendors, thought leaders, and so called experts, selecting the right vendor can be a daunting task. Making the wrong choice can have long term cost implications and hamper your ability to achieve your goals.\n\nTo help firms navigate this process, ESG Global \u2014 a leading IT analyst and business strategy firm \u2014 has produced <i>The Definitive Guide to Evaluating Cloud-based Apache Spark Platforms.</i>\n\n<a href=\"http://go.databricks.com/apache-spark-buyers-guide\"><img src=\"https://databricks.com/wp-content/uploads/2016/09/esg-buyers-guide-thumbnail.jpg\" alt=\"The Definitive Guide to Evaluating Cloud-based Apache Spark Platforms\" width=\"200\" height=\"267\" class=\"alignnone size-full wp-image-11742\" /></a>\n\n<a href=\"http://go.databricks.com/apache-spark-buyers-guide\">You can download the buyer\u2019s guide here</a>.\n\nThis buyer's guide provides actionable best practices and tools to evaluate the aforementioned solutions and to understand the differences between them by providing the tools to define buying criteria, understand the differences between the various options available, manage a successful pilot or PoC, estimate potential total cost of ownership (TCO), and assess your return on investment (ROI).\n\n<a href=\"http://go.databricks.com/apache-spark-buyers-guide\"><img class=\"alignnone wp-image-9158\" src=\"https://databricks.com/wp-content/uploads/2016/09/2016-09-20_1612.png\" alt=\"2016-09-20_1612\" width=\"317\" height=\"425\" /></a> <a href=\"http://go.databricks.com/apache-spark-buyers-guide\"><img class=\"alignnone wp-image-9159\" src=\"https://databricks.com/wp-content/uploads/2016/09/2016-09-20_1617.png\" alt=\"2016-09-20_1617\" width=\"317\" height=\"426\" /></a>\n\n<a href=\"http://go.databricks.com/apache-spark-buyers-guide\">Download the buyer\u2019s guide</a> to get started on your evaluation process today!"}
{"status": "publish", "description": "The survey shows that the Apache Spark community is growing fast: meetup members have tripled; contributors to the project has grown by 67%; and users build diverse apps, with a huge growth in machine learning and streaming.", "creator": "jules_damji", "link": "https://databricks.com/blog/2016/09/27/spark-survey-2016-released.html", "authors": null, "id": 9177, "categories": ["Announcements", "Company Blog"], "dates": {"publishedOn": "2016-09-27", "tz": "UTC", "createdOn": "2016-09-27"}, "title": "Apache Spark Survey 2016 Results Now Available", "slug": "spark-survey-2016-released", "content": "<a href=\"http://go.databricks.com/2016-spark-survey\"><img class=\"aligncenter wp-image-9196 size-full\" src=\"https://databricks.com/wp-content/uploads/2016/09/SURVEY2016-OGIMAGE.png\" alt=\"Apache Spark Survey 2016 Report\" width=\"1200\" height=\"630\" /></a>\n\nIn July 2016, we conducted our Apache Spark Survey to identify insights on how organizations are using Spark and highlight growth trends since our last Spark Survey 2015. The 2016 survey results reflect answers from 900 distinct organizations and 1615 respondents, who were predominantly Apache Spark users.\n\nThe results show that the Spark community is still growing fast: the number of meetup members worldwide has tripled, and the number of contributors to the project has grown by 67% since last year. In addition, users build diverse apps, with significant growth in machine learning and streaming.\n\nSpark has moved well beyond the early-adopter phase at high-tech companies and is now mainstream in large data-driven enterprises, such as banking and health, the results reveal.\nAnd with the rise of public cloud computing, the survey findings reflect users\u2019 affinity toward deploying Spark in the public cloud.\n\n<a href=\"http://www.marketwired.com/press-release/apache-spark-growth-is-pervasive-as-users-embrace-public-cloud-machine-learning-2161564.htm\">You can read the press release here.</a>\n<h2>Report Highlights</h2>\n<h3>Spark community growth and adoption accelerates</h3>\nOver the year, we have seen growth in number of contributors and meetup members: code contributors almost doubled, and Apache Spark Meetup members tripled, from 66K to 225K. Also, since the release of DataFrames in 2015, its usage has doubled, from 15% to 38%; Windows users jumped from 23% to 32%. All this indicate a diverse thriving community and growing adoption of Spark.\n<h3>Spark Streaming and Machine Learning usage surge</h3>\nInterest in developing real-time applications and advanced analytics is on the rise. More than half (51%) of the respondents in this survey consider Spark Streaming as an essential component for building real-time streaming use cases, and 82% of respondents say the same for advanced analytics. This year, the production use of Spark Streaming jumped from 14% (in 2015) to 22% (in 2016), along with Machine Learning from 13% (in 2015) to 18% (2016).\n<h3>Spark\u2019s deployment in the public cloud rises</h3>\nThe rise of cloud computing is rapid in the tech industry. We observed this trend reflected in the survey results, as many respondents elected to deploy Spark in the public cloud, reaping its many benefits. Spark deployments in the cloud this year is at 61%, up from 51% last year. By contrast, the Spark deployments using on-premises cluster managers fell by an average of 5%.\n<h3>Spark\u2019s usage increases in production</h3>\nOverall, the use of Spark components in production has gone up. Moreover, Spark developers often combine multiple Spark components for building sophisticated applications. Seventy-four percent of respondents use more than two components, while 64% use three or more in production. Along with Spark Streaming and Machine Learning, 38% use DataFrames, while 40% use Spark SQL in production.\n\n<strong>To learn more details, download <a href=\"http://go.databricks.com/2016-spark-survey\">The Databricks Apache Spark Survey 2016 Report</a>.</strong>\n<h2>Conclusion</h2>\nAs Apache Spark becomes easier, faster, and smarter, a newer audience across diverse industries is adopting it. From the results revealed in the 2016 survey, we got a glimpse into the growth and trends of who\u2019s using Spark, how they are using it, what\u2019s important, what new features they use, and what they are using it for.\n\nAll the feedback will help us and the community as we move forward with the development of Spark, just as feedback from surveys has done the past few years. Thank you to everyone who participated in our Apache Spark Survey 2016.\n\nDownload the <a href=\"http://cdn2.hubspot.net/hubfs/438089/DataBricks_Surveys_-_Content/2016_Spark_Survey/2016_Spark_Infographic.pdf\">Apache Spark Survey 2016 Infographic</a>.\n\n<a href=\"http://cdn2.hubspot.net/hubfs/438089/DataBricks_Surveys_-_Content/2016_Spark_Survey/2016_Spark_Infographic.pdf\"><img class=\"aligncenter size-full wp-image-9200\" src=\"https://databricks.com/wp-content/uploads/2016/09/spark-survey-2016-infographic-thumbnail.jpg\" alt=\"Apache Spark Survey 2016 Report Highlights Infographic\" width=\"1304\" height=\"2127\" /></a>"}
{"status": "publish", "description": "This is a guest post from CERN, the European Organization for Nuclear Research. In this blog, Luca Canali of CERN investigates performance improvements in Apache Spark 2.0 from whole-stage code generation using Flame Graphs.", "creator": "jules_damji", "link": "https://databricks.com/blog/2016/10/03/voice-from-cern-apache-spark-2-0-performance-improvements-investigated-with-flame-graphs.html", "authors": null, "id": 9215, "categories": ["Apache Spark", "Ecosystem", "Engineering Blog"], "dates": {"publishedOn": "2016-10-03", "tz": "UTC", "createdOn": "2016-10-03"}, "title": "Voice from CERN: Apache Spark 2.0 Performance Improvements Investigated With Flame Graphs", "slug": "voice-from-cern-apache-spark-2-0-performance-improvements-investigated-with-flame-graphs", "content": "<em>This is a guest post from CERN, the\u00a0European Organization for Nuclear Research. In this blog, Luca Canali of CERN investigates performance improvements in Apache Spark 2.0 from whole-stage code generation using flame graphs. The blog was originally posted on the <a href=\"http://db-blog.web.cern.ch/blog/luca-canali/2016-09-spark-20-performance-improvements-investigated-flame-graphs\" target=\"_blank\">CERN website.</a>\u00a0 Luca will be speaking at <a href=\"https://spark-summit.org/eu-2016/\">Spark Summit Europe</a> Oct 25 - 27 on this topic.</em>\n<h2>Introduction</h2>\nThe idea for this post comes from a <strong>performance</strong> troubleshooting case that has come up recently at CERN database services. It started with a user reporting slow response time from a query for a custom report in a relational database. After investigations and initial troubleshooting, the query was still running slow (running in about 12 hours). It was understood that the query was mostly running \"<strong>on CPU</strong>\" and spending most of its time in evaluating a non-equijoin condition repeated 100s of millions of times. Most importantly it was also found that the query was easily <strong>parallelizable</strong>, this was good news as it meant that we could simply <strong>\"throw hardware at it\"</strong> to make it run faster. One way that the team (see the acknowledgments section at the end of the post) used to parallelize the workload (without affecting the production database), is to export the data to a Hadoop cluster and run the query there using <strong>Spark SQL</strong> (the cluster used has 14 nodes, installed with CDH 5.7, Spark version 1.6). This way it was possible to bring the execution time down to less than 20 minutes. All this with relatively low effort, as the query could be run basically <strong>unchanged</strong>.\n\n<h2>Apache Spark 2.0 enters the scene</h2>\nAs I write this post, Apache Spark 1.6 is installed in our production clusters and Apache Spark 2.0 is still relatively new (it has been released at the end of July 2016). Notably Spark 2.0 has very interesting <strong>improvements</strong> over the previous versions, among others improvements in the area of performance that I was eager to test (<a href=\"https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html\" target=\"_blank\">see this blog post by Databricks</a>)\n\nMy first test was to try the query discussed in the previous paragraph on a test server with Spark 2.0 and I found that it was running considerably faster than in the tests with Spark 1.6. The best result I achieved, this time a large box with 60 CPU cores and using Spark 2.0, was an elapsed time of about 2 minutes (to be compared with 20 minutes in Spark 1.6). I\u00a0have previously noticed Spark 1.6 and Impala 2.5 performed comparably on our workloads, but I\u2019m impressed by Spark 2.0\u2019s speedups over Spark 1.6, so I decided to <strong>investigate</strong> further.\n<h2>The test case</h2>\nRather than using the original query and data, I will report here on a synthetic test case that hopefully illustrates the main points of the original case and at the same is simple and easy to reproduce on your test systems, if you wish to do so. This test uses <strong>pyspark</strong>, the Python interface to Spark (if you are not familiar with how to run Spark, see further on in this post some hints on how to build a test system).\n\nThe preparation of the test data proceeds as follows: (1) it creates a DataFrame and registers it as table \"t0\" with 10 million rows. (2) Table t0 is used to create the actual test data, which is composed of an \"id\" column and three additional columns of randomly generated data, all integers. The resulting <strong>DataFrame</strong> is <strong>cached</strong> in memory and \"registered\" as a temporary table called \"t1\". <strong>Spark SQL</strong> interface for DataFrames makes this preparation task straightforward:\n<pre>$ pyspark --driver-memory 2g\n \ntest_numrows = 1e7\n \nsqlContext.range(0, test_numrows, 1).registerTempTable(\"t0\")\n \nsqlContext.sql(\"select id, floor(200*rand()) bucket, floor(1000*rand()) val1, floor(10*rand()) val2 from t0\").cache().registerTempTable(\"t1\")</pre>\nThe following commands are additional checks to make sure the table t1 has been created correctly and is first read into memory. In particular, note that \"t1\" has the required test_numrows (10M) rows and the description of its column from the output of the command \"desc\":\n<pre>sqlContext.sql(\"select count(*) from t1\").show()\n \n+--------+\n|count(1)|\n+--------+\n|10000000|\n+--------+\n \nsqlContext.sql(\"desc t1\").show()\n\n+--------+---------+-------+\n|col_name|data_type|comment|\n+--------+---------+-------+\n|      id|   bigint|       |\n|  bucket|   bigint|       |\n|    val1|   bigint|       |\n|    val2|   bigint|       |\n+--------+---------+-------+\n</pre>\nThe actual <strong>test query</strong> is here below. It consists of a <strong>join</strong> with two conditions: an equality predicate on the column bucket, which becomes an obvious point of where the query can be executed in <strong>parallel</strong>, and a more resource-intensive <strong>non-equality</strong> condition. Notably the query has also an <strong>aggregation</strong> operation. Some additional boilerplate code is added for timing the duration of the query:\n<pre>import time\nstarttime=time.time()\n \nsqlContext.sql(\"select a.bucket, sum(a.val2) tot from t1 a, t1 b where a.bucket=b.bucket and a.val1+b.val1<1000 group by a.bucket order by a.bucket\").show()\n\nprint(\"Delta time = %f\" % (time.time()-starttime))</pre>\nThe <strong>results</strong> are that <strong>Spark 2.0 is about 7 times faster than Spark 1.6</strong> when running the test query on the test server (see details below): Spark 2.0 completed the job in about 15 minutes of elapsed time and 390 minutes of CPU time, while Spark 1.6 took about 100 minutes of elapsed time and 2840 minutes of CPU time. There are fluctuations on the actual job execution time between runs, however you can ignore the fine details and focus on the main finding that the performance <strong>difference is striking</strong> between runs using Spark 1.6 and Spark 2.0 (Spark 2.0 being much faster). This is worth further investigations into the internals and <strong>root causes</strong>.\n\nI have run the tests using Spark in its simplest configuration (local mode) using a standalone (non-clustered) server with 16 cores (2 x E5-2650) and 128 GB of RAM (the virtual memory allocated by the test workload is about 16 GB) running <strong>Linux</strong> (kernel 2.6.32, RHEL 6.7). If you want to run it on a smaller machine you can scale down the preparation phase by setting <em>test_numrows </em>to a smaller value (for example to 1e6). In that case you probably could do also with using the default value of 1g for the driver-memory.\n\nThe tests have been performed on a single server alternating runs with Spark 1.6 and 2.0. In both cases monitoring with OS tools showed that the jobs were <strong>CPU-bound</strong>, that is with 32 threads (16 cores x 2 for multithreading) running on CPU and utilizing the available resources on the test box. During the tests, no additional significant workload was running on the box.\n<h2>Drilling down into the execution plans</h2>\nThe <strong>physical execution</strong> plan generated and executed by Spark (in particular by <strong>Catalyst</strong>, the optimizer and <strong>Tungsten</strong>, the execution engine) has important differences in Spark 2.0 compared to Spark 1.6. The logical plan for executing the query however deploys a <strong>sort merge join</strong> in both cases. Please note in the execution plans reported below that in the case of Spark 2.0 several steps in the execution plan are marked with a star <strong>(*)</strong> around them. This marks steps optimized with <strong>whole-stage code</strong> generation.\n<h3>Physical execution plan in Spark 1.6</h3>\nNote that a sort merge join operation is central to the execution plan of this query. Another important step after the join is the aggregation operation, used to compute \"sum(a.val2)\" as seen in the query text:\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/09/Spark16_blog_execplan_wholestagecodegeneration.png\"><img class=\"aligncenter size-full wp-image-9237\" src=\"https://databricks.com/wp-content/uploads/2016/09/Spark16_blog_execplan_wholestagecodegeneration.png\" alt=\"spark16_blog_execplan_wholestagecodegeneration\" width=\"1053\" height=\"326\" /></a>\n<h3>Physical execution plan in Spark 2.0</h3>\nNote in particular the steps marked with <strong>(*)</strong>, they are optimized with <strong>whole-stage code generation</strong>:\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/09/Spark20_blog_execplan_wholestagecodegeneration.png\"><img class=\"aligncenter size-full wp-image-9239\" src=\"https://databricks.com/wp-content/uploads/2016/09/Spark20_blog_execplan_wholestagecodegeneration.png\" alt=\"spark20_blog_execplan_wholestagecodegeneration\" width=\"1055\" height=\"382\" /></a>\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/09/WebUI_Spark_annotated.png\"><img class=\"aligncenter size-full wp-image-9241\" src=\"https://databricks.com/wp-content/uploads/2016/09/WebUI_Spark_annotated.png\" alt=\"webui_spark_annotated\" width=\"1097\" height=\"1263\" /></a>\n\nDetails of the SQL execution from the Spark Web UI, Spark 1.6. vs. Spark 2.0. This reproduces the physical execution plan with additional metrics gathered at run-time. Note in particular in Spark 2.0 the steps marked as \"Whole Stage Codegen.\"\n<h2>Code generation is the key</h2>\nThe key to understand the improved performance is with the new features in Spark 2.0 for whole-stage code generation. This is expected and detailed for example in the blog post by DataBricks Engineering <a href=\"https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html\" target=\"_blank\">Apache Spark as a Compiler: Joining a Billion Rows per Second on a Laptop Deep dive into the new Tungsten execution engine</a>. The main point is that Spark 2.0 compiles query execution into bytecode that is then executed, as opposed to looping with an iterator over result sets. A detailed discussion on the benefits of query compilation and code generation vs. the \"traditional approach\" to query execution, also called volcano model, can be found in the lecture by <a href=\"https://www.youtube.com/watch?v=y_quuGMRcds&amp;index=19&amp;list=PLSE8ODhjZXjbisIGOepfnlbfxeH7TW-8O\" target=\"_blank\">Andy Pavlo on Query Compilation</a>.\n<h2>Runtime investigations with flame graphs</h2>\n<a href=\"http://www.brendangregg.com/flamegraphs.html\" target=\"_blank\">Flame graphs</a> visualization of stack profiles provide additional <strong>insights</strong> on what part of the code are executed on CPU. The upper layers of the flame graph highlight where <strong>CPU cycles</strong> are spent. The lower layers add context by detailing the information on the parent functions/methods that called the \"upper layers\". The <strong>idea</strong> for this paragraph is to use stack profiles and flame graphs to further drill down on the <strong>differences</strong> in the execution model between Spark 2.0 and Spark 1.6.\n\nTo collect and generate the flame graphs I have used the methods described by <a href=\"https://gist.github.com/kayousterhout/7008a8ebf2babeedc7ce6f8723fd1bf4\" target=\"_blank\">Kay Ousterhout in \"Generating Flame Graphs for Apache Spark using Java Flight Recorder\"</a>. I have used the Java flight recorder on Oracle's Java 8, starting pyspark with the following options:\n\n<strong>pyspark --conf \"spark.driver.extraJavaOptions\"=\"-XX:+UnlockCommercialFeatures -XX:+FlightRecorder\" --conf \"spark.executor.extraJavaOptions\"=\"-XX:+UnlockCommercialFeatures -XX:+FlightRecorder\"\n</strong>\n\nHere below you can find two flame graphs that visualize the stack profiles collected for Spark 1.6 and Spark 2.0 while running the test workload/query. The graphs represent samples collected over 100 seconds. The major differences you should notice between the two flame graphs are that on Spark 1.6 the execution iterates over rows of data, looping on <strong>Row Iterator to Scala</strong> for example. In the <strong>Spark 2.0</strong> example, however, you can see in the flame graph that the methods executing the bulk of the work are built/<strong>optimized</strong> with <strong>whole-stage code generation</strong>. For example the method where most time is spent during execution is code-generated and performs operations on Hash Maps in <strong>vector</strong> form.\n\n<strong>What you can learn</strong> from the flame graphs:\n<ul>\n \t<li>The flame graph for Spark 1.6 shows that a considerable amount of CPU cycles are spent on the <strong>Scala collection iterator</strong>. This can be linked with Spark 1.6 using the \"traditional volcano model\" for SQL execution. This is the part that is optimized in Spark 2.0 (see next bullet points).</li>\n \t<li>Spark 2.0 is making use of<strong> whole-stage code generation </strong>and does not use Scala collection iterator.</li>\n \t<li>Spark 2.0 is also using Vectorized Hash Maps to perform aggregations that are also code generated. The use of <strong>vectorized</strong> operations is likely introducing further performance improvements.</li>\n</ul>\n<strong>Spark 1.6:</strong>\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/09/Flamegraph_Spark16_blog_wholestagecodegeneration_annotated.png\"><img class=\"aligncenter size-full wp-image-9245\" src=\"https://databricks.com/wp-content/uploads/2016/09/Flamegraph_Spark16_blog_wholestagecodegeneration_annotated.png\" alt=\"flamegraph_spark16_blog_wholestagecodegeneration_annotated\" width=\"1600\" height=\"799\" /></a>\n\nFlame graph for a sample of the execution of the test query using Spark 1.6 in local mode (on a machine with 16 cores). Note that most of the time is spent processing data on a iterative way (which is not optimal). <a href=\"http://canali.web.cern.ch/canali/svg/Flamegraph_spark16_blog_sparkwholestagecodegeneration.svg\" target=\"_blank\">Click on this link for a SVG version of the graph</a> where you can drill down on the details of each step.\n\n<strong>Spark 2.0:</strong>\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/09/Flamegraph_Spark20_blog_wholestagecodegeneration_annotated.png\"><img class=\"aligncenter size-full wp-image-9246\" src=\"https://databricks.com/wp-content/uploads/2016/09/Flamegraph_Spark20_blog_wholestagecodegeneration_annotated.png\" alt=\"flamegraph_spark20_blog_wholestagecodegeneration_annotated\" width=\"1600\" height=\"882\" /></a>\n\nFlame graph for a sample of the execution of the test query using Spark 2.0 in local mode (on a machine with 16 cores). Note that most of the time is spent executing code that is generated dynamically via whole-stage code generation. <a href=\"http://canali.web.cern.ch/canali/svg/Flamegraph_spark20_blog_sparkwholestagecodegeneration.svg\" target=\"_blank\">Click on this link for a SVG version of the graph</a> where you can drill down on the details of each step.\n\n<strong>Note</strong>: the process of collecting stack profiles for Spark in this test is made easier by the fact that I have used Spark in local mode, which results in only one (multi-threaded) process to trace in a single box . In the general case tracing Spark is more complicated due to the distributed nature of the workload when running on a cluster for example.\n<h2>Linux Perf stat counters</h2>\nIn this paragraph you can find the output of <a href=\"https://perf.wiki.kernel.org/index.php/Tutorial#Counting_with_perf_stat\" target=\"_blank\">Linux Perf stat</a> counters measured during the execution of the test query. The idea is to find differences in the run-time usage of resources that can further highlight the origin of the performance improvement that was measured in Spark 2.0 compared to Spark 1.6. The selection of stat counters to measure is taken from Tanel Poder's blog post \"<a href=\"http://blog.tanelpoder.com/2015/09/21/ram-is-the-new-disk-and-how-to-measure-its-performance-part-2-tools/\" target=\"_blank\">RAM is the new disk \u2013 and how to measure its performance \u2013 Part 2 \u2013 Tools</a>.\" Notably you can find there also a short explanation of meaning of the counters.\n<pre># perf stat -e task-clock,cycles,instructions,branches,branch-misses \\\n          -e stalled-cycles-frontend,stalled-cycles-backend \\\n          -e cache-references,cache-misses \\\n          -e LLC-loads,LLC-load-misses,LLC-stores,LLC-store-misses \\\n          -e L1-dcache-loads,L1-dcache-load-misses,L1-dcache-stores,L1-dcache-store-misses \\\n          -p  sleep 100</pre>\n<strong>What you can learn</strong> from comparing perf stat counters between Spark 1.6 and Spark 2.0 runs:\n<ul>\n \t<li>In both cases the workload is CPU-bound. The machine has 16 cores and is configured with multi-threading support (i.e. 32 execution threads). Perf stat counters report an average CPU utilization of about 31 CPU threads in both cases, which confirms the fact that the workload is CPU bound.</li>\n \t<li>Reading from main memory seems to be key and <strong>Spark 2.0</strong> appears to access memory with much <strong>higher throughput</strong> than Spark 1.6. In particular, I believe it is important to look at the metrics <strong>LLC-loads</strong> and <strong>LLC-load-misses</strong>, those count respectively how many time a cache line was requested from last level cache (LLC) and the fraction of those requests that resulted in access from main memory. Notably Spark 2.0 in the given sample reports 33 M/sec LLC-loads with ~63% of loads resulting in misses (reads from main memory) while Spark 1.6 has 0,7 M/sec LLC-loads and also ~60% misses. I have noticed that these values fluctuate over different samples, but <strong>Spark 2.</strong>0 presents always much <strong>higher access rate to LLC and memory</strong> than Spark 1.6.</li>\n \t<li>It is interesting to note that the measurements in the case of Spark 1.6 run present a higher ratio of <strong>instructions per cycle</strong> than the run with Spark 2.0. Spark 2.0 workload is <strong>stalling for memory access</strong> more frequently. A higher ratio of instructions per cycle is often an indicator of better performance, however, in this case the opposite appears to be true. I believe a possible interpretation of what is happening is that <strong>Spark 2.0 is more efficient</strong> at using CPU resources and high throughput to memory, therefore it quickly gets into what appears to be the <strong>bottleneck</strong> for this workload: stalling for<strong> memory access</strong>.</li>\n</ul>\nThis is the output of perf stat while running the test workload with Spark 1.6:\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/09/Screen-Shot-2016-09-29-at-5.29.04-PM.png\"><img class=\"aligncenter size-full wp-image-9266\" src=\"https://databricks.com/wp-content/uploads/2016/09/Screen-Shot-2016-09-29-at-5.29.04-PM.png\" alt=\"screen-shot-2016-09-29-at-5-29-04-pm\" width=\"1050\" height=\"402\" /></a>\n\nThis is the output of perf stat while running the test workload with Spark 2.0:\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/09/Screen-Shot-2016-09-29-at-5.29.55-PM.png\"><img class=\"aligncenter size-full wp-image-9267\" src=\"https://databricks.com/wp-content/uploads/2016/09/Screen-Shot-2016-09-29-at-5.29.55-PM.png\" alt=\"screen-shot-2016-09-29-at-5-29-55-pm\" width=\"1033\" height=\"415\" /></a>\n<h2>Source code</h2>\nIf you want to further drill down on the changes in Spark 2.0 that benefit the performance of the test workload you can head to <a href=\"https://github.com/apache/spark\" target=\"_blank\">GitHub and browse the source code of Spark</a>. For example from the flame graphs you can find the name of the relevant classes with path and/or you can use the search function in GitHub. So far I have only skimmed through the source code with these methods and found a few links that I believe are interesting as an example of the drill-down analysis that one can do thanks to the fact that Spark is an open source project:\n<ul>\n \t<li>One link of interest is \"org.apache.sql.execution.<a href=\"https://github.com/apache/spark/blob/branch-2.0/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala\" target=\"_blank\">WholeStageCodegenExec</a>\". This is code introduced in the Spark 2.0 branch, you can find there also comments that shed some light on the mechanism used for code generation.</li>\n \t<li>Another interesting point is about the use of \"vectorized hash maps\" in Spark 2.0, which appears important as it is on the top line of the Spark 2.0 flame graph: \"org.apache.spark.sql.executio.aggregate.<a href=\"https://github.com/apache/spark/blob/branch-2.0/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator.scala\" target=\"_blank\">VectorizedHashMapGenerator.scala</a>\" has additional details about the implementation. You can find there that this is an implementation for fast index lookup, also introduced in the Spark 2.0 branch. It is also mentioned there that the execution can be code generated for boosting its <strong>performance</strong>, that is what you can see happening in the flame graph of Spark 2.0 workload.</li>\n</ul>\n<h2>Tips on how to build a test environment</h2>\nFor the readers who are not familiar with running Spark, here some tips on how to build a test environment:\n<ul>\n \t<li>Download Spark from <a href=\"http://spark.apache.org/downloads.html\">Spark's website</a>.</li>\n \t<li>You will not need to have Hadoop and/or a YARN cluster to run the tests described in this post.</li>\n \t<li>An easy way to install Python 2.7 is by downloading <a href=\"https://www.continuum.io/downloads\">Anaconda</a>.</li>\n \t<li>You can download Java 8 from <a href=\"http://www.oracle.com/technetwork/java/javase/downloads/index.html\">Oracle technet</a>.</li>\n \t<li>Code for generating flame graphs for Spark using Java Flight Recorder (see the recipe at <a href=\"https://gist.github.com/kayousterhout/7008a8ebf2babeedc7ce6f8723fd1bf4\" target=\"_blank\">this link</a>) at:<a href=\"https://github.com/brendangregg/FlameGraph\" target=\"_blank\"> https://github.com/brendangregg/FlameGraph</a> and <a href=\"https://github.com/chrishantha/jfr-flame-graph\" target=\"_blank\">https://github.com/chrishantha/jfr-flame-graph</a></li>\n</ul>\n<h2>Summary</h2>\n<strong>Apache Spark 2.0</strong> has important optimizations for performance compared to Spark version 1.6. Notably Spark optimizer and execution engine in version 2.0 can take advantage of <strong>whole-stage code</strong> generation and of vector operations to make <strong>more efficient use of CPU</strong> cycles and memory bandwidth for improved performance. This post briefly discusses an example how Spark SQL and its parallel execution engine have been useful to tune a query from a production RDBMS. Moreover an example comparing Spark 1.6 and Spark 2.0 performance has been discussed and drilled-down using execution plan details, <strong>flame graphs</strong> and Linux Perf stat counters.\n<h2>Additional comments and my take-away from the tests in this post</h2>\nThe <strong>Hadoop ecosystem</strong> provides a powerful and easy-to-use environment for running reports and analytics queries. The point is nicely illustrated for me by the fact that we could simply take data and a <strong>query</strong> from production RDBMS and run it on the Hadoop cluster (with Spark and Impala) to make it <strong>run with parallelism</strong> and fast. This provides a simple and quick way to throw HW at a performance problem.\n\nI am impressed by the work on <strong>Spark 2.0</strong> optimizations for whole-stage code generation, in particular by how these new features address the important point of how to <strong>optimize CPU-bound workloads</strong>. This makes a great addition to Spark and strengthen its position as a leading player in data processing a scale.\n\n<strong>Query compilation</strong> and/or code generation for executing <strong>SQL</strong> has become a common feature for many of the new databases appearing on the market optimized for \"<strong>in memory</strong>\" (i.e. processing an important fraction of their workload in main memory). This is implemented in various forms for different products, however it is proven to give significant gains in performance, typically of the order of one order of magnitude, for queries where it is applicable. The test case examined in this post provides an example of this type of optimization.\n\nHow are the mainstream <strong>RDBMS</strong> engines, that typically process result sets in an iterative way (similarly to what was found in this post with Spark 1,6 and often referred to as the <strong>volcano model</strong>) going to respond to this performance-based <strong>challenge</strong>?\n<h2>Acknowledgements and references</h2>\nThis work has been made possible and funded by CERN IT, in particular in the context of the <strong>CERN IT Hadoop Service</strong> and Database Services. In particular I would like to thanks CERN colleagues who have contributed to the performance troubleshooting case mentioned in this post: <strong>Raul</strong> Garcia Martinez, <strong>Zbigniew</strong> Baranowski and <strong>Luca</strong> Menichetti.\n<ul>\n \t<li>On the topic of Spark 2.0 improvements for code generation, see the blog post \"<a href=\"https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html\" target=\"_blank\">Apache Spark as a Compiler: Joining a Billion Rows per Second on a Laptop</a>\" and the references therein, notably including \"<a href=\"https://blog.acolyer.org/2016/05/23/efficiently-compiling-efficient-query-plans-for-modern-hardware/\" target=\"_blank\">Efficiently compiling efficient query plans for modern hardware</a>\" and <a href=\"https://issues.apache.org/jira/browse/SPARK-12795\" target=\"_blank\">JIRA ticket SPARK-12795</a>.</li>\n \t<li>On flame graphs for Spark: Kay Ousterhout in \"<a href=\"https://gist.github.com/kayousterhout/7008a8ebf2babeedc7ce6f8723fd1bf4\" target=\"_blank\">Generating Flame Graphs for Apache Spark using Java Flight Recorder</a>\". See also <a href=\"https://db-blog.web.cern.ch/blog/joeri-hermans/2016-04-hadoop-performance-troubleshooting-stack-tracing-introduction\" target=\"_blank\">\"Hadoop performance troubleshooting with stack tracing, an introduction.\"</a></li>\n \t<li>On the topic of query compilation on modern database systems vs. the volcano model, see also the lecture by <a href=\"https://www.youtube.com/watch?v=y_quuGMRcds&amp;index=19&amp;list=PLSE8ODhjZXjbisIGOepfnlbfxeH7TW-8O\" target=\"_blank\">Andy Pavlo on Query Compilation</a>.</li>\n \t<li><a href=\"http://www.brendangregg.com/flamegraphs.html\" target=\"_blank\">Flame graphs </a>are the brain child of <a href=\"https://twitter.com/brendangregg\" target=\"_blank\">Brendan Gregg</a>.</li>\n \t<li>Additional links on using Linux Perf to measure performance counters: <a href=\"http://www.brendangregg.com/linuxperf.html\" target=\"_blank\">this article by Brendan Gregg</a> and the <a href=\"http://blog.tanelpoder.com/2015/09/21/ram-is-the-new-disk-and-how-to-measure-its-performance-part-2-tools/\" target=\"_blank\">3-part blog posts by Tanel Poder</a> on \"<strong>RAM is the new disk</strong>\".</li>\n \t<li>On the topic of connecting Hadoop and relational databases see also Tanel's presentation \"<a href=\"http://blog.tanelpoder.com/2015/10/27/connecting-hadoop-and-oracle/\" target=\"_blank\">Connecting Hadoop and Oracle</a>\".</li>\n</ul>"}
{"status": "publish", "description": "Today we are excited to announce that Databricks has successfully achieved SOC 2 Type 1 certification.", "creator": "dave_wang", "link": "https://databricks.com/blog/2016/10/04/databricks-completes-soc-2-type-1-certification.html", "authors": null, "id": 9294, "categories": ["Announcements", "Company Blog"], "dates": {"publishedOn": "2016-10-04", "tz": "UTC", "createdOn": "2016-10-04"}, "title": "Databricks Completes SOC 2 Type 1 Certification", "slug": "databricks-completes-soc-2-type-1-certification", "content": "<img class=\"aligncenter size-full wp-image-8541\" src=\"https://databricks.com/wp-content/uploads/2016/07/OG_Whitepaper-Protecting_Enterprise_data.jpg\" alt=\"OG_Whitepaper-Protecting_Enterprise_data\" width=\"1200\" height=\"630\" />\n\nAs the first and only company to provide true end-to-end enterprise security on top of Apache Spark in the cloud, Databricks has been <a href=\"https://www.iqt.org/databricks-secures-strategic-investment-from-in-q-tel-to-deliver-cloud-based-apache-spark-platform/\">partnering with enterprises and government agencies</a> to securely process mission-critical workloads with Spark. Today we are excited to announce that Databricks has successfully completed SOC 2 Type 1 certification. The audit was done by Coalfire, a leading third-party cyber risk management and compliance audit firm. The report assures that Databricks\u2019 controls were designed and implemented to meet the criteria for:\n<ul>\n \t<li><em>Security.</em> The system is protected against unauthorized access.</li>\n \t<li><em>Availability.</em> The system is available for operation and use as committed or agreed.</li>\n \t<li><em>Processing integrity.</em> System processing is complete, valid, accurate, timely, and authorized.</li>\n \t<li><em>Confidentiality.</em> Information designated as confidential is protected as committed or agreed\u00a0<sup>1</sup>.</li>\n</ul>\nThis achievement is an important milestone in rolling out <em><a href=\"https://databricks.com/blog/2016/06/08/achieving-end-to-end-security-for-apache-spark-with-databricks.html\" target=\"_blank\">Databricks Enterprise Security (DBES)</a></em>. As the team who created Apache Spark, we architected DBES to address enterprise data security needs by building all the facets of security \u2014 encryption, identity management, role-based access control, data governance, and compliance standards \u2014 natively into a single data platform on top of Spark.\n<h2>What is SOC 2?</h2>\nAs companies increasingly shift their workloads into the cloud-based data platforms, they need objective assurance that their confidential data, which often includes Personally Identifiable Information (PII) and intellectual property, is adequately protected. SOC 2 is a standard audit made by a trusted third party that provides the assurance. Issued by an independent auditor, the SOC 2 report includes a detailed description of the architecture, data flow, processes, controls, and audit opinion.\n<h2>What this audit demonstrates</h2>\nThe audit completed at this time is SOC 2 Type 1. It is an independent validation of Databricks\u2019 commitment to meeting customers\u2019 requirements and to implementing a robust compliance program. Specifically, the auditors determined that Databricks has been architected according to security best practices from the ground up. Achieving SOC 2 Type 1 requires meticulous documentation of the controls that were already in place, such as:\n<ul>\n \t<li>Secure product development lifecycle.</li>\n \t<li>Stringent access control based on the least privileged access principle.</li>\n \t<li>Robust logging, monitoring, events correlation, and alerts.</li>\n \t<li>Comprehensive vulnerability management with internal and external scans, penetration testing, and code reviews.</li>\n \t<li>Extensive employee security awareness training.</li>\n</ul>\n<h2>What\u2019s Next</h2>\nThe SOC 2 Type 1 report provides the auditors\u2019 opinion on the design of controls. In the coming months they will further validate the operating effectiveness of these controls with a Type 2 report, which is based on an assessment after the issuance of the Type 1 report. With the achievement of these compliance standards, Databricks is on track to execute an aggressive roadmap to realize the full vision of DBES, which includes a multitude of additional compliance standards including HIPAA, FedRAMP, and PCI as well as many cutting-edge security features.\n\nTo learn more about how Databricks can help you secure your Apache Spark workloads, <a href=\"http://go.databricks.com/protecting-enterprise-data-on-apache-spark-with-databricks\" target=\"_blank\">download the whitepaper</a>. To try Databricks, <a href=\"https://databricks.com/try-databricks\" target=\"_blank\">sign-up for a trial</a> or <a href=\"http://go.databricks.com/contact-databricks\" target=\"_blank\">contact a solution engineer</a>.\n<h2>References</h2>\n<sup>1</sup> American Institute of CPAs, <a href=\"https://www.aicpa.org/InterestAreas/FRC/AssuranceAdvisoryServices/Pages/TrustDataIntegrityTaskForce.aspx\" target=\"_blank\">Trust Services and Information Integrity</a>"}
{"status": "publish", "description": null, "creator": "jules_damji", "link": "https://databricks.com/blog/2016/10/04/databricks-bi-weekly-apache-spark-digest-10316.html", "authors": null, "id": 9308, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2016-10-04", "tz": "UTC", "createdOn": "2016-10-04"}, "title": "Databricks Bi-Weekly Apache Spark Digest: 10/4/16", "slug": "databricks-bi-weekly-apache-spark-digest-10316", "content": "Here\u2019s our recap of what\u2019s transpired with Apache Spark since our <a href=\"https://databricks.com/blog/2016/08/31/databricks-bi-weekly-digest-83116.html\" target=\"_blank\">previous digest</a>.\n\n<ul>\n\t<li>Databricks <a href=\"https://databricks.com/blog/2016/09/27/spark-survey-2016-released.html\" target=\"_blank\">Apache Spark Survey 2016 Report</a> published and now available.</li>\n\t<li>Databricks\u2019 Chief Architect Reynold Xin joined Intel\u2019s Michael Greene in a keynote at the JavaOne Conference: <a href=\"https://jaxenter.com/javaone-2016-java-keynote-live-129183.html\" target=\"_blank\">Apache Spark is the Pokemon Go of Big Data</a>.</li>\n\t<li>Apache Spark and Databricks won <a href=\"https://databricks.com/blog/2016/09/19/apache-spark-earns-datanami-awards-for-machine-learning-real-time-analytics-and-more.html\" target=\"_blank\">Datanami Readers\u2019 and Editors\u2019 Choice Awards for Machine Learning, Real-time Analytics, Best Startup and More..</a></li>\n\t<li>Databricks\u2019 Chief Technologist and Co-founder Matei Zaharia presented \u201c<a href=\"https://atscaleconference.com/videos/unifying-big-data-workloads-in-apache-spark/\" target=\"_blank\">Unifying big data workloads in Apache Spark</a>\u201d at <a href=\"https://atscaleconference.com/events/main-event/\" target=\"_blank\">@Scale Conference</a>. </li>\n\t<li>Apache Spark Community Voice from CERN: Luca Canali of CERN shared <a href=\"https://databricks.com/blog/2016/10/03/voice-from-cern-apache-spark-2-0-performance-improvements-investigated-with-flame-graphs.html\" target=\"_blank\">Apache Spark 2.0 Performance Improvements Investigated With Flame Graphs</a> in his blog.</li>\n\t<li>Databricks\u2019 Jules S. Damji and Sameer Farooqui published <a href=\"http://www.kdnuggets.com/2016/09/7-steps-mastering-apache-spark.html\" target=\"_blank\">7 Steps to Mastering Apache Spark 2.0</a> in KDnuggets.</li>\n\t<li>Databricks released integration utilities for using <a href=\"https://spark-packages.org/package/databricks/spark-avro\" target=\"_blank\">Apache Spark with Apache Avro data</a> as a Spark package.</li>\n\t<li>Databricks\u2019 Ram Sriharsha and Xiangrui Meng conducted <a href=\"http://conferences.oreilly.com/strata/hadoop-big-data-ny/public/schedule/detail/55547\" target=\"_blank\">Ask Me Anything: State of Apache Spark</a> at Strata + Hadoop World NYC.</li>\n\t<li>Databricks\u2019 Xiangrui Meng presented <a href=\"http://conferences.oreilly.com/strata/hadoop-big-data-ny/public/schedule/detail/52238\" target=\"_blank\">Recent Development in SparkR for Advanced Analytics</a> at Strata + Hadoop World NYC</li>\n\t<li>Databricks\u2019 Ram Sriharsha presented <a href=\"http://conferences.oreilly.com/strata/hadoop-big-data-ny/public/schedule/detail/52312\" target=\"_blank\">The State of Apache Spark and What\u2019s Next after Apache Spark 2.0</a> at Strata + Hadoop World NYC.</li>\n\t<li><a href=\"http://spark.apache.org/releases/spark-release-2-0-1.html\" target=\"_blank\">Apache Spark 2.0.1 Released.</a> Try it on <a href=\"http://databricks.com/try\" target=\"_blank\">Databricks Community Edition</a>.\n</li>\n</ul>\n\n<h2> What's Next</h2>\nTo stay abreast with what\u2019s happening with Apache Spark, follow us on Twitter <a href=\"https://twitter.com/databricks\" target=\"_blank\">@databricks</a> and visit <a href=\"http://sparkhub.databricks.com\" target=\"_blank\">SparkHub</a>."}
{"status": "publish", "description": "Learn how Databricks helps Edmunds.com simplify the management of their Apache Spark infrastructure while accelerating data exploration at scale.", "creator": "Wayne Chan", "link": "https://databricks.com/blog/2016/10/05/new-webinar-how-edmunds-com-leverages-apache-spark-on-databricks-to-improve-customer-conversion.html", "authors": null, "id": 9289, "categories": ["Company Blog", "Customers", "Events", "Product"], "dates": {"publishedOn": "2016-10-05", "tz": "UTC", "createdOn": "2016-10-05"}, "title": "New Webinar: How Edmunds.com leverages Apache Spark on Databricks to Improve Customer Conversion", "slug": "new-webinar-how-edmunds-com-leverages-apache-spark-on-databricks-to-improve-customer-conversion", "content": "<a href=\"https://databricks.com/wp-content/uploads/2016/07/Edmunds_Databricks.png\"><img class=\" wp-image-8136 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2016/07/Edmunds_Databricks.png\" alt=\"Edmunds_Databricks\" width=\"469\" height=\"247\" /></a>\n<ul>\n \t<li><b>Date &amp; Time: </b>October 19th, 2016 at 10:00am PT / 1:00pm ET / 5:00pm UTC</li>\n \t<li><b>Presenters: </b>Shaun Elliott - Senior Software Engineer &amp; Team Lead, Edmunds.com and Christian Lugo - Software Engineer, Edmunds.com</li>\n</ul>\n\n<strong><a href=\"http://go.databricks.com/how-edmunds-leverages-apache-spark-on-databricks-to-improve-customer-conversion\">Register for this webinar now.</a></strong>\n\nEdmunds.com is a leading online car information and shopping marketplace serving nearly 20 million visitors each month to their website. Their ability to drive revenue is directly correlated to the user experience of their web and mobile applications. One of the most impactful ways to increase customer engagement is to ensure the highest levels of data quality on their auto listings pages.\n\nHowever, identifying missing and inaccurate details within the thousands of auto listing pages was difficult to keep under control and do so in a cost effective manner, primarily because of the 10x increase in data to 100+ TBs in the past four years across various siloed data sources including both internal and paid external sources. For example, what percentage of Subarus have \u201csunroof\u201d inaccurately listed under options? These are the questions Edmunds\u2019 data teams are trying to solve.\n\nExecuting on this approach introduced technical challenges around the amount of DevOps time spent integrating a growing number of data sources and maintaining resource intensive MapReduce jobs required to deliver the insights they needed to make the right data source decisions.\n\nEdmunds.com turned to Databricks to simplify the management of their Apache Spark infrastructure while accelerating data exploration at scale by 6x. Now they can quickly analyze large datasets to determine the best sources for car data on their website.\n\nJoin this webinar to learn:\n<ul>\n \t<li>Why Edmunds.com moved from MapReduce to Databricks for ad hoc data exploration.</li>\n \t<li>How Databricks democratized data access across teams to improve decision making and feature innovation.</li>\n \t<li>Best practices for doing ETL and building a robust data pipeline with Databricks.</li>\n</ul>\n<strong><a href=\"http://go.databricks.com/how-edmunds-leverages-apache-spark-on-databricks-to-improve-customer-conversion\">Register Now</a></strong>"}
{"status": "publish", "description": "Learn how Databricks allows data engineering teams to overcome common obstacles while building production-quality data pipelines with Apache Spark.", "creator": "Wayne Chan", "link": "https://databricks.com/blog/2016/10/12/new-webinar-databricks-for-data-engineers.html", "authors": null, "id": 9323, "categories": ["Company Blog", "Events", "Product"], "dates": {"publishedOn": "2016-10-12", "tz": "UTC", "createdOn": "2016-10-12"}, "title": "New Webinar: Databricks for Data Engineers", "slug": "new-webinar-databricks-for-data-engineers", "content": "<a href=\"https://databricks.com/wp-content/uploads/2016/10/2016-10-04_1646.png\"><img class=\"alignnone wp-image-9324\" src=\"https://databricks.com/wp-content/uploads/2016/10/2016-10-04_1646.png\" alt=\"Databricks for Data Engineers Webinar Banner\" width=\"724\" height=\"408\" /></a>\n<ul>\n \t<li><b>Date &amp; Time: </b>October 26th, 2016 at 10:00am PT / 1:00pm ET / 5:00pm UTC</li>\n \t<li><b>Presenter: </b>Prakash Chockalingam - Product Manager, Databricks</li>\n</ul>\n\n[btn href=\"http://go.databricks.com/databricks-for-data-engineers\"]Register Now[/btn]\n<br class=\"\">\n\nAs we\u2019ve seen from the results of our <a href=\"http://go.databricks.com/2016-spark-survey\">recent survey</a> of 1,651 Apache Spark users across 900 distinct organizations, the growth of the Spark community continues to accelerate\u2014transitioning from the darling of the big data industry to the de-facto analytics technology in data-driven companies. However, building production data pipelines can be challenging because data engineers often cannot efficiently debug and optimize Spark jobs.\n\nAt Databricks, we aim at making it easy to productionize Spark code with a fully managed platform in the cloud. The big advantage of running Apache Spark on Databricks for data engineers is that it provides developer-friendly functionalities out of the box:\n\n<ul>\n \t<li>REST APIs to programmatically manage Spark clusters and production jobs.</li>\n \t<li>Automated monitoring that sends alerts when a production job starts, fails, and completes.</li>\n \t<li>Integrated workspace with Spark UI and history server to facilitate easier debugging.</li>\n \t<li>One-click deployment of Spark code into resilient production jobs.</li>\n</ul>\n\nIn this webinar, Prakash Chockalingam\u2014seasoned data engineer and product manager\u2014will discuss how Databricks allows data engineering teams to overcome common obstacles while building production-quality data pipelines with Spark.\n\n<strong>Join this webinar to learn:</strong>\n\n<ul>\n \t<li>Obstacles faced by data engineering teams while building ETL pipelines;</li>\n \t<li>How Databricks simplifies Spark development;</li>\n \t<li>A demonstration of key Databricks functionalities geared towards making data engineers more productive</li>\n</ul>\n\n[btn href=\"http://go.databricks.com/databricks-for-data-engineers\"]Register Now[/btn]\n\n&nbsp;"}
{"status": "publish", "description": "This blog shows how to leverage AWS Lambda and Databricks together to tackle two use cases: an event-based ETL automation and serving Machine Learning model results trained with Apache Spark.", "creator": "admin", "link": "https://databricks.com/blog/2016/10/11/using-aws-lambda-with-databricks-for-etl-automation-and-ml-model-serving.html", "authors": null, "id": 9349, "categories": ["Company Blog"], "dates": {"publishedOn": "2016-10-11", "tz": "UTC", "createdOn": "2016-10-11"}, "title": "Using AWS Lambda with Databricks for ETL Automation and ML Model Serving", "slug": "using-aws-lambda-with-databricks-for-etl-automation-and-ml-model-serving", "content": "As a Data Solutions Architect, I partner with clients to build end-to-end solutions with Databricks. Databricks is built on top of AWS, is natively compatible with all the AWS offerings, and all our clients are avid AWS users. Naturally, this means I often advise them on architectural issues such as how to integrate Databricks with the broader AWS ecosystem.\n\nIn this blog, I will show how to leverage AWS Lambda and Databricks together to tackle two use cases: an event-based ETL automation (e.g., partition creations for a Spark SQL table or job trigger using Databricks\u2019 REST API) and serving Machine Learning model results trained with Apache Spark.\n<h2>A Little Background on AWS Lambda</h2>\nLambda is a managed compute service that allows you to run a custom function in response to an event (e.g., writes to a specific AWS S3 bucket) without having to set up a server. Compute resources, capacity provisioning, automatic scaling, code monitoring, logging, and code and security patch deployment are all managed by AWS. It supports three programming languages, Java, Python and Node.Js.\n\nLambda is the perfect complement to Databricks to trigger an action based on an events in other AWS services. The Databricks REST API provides a mechanism to connect your Spark clusters with Lambda.\n<h2>An Introduction to Databricks\u2019 REST API</h2>\nThe Databricks REST API enables programmatic access to Databricks, (instead of going through the Web UI). It can automatically create and run jobs, productionalize a data flow, and much more. For more information on how the API works, read the <a href=\"http://docs.databricks.com/api/index.html\">documentation</a> or <a href=\"https://databricks.com/blog/2016/03/30/announcing-new-databricks-apis-for-faster-production-apache-spark-application-deployment.html\">this blog</a>.\n\nFor this next example, I will demonstrate how to use the API to automate an ETL job.\n<h2>Example #1: ETL Automation</h2>\nThere are cases where a daily ETL job cannot be scheduled on a set time. For example, sometimes you need a specific number of data points to be available, or there could be considerable daily variability - making a simple CRON job not a good option. In this and other similar situations, an AWS Lambda function can be used to check for the condition(s) across a variety of systems (e.g. whether data landing is in S3 or Kinesis) and start the job via Databricks\u2019 REST API.In the example illustrated by the diagram below, the custom function is triggered by S3 as new data lands in a bucket. The lambda function triggers Databricks\u2019 job is using the REST API. Specifically, a variety of data lands in S3 (step 1); an event notification is pushed to the custom function in Amazon Lambda (step 2); a custom function makes a REST API call to Databricks to start a new job (step 3); and as part of the ETL job Databricks reads and write data to/from S3 (step 4).\n\nIn the example illustrated by the diagram below, the custom function is triggered by S3 as new data lands in a bucket. AWS Lambda \u00a0triggers Databricks\u2019 job is using the REST API. Specifically, a variety of data lands in S3 (step 1); an event notification is pushed to the custom function in AWS Lambda (step 2); a custom function makes a REST API call to Databricks to start a new job (step 3); and as part of the ETL job Databricks reads and write data to/from S3 (step 4).\n\n<img class=\"aligncenter size-full wp-image-9351\" src=\"https://databricks.com/wp-content/uploads/2016/10/01-etl-automation-with-aws-lambda-and-databricks.jpg\" alt=\"Diagram showing ETL automation with AWS S3, Lambda, and Databricks.\" width=\"836\" height=\"388\" />\n\n<em>Figure 1: ETL automation: 1) Data lands is S3 from variety of sources, 2) An event is triggered and a call is made to the custom function in AWS Lambda, 3) Custom function makes a REST API call to Databricks to start a new job, 4) As part of the ETL job Databricks reads and writes data to/from S3.</em>\n\n<img class=\"aligncenter size-full wp-image-9352\" src=\"https://databricks.com/wp-content/uploads/2016/10/02-configuring-triggers-on-aws-lambda.jpg\" alt=\"Configuring triggers in AWS Lambda\" width=\"1000\" height=\"535\" />\n\n<em>Figure 2: Screen-shot of Amazon Lambda\u2019s configuration page. In the <strong>Prefix</strong> and <strong>Suffix</strong> fields, you could further limit the scope that will trigger the notifications by providing a prefix or suffix such as file extension. If not specified, all the objects created in the bucket trigger the notification.</em>\n\nThe code below shows the custom lambda function written in Node.js. It makes an HTTPs post call to a REST endpoint in Databricks. The JSON load is a key / value (job_id and the actual job number), of a preconfigured Spark job in Databricks. You can learn how Spark jobs work in Databricks in <a href=\"https://vimeo.com/156886719\">this video</a>.\n<pre>const https = require(\"https\");\n\nexports.handler = (event, context, callback) => {\n  var data = JSON.stringify({\n    \"job_id\": job_id\n  });\n\n  var options = {\n     host: \"xxx.cloud.databricks.com\",\n     port: 443,\n     path: \"/api/2.0/jobs/run-now\",\n     method: \"POST\",\n     // authentication headers\n     headers: {\n      \"Authorization\": \"Basic \" + new Buffer(\"USER:PASS\").toString(\"base64\"),\n      \"Content-Type\": \"application/json\",\n      \"Content-Length\": Buffer.byteLength(data)\n     }\n  };\n\n  var request = https.request(options, function(res){\n    var body = \"\";\n\n    res.on(\"data\", function(data) {\n      body += data;\n    });\n\n    res.on(\"end\", function() {\n      console.log(body);\n    });\n\n    res.on(\"error\", function(e) {\n      console.log(\"Got error: \" + e.message);\n    });\n\n  });\n\n  request.write(data);\n  request.end();\n};</pre>\n<h2>Example #2: Machine Learning Model Serving</h2>\nIn this example, we use a prediction model trained in Databricks using Spark ML\u2019s Random Forest Regressor. This data is an hourly snapshot of riders in a bike sharing system. For each hour, we are given the count of registered, casual, and total riders currently using a bike as well as information regarding the date and weather. Based on the data, we train a machine learning model to predict the number of riders in the D.C. bike sharing system for a given hour. Once the model is trained, we apply it to a test set of data and write the resulting predictions to a NoSQL database (in this case Riak TS). For more information about this model, take a look at <a href=\"https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html#03%20Data%20Sources/5%20Databases%20%26%20Other%20Data%20Sources/7%20RiakTS%20Tutorial.html\">part 3 and 4 of the Data Modeling notebook</a>.\n\nIn the example above, training data are stored in S3 and model creation and prediction results are written to Riak TS in batch mode. AWS Lambda is a good solution to serve the results of model prediction out of a persistence layer without dealing with any headaches around scaling, versioning, and security. Similarly, Lambda can be used to serve prediction results out of DynamoDB, Redis, or other appropriate data storage systems. This approach is not limited to regression models: it can be used for building a recommender system or a classifier just as well. One of the issues with this approach is that it is limited to prediction with categorical features (e.g. city, and state in this case). For continuous features like temperature with unbounded number of possible values, one can discretize the continuous feature (i.e., using QuantileDiscretizer), details of which are beyond the scope of this blog.\n\nIt is also possible to expand on this use case and ingest training data as a stream using Kinesis (or Kafka) with Spark Streaming. In the cases where the ML algorithm allows streaming updates (e.g. K-Means or Logistic Regression), we can update the model in near real-time. There is going to be some latency between near real-time model updates and refreshing of the prediction results in the database.\n\n<img class=\"aligncenter size-full wp-image-9353\" src=\"https://databricks.com/wp-content/uploads/2016/10/03-diagram-of-machine-learning-model-serving-with-databricks-and-other-services.jpg\" alt=\"Diagram of Machine Learning model serving with Databricks and various APIs.\" width=\"900\" height=\"507\" />\n\n<em>Figure 3: Machine Learning Model Serving: 1) real-time data feed, e.g. logs, pixels or sensory data land on Kinesis, 2) Spark\u2019s Structured Streaming pulls data for storage and processing, both batch or near-real time ML model creation / update, 3) Output model predictions are written to Riak TS, 4) AWS Lambda and AWS API Gateway are used to serve the prediction results to a variety of clients.</em>\n\nThe code below shows a custom lambda function written in Java. It makes a call to the Riak server to pull the predicted number of riders based on the city, state and time information. The JSON load is, e.g. <code>{\"city\": \"Washington\", \"state\": \"D.C.\", \"time\": \"1356436800000\"}</code>, same parameters as the notebook.\n<pre>public class RiakModelServer implements RequestHandler<Request, Response> {\n  static String tableName = \"spark-riak-predictions-time\";\n  static String hostName = \"hostname\";\n  \n  public Response handleRequest(Request request, Context context) {\n    double predictedCount = 0;\n    \n    try {\n      RiakClient client = RiakClient.newClient(hostName);\n\n      List inputPKs = new ArrayList();\n      inputPKs.add(new Cell(request.getCity()));\n      inputPKs.add(new Cell(request.getState()));\n      inputPKs.add(Cell.newTimestamp(Long.parseLong(request.getTime())));\n      Fetch fetch = new Fetch.Builder(tableName, inputPKs).build();\n      QueryResult queryResult = client.execute(fetch);\n\n      if (queryResult.getRowsCount() != 0) {\n        predictedCount = queryResult.getRowsCopy().get(0).getCellsCopy().get(3).getDouble();\n      }\n    } catch (Exception ex) {\n      ex.printStackTrace();\n    }\n    \n    return new Response(\"\" + predictedCount);\n  }\n}</pre>\n<img class=\"aligncenter size-full wp-image-9354\" src=\"https://databricks.com/wp-content/uploads/2016/10/04-screenshot-of-amazon-lambda-configuration-page.jpg\" alt=\"Screenshot of Amazon Lambda configuration page\" width=\"1000\" height=\"438\" />\n\n<em>Figure 4: Screenshot of AWS Lambda\u2019s configuration page. Predicted count for Christmas 2012 at 12 noon riders matches section 4 of the notebook.</em>\n<h2>What\u2019s Next</h2>\nNow that you have seen how Databricks can work with AWS Lambda, you can try it for yourself. <a href=\"http://databricks.com/try-databricks\">Sign-up for a free trial</a> to start experimenting with Databricks\u2019 Apache Spark clusters, APIs, and more (choose the platform trial to get full access to the API).\n\nThis is the first in a series of blogs on how to use Databricks with other services in the AWS ecosystem, <a href=\"https://twitter.com/databricks\">follow us on Twitter</a> or <a href=\"http://go.databricks.com/newsletter-registration\">sign-up for our newsletter</a> to get notified when a new blog gets posted."}
{"status": "publish", "description": "Working with large datasets is hardly ever that easy, here are some tips for debugging your Spark programs with Databricks.", "creator": "vida", "link": "https://databricks.com/blog/2016/10/18/7-tips-to-debug-apache-spark-code-faster-with-databricks.html", "authors": null, "id": 9395, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2016-10-18", "tz": "UTC", "createdOn": "2016-10-18"}, "title": "7 Tips to Debug Apache Spark Code Faster with Databricks", "slug": "7-tips-to-debug-apache-spark-code-faster-with-databricks", "content": "[sidenote]Vida Ha is a lead solution architect at Databricks. She has over a decade of experience building big data applications at Google, Square, and Databricks. She is an early user of Apache Spark and has been helping Databricks customers building production applications for over two years.[/sidenote]\n\n<hr />\n\nIn a perfect world, we all write perfect Apache Spark code and everything runs perfectly all the time, right? Just kidding - in practice, we know that working with large datasets is hardly ever that easy - there is inevitably some data point that will expose any corner cases with your code. Here are some tips for debugging your Spark programs with Databricks.\n<h2>Tip 1: Use count() to call actions on intermediary RDDs/Dataframes.</h2>\nWhile it\u2019s great that Spark follows a lazy computation model so it doesn\u2019t compute anything until necessary, the downside is that when you do get an error, it may not be clear exactly where the error in your code appeared. Therefore, you\u2019ll want to factor your code such that you can store intermediary RDDs / Dataframes as a variable. When debugging, you should call <code>count()</code> on your RDDs / Dataframes to see what stage your error occurred. This is a useful tip not just for errors, but even for optimizing the performance of your Spark jobs. It will allow you to measure the running time of each individual stage and optimize them.\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/10/01-use-count-to-call-actoins-on-interim-rdds.png\"><img class=\"aligncenter size-full wp-image-9396\" src=\"https://databricks.com/wp-content/uploads/2016/10/01-use-count-to-call-actoins-on-interim-rdds.png\" alt=\"Use count to call actions on interim RDDs\" width=\"546\" height=\"117\" /></a>\n<h2>Tip 2: Working around bad input.</h2>\nWhen working with large datasets, you will have bad input that is malformed or not as you would expect it. I recommend being proactive about deciding for your use case, whether you can drop any bad input, or you want to try fixing and recovering, or otherwise investigating why your input data is bad.\n\nA filter command is a great way to get only your good input points or your bad input data (If you want to look into that more and debug). If you want to fix your input data or to drop it if you cannot, then using a <code>flatMap()</code> operation is a great way to accomplish that.\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/10/02-working-around-bad-input.png\"><img class=\"aligncenter size-full wp-image-9397\" src=\"https://databricks.com/wp-content/uploads/2016/10/02-working-around-bad-input.png\" alt=\"Working around bad input\" width=\"565\" height=\"61\" /></a>\n<h2>Tip 3: Use the debugging tools in Databricks notebooks.</h2>\nThe Databricks notebook is the most effective tool in Spark code development and debugging. When you compile code into a JAR and then submit it to a Spark cluster, your whole data pipeline becomes a bit of a black box that is slow to iterate on. The notebooks allow you to isolate and find the parts of your data pipeline that are buggy or slow, and it also allows you to quickly try different fixes. In Databricks, we have a number of additional built-in features to make debugging very easy for you:\n<h3>Commenting</h3>\nOther users in your organization can comment on your code and suggest improvements. You could even do code reviews directly within notebooks or just share comments on the features.\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/10/03-databricks-notebook-debugging-commenting.png\"><img class=\"aligncenter size-full wp-image-9398\" src=\"https://databricks.com/wp-content/uploads/2016/10/03-databricks-notebook-debugging-commenting.png\" alt=\"Databricks offers real-time collaborative features in Notebooks.\" width=\"673\" height=\"119\" /></a>\n<h3>Version Control</h3>\nDatabricks notebooks have two different types of version control. The first is the traditional method of syncing your notebooks directly into GitHub.\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/10/04-databricks-notebook-debugging-version-control-setup.png\"><img class=\"aligncenter size-full wp-image-9399\" src=\"https://databricks.com/wp-content/uploads/2016/10/04-databricks-notebook-debugging-version-control-setup.png\" alt=\"Setting up GitHub version control in Databricks.\" width=\"249\" height=\"185\" /></a>\n\nThe other is a history of what your notebook looked like at previous points in time, and allows you to revert to an older version with the click of a button.\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/10/05-databricks-notebook-debugging-version-control-view.png\"><img class=\"aligncenter size-full wp-image-9400\" src=\"https://databricks.com/wp-content/uploads/2016/10/05-databricks-notebook-debugging-version-control-view.png\" alt=\"Viewing commit history in a Databricks Notebook.\" width=\"435\" height=\"280\" /></a>\n<h3>Condensed view of the number of partitions.</h3>\nWhen are you running a Spark Job, you can drill down and see the jobs and stages needed to run your job and how far along they are. In the workload below, for Stage 11, there are 200 partitions, 42 have completed, and 8 are currently running. If this stage were really slow, a larger Spark cluster would allow you to run more of the partitions at once and make the overall job finish faster.\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/10/06-databricks-notebook-debugging-partition-condensed-view.png\"><img class=\"aligncenter size-full wp-image-9401\" src=\"https://databricks.com/wp-content/uploads/2016/10/06-databricks-notebook-debugging-partition-condensed-view.png\" alt=\"Condensed view of Spark Job partitions\" width=\"545\" height=\"294\" /></a>\n<h3>Spark UI Popout</h3>\nIf you click on the \u201cView\u201d link above for the job, the whole Spark UI will pop up for you to debug with. In tip #4, we\u2019ll cover the Spark UI. We did a blog on this feature, <a href=\"https://databricks.com/blog/2015/09/23/easier-spark-code-debugging-real-time-progress-bar-and-apache-spark-web-ui-integration-in-databricks.html\">check it out</a> for more details.\n<h2>Tip 4: Understanding how to debug with the Databricks Spark UI.</h2>\nThe Spark UI contains a wealth of information you can use for debugging your Spark jobs. There are a bunch of great visualizations, and we have a <a href=\"https://databricks.com/blog/2015/06/22/understanding-your-spark-application-through-visualization.html\">blog post</a> here about those features.\n\nGenerally, I find the Spark UI intuitive to use. The only thing I see is that sometimes if a job fails, users will only look at the error that is thrown up to the cell in the notebook. When you have a Spark stage with a ton of tasks, if even a single task consistently fails, your whole job will fail. So, I advise drilling all the way down to the task page, sorting your page by the status, and examining the \u201cErrors\u201d column for the tasks that have failed. You\u2019ll get a detailed error message there. The task page is shown below - although the stage depicted here has completed successfully:\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/10/07-debug-spark-ui.png\"><img class=\"aligncenter size-full wp-image-9402\" src=\"https://databricks.com/wp-content/uploads/2016/10/07-debug-spark-ui.png\" alt=\"Using the Spark UI in Databricks to debug Spark Jobs.\" width=\"574\" height=\"427\" /></a>\n<h2>Tip 5: Scale up Spark jobs slowly for really large datasets.</h2>\nIf you have a really large dataset to analyze and run into errors, you may want to try debugging/testing on a portion of your dataset first. And then when you get that running smoothly, go back to the full dataset. Having a smaller dataset makes it quick to reproduce any errors, understand the characteristics of your dataset during various stages of your data pipeline, and more. Note - you can definitely run into more problems when you run the larger dataset - the hope is just that if you can reproduce the error at a smaller scale, it\u2019s easier for you to fix than if you needed the full dataset.\n\nFor an example on how to do this efficiently, check out the <a href=\"https://databricks.com/blog/2015/11/05/using-databricks-to-transition-from-concept-to-product.html\">blog</a> and <a href=\"http://go.databricks.com/webinar-better-sales-performance-with-databricks\">webinar</a> from one of our customers - Yesware.\n<h2>Tip 6: Reproduce errors or slow Spark jobs using Databricks Jobs.</h2>\nAs with any bug, having a reliable reproduction of the bug is half the effort of solving the bug. For that, I recommending reproducing errors and slow Spark jobs using the <a href=\"https://databricks.com/blog/2015/03/18/databricks-launches-jobs-feature-for-production-workloads.html\">Databricks Jobs</a> feature. This will help you capture the conditions for the bug/slowness and understand how flakey the bug is. You\u2019ll also capture the output permanently to look at - including the running times of each individual cell, the output of each cell, and any error message. And since our jobs feature contains a history UI, you can view any log files and the Spark UI even after your cluster has been shut down. You can also experiment with different Spark versions, instances types, cluster sizes, alternate ways to write your code, etc. in a very controlled environment to figure out how they affect your Spark job.\n<h2>Tip 7: Examine the partitioning for your dataset.</h2>\nWhile Spark chooses good reasonable defaults for your data, if your Spark job runs out of memory or runs slowly, bad partitioning could be at fault. For me, I start with trying different partitioning sizes to see how they affect your job.\n\nIf your dataset is large, you can try repartitioning to a larger number to allow more parallelism on your job. A good indication of this is if in the Spark UI - you don\u2019t have a lot of tasks, but each task is very slow to complete.\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/10/08-examine-dataset-partition.png\"><img class=\"aligncenter size-full wp-image-9403\" src=\"https://databricks.com/wp-content/uploads/2016/10/08-examine-dataset-partition.png\" alt=\"Examining Dataset partitions in a Spark job\" width=\"567\" height=\"70\" /></a>\n\nOn the other hand, if you don\u2019t have that much data and you have a ton of partitions, the overhead of the having too many partitions can also cause your job to be slow. You can repartition to a smaller number, but coalesce may be faster since that will try to combine partitions on the same machines rather than shuffle your data around again.\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/10/09-coalesce-dataframe.png\"><img class=\"aligncenter size-full wp-image-9404\" src=\"https://databricks.com/wp-content/uploads/2016/10/09-coalesce-dataframe.png\" alt=\"Using coalesce() can be faster than repartition()\" width=\"565\" height=\"72\" /></a>\n\nIf you are using Spark SQL, you can set the partition for shuffle steps by setting spark.sql.shuffle.partitions.\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/10/10-shuffle-partitions.png\"><img class=\"aligncenter size-full wp-image-9405\" src=\"https://databricks.com/wp-content/uploads/2016/10/10-shuffle-partitions.png\" alt=\"Setting shuffle partition steps\" width=\"567\" height=\"128\" /></a>\n<h2>What\u2019s Next</h2>\nThis is the first blog in a series on how to debug and optimize Apache Spark code on Databricks. To get notified when the next blog comes out, <a href=\"https://twitter.com/databricks\" target=\"_blank\">follow us on Twitter</a> or <a href=\"http://go.databricks.com/newsletter-registration\" target=\"_blank\">subscribe to the newsletter</a>.\n\nIf you are interested in trying Databricks, <a href=\"http://databricks.com/try\">sign-up for a free trial</a> or <a href=\"http://go.databricks.com/contact-databricks\">contact us</a>."}
{"status": "publish", "description": "With Spark deployments tuned for GPUs, plus pre-installed libraries and examples, Databricks offers a simple way to leverage GPUs to power image processing, text analysis, and other Machine Learning tasks.", "creator": "joseph", "link": "https://databricks.com/blog/2016/10/27/gpu-acceleration-in-databricks.html", "authors": null, "id": 9420, "categories": ["Apache Spark", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2016-10-27", "tz": "UTC", "createdOn": "2016-10-27"}, "title": "GPU Acceleration in Databricks", "slug": "gpu-acceleration-in-databricks", "content": "Databricks is adding support for Apache Spark clusters with Graphics Processing Units (GPUs), ready to accelerate Deep Learning workloads (<a href=\"http://www.marketwired.com/press-release/databricks-adds-deep-learning-support-to-cloud-based-apache-spark-platform-2170158.htm\">read press release</a>). With Spark deployments tuned for GPUs, plus pre-installed libraries and examples, Databricks offers a simple way to leverage GPUs to power image processing, text analysis, and other Machine Learning tasks. Users will benefit from 10x speedups in Deep Learning, automated configuration of GPU machines, and smooth integration with Spark clusters. The feature is available <a href=\"http://go.databricks.com/contact-databricks\">by request</a>, and will be generally available within weeks.\n<h2>Speeding up Machine Learning with GPUs</h2>\nDeep Learning is an extremely powerful tool for modeling data, but it comes at the price of expensive computations. GPUs can drastically lower the cost because they support efficient parallel computation. To demonstrate these benefits, we benchmarked a simple numerical task (<a href=\"https://en.wikipedia.org/wiki/Kernel_density_estimation\" target=\"_blank\">kernel density estimation</a>). We compared optimized code written in Scala and run on top-of-the-line compute intensive machines in AWS (c3.8xlarge) against standard GPU hardware (g2.2xlarge). Using <a href=\"https://www.tensorflow.org/\" target=\"_blank\">TensorFlow</a> as the underlying compute library, the code is 3X shorter, and about 4X less expensive (in $ cost) to run on a GPU cluster. More details about achieving this level of performance will be published in a future blog post.\n\n<img class=\"aligncenter size-full wp-image-9429\" src=\"https://databricks.com/wp-content/uploads/2016/10/cpu-and-gpu-deep-learning-comparison.png\" alt=\"Chart showing cost comparison running expensive computations on compute-intensive machines (c3.8xlarge) vs standard GPU hardware (g2.2xlarge) on AWS.\" width=\"371\" height=\"284\" />\n<h2>Using GPUs in Databricks</h2>\nDatabricks\u2019 GPU offering provides tight integration with Apache Spark. When you create a GPU cluster in Databricks, we configure the Spark cluster to use GPUs, and we preinstall the libraries required to access the GPU hardware. In addition, we provide scripts that install popular Deep Learning libraries so that you can immediately get started with Deep Learning or other GPU-accelerated tasks.\n\nHere are some specifics on our current GPU offering:\n<ul>\n \t<li>Amazon EC2 g2.2xlarge (1 GPU) and g2.8xlarge (4 GPUs) instance types. p2 (1-16 GPUs) instance types coming soon <em>(UPDATE: We are supporting P2 instances instead of G2 instances because P2 generally provide more memory and GPU cores per dollar than G2)</em>.</li>\n \t<li>Pre-installed CUDA \u00ae and cuDNN libraries.</li>\n \t<li>Support for GPUs on both driver and worker machines in Spark clusters.</li>\n \t<li>Simplified installation of Deep Learning libraries, via provided and customizable init scripts.</li>\n</ul>\n<h2>How Databricks integrated Spark with GPUs</h2>\nApache Spark does not provide out-of-the-box GPU integration. One of the key benefits of using GPUs on Databricks is our work on configuring Spark clusters to utilize GPUs. When you run Spark on Databricks, you will notice a few things that make your life easier:\n\n<strong>Cluster setup:</strong> GPU hardware libraries like CUDA and cuDNN are required for communication with the graphics card on the host machine. Simply downloading and installing these libraries takes time, especially in cloud-based offerings which create and tear down clusters regularly. By providing pre-installed libraries, Databricks reduces cluster setup time (and the EC2 cost of setup) by about 60%.\n\n<strong>Spark configuration:</strong> We configure GPU Spark clusters to prevent contention on GPU devices. Essentially, GPU context switching is expensive, and GPU libraries are generally optimized for running single tasks. Therefore, reducing Spark parallelism per executor results in higher throughput.\n\n<strong>Cluster management:</strong> Databricks provides these capabilities within a secure, containerized environment. We isolate users from each other, and we reuse EC2 instances when you launch a cluster to minimize your cost.\n<h2>Using Deep Learning libraries on Databricks</h2>\nDatabricks users may take advantage of many Deep Learning libraries. For example, the above performance benchmark plot used TensorFlow. We have published an open source Spark Package <a href=\"https://github.com/databricks/tensorframes\" target=\"_blank\">TensorFrames</a> that integrates Spark with TensorFlow. To learn more, check out <a href=\"https://www.youtube.com/watch?v=gXItObf-qaI\">Tim Hunter\u2019s talk at Spark Summit Europe</a>. Databricks users can also take advantage of other popular libraries such as <a href=\"http://caffe.berkeleyvision.org/\" target=\"_blank\">Caffe</a>. Our follow-up blog post will dive into more details and tutorials.\n<h2>Getting started</h2>\n<a href=\"http://go.databricks.com/contact-databricks\" target=\"_blank\">Contact us</a> if you want to get started with deep learning on Databricks. We are working on example notebooks showing how to get started with GPU-accelerated Deep Learning on Spark. In the meantime, check out our <a href=\"https://github.com/databricks/tensorframes\" target=\"_blank\">TensorFrames package</a> and example notebooks from our <a href=\"https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html\" target=\"_blank\">previous blog post</a>. Sign up for our <a href=\"https://databricks.com/resources/type/newsletters\" target=\"_blank\">newsletter</a> and <a href=\"https://twitter.com/databricks\" target=\"_blank\">follow us on Twitter</a> to be notified when the next blog in the series comes out in a few weeks!"}
{"status": "publish", "description": "Using Amazon EC2 Spot instances to launch your Apache Spark clusters can significantly reduce the cost of running your big data applications in the cloud. But how do you manage the risks of nodes terminating and balance that with cost savings?", "creator": "jakebellacera", "link": "https://databricks.com/blog/2016/10/25/running-apache-spark-clusters-with-spot-instances-in-databricks.html", "authors": null, "id": 9435, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2016-10-25", "tz": "UTC", "createdOn": "2016-10-25"}, "title": "Running Apache Spark Clusters with Spot Instances in Databricks", "slug": "running-apache-spark-clusters-with-spot-instances-in-databricks", "content": "Using Amazon EC2 Spot instances to launch your Apache Spark clusters can significantly reduce the cost of running your big data applications in the cloud. But how do you manage the risks of nodes terminating and balance that with cost savings?\n\nIn this blog post, I\u2019ll cover what Spot instances are and how they can dramatically lower your computing costs for jobs that are time flexible and tolerant to interruption. Then I\u2019ll explain how you can combine Spot and On-Demand instances in the same cluster to get the best of both worlds. \n\nWith Databricks, you can run the Spark Master node and some dedicated workers On-Demand supplemented by more workers on Spot instances. The On-Demand workers guarantee your jobs will eventually complete (ensuring predictability), while the Spot instances accelerate the job completion times.\n\n<h2>What are EC2 Spot instances?</h2>\n\nThe idea behind Spot instances is to allow you to bid on spare Amazon EC2 compute capacity. You choose the max price you\u2019re willing to pay per EC2 instance hour. If your bid meets or exceeds the Spot market price, you win the Spot instances. However, unlike traditional bidding, when your Spot instances start running, you pay the live Spot market price (not your bid amount). Spot prices fluctuate based on the supply and demand of available EC2 compute capacity and are specific to different regions and availability zones.\n\nSo, although you may have bid 0.55 cents per hour for a r3.2xlarge instance, you\u2019ll end up paying only 0.10 cents an hour if that\u2019s what the going rate is for the region and availability zone.\n\nThe first hour of your Spot instance price is based on the current market price at the time of launch. AWS then re-evaluates this market price every hour.\n\nIf the Spot market price increases above your bid price at any point, your EC2 instance could be automatically terminated and you are not charged for that last partial hour. However, if you manually terminate your Spot instances, you will be billed for the full last hour.\n\nThis is different from the traditional On-Demand instances, where Amazon guarantees the availability of instances (99.95% SLA) but dictates the price they'll charge you per hour. On-demand pricing is statically set and rarely changes.\n\n<h2>How significant can the cost savings be?</h2>\n\nAt the time of writing this (October 2016), an On-Demand r3.2xlarge instance (with 8 vCPUs and 61 GB RAM) costs $0.665 per hour in the US East region. However, the current Spot price for that instance is only $0.10\u2026 a cost savings of over 80%!\n\n<h2>Launching Spark Clusters with Spot instances</h2>\n\nThere are several ways to use Spot instances in your Spark clusters:\n\n<ul>\n<li>Mix of On-Demand and Spot instances in a single cluster (Hybrid model):<ul>\n<li>Only Master node (with Spark Driver) is On-Demand, with all workers running on Spot instances</li>\n<li>Master node and some dedicated workers are On-Demand, while other workers are on Spot instances</li>\n<li>In either of these two hybrid models, terminated Spot instances could optionally be replaced by On-Demand instances</li></ul></li>\n<li>100% Spot instance based cluster:<ul>\n<li>If prices spike, terminate cluster</li>\n<li>If prices spike, try to relaunch nodes with On-Demand instances so cluster fully recovers, but costs more to run (this option can only take place if the Spot based Driver is not terminated because of the price spike)</li></ul></li>\n</ul>\n\nDatabricks supports all of the variations above. I\u2019ll first discuss the pros/cons and use cases of the hybrid model, which is our recommendation for most customers. Then I\u2019ll discuss launching 100% Spot instance clusters. Finally, I\u2019ll show you how to set the actual bid price in Databricks and research the historic and live Spot market prices for different instance types.\n\n<h2>Mixing On-Demand and Spot instances in the same Cluster (Default and Recommended Approach)</h2>\n\nWhat if you want to get the best of both worlds\u2026 a single Spark cluster with a combination of On-Demand and Spot instances?\n\nThis would allow you to launch the Spark Driver On-Demand to ensure the stability of your Spark cluster. The Worker nodes could be a mix of On-Demand and Spot. The On-Demand instances guarantee your Spark jobs will run to completion, while the Spot instances let you accelerate batch workloads affordably.\n\nCreating Spark clusters with a mix of On-Demand and Spot EC2 instances is simple in Databricks. On the Create Cluster page,  just choose the default of \u201cOn-Demand and Spot\u201d Type from the drop-down and pick the number of On-Demand vs Spot instances you want:\n\n<img src=\"https://databricks.com/wp-content/uploads/2016/10/01-create-cluster-screen.png\" alt=\"Create Cluster Screen\" width=\"666\" height=\"470\" class=\"aligncenter size-full wp-image-9438\" />\n\nThe screenshot above shows a minimum of 5 On-Demand worker instances and a variable amount of 15 Spot instances.\n\nIn \u201cOn-Demand and Spot\u201d type clusters, the Driver is always run on an On-Demand instance (see green line above). The numbers you choose for On Demand and Spot only affect the worker nodes.\n\nThe \u201cFall back to On-demand\u201d option will be turned on by default:\n\n<img src=\"https://databricks.com/wp-content/uploads/2016/10/02-fall-back-to-on-demand-cluster.png\" alt=\"Fall back to on-demand node option\" width=\"550\" height=\"251\" class=\"aligncenter size-full wp-image-9439\" />\n\nWhen enabled, this setting will launch your Spark cluster with On-Demand instances if the Spot market price is greater than your bid price at the cluster creation time. \n\nAdditionally, while your Spot instances are running, if the Spot price increases above your bid price, the \u201cFall back to On-Demand\u201d feature will re-launch your terminated Spot instances as On-Demand instances.\n\nOnce a node falls back to On-Demand, it remains On-Demand for the duration of the cluster\u2019s lifetime.\n\n\u201cFall back to On-Demand\u201d is recommended for use cases where you want Databricks to always aim to keep the cluster at the desired size, even if that means launching On-Demand nodes. This brings predictability to your workloads, as you can pre-calculate how long it takes to run a Job on a specific size cluster.\n\nIf you\u2019re planning on doing exploratory data analysis and are okay with the Spot nodes in the cluster terminating and running in a degraded state, then uncheck the \u201cFall back to On-demand\u201d option.\n\nIf you turn off the \u201cFall back to On-demand\u201d option, you will never pay more than your bid price for each EC2 Spot instance (see how to set your bid price below).\n\nOnce your cluster has launched, you can check how many nodes are On-Demand vs. Spot in the active clusters dashboard:\n\n<img src=\"https://databricks.com/wp-content/uploads/2016/10/03-active-clusters-screen.png\" alt=\"Active Clusters Screen in Databricks showing split between On-Demand and Spot nodes\" width=\"605\" height=\"154\" class=\"aligncenter size-full wp-image-9440\" />\n\nFor example, to run a hybrid cluster with only the Master/Driver node On-Demand with 20 worker Spot instances, use the following settings:\n\n<img src=\"https://databricks.com/wp-content/uploads/2016/10/04-hybrid-cluster-spot-only.png\" alt=\"Screenshot of a cluster configuration with 20 spot nodes.\" width=\"437\" height=\"251\" class=\"aligncenter size-full wp-image-9441\" />\n\nNote in the screenshot above that \u201cFall back to On-Demand\u201d is enabled, so if any of the 20 Spot instances terminate, they will automatically be replaced by On-Demand instances so the cluster will aim to always run with 20 nodes. If you\u2019re okay running in a degraded state with only a fraction of the 20 Spot instances, uncheck \u201cFall back to On-Demand\u201d.\n\n<h2>100% Spot instance based clusters</h2>\n\nUsing 100% Spot instance based Spark clusters is not recommended for production workloads because of the threat of losing production jobs to unpredictable Spot prices. Specifically, if the EC2 Spot instance running the Master node (with Spark Driver) is terminated, the entire cluster is also immediately terminated.\n\nNevertheless, 100% Spot instances are commonly used when Data Analysts or Data Scientists are exploring new data sets. If their Spot cluster terminates, they can quickly launch a new cluster with a higher bid price or with On-Demand instances instead. In either recovery attempt, the analyst or scientist can easily re-run their Notebook with one click (the \u2018Run All\u2019 button in Databricks). \n\nCreating Spark clusters with only EC2 Spot instances is simple in Databricks. On the Create Cluster page, choose \u201cSpot\u201d for the Instance type:\n\n<img src=\"https://databricks.com/wp-content/uploads/2016/10/05-how-to-create-spot-only.png\" alt=\"Screenshot of a spot-only cluster configuration\" width=\"437\" height=\"252\" class=\"aligncenter size-full wp-image-9442\" />\n\nIf you want the most affordable Spark cluster that can run Spark commands, use a size of 1:\n\n<img src=\"https://databricks.com/wp-content/uploads/2016/10/06-single-spot-cluster.png\" alt=\"Screenshot of a single spot cluster\" width=\"438\" height=\"252\" class=\"aligncenter size-full wp-image-9443\" />\n\nSpot Clusters with size 1 will use two EC2 spot instances, one for the Spark Driver and another for the Spark Worker.\n\nHow do you know if a cluster terminated because Spot market price exceeded your bid price? Hover over a cluster\u2019s state to see why it was terminated:\n\n<img src=\"https://databricks.com/wp-content/uploads/2016/10/07-terminated-clusters-screen.png\" alt=\"Terminated clusters screen\" width=\"643\" height=\"185\" class=\"aligncenter size-full wp-image-9444\" />\n\n<h2>How do you set your bid price?</h2>\n\nNow that you\u2019ve seen the different types of clusters Databricks supports, you may be wondering how to control the bid price.\n\nOn the Create Cluster page, scroll down and click on \u201cShow advanced settings\u201d, then notice the Spot Bid Price field:\n\n<img src=\"https://databricks.com/wp-content/uploads/2016/10/08-advanced-cluster-settings-screen.png\" alt=\"Screenshot of advanced cluster settings in Databricks\" width=\"881\" height=\"380\" class=\"aligncenter size-full wp-image-9445\" />\n\nDatabricks defaults to a bid price that is equal to the current on-demand price. So if the on-demand price for your instance type is $0.665 cents per hour, then the default bid price is also set to $0.665 cents per hour. \n\nRecall that with Spot instances, you don\u2019t necessarily pay your actual bid price -  you pay the Spot market price for your instance type. So, even though you may have bid $0.665 cents, in reality you may only be charged $0.10 for the hour.\n\nBy setting your bid price to 100% of the on-demand price, you\u2019re essentially saying \u201ccharge me whatever the live Spot market price is, but never more than the on-demand price for the same instance type\u201d.\n\nHowever, sometimes achieving cost savings is the top priority. In this case Databricks lets you set your own bid price. Let\u2019s say you actually are only willing to pay 50% of the On-Demand price. You can change your bid accordingly by clicking on \u201cShow advanced settings\u201d and changing the \u201cSpot Bid Price\u201d field:\n\n<img src=\"https://databricks.com/wp-content/uploads/2016/10/09-setting-spot-bid-price.png\" alt=\"Setting a spot node bid price in Databricks\" width=\"585\" height=\"62\" class=\"aligncenter size-full wp-image-9446\" />\n\nOn the other hand, if stability is more important and if you don't mind occasionally paying more than the On-Demand instance price so that the overall amortized cost over time is cheaper than On-Demand, you can also bid at more than 100%... say 120% or 150%. In these cases, you are still billed the Spot market price, and although some hours may cost more than the On-Demand equivalent, the total cost for your cluster should generally be cheaper than the On-Demand costs would have been.\n\n<h2>How much is the On-Demand price vs the Spot market price right now?</h2>\n\nYou can look up the current On-Demand instance prices on the <a href=\"https://aws.amazon.com/ec2/pricing/\" target=\"_blank\">AWS EC2 Pricing page</a>. To see the live Spot market price for your instance type, one way is to attempt launching that instance via the EC2 console and check \u201cRequest Spot instances\u201d during Step 3:\n\n<img src=\"https://databricks.com/wp-content/uploads/2016/10/10-aws-ec2-pricing-page-screenshot.png\" alt=\"Screenshot of the AWS EC2 pricing page\" width=\"526\" height=\"309\" class=\"aligncenter size-full wp-image-9447\" />\n\nFor historic trend analysis of Spot market prices, I like using <a href=\"https://aws.amazon.com/ec2/pricing/\" target=\"_blank\">Amazon\u2019s official Pricing History</a> and <a href=\"https://aws.amazon.com/ec2/spot/bid-advisor/\" target=\"_blank\">Spot Bid Advisor</a>, along with this <a href=\"https://ec2price.com/\" target=\"_blank\">unofficial EC2 Spot Prices website</a>.\n\nTo access the official Spot Pricing History, log into your AWS console, go to the EC2 services dashboard, choose \u201cSpot Requests\u201d in the left pane, then click the Pricing History button at the top.\n\n<img src=\"https://databricks.com/wp-content/uploads/2016/10/11-official-ec2-spot-instance-pricing-history.png\" alt=\"Screenshot of the official AWS EC2 Spot Instance pricing history page.\" width=\"808\" height=\"454\" class=\"aligncenter size-full wp-image-9448\" />\n\nAs you hover your mouse across the chart, the prices in the bottom left change based on their historical values. You can choose date ranges of 1 day, 1 week, 1 month and 3 months.\n\nIf I zoom out to 1 week, I can see that the us-east-1e availability zone has experienced multiple price spikes, so perhaps I\u2019ll avoid this zone when launching my Spot cluster:\n\n<img src=\"https://databricks.com/wp-content/uploads/2016/10/12-official-ec2-spot-instance-pricing-history-zoomed-out.png\" alt=\"Zoomed-out screenshot of the official AWS EC2 Spot pricing history page.\" width=\"806\" height=\"447\" class=\"aligncenter size-full wp-image-9449\" />\n\nI also find <a href=\"https://aws.amazon.com/ec2/spot/bid-advisor/\" target=\"_blank\">Amazon\u2019s Spot Bid Advisor</a> very useful in helping me decide my bid price, based on how likely I am to be out-bid: \n\n<img src=\"https://databricks.com/wp-content/uploads/2016/10/13-official-ec2-spot-bid-advisor-page.png\" alt=\"Screenshot of the official AWS EC2 Spot Bid Advisor utility.\" width=\"873\" height=\"505\" class=\"aligncenter size-full wp-image-9450\" />\n\n<h2>What\u2019s Next</h2>\n\nDatabricks makes it simple to manage On-Demand and Spot instances within the same Spark cluster. Our users can create purely Spot clusters, purely On-Demand clusters, or hybrid clusters with just a few clicks.\n\nGet started with a <a href=\"https://databricks.com/try\">free Databricks trial</a>."}
{"status": "publish", "description": null, "creator": "jules_damji", "link": "https://databricks.com/blog/2016/10/26/day-1-databricks-voices-spark-summit-eu.html", "authors": null, "id": 9470, "categories": ["Company Blog", "Events"], "dates": {"publishedOn": "2016-10-26", "tz": "UTC", "createdOn": "2016-10-26"}, "title": "Databricks Voices From Spark Summit EU 2016 Day 1", "slug": "day-1-databricks-voices-spark-summit-eu", "content": "<strong>Update: The videos of the presentations are now available. Find them below.</strong>\n\n<hr />\n\n<h2>Spark Summit Keynotes</h2>\nBrussels\u2019 October morning overcast or morning-commute traffic did not faze more than 1000 Apache Spark enthusiasts to converge at the SQUARE convention center to hear keynote from the Spark creator Matei Zaharia.\n\n<img class=\"aligncenter size-full wp-image-9475\" src=\"https://databricks.com/wp-content/uploads/2016/10/demo-whole-room-sm.jpg\" alt=\"Attendees at Day 1 of Spark Summit EU's Keynotes\" width=\"680\" height=\"453\" />\n<h3>Simplifying Big Data Applications with Apache Spark 2.0</h3>\nEarly in the year when Matei Zaharia took the stage at Spark Summit East, he shared his vision where the community was heading with Apache Spark and coined the term <a href=\"https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html\" target=\"_blank\">continuous applications</a>, as a way for developers to write end-to-end real-time applications that reacts to real-time data. In July 2016, <a href=\"https://databricks.com/blog/2016/07/26/introducing-apache-spark-2-0.html\" target=\"_blank\">Apache Spark 2.0 was released</a>.\n\nAnd today in Brussels, in his keynote at Spark Summit, the biggest conference dedicated to Apache Spark, Zaharia shared how all the work that went into Spark 2.0, to unify a single interface based on DataFrames and Datasets, facilitate and simplify not only how to write continuous applications but also simplify how to write big data applications.\n\n<img class=\"aligncenter size-full wp-image-9471\" src=\"https://databricks.com/wp-content/uploads/2016/10/matei-zaharia-spark-summit-eu-2016.jpg\" alt=\"Matei Zaharia's keynote at Spark Summit Europe 2016\" width=\"680\" height=\"453\" />\n\nZaharia said that writing big data applications is hard. For one it entails a complex combination of processing tasks, storage systems and modes such as ETL, aggregation, machine learning and streaming. Second, it\u2019s hard to get both the productivity and performance.\n\nYet he outlined a number of ways in which the Spark 2.0 approach simplifies writing big data applications.\n\nFirst, Spark\u2019s unified engine allows you to express an entire workflow in a single API, connecting existing libraries and storage. Second, the high-level APIs in DataFrames and ML pipeline result in optimized code. And finally, Structured Streaming APIs enable a developer to write a multi-faceted continuous applications.\n\n<img class=\"aligncenter size-full wp-image-9472\" src=\"https://databricks.com/wp-content/uploads/2016/10/apache-spark-approach.png\" alt=\"Outline of Apache Spark's approach to simplify writing big data applications\" width=\"1000\" height=\"516\" />\n\nShowing what Matei alluded as <em>continuous applications</em> - a way to interact with both batch and streaming data - Databricks software engineer Greg Owen demonstrated how you can combine real-time streaming data aggregation with machine learning models, to garner newer insights. All this is possible because of Structured Streaming.\n\nIn his demo, using Sentiment Analysis, Greg showed how real-time tweets related to Brexit affected people's sentiments about Marmite\u2019s rising prices.\n\n<a href=\"https://vimeo.com/188960545\" target=\"_blank\"><img class=\"aligncenter size-full wp-image-9467\" src=\"https://databricks.com/wp-content/uploads/2016/10/OG_SSEu2016-Demo.jpg\" alt=\"og_sseu2016-demo\" width=\"600\" height=\"314\" /></a>\n\nIn summation, Spark 2.0 lays the foundation for the future of unifying APIs based on DataFrames and Datasets across all Spark components for the aforementioned reasons. In the end big data developers write less code, grapple with fewer concepts, and get better performance.\n\n<strong>Update: You can <a href=\"https://www.youtube.com/watch?v=Zb9YW8XjxnE\">watch the full presentation here</a>.</strong>\n<h3>The Next AMPLab: Real-time Intelligent Secure Execution</h3>\nFollowing the Matei\u2019s keynote, Databricks co-founder and executive chairman Ion Stoica shared his vision of what\u2019s the next phase in distributed computing.\n<p style=\"background-color: #000;\"><img class=\"aligncenter size-full wp-image-9473\" src=\"https://databricks.com/wp-content/uploads/2016/10/ion-stocia-spark-summit-eu-2016.jpg\" alt=\"Ion Stocia's keynote at Spark Summit Europe 2016\" width=\"340\" height=\"511\" /></p>\nAn incubator of Apache Spark, Alluxio, and Apache Mesos, AMPLab\u2019s reign ends this year. But it transitions into new phase of innovation: RISELab (Real-time Intelligent Secure Execution).\n\n<img class=\"aligncenter size-full wp-image-9474\" src=\"https://databricks.com/wp-content/uploads/2016/10/amplab-transition-to-riselab.png\" alt=\"Diagram showing the transition from AMPLab to RISELab\" width=\"654\" height=\"328\" />\n\nCommitted to the goal of building open-source frameworks, tools, and algorithms that make building real-time applications decisions on live data with stronger security, this new phase is set to innovate and enhance Spark with two projects\u2014Drizzle and Opaque\u2014Stoica said.\n\nWhile Drizzle reduces Apache Spark\u2019s streaming latency by a factor of ten and bolsters fault-tolerance, Opaque enhances Spark\u2019s data encryption at-rest or in-motion, offering stronger protection in cloud or on-premise.\n\n<strong>Update: You can<a href=\"https://www.youtube.com/watch?v=lQrByGOpbZw\"> watch the full presentation here</a>.</strong>\n<h2>Developer Track Sessions</h2>\n<h3>Spark\u2019s Performance: past, present and future</h3>\nKicking off summit\u2019s first day\u2019s developer track, Sameer Agarwal surveyed Apache Spark\u2019s performance\u2014past, present and future. In their short historical exploration of Spark\u2019s performance, the team asked a basic question: \u201cSpark is already pretty fast, but can we make it 10x faster?\u201d The answer, in short, led to Apache Spark 2.0 shipping with the <a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\" target=\"_blank\">second generation Tungsten engine</a>, built upon ideas from <a href=\"https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html\" target=\"_blank\">modern compilers and MPP databases and applied to data processing queries</a> (SQL / DataFrames).\n\nAgarwal shared an insightful glimpse into Spark Internals and covered how Tungsten emits optimized bytecode at runtime that collapses the entire query into a single function, eliminating virtual function calls and leveraging CPU registers for intermediate data. He also discussed how Tungsten laid out the cached data in memory in a column oriented format for more efficient storage and higher performance.\n\n<strong>Update: You can <a href=\"https://www.youtube.com/watch?v=RlbBPrWJEEM\">watch the full presentation here.</a></strong>\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/10/columnar-in-memory-format.png\" target=\"_blank\"><img class=\"aligncenter size-full wp-image-9476\" src=\"https://databricks.com/wp-content/uploads/2016/10/columnar-in-memory-format.png\" alt=\"Visualization of how Tungsten lays out cached data in memory\" width=\"680\" height=\"383\" /></a>\n<h3>A Deep Dive into the Catalyst Optimizer</h3>\nWhether you express your query or computation in SQL, DataFrame or Dataset, it undergoes the same Spark SQL engine with the Catalyst Optimizer. Through careful analysis, the Catalyst optimizer generates logical and physical plans, followed by an RDD-level byte code, optimized for optimal performance, for the JVM.\n\nHerman van Hovell, a Spark committer and Databricks software engineer, took his audience on a deep dive on the entire pipeline and transformation that a query undergoes. He explained that the main data type in Catalyst is a tree composed of node objects or operations with zero or more children. Next he discussed how rules can be applied to manipulate the tree, and finally how rules may rearrange the nodes in the tree for optimal code generation.\n\nImmediately following the talk, the audience got a hands-on lab session and walked away with an in-depth understanding of how Catalyst works under the hood.\n\n<strong>Update: You can <a href=\"https://www.youtube.com/watch?v=GDeePbbCz2g\">watch the full presentation here</a>, and the <a href=\"https://www.youtube.com/watch?v=MUZEfdvw5pU\">hands-on lab session</a> here.</strong>\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/10/overview-of-how-catalyst-works.png\" target=\"_blank\"><img class=\"aligncenter size-full wp-image-9477\" src=\"https://databricks.com/wp-content/uploads/2016/10/overview-of-how-catalyst-works.png\" alt=\"Diagram showing an overview of how Catalyst works\" width=\"680\" height=\"383\" /></a>\n<h2>What\u2019s Next</h2>\n<a href=\"https://databricks.com/blog/2016/10/27/day-2-databricks-voices-spark-summit-eu-2016.html\">A recap of the second day is available</a>, read it in our blog. Also, shortly after the Spark Summit, all the keynotes and sessions talks and slides will available at the <a href=\"https://spark-summit.org/eu-2016/\" target=\"_blank\">Spark Summit EU website</a>."}
{"status": "publish", "description": null, "creator": "jules_damji", "link": "https://databricks.com/blog/2016/10/27/day-2-databricks-voices-spark-summit-eu-2016.html", "authors": null, "id": 9521, "categories": ["Company Blog", "Events"], "dates": {"publishedOn": "2016-10-27", "tz": "UTC", "createdOn": "2016-10-27"}, "title": "Databricks Voices From Spark Summit EU 2016 Day 2", "slug": "day-2-databricks-voices-spark-summit-eu-2016", "content": "[sidenote]<a href=\"https://databricks.com/blog/2016/10/26/day-1-databricks-voices-spark-summit-eu.html\" target=\"_blank\">Read the recap from Day 1 of Spark Summit EU.</a>[/sidenote]\n\n<strong>Update: The videos of the presentations are now available. Find them below.</strong>\n\n<hr />\n\n<h2>Spark Summit Keynotes</h2>\nAlthough the October overcast persisted over Brussels, inside the SQUARE\u2019s convention center attendees lined up, with coffee in one hand and pastry in the other, to hear how other organizations employ Apache Spark for their use cases.\n\n<img class=\"aligncenter size-full wp-image-9522\" src=\"https://databricks.com/wp-content/uploads/2016/10/spark-summit-eu-2016-day-2-crowd.jpg\" alt=\"The crowd at Spark Summit EU 2016 Day 2\" width=\"680\" height=\"452\" />\n<h2>Democratizing AI with Apache Spark</h2>\nThe second day kicked off with the a keynote from Databricks CEO Ali Ghodsi on the topic of artificial intelligence (AI).\n\n<iframe style=\"margin: 0 auto;\" src=\"https://player.vimeo.com/video/189123192\" width=\"640\" height=\"360\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\nAli observed that the machine learning algorithm itself is rarely the main barrier in building AI applications. Instead, the real culprit is the set of complex systems that manages the infrastructure and prepares the data for the ML algorithms.\n\nAccording to Ali, Spark is a huge leap forward in democratizing AI because of its speed, flexibility, and scalability. However, Spark cannot solve all the problems around AI by itself - and this is where Databricks comes in. Databricks\u2019 vision is to build a platform around Spark that allows organizations to easily capitalize on Spark\u2019s inherent speed, flexibility, and scalability for advanced analytics and beyond.\n\nAs another step towards Databricks\u2019 goal, Ali announced the addition of GPU support and integration of popular deep learning libraries to the Databricks\u2019 big data platform. This allows organizations to easily conduct deep learning on Spark using the popular TensorFlow framework on top of highly optimized GPU hardware.\n\nThe deep learning functionality works in concert with other components of the Databricks platform, enabling organizations to seamlessly perform data wrangling, feature extraction, interactive exploration, and model training in an end-to-end machine learning pipeline. <a href=\"https://databricks.com/blog/2016/10/27/gpu-acceleration-in-databricks.html\" target=\"_blank\">Read the blog on GPU support to learn more</a>, or <a href=\"http://go.databricks.com/contact\" target=\"_blank\">contact us to get started</a>.\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/10/Deep-Learning-on-Databricks.png\"><img class=\"aligncenter size-full wp-image-9523\" src=\"https://databricks.com/wp-content/uploads/2016/10/Deep-Learning-on-Databricks.png\" alt=\"Deep Learning on Databricks\" width=\"680\" height=\"387\" /></a>\n<h2>Developer Track Sessions</h2>\n<h3>TensorFrames: Deep Learning with TensorFlow on Apache Spark</h3>\n<img class=\"aligncenter size-full wp-image-9524\" src=\"https://databricks.com/wp-content/uploads/2016/10/image02.jpg\" alt=\"Tim Hunter speaking at Spark Summit EU 2016\" width=\"680\" height=\"453\" />\n\nDatabricks engineer Tim Hunter revealed more details behind Ali\u2019s keynote with a presentation focused on deep learning on Apache Spark. He discussed how to combine Apache Spark with <a href=\"https://www.tensorflow.org/\" target=\"_blank\">TensorFlow</a>, a popular framework from Google that provides the building blocks for Machine Learning computations on GPUs. Tim demonstrated how to use GPUs with TensorFlow on Apache Spark to achieve extremely fast performance.\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/10/image03.png\"><img class=\"aligncenter size-full wp-image-9525\" src=\"https://databricks.com/wp-content/uploads/2016/10/image03.png\" alt=\"Tensorframes slide\" width=\"680\" height=\"384\" /></a>\n\nThe trick, as Hunter explained, is <a href=\"https://github.com/databricks/tensorframes\" target=\"_blank\">TensorFrames</a>. It is a library he wrote that allows Spark developers to easily pass data between Spark DataFrames and the TensorFlow runtime while taking advantage of the latest performance optimizations in <a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\" target=\"_blank\">Project Tungsten</a>. The result is faster and simpler code.\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/10/tensorframes-speedup-slide.png\"><img class=\"aligncenter size-full wp-image-9526\" src=\"https://databricks.com/wp-content/uploads/2016/10/tensorframes-speedup-slide.png\" alt=\"Tensorframes speedup slide\" width=\"680\" height=\"384\" /></a>\n\n<strong>Update: You can <a href=\"https://www.youtube.com/watch?v=gXItObf-qaI\">watch the full presentation here.</a></strong>\n<h2>Data Science Track</h2>\n<h3>Online Learning with Structured Streaming</h3>\nIn his session talk, Databricks Product Manager Ram Sriharsha defined online machine learning as an ability to learn efficiently over a data stream on a single pass, especially when you cannot replay or revisit your data point. This ability is important for two sets of problems. First, for large scale learning in which achieving accuracy in a given time is important, machine learning algorithms might achieve that accuracy faster. And second, when data distribution changes over time, online algorithms can adapt to changing algorithms.\n\n<img class=\"aligncenter size-full wp-image-9527\" src=\"https://databricks.com/wp-content/uploads/2016/10/Ram-Sriharsha-Spark-Summit-EU-2016.png\" alt=\"Ram Sriharsha speaking at Spark Summit EU 2016\" width=\"680\" height=\"452\" />\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/10/what-is-online-learning-slide.png\"><img class=\"aligncenter size-full wp-image-9528\" src=\"https://databricks.com/wp-content/uploads/2016/10/what-is-online-learning-slide.png\" alt=\"Slide discussing the overview of Online Learning\" width=\"680\" height=\"381\" /></a>\n\nImplementing online Machine Learning (ML) on top Structured Streaming makes it all possible, Sriharsha said, because we can leverage its fault tolerance and interoperability with MLlib for invoking feature transformation and online algorithms within the same ML pipeline.\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/10/why-structured-streaming-slide.png\"><img class=\"aligncenter size-full wp-image-9529\" src=\"https://databricks.com/wp-content/uploads/2016/10/why-structured-streaming-slide.png\" alt=\"Slide providing an overview of why to use Structured Streaming\" width=\"680\" height=\"378\" /></a>\n\n<strong>Update: You can <a href=\"https://www.youtube.com/watch?v=r0hyjmLMMOc\">watch the full presentation here.</a></strong>\n<h2>Enterprise Track</h2>\n<h3>Paddling Up the Stream</h3>\nWhen building a real-time streaming application, how do you untangle the challenges around upgrading versions, migrating between languages and integrating with peripheral systems?\n\n<img class=\"aligncenter size-full wp-image-9530\" src=\"https://databricks.com/wp-content/uploads/2016/10/Miklos-Christine-Spark-Summit-EU-2016.jpg\" alt=\"Miklos Christine speaking at Spark Summit EU 2016\" width=\"680\" height=\"453\" />\n\nMiklos Christine, systems engineer at Databricks, today discussed the top 5 issues that he has seen customers run into and how they resolved them. In the first four issues, he showed how to fix common stacktraces seen in the wild such as type mismatches, \u201ccouldn\u2019t find leader offsets\u201d errors, \u201ctoDF not member of RDD\u201d and \u201ctask not serializable.\u201d\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/10/Miklos-Spark-Summit-EU-2016-slide-issue-2.png\"><img class=\"aligncenter size-full wp-image-9531\" src=\"https://databricks.com/wp-content/uploads/2016/10/Miklos-Spark-Summit-EU-2016-slide-issue-2.png\" alt=\"Issue 2: Couldn't find leader offsets\" width=\"680\" height=\"387\" /></a>\n\nIn the fifth issue, Miklos covered how to efficiently push JSON records to Kinesis or Kafka.\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/10/Miklos-Spark-Summit-EU-2016-slide-issue-5.png\"><img class=\"aligncenter size-full wp-image-9532\" src=\"https://databricks.com/wp-content/uploads/2016/10/Miklos-Spark-Summit-EU-2016-slide-issue-5.png\" alt=\"Issue 5: Push JSON Records\" width=\"680\" height=\"378\" /></a>\n\nIf you\u2019ve been using Spark Streaming, this session was an exemplary way to learn to avoid common pitfalls that developers can run into.\n\n<strong>Update: You can <a href=\"https://www.youtube.com/watch?v=2_USi55SnmQ\">watch the full presentation here.</a></strong>\n<h2>What\u2019s Next</h2>\nAlso, presentation slides and recordings from this event will be available on the <a href=\"https://spark-summit.org/eu-2016/\" target=\"_blank\">Spark Summit website</a> by November 4th."}
{"status": "publish", "description": null, "creator": "Wayne Chan", "link": "https://databricks.com/blog/2016/11/01/demand-webinar-faq-databricks-data-engineers.html", "authors": null, "id": 9546, "categories": ["Company Blog", "Events", "Product"], "dates": {"publishedOn": "2016-11-01", "tz": "UTC", "createdOn": "2016-11-01"}, "title": "On-Demand Webinar and FAQ: Databricks for Data Engineers", "slug": "demand-webinar-faq-databricks-data-engineers", "content": "Last week, we held a live webinar\u2014<a href=\"http://go.databricks.com/databricks-for-data-engineers\" target=\"_blank\">Databricks for Data Engineers</a>\u2014to provide an overview of the data engineering role, common challenges data engineers face while building ETL pipelines, and how Databricks can help data engineers easily build production-quality data pipelines with Apache Spark. \n\nPrakash Chockalingam, product manager at Databricks, also gave a live demonstration of Databricks and features that data engineers would benefit from such as:\n\n<ul>\n<li>Advanced cluster management functionalities that suit any workload requirements.</li>\n<li>The ability to interactively build an ETL pipeline via an integrated workspace.</li>\n<li>Simplified troubleshooting of jobs with monitoring alerts.</li>\n<li>Job scheduling with helpful features like alerting, custom retry policies, and parallel runs.</li>\n<li>Notebook workflows which allow you to build multi-stage production Spark pipelines directly from Databricks notebooks.</li>\n</ul>\n\n\nThe webinar is now <a href=\"http://go.databricks.com/databricks-for-data-engineers\" target=\"_blank\">accessible on-demand</a>, and the slides used in the webinar are also downloadable as attachments to the webinar. \n\n\nWe have also answered the common questions raised by webinar viewers below. If you have additional questions, check out the <a href=\"https://forums.databricks.com/\" target=\"_blank\">Databricks Forum</a> or the new <a href=\"http://docs.databricks.com\" target=\"_blank\">documentation resource</a>.\n\n\nIf you\u2019d like free access to Databricks, you can access the <a href=\"https://databricks.com/try\" target=\"_blank\">free trial here</a>.\n\n<h3>Common webinar questions and answers</h3>\n\n<em>Click on the question to see answer</em>\n\n<a href=\"https://forums.databricks.com/questions/10101/from-webinar-databricks-for-data-engineers-how-wou.html\" target=\"_blank\">How would you integrate an ETL pipeline in production with tools like Chef or Puppet, automatic testing tools for Continuous integration, and include other services?</a>\n\n<a href=\"https://forums.databricks.com/questions/10102/from-webinar-databricks-for-data-engineers-do-you.html\" target=\"_blank\">Do you have any recommendations on the best architecture for integrating IoT data into Databricks using Apache NiFi to S3?</a>\n\n<a href=\"https://forums.databricks.com/questions/10103/from-webinar-databricks-for-data-engineers-can-you.html\" target=\"_blank\">Can you please explain any one scenario where Spark with Yarn or Spark with Mesos can be a justified choice?</a>\n\n<a href=\"https://forums.databricks.com/questions/10104/from-webinar-databricks-for-data-engineers-can-you-1.html\" target=\"_blank\">Can you please clarify R as a component of Spark?</a>\n\n<a href=\"https://forums.databricks.com/questions/10105/from-webinar-databricks-for-data-engineers-does-yo.html\" target=\"_blank\">Does your analytic layer include Spotfire?</a>\n\n<a href=\"https://forums.databricks.com/questions/10106/from-webinar-databricks-for-data-engineers-can-you-2.html\" target=\"_blank\">Can you SSH into your EC2 instances?</a>\n\n<a href=\"https://forums.databricks.com/questions/10107/from-webinar-databricks-for-data-engineers-how-doe.html\" target=\"_blank\">How does Spark compare to Sqoop in transferring data from Oracle to HDFS?</a>\n\n<a href=\"https://forums.databricks.com/questions/10108/from-webinar-databricks-for-data-engineers-is-it-p.html\" target=\"_blank\">Is it possible to restart a job from the failed notebook?</a>\n\n<a href=\"https://forums.databricks.com/questions/10109/from-webinar-databricks-for-data-engineers-does-da.html\" target=\"_blank\">Does Databricks provides any APIs for notebook execution monitoring?</a>\n\n<a href=\"https://forums.databricks.com/questions/10110/from-webinar-databricks-for-data-engineers-is-spar.html\" target=\"_blank\">Is SparkSQL the only component used to build ETL pipelines?</a>\n\n<a href=\"https://forums.databricks.com/questions/10111/from-webinar-databricks-for-data-engineers-can-we.html\" target=\"_blank\">Can we implement Type 2 logic using Spark and do inserts and updates to target an RDBMS?</a>\n\n<a href=\"https://forums.databricks.com/questions/10112/from-webinar-databricks-for-data-engineers-whats-t.html\" target=\"_blank\">What\u2019s the main difference between Storm and Spark? Can data be processed in real time using Spark?</a>"}
{"status": "publish", "description": null, "creator": "Wayne Chan", "link": "https://databricks.com/blog/2016/11/03/how-homeaway-is-transforming-the-vacation-rental-industry-with-databricks.html", "authors": null, "id": 9581, "categories": ["Company Blog", "Customers", "Product"], "dates": {"publishedOn": "2016-11-03", "tz": "UTC", "createdOn": "2016-11-03"}, "title": "How HomeAway is Transforming the Vacation Rental Industry with Databricks", "slug": "how-homeaway-is-transforming-the-vacation-rental-industry-with-databricks", "content": "<img class=\"aligncenter size-full wp-image-9583\" src=\"https://databricks.com/wp-content/uploads/2016/11/homeaway_databricks.png\" alt=\"HomeAway and Databricks\" width=\"473\" height=\"47\" />\n\nWe are proud to announce that HomeAway, a subsidiary of Expedia and one of the world's leading online marketplaces for the vacation rental industry, has selected Databricks to simplify its big data needs and improve the company\u2019s ability to match a traveler with the right property.\n\nTravelers from around the world use HomeAway\u2019s online marketplace to search for vacation rentals. To facilitate a match between the traveler and vacation rental, HomeAway must show search results that are relevant to the traveler\u2019s specific interests.\n\nHomeAway analyzes a high volume of unstructured data \u2014 log events, text (in multiple languages), and images \u2014 to provide search results in order of relevance to the traveler. They also leverage contextual image classification to interprets contextual information within an image so they can map the image with the highest relevance to the search criteria. For example, when a traveler selects \u201cbeachfront\u201d as a filter, they are able to curate and prioritize photos that contain the images of the beach.\n\nThe challenge of search and content relevancy requires the ability to ETL a large variety of unstructured data quickly. In HomeAway\u2019s case, they needed to move data from their on-premises HDFS to AWS S3 for exploration and analysis. They also had the growing need to merge their data with Expedia\u2019s to create predictive models across all of Expedia\u2019s websites.\n\nInitially, they tried using open source Apache Spark, bundled in JARs, and executed via a <em>spark-submit</em> script and Zeppelin notebooks. But they quickly found that their lack of Spark expertise, pains of upgrading Hadoop, and the challenges of using R to calculate prediction-based document similarity tasks on samples of data on a single machine proved to be too time consuming and resource intensive.\n\nWith Expedia\u2019s strategic initiative to move completely into the cloud, HomeAway needed a platform that enabled them to access large volumes of data on S3, while providing an interactive and highly scalable environment that allows for rapid prototyping and question asking to uncover future machine learning and streaming use cases.\n\nWith Databricks simplifying their Spark infrastructure, HomeAway\u2019s data science team can now focus on delivering innovative new features that enhance the overall user experience. Looking into the future, HomeAway is also exploring more joint analytic opportunities with the rest of Expedia to deliver a unified user experience that takes advantage of all the data under their collective umbrella.\n\n<a href=\"http://go.databricks.com/case-studies/homeaway\" target=\"_blank\">Download this case study</a> to learn more about how HomeAway is using Databricks."}
{"status": "publish", "description": null, "creator": "bill", "link": "https://databricks.com/blog/2016/11/10/databricks-launches-comprehensive-guide-product-apache-spark.html", "authors": null, "id": 9611, "categories": ["Announcements", "Company Blog", "Product"], "dates": {"publishedOn": "2016-11-10", "tz": "UTC", "createdOn": "2016-11-10"}, "title": "Databricks Launches a Comprehensive Guide for Its Product and Apache Spark", "slug": "databricks-launches-comprehensive-guide-product-apache-spark", "content": "<a href=\"http://docs.databricks.com\"><img class=\"aligncenter size-full wp-image-9615\" style=\"border: 1px solid #c2c2c2;\" src=\"https://databricks.com/wp-content/uploads/2016/11/databricks-documentation-home-page.png\" alt=\"Screenshot of the Databricks product documentation homepage.\" width=\"708\" height=\"365\" /></a>\n\nWe are proud to announce the launch of a new online guide for Databricks and Apache Spark at <a href=\"http://docs.databricks.com\">docs.databricks.com</a>. Our goal is to create a definitive resource for Databricks users and the most comprehensive set of Apache Spark documentation on the web. As a result, we've dedicated a large portion of the guide to Spark tutorials and How-Tos, in addition to Databricks product documentation.\n\nThe content of the guide\u00a0falls into three broad categories:\n<ul>\n \t<li><a href=\"http://docs.databricks.com/user-guide/index.html\">How to use Databricks</a></li>\n \t<li><a href=\"http://docs.databricks.com/administration-guide/index.html\">How to administer Databricks</a></li>\n \t<li>How to learn and use Apache Spark, including references, example code, and training material not available anywhere else such as:\n<ul>\n \t<li><a href=\"http://docs.databricks.com/spark/latest/spark-sql/index.html#spark-sql-language-manual\">Detailed Spark SQL Language Manual</a></li>\n \t<li><a href=\"http://docs.databricks.com/spark/latest/gentle-introduction/index.html\">Introductory Spark Tutorials for Beginners</a></li>\n \t<li><a href=\"https://docs.databricks.com/spark/latest/training/index.html\">Spark Training Materials</a></li>\n \t<li><a href=\"http://docs.databricks.com/spark/latest/data-sources/index.html\">Working with a variety of Spark Data Sources</a></li>\n</ul>\n</li>\n</ul>\nIn this blog, I will provide an overview of this new resource and highlight a few key sections.\n<h2>Documentation on How to Use Databricks</h2>\nWe\u2019ve consolidated product documentation into simple tutorials that walk you through everything you need to write and run Spark code in Databricks. This includes <a href=\"https://docs.databricks.com/user-guide/clusters/index.html\">how to spin up a cluster</a>, <a href=\"https://docs.databricks.com/user-guide/notebooks/index.html\">analyze data in notebooks</a>, <a href=\"https://docs.databricks.com/user-guide/jobs.html\">run production jobs with notebooks or JARs</a>, <a href=\"https://docs.databricks.com/api/index.html\">Databricks APIs</a>, and much more. There are detailed walkthroughs of every aspect of the Databricks UI as well as introductory tutorial videos.\n\n<a href=\"https://docs.databricks.com/user-guide/getting-started.html\"><img class=\"aligncenter size-full wp-image-9616\" src=\"https://databricks.com/wp-content/uploads/2016/11/how-to-use-databricks-videos.png\" alt=\"How to use Databricks videos\" width=\"1029\" height=\"180\" /></a>\n\nIn particular, <a href=\"https://docs.databricks.com/user-guide/faq/index.html\">the FAQ and best practices section</a> has many tips to help new users get the most out of Databricks such as:\n<ul>\n \t<li><a href=\"https://docs.databricks.com/user-guide/faq/ide-plugin.html\">Using an IDE with Databricks</a></li>\n \t<li><a href=\"https://docs.databricks.com/user-guide/faq/tableau.html\">Integrating Databricks with Tableau</a></li>\n \t<li><a href=\"https://docs.databricks.com/user-guide/faq/xgboost.html\">Using XGBoost and Spark</a></li>\n</ul>\n<h2>Documentation on How to Administer Databricks</h2>\nThese documents walk through how to manage the configuration, administration, and other housekeeping aspects of a Databricks account. For example, account owners can learn how to change AWS credentials and view billing details, while account administrators can learn how to add new Databricks users, set up access control, and configure SAML 2.0 compatible identity service providers.\n\n<a href=\"https://docs.databricks.com/administration-guide/index.html\"><img class=\"aligncenter size-full wp-image-9614\" style=\"border: 1px solid #c2c2c2;\" src=\"https://databricks.com/wp-content/uploads/2016/11/databricks-documentation-administration-guide-page.png\" alt=\"Screenshot of the Databricks Administration Guide homepage\" width=\"715\" height=\"368\" /></a>\n<h2>Guide for Apache Spark Developers</h2>\nWhile improving the documentation for the Databricks product was essential, we also wanted to give back to the Apache Spark community. We have created Apache Spark\u2019s only <a href=\"https://docs.databricks.com/spark/latest/spark-sql/index.html#spark-sql-language-manual\">Spark SQL Language Manual</a>, simple examples for <a href=\"https://docs.databricks.com/spark/latest/spark-sql/udf-scala.html\">Apache Spark UDFs in Scala</a> and <a href=\"https://docs.databricks.com/spark/latest/spark-sql/udf-in-python.html\">Python</a>, <a href=\"https://docs.databricks.com/spark/latest/sparkr/index.html#sparkr-function-reference\">SparkR Function Reference</a>, as well as <a href=\"https://docs.databricks.com/spark/latest/structured-streaming/index.html\">Structured Streaming Examples</a>.\n\n<img class=\"aligncenter size-full wp-image-9613\" src=\"https://databricks.com/wp-content/uploads/2016/11/code-sample-copy.png\" alt=\"Screenshot of a code sample in the Databricks documentation featuring a one-click copy to clipboard function.\" width=\"1037\" height=\"323\" />\n\nThere are many\u00a0practical code examples throughout the guide and you can easily drop them into your environment to test them out with a simple \u201ccopy\u201d button.\n<h2>Accessing the Guide in Databricks</h2>\nTo make it easy for users to find and use the code they need, we have also revamped the in-product search to make it easy to use and reference the code examples that you need. For example, <a href=\"https://docs.databricks.com/spark/latest/spark-sql/udf-in-python.html\">finding an example for a Python Spark UDF</a> is just a couple of keystrokes away!\n\n<img class=\"aligncenter size-full wp-image-9612\" style=\"border: 1px solid #c2c2c2;\" src=\"https://databricks.com/wp-content/uploads/2016/11/accessing-the-guide-in-notebooks.gif\" alt=\"You can access the documentation directly from Databricks.\" width=\"868\" height=\"372\" />\n<h2>What\u2019s Next</h2>\nThis release has been a couple of months in the making and we are just getting started. We will be constantly adding new content to answer the most common questions as well as deep dives into more sophisticated use cases. Be sure to <a href=\"http://docs.databricks.com\">browse the docs</a>, bookmark it, and share with friends because this resource will only continue to grow!"}
{"status": "publish", "description": null, "creator": "Wayne Chan", "link": "https://databricks.com/blog/2016/11/09/new-webinar-evaluate-cloud-based-apache-spark-platforms.html", "authors": null, "id": 9619, "categories": ["Company Blog", "Events", "Product"], "dates": {"publishedOn": "2016-11-10", "tz": "UTC", "createdOn": "2016-11-10"}, "title": "New Webinar: How to Evaluate Cloud-based Apache Spark Platforms", "slug": "new-webinar-evaluate-cloud-based-apache-spark-platforms", "content": "<ul>\n \t<li><strong>Date &amp; Time:</strong> November 16th, 2016 at 10:00am PT / 1:00pm ET / 5:00pm UTC</li>\n \t<li><strong>Presenter:</strong> Nik Rouda - Senior Analyst, ESG</li>\n</ul>\n[btn href=\"http://go.databricks.com/spark-buyers-guide-webinar\"]Register Now[/btn]\n\n&nbsp;\n\nApache Spark has quickly become the de-facto big data engine in data-driven companies for its performance, flexibility, and ease of use. This rise in popularity is causing an increasing number of companies across industries to evaluate Spark for their business. Unfortunately, few companies have the domain expertise and resources to build their own Spark-based infrastructure \u2014 often times resulting in a mix of tools that are complex to stand up and time consuming to maintain.\n\nThere are a number of cloud-based platforms available that allow you to harness the power of Spark while reaping the advantages of the cloud. The challenge is understanding the key decision criteria in choosing a Spark platform and how to evaluate each vendor\u2019s platform against those requirements.\n\nThis webinar, featuring ESG senior analyst Nik Rouda, will help you navigate these options and provide practical best practices and tools to evaluate and compare the most popular cloud-based Spark solutions.\n\nThe webinar will provide you with the tools to:\n<ul>\n \t<li>Define the evaluation criteria and compare common options.</li>\n \t<li>Set up a successful proof of concept (PoC).</li>\n \t<li>Estimate the total cost of ownership (TCO) and assess the return on investment (ROI).</li>\n</ul>\n\nPlease <a href=\"http://go.databricks.com/spark-buyers-guide-webinar\">register for this webinar</a> today!"}
{"status": "publish", "description": null, "creator": "rxin", "link": "https://databricks.com/blog/2016/11/14/setting-new-world-record-apache-spark.html", "authors": null, "id": 9638, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2016-11-15", "tz": "UTC", "createdOn": "2016-11-15"}, "title": "$1.44 per terabyte: setting a new world record with Apache Spark", "slug": "setting-new-world-record-apache-spark", "content": "We are excited to share with you that a joint effort by Nanjing University,\u00a0Alibaba Group, and Databricks set a new world record in <a href=\"http://sortbenchmark.org/\" target=\"_blank\">CloudSort</a>, a well-established third-party benchmark. We together architected the most efficient way to sort 100 TB of data, using only $144.22 USD worth of cloud resources, 3X more cost-efficient than the previous world record. Through this benchmark, Databricks, along with our partners, demonstrated that the most efficient way to process data is to use Apache Spark in the cloud.\n\n<img src=\"https://databricks.com/wp-content/uploads/2016/11/CloudSort-Benchmark-Image.jpg\" alt=\"Databricks, in collaboration with the Alibaba Group and Nanjing University, set a new CloudSort benchmark of $1.44 per terabyte.\" width=\"652\" height=\"335\" class=\"aligncenter size-full wp-image-9651\" />\n\nThis adds to the 2014 GraySort record we won, and validates two major technological trends we believe in:\n<ol>\n \t<li>Open source software is the future of software evolution, and Spark continues to be the most efficient engine for data processing.</li>\n \t<li>Cloud computing is becoming the most cost-efficient and enabling architecture to deploy big data applications.</li>\n</ol>\nIn the remainder of the blog post, we will explain the significance of the two benchmarks (CloudSort and GraySort), our records, and how they inspired major improvements in Spark such as <a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\">Project Tungsten</a>.\n<h2>GraySort Benchmark</h2>\nIt is easy to take for granted the incredible amount of computing power available today. As a beacon, the Sort Benchmark is a constant reminder of the journey taken and work accomplished by pioneers in computer systems of how we got here.\n\nStarted in 1985 by Turing Award winner Jim Gray, the benchmark measures the advances in computer software and hardware systems, and it is one of the most challenging benchmarks for computer systems. Participants in the past typically built specialized software and even hardware (e.g. special network topology) to maximize their chances of winning.\n\nThe original benchmark, called Datamation Sort, in 1985 required competing systems to sort 100 MB of data as fast as possible, regardless of the hardware resources used. 100 MB fits easily in the smallest USB stick you can find today, but back in 1985, it was a big challenge. The 1985 winning entry took 980 seconds. By 2001, this number was reduced to 0.44 seconds! To make the benchmark challenging, Datamation Sort was deprecated and a new TeraSort benchmark was created in 1998, changing the data size to 1TB.\n\nIn 2009, the 100 TB GraySort benchmark was created in honor of Jim Gray. Yahoo won the 2013 record using a 2100-node Hadoop cluster, sorting 100 TB of data in 72 minutes.\n\nIn 2014, our team at <a href=\"https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html\" target=\"_blank\">Databricks entered the competition</a> using a distributed program built on top of Spark. Our system sorted 100 TB of data in 23 minutes, using only 207 machines on EC2. That is to say, Spark sorted the same data 3X faster using 10X fewer resources than the 2013 Hadoop entry. In addition to winning the benchmark, we also sorted 1 PB of data in 4 hours using a similar setup and achieved near linear scalability.\n\nThis 2014 win set a milestone: it was the first time a combination of open source software and cloud computing won the benchmark. In the past, only well-funded organizations could compete in the sort benchmark, because participants would need to acquire a sizable, costly cluster. But with the rise of the public cloud, anybody can leverage the elasticity and accomplish what was once only possible in a handful of companies.\n<h2>CloudSort Benchmark</h2>\nThe GraySort Benchmark measures the time it takes to sort 100 TB of data regardless of the hardware resources used. A well designed sorting system is relatively linearly scalable. Once an organization creates a linearly scalable system, the chance of winning then depends to a large degree on the amount of money the organization is willing to spend in acquiring hardware resources.\n\nTo mitigate this, the benchmark committee in 2014 created a new category called CloudSort. Unlike the GraySort which picks the winner based purely on time, CloudSort favors the entry with the lowest cost, measured by public cloud pricing. This benchmark effectively measures the efficiency, i.e. ratio of performance to cost, of the cloud architecture (combination of software stack, hardware stack, and tuning).\n\nThe spirit of the CloudSort is that it should be accessible enough that everyone can \u201cplay\u201d at all scales, and let the best system win. In its inception year, the research team from the University of California, San Diego built a specialized sorting program called TritonSort and sorted 100 TB of data on Amazon EC2 for only $451. The system was so efficient that no one was able to challenge it even a year later.\n\nThat finally changed this week when our joint effort beat it and set the new CloudSort record.\n<h2>A New Cloud Record</h2>\nDatabricks, together with Nanjing University and Alibaba, formed a team called NADSort to compete this year. NADSort ran on 394 ecs.n1.large nodes on the AliCloud, each equipped with an Intel Haswell E5-2680 v3 processor, 8 GB of memory, and 4x135 GB SSD Cloud Disk. The sorting program was based on Databricks\u2019 2014 GraySort record-setting entry and adapted for better efficiency. You can find the details of NADSort in <a href=\"http://sortbenchmark.org/NADSort2016.pdf\" target=\"_blank\">this technical report</a>.\n\nIn the previous record for CloudSort, it cost nearly five dollars ($4.51) per terabyte, to sort 100 terabytes of data. Our optimizations and the cloud have reduced the per terabyte cost by two-thirds, and our new record stands at $1.44 per terabyte!\n\nWhat made this cost efficiency improvement possible?\n<ul>\n \t<li><strong>Cost-effectiveness of cloud computing:</strong> Increased competition among major cloud providers has lowered the cost of resources, making deploying applications in the cloud economically feasible and scalable. In addition, cloud computing also enables use cases that were simply not possible before with on-demand resources and elasticity.</li>\n \t<li><strong>Efficiency of software:</strong> We have invested heavily in performance optimizations, with some of the most significant changes in the last two years. Innovations such as <a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\" target=\"_blank\">Project Tungsten</a>, <a href=\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\" target=\"_blank\">Catalyst</a>, and <a href=\"https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html\" target=\"_blank\">whole-stage code generation</a>, have benefited Apache Spark enormously, improving all aspects of the stack. In addition to the general improvements in Spark, our collaborators have also worked on improving shuffle specifically for the AliCloud environment.</li>\n \t<li><strong>Expertise in optimizing Spark and cloud-native architecture:</strong> As the creators of Spark, we continue to lead the way in execution optimization. In addition, we have developed deep expertise in operating and tuning cloud-native data architecture by operating tens of thousands of clusters at once for our customers and users. The expertise we developed in building the most efficient cloud architecture for data processing enabled us to push the boundary of cost-efficiency in addition to the innovations from providers.</li>\n</ul>\n<h2>Significance of Sorting in Distributed Data Processing</h2>\nYou might ask: what\u2019s the significance of sorting, and why would we sort so much data?\n\nAt the core of sorting is the shuffle operation, which moves data across all machines. Shuffle underpins almost all distributed data processing workloads. For example, a SQL query joining two disparate data sources uses shuffle to move tuples that should be joined together onto the same machine, and collaborative filtering algorithms such as ALS rely on shuffle to send user/product ratings and weights across the network.\n\nMost data pipelines start with a large amount of raw data, but as the pipeline progresses, the amount of data is reduced due to filtering out irrelevant data or more compact representation of intermediate data. A SQL query on 100 TB of raw input data most likely only shuffles a tiny fraction of the 100 TB across the network, and the query optimizer can be smart to reduce the amount of data scanned. This pattern is also reflected in the naming of MapReduce itself.\n\nWe evaluate Spark using a lot of different benchmarks, e.g. TPC-H, TPC-DS. The sort benchmark, however, is by far the most challenging due to\u00a0the way it stresses the execution path. Sorting 100 TB of input data requires shuffling 100 TB of data across the network. We can\u2019t hide behind clever tricks such as data skipping or query optimizations to reduce the scale.\n\nThe above attributes make sorting a metric we frequently refer to when we seek to measure and improve Spark.\n\nThe optimizations we did for our 2014 GraySort record have led to important improvements to Spark itself in the last 2 years: Netty-based network module to improve network throughput, off-heap memory management to remove GC, Project Tungsten to improve CPU efficiency. We have learned more with our 2016 CloudSort record, and we will apply those learnings to our development on Spark and the Databricks platform.\n\nThe achievements of two world records in two years leave us humbled, yet they validate the technology trends we\u2019ve invested in heavily: Spark and cloud computing. These achievements are also testament to the engineering prowess we posses. To leverage this expertise and get the best performance out of Spark in the cloud, <a href=\"https://databricks.com/try-databricks\" target=\"_blank\">sign up for a Databricks account to build your data platform</a>.\n\nLast but not least, we would like to thank Qian Wang, Rong Gu, Yihua Huang from Nanjing University, Wei Wu, Jun Song, Junluan Xia from Alibaba for pushing forward state-of-the-art with us. We also thank the benchmark committee members Chris Nyberg, Mehul Shah, and Naga Govindaraju for their support. It was certainly fun to work together and compete!"}
{"status": "publish", "description": null, "creator": "jules_damji", "link": "https://databricks.com/blog/2016/11/16/databricks-bi-weekly-apache-spark-digest-111616.html", "authors": null, "id": 9665, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2016-11-16", "tz": "UTC", "createdOn": "2016-11-16"}, "title": "Databricks Bi-Weekly Apache Spark Digest: 11/16/16", "slug": "databricks-bi-weekly-apache-spark-digest-111616", "content": "<h2>Spark Summit Talks and Apache Spark Roundup</h2>\n<ul>\n\t<li>Databricks and partners set a new world record for <a href=\"https://databricks.com/blog/2016/11/14/setting-new-world-record-apache-spark.html\" target=\"_blank\">CloudSort 2016 Benchmark using Apache Spark</a>, wrote Reynold Xin, chief architect.</li>\n\t<li>Databricks Chief Technologist Matei Zaharia delivered a keynote, \u201c<a href=\"https://spark-summit.org/eu-2016/events/simplifying-big-data-applications-with-apache-spark-20/\" target=\"_blank\">Simplifying Big Data Applications with Apache Spark 2.0</a>,\u201d at Spark Summit 2016 EU in Brussels, followed by a demo of continuous application by Databricks software engineer Greg Owen.</li>\n\t<li>Databricks CEO Ali Ghodsi shared his vision of \u201c<a href=\"https://spark-summit.org/eu-2016/events/democratizing-ai-with-apache-spark/\" target=\"_blank\">Democratizing AI with Apache Spark</a>\u201d in his keynote at Spark Summit 2016 EU in Brussels.</li>\n\t<li>Executive Chairman of Databricks Ion Stoica announced \u201c<a href=\"https://spark-summit.org/eu-2016/events/the-next-amplab-real-time-intelligent-and-secure-computing/\" target=\"_blank\">The Next AmpLAB: Real-time, Intelligent, and Secure Computing</a>,\u201d in his keynote at Spark Summit 2016 EU in Brussels.</li>\n\t<li>Sameer Agarwal, software engineer at Databricks, presented \u201c<a href=\"https://spark-summit.org/eu-2016/events/sparks-performance-the-past-present-and-future/\" target=\"_blank\">Apache Spark\u2019s Performance: Project Tungsten and Beyond</a>,\u201d at Spark Summit 2016 EU in Brussels.</li>\n\t<li>Herman Van Hovell, software engineer at Databricks, gave a \u201c<a href=\"https://spark-summit.org/eu-2016/events/a-deep-dive-into-the-catalyst-optimizer/\" target=\"_blank\">Deep Dive into the Catalyst Optimizer</a>\u201d talk and a <a href=\"https://spark-summit.org/eu-2016/events/a-deep-dive-into-the-catalyst-optimizer-hands-on-lab/\" target=\"_blank\">hands-on lab</a> at Spark Summit 2016 EU in Brussels. </li>\n\t<li>Echoing Ali Ghodsi\u2019s keynote above, Tim Hunter, Databricks software engineer, showed how to use Apache Spark with TensorFlow: \u201c<a href=\"https://spark-summit.org/eu-2016/events/tensorframes-deep-learning-with-tensorflow-on-apache-spark/\" target=\"_blank\">TensorFrames: Deep Learning with TensorFlow on Apache Spark,</a>\u201d at Spark Summit 2016 EU in Brussels.</li>\n\t<li>Databricks Solution Architect Mikos Christine shared challenges and pitfalls you can avoid with Spark Streaming in his talk \u201c<a href=\"https://spark-summit.org/eu-2016/events/paddling-up-the-stream/\" target=\"_blank\">Paddling up the Stream</a>,\u201d at Spark Summit 2016 EU in Brussels.</li>\n\t<li>Facebook\u2019s Big Compute Team software engineer Sital Kedia described how Apache Spark scales in production in his talk: \u201c<a href=\"https://spark-summit.org/eu-2016/events/apache-spark-at-scale-a-60-tb-production-use-case/\" target=\"_blank\">Apache Spark at Scale: A 60 TB+ Production Use Case</a>\u201d at Spark Summit 2016 EU in Brussels.</li>\n\t<li><a href=\"https://blog.acolyer.org/2016/11/04/scaling-spark-in-the-real-world-performance-and-usability/\" target=\"_blank\">Morning Paper</a> blogger Adrian Colyer commented on Michael Armbrust et. al. article \u201c<a href=\"https://cs.stanford.edu/~matei/papers/2015/vldb_spark.pdf\" target=\"_blank\">Scaling Spark in the Real World: Performance and Usability.</a>\u201d</li>\n\t<li>Matei Zaharia, Reynold Xin et.al. contributed to Communications of ACM: \u201c<a href=\"http://cacm.acm.org/magazines/2016/11/209116-apache-spark/fulltext\" target=\"_blank\">Apache Spark: A Unified Engine for Big Data Processing.</a>\u201d</li>\n\t<li>Tim Hunter, Databricks software engineer, participated on the panel \"<a href=\"https://scalaebythebay2016.sched.org/event/8auf?iframe=no\" target=\"_blank\">Modern Software Architectures and Data Pipelines</a>\" at <a href=\"https://scalaebythebay2016.sched.org/\" target=\"_blank\">Scala by the Bay</a>.\n</ul>\n\n<h2>Releases</h2>\n<ul>\n\t<li><a href=\"https://spark-packages.org/package/graphframes/graphframes\" target=\"_blank\">GraphFrames 0.3.0</a> released as a spark package. Find out more from <a href=\"http://graphframes.github.io/\" target=\"_blank\">graphframes.github.io</a>.</li>\n\t<li><a href=\"http://spark.apache.org/news/spark-1-6-3-released.html\" target=\"_blank\">Apache Spark 1.6.3 Released</a>. Try it on <a href=\"https://databricks.com/try-databricks\" target=\"_blank\">Databricks Community Edition</a>.</li>\n\t<li><a href=\"https://spark.apache.org/news/spark-2-0-2-released.html\" target=\"_blank\">Apache Spark 2.0.2 Released</a>. Kafka 0.10 support and runtime metrics are the two notable features in Structured Streaming in this release. Try it on <a href=\"https://databricks.com/try-databricks\" target=\"_blank\">Databricks Community Edition</a>.</li>\n\t<li> Databricks released <a href=\"https://github.com/databricks/spark-redshift/tree/v3.0.0-preview1\" target=\"_blank\">spark-redshift v3.0.0-preview1</a> spark package with usability improvements. Learn more about its improvements at <a href=\"https://github.com/databricks/spark-redshift/releases/tag/v3.0.0-preview1\" target=\"_blank\">Redshift Data Source for Apache Spark</a>.</li>\n</ul>\n\n<h2>What\u2019s Next</h2>\nTo stay abreast with what\u2019s happening with Apache Spark, follow us on Twitter <a href=\"https://twitter.com/databricks\" target=\"_blank\">@databricks</a> and visit <a href=\"http://sparkhub.databricks.com\" target=\"_blank\">SparkHub</a>."}
{"status": "publish", "description": null, "creator": "jakebellacera", "link": "https://databricks.com/blog/2016/11/16/oil-gas-asset-optimization-aws-kinesis-rds-databricks.html", "authors": null, "id": 9666, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2016-11-16", "tz": "UTC", "createdOn": "2016-11-16"}, "title": "Oil and Gas Asset Optimization with AWS Kinesis, RDS, and Databricks", "slug": "oil-gas-asset-optimization-aws-kinesis-rds-databricks", "content": "[dbce_cta href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/blogs/Databricks%20Asset%20Optimization%20Example.html\"]Try this notebook in Databricks[/dbce_cta]\n\nThe key to success is consistently making good decisions, and the key to making good decisions is having good information. This belief is the main impetus behind the explosive interest in Big Data. We all know intuitively that access to more data presents the potential to obtain better data and therefore better decisions, yet more data in-and-of itself does not necessarily result in better decisions. We must also sift through the data and discover the good information. Doing so effectively is especially important in capital intensive industries.\n\nThe oil and gas industry is an asset-intensive business with capital assets ranging from drilling rigs, offshore platforms and wells to pipelines, LNG terminals, and refineries (Figure 1). These assets are costly to design, build, operate, and maintain. Analysis of the financial statements of the five super-majors (BP, ConocoPhillips, ExxonMobil, Shell, Total) shows that plant, property and equipment on average accounts for 51% of total assets. Effectively managing these assets requires oil and gas industry to leverage advanced machine learning and analytics on extreme large volumes of data, in batch and real-time. Apache Spark is ideal for handling this type of workload and Databricks is the ideal platform for building Apache Spark solutions.\n\nIn this blog we will solve a typical problem in the oil and gas industry - asset optimization. We will demonstrate a solution with three components:\n\n<ul>\n    <li>AWS Kinesis to stream the real time data;</li>\n    <li>AWS RDS to store our historical data;</li>\n    <li>Databricks to process the data from RDS and Kinesis to determine the optimal asset levels.</li>\n</ul>\n\n<img class=\"aligncenter size-full wp-image-9672\" src=\"https://databricks.com/wp-content/uploads/2016/11/project-pipeline-overview.png\" alt=\"A visualization of the project we will be embarking on.\" width=\"602\" height=\"177\" />\n\n<h2>Background on asset optimization</h2>\n\n\u201cAsset\u201d refers to tangible goods used by the business to generate revenue - raw material, equipment, etc. Business operations consume assets (i.e., wearing down a piece of equipment), and must replenish them to continue revenue generation. The estimation of timing and quantity of the replenishment is the heart of asset optimization because errors are costly: revenue stops flowing if the business runs out of raw materials, while excess stockpiles incur holding costs. Ideally, asset optimization accurately determines the correct asset levels based on analytics of near real-time consumption data. The goal is to precisely estimate how much stock will be used in the time it takes for an order to arrive with pinpoint accuracy.\n\n<h2>Asset optimization example</h2>\n\nIn the capital intensive oil and gas industry, every single hour of inefficient asset operation or unscheduled downtime cost millions. In the current Internet-of-Things (IoT) Big Data era, asset optimization focuses on continuously monitoring key operating characteristics of assets and applying advanced machine learning to maximize asset performance and minimize unplanned outages. That is where Big Data and advance analytics come in. The remainder of the blog we will look at a power generation plant example, where we monitor asset meters in real-time and model key measurements to determine whether assets are functioning optimally.\n\nWe model this by fitting a distribution to the limited lead time data we have and then sampling from that distribution. Fitting the distribution is the slowest part as it must be done numerically using Markov chain Monte Carlo (MCMC), for our asset this requires a loop of 100,000 iterations which cannot be done in parallel. This whole process must be done for each material in the data set, depending on the plant this can be 3,000+. Each material can be analyzed independently and in parallel.\n\nLet\u2019s see how we can do this with AWS Kinesis, RDS, and Databricks together (Note: You can also <a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/blogs/Databricks%20Asset%20Optimization%20Example.html\" target=\"_blank\">see the sample code in a Databricks notebook</a>).\n\n<h2>Streaming sensor readings with AWS Kinesis</h2>\n\n<h3>Step 1: Import the proper Kinesis Libraries.</h3>\n\nThis example assumes a Spark 2.0.1 (Scala 2.11). In this particular notebook, make sure you have attached Maven dependencies spark-streaming-kinesis for same version of Spark as your cluster and corresponding kinesis-client library.\n\n<img src=\"https://databricks.com/wp-content/uploads/2016/11/search-packages-screenshot.png\" alt=\"Selecting the kinesis-client and spark-streaming-kinesis packages in Databricks.\" width=\"705\" height=\"204\" class=\"aligncenter size-full wp-image-9725\" />\n\n<h3>Step 2: Configure your Kinesis Stream.</h3>\n\n<pre><code class=\"scala\">// === Configuration to control the flow of the application ===\nval stopActiveContext = true     \n// \"true\"  = stop if any existing StreamingContext is running;              \n// \"false\" = dont stop, and let it run undisturbed, but your latest code may not be used\n\n// === Configurations for Spark Streaming ===\nval batchIntervalSeconds = 10 \nval eventsPerSecond = 1000    // For the dummy source\n\n// Verify that the attached Spark cluster is 1.4.0+\nrequire(sc.version.replace(\".\", \"\").toInt &gt;= 140)\n</code></pre>\n\n<h3>Step 3: Define the function that consumes the Stream.</h3>\n\nThis function consumes a dummy stream that we have created for the sake of demonstrating Kinesis. The data that we use latter is staged as JSON files.\n\n<pre><code class=\"scala\">import scala.util.Random\nimport org.apache.spark.streaming.receiver._\n\nclass DummySource(ratePerSec: Int) extends Receiver[String](StorageLevel.MEMORY_AND_DISK_2) {\n  def onStart() {\n    // Start the thread that receives data over a connection\n    new Thread(\"Dummy Source\") {\n      override def run() { receive() }\n    }.start()\n  }\n\n  def onStop() {\n   // There is nothing much to do as the thread calling receive()\n   // is designed to stop by itself isStopped() returns false\n  }\n\n  /** Create a socket connection and receive data until receiver is stopped */\n  private def receive() {\n    while(!isStopped()) {      \n      store(\"I am a dummy source \" + Random.nextInt(10))\n      Thread.sleep((1000.toDouble / ratePerSec).toInt)\n    }\n  }\n}\n</code></pre>\n\n<h2>Storing historical data in AWS RDS</h2>\n\nLet's connect to a relational database to look at our master data and choose the Power Plant we want to create our first model for. We will be using Redshift as our database but the steps are essentially the same for connecting to any database. For our simulation, Redshift is where master data regarding the assets is stored. In the real world, this data could be stored in any relational database.\n\n<h3>Step 1: Create a DataFrame from an entire Redshift table</h3>\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/11/create-dataframe-from-redshift-table.png\" target=\"_blank\"><img class=\"aligncenter size-full wp-image-9676\" src=\"https://databricks.com/wp-content/uploads/2016/11/create-dataframe-from-redshift-table.png\" alt=\"Screenshot from a Databricks notebook demonstrating how to create a Dataframe from an entire Redshift table.\" width=\"820\" height=\"176\" /></a>\n\n<h3>Step 2: Create a Temporary View</h3>\n\n<img class=\"alignleft size-full wp-image-9677\" src=\"https://databricks.com/wp-content/uploads/2016/11/create-temporary-view.png\" alt=\"Screenshot of code from a notebook showing how to create a temporary view from a DataFrame.\" width=\"544\" height=\"59\" />\n\n<h3>Step 3: Select and View list of Power Plants</h3>\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/11/sql-table-visualization-from-view.png\" target=\"_blank\"><img class=\"aligncenter size-full wp-image-9678\" src=\"https://databricks.com/wp-content/uploads/2016/11/sql-table-visualization-from-view.png\" alt=\"Table visualization from a DataFrame view.\" width=\"1000\" height=\"238\" /></a>\n\nWe can use ANSI SQL to explore our master data and decide what asset we would like to use for our initial analysis.\n\n<h2>Monitoring and anomaly detection with Databricks</h2>\n\n<h3>Step 1: Let's Load Our Data</h3>\n\nSource measurement data from staged JSON data. In the real world, this would be sourced directly from Kinesis or another streaming technology as I showed with the dummy example above.\n\nLoad staged data from JSON files:\n\n<pre><code class=\"scala\">mounts_list = [\n{'bucket':'databricks-corp-training/structured_streaming/devices', 'mount_folder':'/mnt/sdevices'}\n]\n\nfor mount_point in mounts_list:\n  bucket = mount_point['bucket']\n  mount_folder = mount_point['mount_folder']\n  try:\n    dbutils.fs.ls(mount_folder)\n    dbutils.fs.unmount(mount_folder)\n  except:\n    pass\n  finally: #If MOUNT_FOLDER does not exist\n    dbutils.fs.mount(\"s3a://\"+ ACCESSY_KEY_ID + \":\" + SECRET_ACCESS_KEY + \"@\" + bucket,mount_folder)\n</code></pre>\n\nDefine a schema for the JSON Device data so that Spark doesn't have to infer it:\n\n<pre><code class=\"scala\">import org.apache.spark.sql.types._\n//fetch the JSON device information uploaded into the Filestore\nval jsonFile = \"dbfs:/mnt/sdevices/\"\nval jsonSchema = new StructType()\n        .add(\"battery_level\", LongType)\n        .add(\"c02_level\", LongType)\n        .add(\"cca3\",StringType)\n        .add(\"cn\", StringType)\n        .add(\"device_id\", LongType)\n        .add(\"device_type\", StringType)\n        .add(\"signal\", LongType)\n        .add(\"ip\", StringType)\n        .add(\"temp\", LongType)\n        .add(\"timestamp\", TimestampType)\n</code></pre>\n\nRead the JSON files from the mounted directory using the specified schema. Providing the schema avoids Spark to infer Schema, hence making the read operation faster:\n\n<pre><code class=\"scala\">val devicesDF = spark\n                  .read\n                  .schema(jsonSchema)\n                  .json(jsonFile)\n</code></pre>\n\n<h3>Step 2: Let\u2019s Explore Our Data</h3>\n\n<pre><code class=\"scala\">display(devicesDF)\n</code></pre>\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/11/display-devices-dataframe-table.png\" target=\"_blank\"><img class=\"aligncenter size-full wp-image-9679\" src=\"https://databricks.com/wp-content/uploads/2016/11/display-devices-dataframe-table.png\" alt=\"Display the Devices DataFrame as a table.\" width=\"1000\" height=\"176\" /></a>\n\n<h3>Step 3: Visualize our Data</h3>\n\n<pre><code class=\"scala\">// import some SQL aggregate and windowing function\nimport org.apache.spark.sql.functions._\n\nval staticCountsDF = devicesDF\n    .select(\"device_type\", \"battery_level\")\n    .where (\"signal &lt;= 15\")\n    .groupBy($\"device_type\", $\"battery_level\")\n    .count()\n// Let's register the DataFrame as table 'static_device_counts'\nstaticCountsDF.createOrReplaceTempView(\"static_device_counts\")\n\ndisplay(staticCountsDF)\n</code></pre>\n\n<img class=\"aligncenter size-full wp-image-9680\" src=\"https://databricks.com/wp-content/uploads/2016/11/visualize-device-type-static-count.png\" alt=\"Visualize the static device counts view as a chart,\" width=\"526\" height=\"216\" />\n\n<h3>Step 4: Stream Processing</h3>\n\nRead the stream.\n\n<pre><code class=\"scala\">val streamingSignalsCountsDF = streamingDevicesDF\n    .select(\"device_type\", \"battery_level\")\n    .where (\"signal &lt;= 15\")\n    .groupBy($\"device_type\", $\"battery_level\")\n    .count()\n// Is this DF actually a streaming DF?\nstreamingSignalsCountsDF.isStreaming\n</code></pre>\n\n<h3>Step 5: Monitor the Stream in Real Time</h3>\n\n<pre><code class=\"scala\">display(streamingSignalsCountsDF)\n</code></pre>\n\n<img class=\"aligncenter size-full wp-image-9681\" src=\"https://databricks.com/wp-content/uploads/2016/11/real-time-stream-visualization.png\" alt=\"Displaying a stream as a chart in real-time.\" width=\"530\" height=\"232\" />\n\n<h3>Step 6: Model the Data and Optimize the Asset</h3>\n\nWe have staged some sensor data as a CSV. In the real world, you would read this off the stream as I have shown above. Lets create a temporary table we will use in our analysis.\n\n<pre><code class=\"scala\">sqlContext.read.format(\"csv\")\n  .option(\"header\", \"true\")\n  .option(\"delimiter\", \"\\t\")\n  .option(\"inferSchema\", \"true\")\n  .load(\"dbfs:/databricks-datasets/power-plant/data/\")\n  .createOrReplaceTempView(\"power_plant_sf\")\n</code></pre>\n\nThe next step is to prepare the data. Since all of this data is numeric and consistent this is a simple task for us today. We will need to convert the predictor features from columns to Feature Vectors using the <code>org.apache.spark.ml.feature.VectorAssembler</code>. The <code>VectorAssembler</code> will be the first step in building our ML pipeline.\n\n<pre><code class=\"scala\">import org.apache.spark.ml.feature.VectorAssembler\nval dataset = sqlContext.table(\"power_plant_sf\")\nval vectorizer =  new VectorAssembler()\n  .setInputCols(Array(\"AT\", \"V\", \"AP\", \"RH\"))\n  .setOutputCol(\"features\")\n</code></pre>\n\nThe linear correlation is not as strong between Exhaust Vacuum Speed and Power Output but there is some semblance of a pattern. Now let's model our data to predict what the power output will be given a set of sensor readings.\n\n<pre><code class=\"scala\">// First let's hold out 20% of our data for testing and leave 80% for training\nvar Array(split20, split80) = dataset.randomSplit(Array(0.20, 0.80), 1800009193L)\n\n// Let's cache these datasets for performance\nval testSet = split20.cache()\ntestSet.count()\nval trainingSet = split80.cache()\ntrainingSet.count()\n\n// ***** LINEAR REGRESSION MODEL ****\n\nimport org.apache.spark.ml.regression.LinearRegression\nimport org.apache.spark.ml.regression.LinearRegressionModel\nimport org.apache.spark.ml.Pipeline\n\n// Let's initialize our linear regression learner\nval lr = new LinearRegression()\n\n// Now we set the parameters for the method\nlr.setPredictionCol(\"Predicted_PE\")\n  .setLabelCol(\"PE\")\n  .setMaxIter(100)\n  .setRegParam(0.1)\n// We will use the new spark.ml pipeline API. If you have worked with scikit-learn this will be very familiar.\nval lrPipeline = new Pipeline()\nlrPipeline.setStages(Array(vectorizer, lr))\n// Let's first train on the entire dataset to see what we get\nval lrModel = lrPipeline.fit(trainingSet)\n\nval predictionsAndLabels = lrModel.transform(testSet)\n\ndisplay(predictionsAndLabels.select(\"AT\", \"V\", \"AP\", \"RH\", \"PE\", \"Predicted_PE\"))\n</code></pre>\n\n<a href=\"https://databricks.com/wp-content/uploads/2016/11/output-of-predictions-and-labels.png\" target=\"_blank\"><img class=\"aligncenter size-full wp-image-9682\" src=\"https://databricks.com/wp-content/uploads/2016/11/output-of-predictions-and-labels.png\" alt=\"Displaying predictions and labels in a table view.\" width=\"842\" height=\"203\" /></a>\n\nNow that we have real predictions we can use an evaluation metric such as Root Mean Squared Error to validate our regression model. The lower the Root Mean Squared Error, the better our model.\n\n<pre><code class=\"scala\">//Now let's compute some evaluation metrics against our test dataset\n\nimport org.apache.spark.mllib.evaluation.RegressionMetrics \n\nval metrics = new RegressionMetrics(predictionsAndLabels.select(\"Predicted_PE\", \"PE\").rdd.map(r =&gt; (r(0).asInstanceOf[Double], r(1).asInstanceOf[Double])))\n\nval rmse = metrics.rootMeanSquaredError\nval explainedVariance = metrics.explainedVariance\nval r2 = metrics.r2\n\n// First we calculate the residual error and divide it by the RMSE\npredictionsAndLabels.selectExpr(\"PE\", \"Predicted_PE\", \"PE - Predicted_PE Residual_Error\", s\"\"\" (PE - Predicted_PE) / $rmse Within_RSME\"\"\").createOrReplaceTempView(\"Power_Plant_RMSE_Evaluation\")\n</code></pre>\n\nNow we can display the RMSE as a Histogram. Clearly this shows that the RMSE is centered around 0 with the vast majority of the error within 2 RMSEs.\n\n<pre><code class=\"sql\">SELECT Within_RSME  from Power_Plant_RMSE_Evaluation\n</code></pre>\n\n<img class=\"aligncenter size-full wp-image-9683\" src=\"https://databricks.com/wp-content/uploads/2016/11/display-rmse-as-histogram.png\" alt=\"Histogram visualization of the RMSE.\" width=\"604\" height=\"286\" />\n\nAs you can see the Predictions are very close to the real data points. Now we can predict the optimal operating parameters for this plant and apply this model to other plants in real-time.\n\n<h2>What\u2019s Next</h2>\n\nThe example code we used in this blog is available as a <a href=\"http://cdn2.hubspot.net/hubfs/438089/notebooks/blogs/Databricks%20Asset%20Optimization%20Example.html\" target=\"_blank\">Databricks notebook</a>, you can try it with a <a href=\"https://databricks.com/try\" target=\"_blank\">free trial of Databricks</a>.\n\nThis just one of many examples of how Databricks can seamlessly work with other AWS components to deliver advanced solutions. To learn how Databricks helped an energy company analyze IoT data, <a href=\"http://go.databricks.com/case-studies/dnvgl\" target=\"_blank\">check out our case study with DNV GL</a>. If you want to get started with Databricks, <a href=\"http://databricks.com/try\" target=\"_blank\">sign-up for a free trial</a> or <a href=\"http://go.databricks.com/contact-databricks\" target=\"_blank\">contact us</a>."}
{"status": "publish", "description": null, "creator": "Wayne Chan", "link": "https://databricks.com/blog/2016/11/23/demand-webinar-faq-evaluate-cloud-based-apache-spark-platforms.html", "authors": null, "id": 9744, "categories": ["Company Blog", "Events", "Product"], "dates": {"publishedOn": "2016-11-23", "tz": "UTC", "createdOn": "2016-11-23"}, "title": "On-Demand Webinar and FAQ: How to Evaluate Cloud-based Apache Spark Platforms", "slug": "demand-webinar-faq-evaluate-cloud-based-apache-spark-platforms", "content": "Last week, we held a live webinar, <a href=\"http://go.databricks.com/spark-buyers-guide-webinar\" target=\"_blank\">How to Evaluate Cloud-based Apache Spark Platforms</a>, to help those who are currently evaluating various platforms understand the key decision criteria in choosing a Spark platform and how to evaluate each vendor\u2019s platform against those requirements.\n\nThis webinar, featuring ESG senior analyst Nik Rouda, provided practical best practices and tools to evaluate and compare the most popular cloud-based Spark solutions such as how to set up a successful proof of concept (PoC) and estimate the total cost of ownership (TCO) and return on investment (ROI) of each solution.\n\nThe webinar is now <a href=\"http://go.databricks.com/spark-buyers-guide-webinar\">accessible on-demand</a>, and the buyer\u2019s guide referenced in the webinar along with the slides are also downloadable as attachments to the webinar.\n\nWe have also answered the common questions raised by webinar viewers below. If you have additional questions, check out the <a href=\"https://forums.databricks.com/\" target=\"_blank\">Databricks Forum</a> or the new <a href=\"http://docs.databricks.com/\" target=\"_blank\">documentation resource</a>.\n\nIf you\u2019d like free access to Databricks, you can sign up for a <a href=\"https://databricks.com/try\" target=\"_blank\">free 14-day trial today</a>.\n\n<h2>Common webinar questions and answers</h2>\n\n<em>Click on the question to see answer</em>\n\n<a href=\"https://forums.databricks.com/questions/10279/from-webinar-how-to-evaluate-a-cloud-based-apache-3.html\" target=\"_blank\">What is the trend for adoption of Python vs. JVM centric (Java/Scala) as a means to plan for development team skills, preferences and hiring?</a>\n\n<a href=\"https://forums.databricks.com/questions/10276/from-webinar-how-to-evaluate-a-cloud-based-apache.html\" target=\"_blank\">Doesn't Spark need to integrate with a file management system, such as HDFS? So comparing Spark Vs. Hadoop seems strange, as Spark as I know it runs on HDFS.</a>\n\n<a href=\"https://forums.databricks.com/questions/10277/from-webinar-how-to-evaluate-a-cloud-based-apache-1.html\" target=\"_blank\">How do you reuse clusters and auto optimize cluster resources in Databricks?</a>\n\n<a href=\"https://forums.databricks.com/questions/10281/from-webinar-how-to-evaluate-a-cloud-based-apache-5.html\" target=\"_blank\">What do you offer to help manage streaming workflows?</a>\n\n<a href=\"https://forums.databricks.com/questions/10280/from-webinar-how-to-evaluate-a-cloud-based-apache-4.html\" target=\"_blank\">How well does Databricks work with MS SQL 2016 or Oracle 12c?</a>\n\n<a href=\"https://forums.databricks.com/questions/10278/from-webinar-how-to-evaluate-a-cloud-based-apache-2.html\" target=\"_blank\">Does Databricks provide AWS SQS integration? For example, if I run a job (notebook) can I send SQS messages to update my solution with job status?</a>"}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2016/12/08/integrating-apache-airflow-databricks-building-etl-pipelines-apache-spark.html", "authors": null, "id": 9779, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2016-12-08", "tz": "UTC", "createdOn": "2016-12-08"}, "title": "Integrating Apache Airflow and Databricks: Building ETL pipelines with Apache Spark", "slug": "integrating-apache-airflow-databricks-building-etl-pipelines-apache-spark", "content": "[sidenote]This is one of a series of blogs on integrating Databricks with commonly used software packages. See the \u201cWhat\u2019s Next\u201d section at the end to read others in the series, which includes how-tos for AWS Lambda, Kinesis, and more.[/sidenote]\n\n<hr />\n\n<h2>Apache Airflow Overview</h2>\n<a href=\"https://airflow.incubator.apache.org/\" target=\"_blank\">Airflow</a> is a platform to programmatically author, schedule, and monitor workflows. It can be used to author workflows as directed acyclic graphs (DAGs) of tasks. The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies.\n\nThere are two key concepts in Airflow: DAGs describe <em>how</em> to run a workflow, while Operators determine <em>what</em> actually gets done by defining a task in a workflow. Operators are usually (but not always) atomic, meaning they can stand on their own and don\u2019t need to share resources with any other operators.\n\nAirflow is a heterogenous workflow management system enabling gluing of multiple systems both in cloud and on-premise. In cases that Databricks is a component of the larger system, e.g., ETL or Machine Learning pipelines, Airflow can be used for scheduling and management.\nAirflow already works with some commonly used systems like S3, MySQL, or HTTP endpoints; one can also extend the base modules easily for other systems.\n\nThis blog assumes there is an instance of Airflow up and running already. See the \u201cReferences\u201d section for readings on how to do setup Airflow.\n<h2>How to use Airflow with Databricks</h2>\nThe <a href=\"https://databricks.com/blog/2016/03/30/announcing-new-databricks-apis-for-faster-production-apache-spark-application-deployment.html\">Databricks REST API</a> enables programmatic access to Databricks, (instead of going through the Web UI). It can automatically create and run jobs, productionalize a data flow, and much more. It will also allow us to integrate Airflow with Databricks through Airflow operators.\n\nAirflow provides operators for many common tasks, and you can use the <strong>BashOperator</strong> and <strong>Sensor</strong> operator to solve many typical ETL use cases, e.g. triggering a daily ETL job to post updates in AWS S3 or row records in a database.\n<h3>The BashOperator</h3>\nThe <strong>BashOperator</strong> executes a bash command. It can be used to integrate with Databricks via the <a href=\"https://docs.databricks.com/api/index.html\" target=\"_blank\">Databricks API</a> to start a preconfigured Spark job, for example:\n<pre>t0 = BashOperator(\n     task_id='dbjob',\n     depends_on_past=False,\n     bash_command='curl -X POST -u username:password https://.cloud.databricks.com/api/2.0/jobs/run-now -d \\'{\\\"job_id\\\":}\\'',\n     dag=dag)</pre>\nYou can test this operator by typing in:\n<pre>%airflow test tutorial dbjob 2016-10-01</pre>\nIn the above example the operator starts a job in Databricks, the JSON load is a key / value (job_id and the actual job number).\n<blockquote>Note: Instead of using <em>curl</em> with the <strong>BashOperator</strong>, you can also use the <strong>SimpleHTTPOperator</strong> to achieve the same results.</blockquote>\n<h3>The Sensor Operator</h3>\nThe <strong>Sensor</strong> operator keeps running until a criteria is met. Examples include: a certain file landing in a S3 bucket (S3KeySensor), or a HTTP GET request to an end-point (HttpSensor); it is important to set up the correct time interval between each retry, \u2018poke_interval\u2019. It is necessary to use a Sensor Operator with the \u2018hook\u2019 to the persistence layer to do a push notification in an ETL workflow.\n<h2>JSON File to Parquet Processing Example</h2>\nBelow is an example of setting up a pipeline to process JSON files and converting them to parquet on a daily basis using Databricks. Airflow is used to orchestrate this pipeline by detecting when daily files are ready for processing and setting \u201cS3 sensor\u201d for detecting the output of the daily job and sending a final email notification.\n\nSetup of the pipeline:\n\n<img class=\"aligncenter size-full wp-image-9780\" src=\"https://databricks.com/wp-content/uploads/2016/12/setup-of-pipeline-in-airflow.png\" alt=\"Screenshot showing the setup of the pipeline in Airflow\" width=\"834\" height=\"396\" />\n\nAs shown above this pipeline has five steps:\n<ol style=\"margin-bottom: .5em;\">\n \t<li>Input S3 Sensor (check_s3_for_file_s3) checks that input data do exist:</li>\n</ol>\n<pre>s3:///input-airflow/input-*</pre>\n<ol style=\"margin-bottom: .5em;\" start=\"2\">\n \t<li>Databricks REST API (dbjob), BashOperator to make REST API call to Databricks and dynamically passing the file input and output arguments. For the purposes of illustrating the point in this blog, we use the command below; for your workloads, there are many ways to maintain security if entering your S3 secret key in the Airflow Python configuration file is a security concern:</li>\n</ol>\n<pre>curl -X POST -u : \\\n     https://.cloud.databricks.com/api/2.0/jobs/run-now \\\n     -d'{\"job_id\":, \"notebook_params\":{\"inputPath\": \"s3a://:@/input/test.json\",\"outputPath\": \"s3a://:@/output/sample_parquet_data\"}}'</pre>\n<img class=\"aligncenter size-full wp-image-9781\" src=\"https://databricks.com/wp-content/uploads/2016/12/nightly-etl-job-in-databricks.png\" alt=\"The nightly-etl-job in Databricks\" width=\"1000\" height=\"413\" />\n\nAbove is the screen-shot of the job within Databricks that is getting called from Airflow. Read the <a href=\"https://docs.databricks.com/user-guide/jobs.html\">Databricks jobs documentation</a> to learn more.\n<ol style=\"margin-bottom: .5em;\" start=\"3\">\n \t<li>Databricks Action involves reading an input JSON file and converting it into parquet:</li>\n</ol>\n<pre>val inputPath = getArgument(\"inputPath\", \"test\")\nval testJsonData = sqlContext.read.json(inputPath)\nval outPath = getArgument(\"outputPath\", \"test\")\ntestJsonData.write.format(\"parquet\").mode(\"overwrite\").save(outPath)</pre>\n<ol style=\"margin-bottom: .5em;\" start=\"4\">\n \t<li>Output S3 Sensor (check_s3_output_for_file_s3) checks that output data do exist:</li>\n</ol>\n<pre>s3:///output-airflow/sample_parquet_data/_SUCCESS</pre>\n<ol start=\"5\">\n \t<li>Email Notification (email_notification), sends out an email to alert when the job is successful.</li>\n</ol>\nBelow is the Python configuration file for this Airflow pipeline:\n<pre>from airflow import DAG\nfrom airflow.operators import BashOperator, S3KeySensor, EmailOperator\nfrom datetime import datetime, timedelta\n\n\ntoday_date = datetime.today()\n\n\ndefault_args = {\n   'owner': 'airflow',\n   'depends_on_past': False,\n   'start_date': today_date,\n   'email': ['&lt;&gt;@databricks.com'],\n   'email_on_failure': False,\n   'email_on_retry': False,\n   'retries': 1,\n   'retry_delay': timedelta(minutes=5),\n}\n\n\ndag = DAG('tutorial', default_args=default_args, schedule_interval= '@once')\n\n\ninputsensor = S3KeySensor(\n   task_id='check_s3_for_file_in_s3',\n   bucket_key='input-airflow/input-*',\n   wildcard_match=True,\n   bucket_name='peyman-datapipeline',\n   s3_conn_id='S3Connection',\n   timeout=18*60*60,\n   poke_interval=120,\n   dag=dag)\n\n\ndbtask = BashOperator(\n    task_id='dbjob',\n    depends_on_past=False,\n    bash_command='curl -X POST -u : https://demo.cloud.databricks.com/api/2.0/jobs/run-now -d \\'{\\\"job_id\\\":, \\\"notebook_params\\\":{\\\"inputPath\\\": \\\":&lt;secrte-key@/input-airflow/input-airflow.json\\\",\\\"outputPath\\\": \\\":&lt;secrte-key@/output-airflow/sample_parquet_data\\\"}}\\'',\n    dag=dag)\n\n\noutputsensor = S3KeySensor(\n   task_id='check_s3_output_for_file_in_s3',\n   bucket_key='output-airflow/sample_parquet_data/_SUCCESS',\n   wildcard_match=False,\n   bucket_name='peyman-datapipeline',\n   s3_conn_id='S3Connection',\n   timeout=18*60*60,\n   poke_interval=120,\n   dag=dag)\n\n\nemailNotify = EmailOperator(\n   task_id='email_notification',\n   to = '',\n   subject = 'ETL Job Done',\n   html_content = 'Airflow ETL Job Done',\n   dag=dag)\n\n\ndbtask.set_upstream(inputsensor)\noutputsensor.set_upstream(dbtask)\nemailNotify.set_upstream(outputsensor)</pre>\n<h2>What\u2019s Next</h2>\nWe hope this simple example shows how you can use Airflow and Databricks to solve your data processing problems. To try Databricks, <a href=\"http://databricks.com/try\">sign-up for a free trial</a> or <a href=\"http://go.databricks.com/contact-databricks\">contact us</a>.\n\nRead other blogs in the series to learn how to integrate Databricks with your existing architecture:\n<ul>\n \t<li><a href=\"https://databricks.com/blog/2016/11/16/oil-gas-asset-optimization-aws-kinesis-rds-databricks.html\">Oil and Gas Asset Optimization with AWS Kinesis, RDS, and Databricks</a></li>\n \t<li><a href=\"https://databricks.com/blog/2016/10/11/using-aws-lambda-with-databricks-for-etl-automation-and-ml-model-serving.html\">Using AWS Lambda with Databricks for ETL Automation and ML Model Serving</a></li>\n</ul>\n<h2>References</h2>\n<ul>\n \t<li><a href=\"http://airflow.incubator.apache.org/\" target=\"_blank\">Airflow</a></li>\n \t<li><a href=\"http://pythonhosted.org/airflow/configuration.html?#connections\" target=\"_blank\">Airflow S3 connection</a></li>\n \t<li><a href=\"https://docs.databricks.com/api/latest/index.html\" target=\"_blank\">Databricks APIs</a></li>\n</ul>"}
{"status": "publish", "description": null, "creator": "jakebellacera", "link": "https://databricks.com/blog/2016/12/12/apache-spark-scala-library-development-with-databricks.html", "authors": null, "id": 9789, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2016-12-12", "tz": "UTC", "createdOn": "2016-12-12"}, "title": "Apache Spark Scala Library Development with Databricks", "slug": "apache-spark-scala-library-development-with-databricks", "content": "[dbce_cta href=\"http://go.databricks.com/hubfs/notebooks/blogs/Just%20enough%20sbt/Prototype%20Date%20Dimension.html\"]Try this notebook in Databricks[/dbce_cta]\n\nThe movie Toy Story was released in 1995 by Pixar as the first feature-length computer animated film. Even though the animators had professional workstations to work with, they started sketching out the story by hand. A practice that is still followed today for all of Pixar\u2019s films. Whenever you\u2019re developing an Apache Spark application, sometimes you need to sketch it out before you develop it fully and ship it to production.\n\nThis blog will explain how to quickly prototype a notebook that generates a Date Dimension for a data mart, and then engineer that code into a Scala library with unit tests and a release process.\n\n<img class=\"aligncenter size-full wp-image-9793\" src=\"https://databricks.com/wp-content/uploads/2016/12/prototype-date-dimension-notebook-storyboard.png\" alt=\"Visualization of the steps we will follow in this blog post.\" width=\"819\" height=\"206\" />\n<h2>Prerequisites</h2>\nBefore you attempt to replicate these instructions, you need to make sure that you have a few things installed on your personal workstation:\n<ul>\n \t<li>Git</li>\n \t<li>Java (version 8 for Spark 2.0)</li>\n \t<li>Maven</li>\n \t<li>Scala</li>\n \t<li>SBT</li>\n \t<li>IntelliJ</li>\n</ul>\n<h2>Prototype In Databricks Notebooks</h2>\nJust like Toy Story was sketched out on paper by hand, you can use Databricks notebooks to quickly prototype your own ideas. You can write a few lines of code, execute them immediately, visualize the result, and iterate. It only takes a few minutes to spin up a cluster to run your code, and you will be able to get your program into a functioning state quickly while testing against real datasets.\n\nUsing a Databricks Notebook, I have prototyped some code that will create a Date Dimension which I will use across several data marts. I use Spark to generate a DataFrame for 100 years worth of dates, and then inspect the schema and the contents of the DataFrame before I create a Spark SQL table from it, from which any cluster can then query or join to it. (<a href=\"http://go.databricks.com/hubfs/notebooks/blogs/Just%20enough%20sbt/Prototype%20Date%20Dimension.html\" target=\"_blank\">See the notebook here</a>)\n<h3>Create a Project in IDE</h3>\nOnce I have a working prototype that I\u2019m happy with, I want to generate it as a library so that I can release it to other teams for use in their own data marts. To do this, I will begin by creating a new Scala / SBT project in IntelliJ on my local workstation.\n\nIf you\u2019re not familiar with IntelliJ, you can follow these instructions:\n<ol style=\"margin-bottom: .5em;\">\n \t<li>Open up IntelliJ and select \u201cCreate New Project\u201d and select \u201cSBT\u201d for the Project.</li>\n \t<li>Set the Java SDK and Scala Versions to match your intended Apache Spark environment on Databricks.</li>\n \t<li>Enable \u201cauto-import\u201d to automatically import libraries as you add them to your build file.</li>\n</ol>\n<img class=\"aligncenter size-full wp-image-9792\" src=\"https://databricks.com/wp-content/uploads/2016/12/new-project-window-intellij.png\" alt=\"A screenshot of the New Project dialog in IntelliJ\" width=\"963\" height=\"529\" />\n<ol style=\"margin-bottom: .5em;\" start=\"4\">\n \t<li>To check the Apache Spark Environment on Databricks, spin up a cluster and view the \u201cEnvironment\u201d tab in the Spark UI:</li>\n</ol>\n<img class=\"aligncenter size-full wp-image-9794\" src=\"https://databricks.com/wp-content/uploads/2016/12/spark-ui-view-databricks.png\" alt=\"Screenshot of the Environment tab selected in the Spark UI view on Databricks \" width=\"731\" height=\"320\" />\n<ol style=\"margin-bottom: .5em;\" start=\"5\">\n \t<li>IntelliJ will create a new project structure for you and a <em>build.sbt</em> file. Go ahead and open the <em>build.sbt</em> file. It should resemble the following:</li>\n</ol>\n<img class=\"aligncenter size-full wp-image-9790\" src=\"https://databricks.com/wp-content/uploads/2016/12/build-sbt-file-intellij.png\" alt=\"Screenshot of the build.sbt file selected in IntelliJ\" width=\"966\" height=\"268\" />\n<ol style=\"margin-bottom: .5em;\" start=\"6\">\n \t<li>Next, add your organization to the build file and include the library dependency for Spark SQL. Whenever you include the library dependencies for Spark libraries, be sure to use <em>%provided%</em>. This will not include the dependency in your compiled JAR as it is assumed that Databricks will already have this library on the classpath for you when you attach it to a cluster.Below is an example of how your <em>build.sbt</em> file should now look:</li>\n</ol>\n<pre>name := \"datamart\"\n\nversion := \"1.0\"\n\nscalaVersion := \"2.11.8\"\n\norganization := \"com.databricks.blog\"\n\nlibraryDependencies ++= Seq(\n \"org.apache.spark\" % \"spark-sql_2.11\" % \"2.0.0\" % \"provided\")</pre>\nIf you need to know how to write the exact string for the libraryDependencies, you can view it from the SBT tab on the project\u2019s Maven Central page. Be sure that you match your Scala build version with the correct version of Spark.\n\nHere is an <a href=\"https://mvnrepository.com/artifact/org.apache.spark/spark-sql_2.11/2.0.0\" target=\"_blank\">example for Spark SQL 2.0 on Scala 2.11</a>.\n<h3>Restructure Prototype Code into Packages</h3>\nWhen I built my prototype, I wrote everything in a single notebook to quickly test things. However, before I package this as a library, I will want to modularize the code into separate classes and packages. I created a single class, <em>DateDimension</em> (<a href=\"https://github.com/pohlposition/datamart/blob/master/src/main/scala/com/databricks/blog/datamart/util/DateDimension.scala\" target=\"_blank\">source code</a>).\n<h3>Push to Git Repo</h3>\nNow is a good time to begin pushing my codebase to GitHub to begin tracking the changes. Before you do, you should first create a .gitignore file in the project root folder so that you can exclude user specific files. There are a number of files that IntelliJ will create for your project, which should not be pushed to a Git repo. You can tell git to ignore these files by creating a <strong><em>.gitignore</em></strong> file in the root of your project. Be sure to include the period at the beginning of the filename (<a href=\"https://github.com/pohlposition/datamart/blob/master/.gitignore\" target=\"_blank\">example .gitignore file</a>).\n<h3>Compile and Package Library</h3>\nAfter creating my <em>DateDimension</em> class in the source code, I am ready to compile and package my code into a JAR file. If you\u2019re not familiar with SBT, you can do so from the terminal by running the <code>sbt package</code> command from the project root. This will create a JAR file under the ./target folder:\n<pre>./target/scala-2.11/datamart_2.11-1.0.jar</pre>\nYou can now take this JAR file, create a Library in Databricks with it, attach it to a cluster, import the package into a notebook and start using it. These are a lot of manual steps, so in the next section, I will show you how to automate this.\n<h2>Automate Deployment to Databricks</h2>\nOnce you start developing your library in IntelliJ, you will likely want to iterate on the code, upload it to Databricks, and retest with your notebooks. Databricks has developed an <a href=\"https://databricks.com/blog/2015/06/05/making-databricks-cloud-better-for-developers-ide-integration.html\" target=\"_blank\">SBT plugin</a> to make this process more seamless. After you configure it, you can run a single command to compile, upload, attach the library to a named cluster, and restart that cluster. You can learn more about how to setup the plugin in the <a href=\"https://docs.databricks.com/user-guide/faq/ide-plugin.html\" target=\"_blank\">Databricks documentation</a>.\n<h3>Create Notebook and Link to GitHub</h3>\nAfter my library is attached to a cluster in Databricks, I created a new notebook to be used by a Job that I can schedule to load the date dimension (<a href=\"http://go.databricks.com/hubfs/notebooks/blogs/Just%20enough%20sbt/Load%20Date%20Dimension.html\" target=\"_blank\">see the notebook here</a>).\n<h4>Now, let\u2019s link this to our GitHub repo:</h4>\nAs a best practice, I treat the notebook code as an entry point into an Apache Spark application and write it to a location in my repository that is separate from the path of the source code that is packaged into a JAR. I prefer to write it to <code>./src/main/notebook</code>\n\nIt is easy to synchronize a notebook in Databricks with your repository using our built in GitHub integration. The full instruction on how to do so can be found in the <a href=\"https://docs.databricks.com/user-guide/notebooks/github-version-control.html\" target=\"_blank\">GitHub Databricks Version Control Documentation</a>.\n<h2>Apply Unit Tests</h2>\nIn order to assure the quality and maintainability of my library, I must create unit tests that can be run each time it is built. To do this, I leveraged the open source spark-testing-base SBT plugin to make testing my code easier. You can leverage the same plugin in your own code by following these instructions:\n<ol style=\"margin-bottom: .5em;\">\n \t<li>Add the following entry to libraryDependencies in your <em>build.sbt</em> file:\n\n<code>\"com.holdenkarau\" %% \"spark-testing-base\" % \"2.0.0_0.4.7\" % \"test\"</code></li>\n \t<li>Add the following line to your <em>build.sbt</em> file:\n\n<code>parallelExecution in Test := false</code>\n\nThe version you supply is dependant on the version of Spark you are building your library for.The <em>% \u201ctest\u201d</em> that is included at the end of your dependency tells SBT to only include this library on your test classpath and not to include it your final packaged JAR.Be advised that this plugin may implicitly pull in an older version of ScalaTest than you might expect.\n\n<strong>Full documentation on the spark-testing-base plugin can be found here:</strong>\n\n<a href=\"https://github.com/holdenk/spark-testing-base\" target=\"_blank\">https://github.com/holdenk/spark-testing-base</a></li>\n \t<li>Create a package structure and Scala Test class under /src/test that matches your <em>DateDimension</em> class (<a href=\"https://github.com/pohlposition/datamart/blob/master/src/test/scala/com/databricks/blog/datamart/util/DateDimensionTest.scala\" target=\"_blank\">source code here</a>):</li>\n</ol>\n<img class=\"aligncenter size-full wp-image-9791\" src=\"https://databricks.com/wp-content/uploads/2016/12/datedimensiontest-in-intellij-sidebar.png\" alt=\"DateDimensionTest file location in the IntelliJ sidebar\" width=\"356\" height=\"385\" />\n<ol style=\"margin-bottom: .5em;\" start=\"4\">\n \t<li>If you\u2019re not familiar with SBT, you can execute the tests from the terminal by running the <code>sbt test</code> command from the project root. You should see output similar to the following:</li>\n</ol>\n<pre>16/10/07 15:22:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n16/10/07 15:22:58 WARN SparkContext: Use an existing SparkContext, some configuration may not take effect.\n[info] DateDimensionTest:                                                       \n[info] - test DateDimension default does NOT generate duplicate date_value\n[info] ScalaCheck\n[info] Passed: Total 0, Failed 0, Errors 0, Passed 0\n[info] ScalaTest\n[info] Run completed in 5 seconds, 396 milliseconds.\n[info] Total number of tests run: 1\n[info] Suites: completed 1, aborted 0\n[info] Tests: succeeded 1, failed 0, canceled 0, ignored 0, pending 0\n[info] All tests passed.\n[info] Passed: Total 1, Failed 0, Errors 0, Passed 1\n[success] Total time: 9 s, completed Oct 7, 2016 3:23:03 PM</pre>\n<h2>Publish Library Release</h2>\nAt this point, my library is successfully compiling and passing tests. Publishing a library as a release will snapshot the functionality to a version. Although you are free to use any kind of versioning standard that you like, a best practice is to follow <a href=\"http://semver.org/\" target=\"_blank\">Semantic Versioning</a>. I used the <a href=\"https://github.com/sbt/sbt-release\" target=\"_blank\"><em>sbt-release</em></a> plugin to better manage the process of releasing the library. You can do so yourself by following these instructions:\n<ol>\n \t<li>Add the following line to <em>project/plugins.sbt</em>:<code>addSbtPlugin(\"com.github.gseitz\" % \"sbt-release\" % \"1.0.3\")</code></li>\n \t<li>Create a <em>version.sbt</em> file in the project root and add the following:<code>version := \"0.1.0-SNAPSHOT\"</code></li>\n \t<li>Remove the same version assignment from your <em>build.sbt</em> file.</li>\n</ol>\nBefore the <em>sbt-release</em> plugin will generate a release, it first requires a <em>build.sbt</em> file in order to publish to an artifact repository, by setting the <em>publishTo</em> variable.\n\nAn artifact repository is typically used by build tools to import dependencies when building projects. If other teams in your organization are going to depend on your library, you may want to leverage one. This allows them to specify which version of your library to use, or to just take the most recent stable version. This could be a repository internal to your network or a cloud hosted one, such as <a href=\"https://www.jfrog.com/artifactory/\" target=\"_blank\">artifactory</a>.\n\nSBT uses the <em>publish</em> action to upload your compiled JAR to an artifact repository. You can find the full documentation for <a href=\"http://www.scala-sbt.org/release/docs/Publishing.html\" target=\"_blank\">publishing with SBT at this link</a>.\n\nIf you don\u2019t have an artifact repository to publish to, you can workaround this by publishing to the Maven repository on your local machine by including the following in your <em>build.sbt</em> file:\n<pre>publishTo := Some(Resolver.file(\"file\",  new File(Path.userHome.absolutePath+\"/.m2/repository\")))</pre>\nThe <em>sbt-release</em> plugin performs a number of checks and requires that all changes are committed and pushed to your remote tracking branch in git before it will continue.\n\nTo cut a new release, run the <code>sbt release</code> command from the project root folder.\n\nThe <em>sbt-release</em> plugin will prompt to first specify the version of the release [0.1.0] and then to specify the next version to continue development on [0.1.1-SNAPSHOT]. You can either use the defaults by pressing enter or specify your own.\n\nThe plugin will then compile, package, run unit tests, update the version number, and ask to commit and push the changes to git. If successful, you should see the version in versions.sbt updated to reflect the new version, the branch tagged with the current version, and all commits pushed to the remote tracking branch.\n<h2>What's Next</h2>\nThis post described how to develop a simple Spark library in Scala to generate a Date Dimension. We started by prototyping in a Databricks notebook. The code was then copied to a local IntelliJ project and unit tests written against it. Several SBT plugins were leveraged to automate the deployment to Databricks, run unit tests, and publish a release of the library. A Databricks notebook was created to imports the library and be scheduled as a Job. The built-in GitHub integration was used to commit the notebook to the same Git repository that hosts our library source code. Hopefully this post will help you to create your own blockbuster feature from an initial prototype all the way to production.\n\nTo try this out in Databricks, <a href=\"https://databricks.com/try\" target=\"_blank\">sign-up for a free trial</a> or <a href=\"http://go.databricks.com/contact-databricks\" target=\"_blank\">contact us</a>."}
{"status": "publish", "description": null, "creator": "joseph", "link": "https://databricks.com/blog/2016/12/14/demand-webinar-faq-apache-spark-mllib-2-x-migrating-ml-workloads-dataframes.html", "authors": null, "id": 9817, "categories": ["Company Blog", "Events"], "dates": {"publishedOn": "2016-12-14", "tz": "UTC", "createdOn": "2016-12-14"}, "title": "On Demand Webinar and FAQ: Apache Spark MLlib 2.x: Migrating ML Workloads to DataFrames", "slug": "demand-webinar-faq-apache-spark-mllib-2-x-migrating-ml-workloads-dataframes", "content": "Last week, we held a live webinar, <a href=\"http://go.databricks.com/apache-spark-mllib-migrating-ml-workloads-to-dataframes-webinar\">Apache Spark MLlib 2.x: Migrating ML Workloads to DataFrames</a>, to demonstrate the ease with which you can migrate your MLlib RDD-based workloads to Spark 2.x MLlib DataFrame-based APIs, gaining all the benefits of simpler APIs, performance and persistence.\n\nMixing presentation and demonstration, we covered migrating workloads from RDDs to DataFrames, showed how ML persistence works across languages for saving and loading models, and shared the roadmap ahead.\n\nWith <a href=\"http://go.databricks.com/apache-spark-mllib-migrating-ml-workloads-to-dataframes-webinar\">the webinar now accessible on-demand</a>, you can view the webinar at will, download the presentation slides via the attachments tab as well as access the two notebooks that demonstrate how to migrate your ML workloads from RDDs to DataFrames.\n\nAlso, we answered many questions raised by webinar attendees below. If you have additional or related questions, check out the <a href=\"https://forums.databricks.com/\">Databricks Forum</a> or the new <a href=\"http://docs.databricks.com/\">documentation resource</a>.\n\n<h2>Common Webinar Questions and Answers</h2>\n\n<em>Click on the question to see the answer.</em>\n\n<ol>\n<li><a href=\"https://forums.databricks.com/questions/10436/from-webinar-apache-spark-mllib-2x-migrating-ml-wo-1.html\">Can you please explain the distinction between (a) mllib vs ml, and (b) DataFrames vs DataSets?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/10437/from-webinar-apache-spark-mllib-2x-migrating-ml-wo-2.html\">Most of the pipeline / dataframe based feature (tokenizer ) already available in 1.6 as well. What are the key / real new thing in MLlib 2.0?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/10440/from-webinar-apache-spark-mllib-2x-migrating-ml-wo-3.html#answer-10441\">What is the difference between old ML features vectors and new ML features vectors? What does the conversion do?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/10442/from-webinar-apache-spark-mllib-2x-migrating-ml-wo-4.html#answer-10443\">Do we need to consider data normalization in the new API? e.g., z-score?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/10444/from-webinar-apache-spark-mllib-2x-migrating-ml-wo-5.html#answer-10445\">Can we expect a k-Nearest-Neighbors implementation in Spark ML/MLlib any time soon?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/10446/from-webinar-apache-spark-mllib-2x-migrating-ml-wo-6.html#answer-10447\">Could you show a few examples to use parameter matrix during model validation?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/10448/from-webinar-apache-spark-mllib-2x-migrating-ml-wo-7.html#answer-10449\">Can I save model from mllib pyspark and load the model in scala?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/10451/from-webinar-apache-spark-mllib-2x-migrating-ml-wo-8.html#answer-10452\">Will the UC Berkeley / edX / Databricks classes be modified to reflect these spark.ml modifications?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/10454/from-webinar-apache-spark-mllib-2x-migrating-ml-wo-9.html#answer-10455\">So spark.mllib is RDD based and new spark.ml is data frame based?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/10456/from-webinar-apache-spark-mllib-2x-migrating-ml-wo-10.html#answer-10457\">What kind of ML algorithms will benefit from Tungsten/Catalyst (once ML is ported onto SparkSQL), just memory heavy ones or also communication-heavy ones? E.g. we see RandomForest being up to 100 times slower than local, sklearn counterparts.</a></li>\n<li><a href=\"https://forums.databricks.com/questions/10458/from-webinar-apache-spark-mllib-2x-migrating-ml-wo-11.html#answer-10459\">While this ML persistence is nice, but do you run into naming collision? i.e., variables/dataframe in current environment having same name with variables/df from loaded pipeline?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/10460/from-webinar-apache-spark-mllib-2x-migrating-ml-wo-12.html#answer-10461\">Can you share a quick example of sharing a pipeline among different languages?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/10462/from-webinar-apache-spark-mllib-2x-migrating-ml-wo-13.html#answer-10463\">Can I save model from mllib pyspark and load the model in scala?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/10476/from-webinar-apache-spark-mllib-2x-migrating-ml-wo-20.html#answer-10477\">Do the Databricks notebooks allow us to experiment with the streaming modeling algorithms? Did those change much from 1.6 to 2.x?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/10465/from-webinar-apache-spark-mllib-2x-migrating-ml-wo-15.html#answer-10466\">Is Random Forest package available for Spark R API?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/10467/from-webinar-apache-spark-mllib-2x-migrating-ml-wo-16.html#answer-10468\">What is upper limit of the length of the feature vector, say for k-means? or is there a limit?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/10469/from-webinar-apache-spark-mllib-2x-migrating-ml-wo-17.html#answer-10470\">Above demo source code is available to public?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/10472/from-webinar-apache-spark-mllib-2x-migrating-ml-wo-18.html#answer-10473\">Will udf transforming Datasets be optimized by Catalyst or Tungsten?</a></li>\n<li><a href=\"https://forums.databricks.com/answers/10475/view.html\">Is the Spark Dataset superseding Dataframes? Will ML run on Datasets?</a></li>\n</ol>\n\n<h2>Read More</h2>\n\n<ul>\n<li>Get the presentation slides: <a href=\"http://www.slideshare.net/databricks/apache-spark-mllib-2x-migrating-ml-workloads-to-dataframes\">Apache Spark MLlib 2.x: Migrating ML Workloads to DataFrames</a></li>\n<li>Get notebook-1: <a href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/1359427237536478/3601578643761083/latest.html\">Topic Modeling with Latent Dirichlet Allocation - Spark 1.6 with RDDs</a></li>\n<li>Get notebook-2: <a href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/1359427237536511/3601578643761083/latest.html\">Topic Modeling with Latent Dirichlet Allocation- Spark 2.0 with DataFrames</a></li>\n</ul>\n\nIf you\u2019d like free access to Databricks, you can sign up for a <a href=\"https://databricks.com/try\">free 14-day trial today</a>."}
{"status": "publish", "description": null, "creator": "wenchen@databricks.com", "link": "https://databricks.com/blog/2016/12/15/scalable-partition-handling-for-cloud-native-architecture-in-apache-spark-2-1.html", "authors": null, "id": 9842, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2016-12-15", "tz": "UTC", "createdOn": "2016-12-15"}, "title": "Scalable Partition Handling for Cloud-Native Architecture in Apache Spark 2.1", "slug": "scalable-partition-handling-for-cloud-native-architecture-in-apache-spark-2-1", "content": "<em>Spark Summit will be held in Boston on Feb 7-9, 2017. Check out the <a href=\"https://spark-summit.org/east-2017/schedule/\" target=\"_blank\">full agenda</a> and <a href=\"https://r.online-reg.com/Spark_Summit_East_2017/register\" target=\"_blank\">get your ticket</a> before it sells out</em>!\n\nApache Spark 2.1 is just around the corner: the community is going through voting process for the release candidates. This blog post discusses one of the most important features in the upcoming release: scalable partition handling.\n\nSpark SQL lets you query terabytes of data with a single job. Often times though, users only want to read a small subset of the total data, e.g. scanning the activity of users in San Francisco rather than the entire world. They do this by partitioning the data files of the table by commonly filtered fields such as date or country. Spark SQL then uses this partitioning information to \u201cprune\u201d or skip over files irrelevant to the user\u2019s queries. However, in previous Spark versions the first read to a table can be slow, if the number of partitions is very large, since Spark must first discover which partitions exist.\n\n<strong>In Spark 2.1, we drastically improve the initial latency of queries that touch a small fraction of table partitions.</strong> In some cases, queries that took tens of minutes on a fresh Spark cluster now execute in seconds. Our improvements cut down on table memory overheads, and make the SQL experience starting cold comparable to that on a \u201chot\u201d cluster with table metadata fully cached in memory.\n\n<strong>Spark 2.1 also unifies the partition management features of DataSource and Hive tables.</strong> This means both types of tables now support the same partition DDL operations, e.g. adding, deleting, and relocating specific partitions.\n\n<h2>Table Management in Spark</h2>\n\nTo better understand why the latency issue existed, let us first explain how table management worked in previous versions of Spark. In these versions, Spark supports two types of tables in the catalog:\n\n<ol>\n<li><a href=\"https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html\">DataSource tables</a> are the preferred format for tables created in Spark. This type of table can be defined on the fly by saving a DataFrame to the filesystem, e.g. <code>df.write.partitionBy(\"date\").saveAsTable(\"my_metrics\")</code>, or via a <a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/create-table.html\">CREATE TABLE statement</a>, e.g. <code>CREATE TABLE my_metrics USING parquet PARTITIONED BY date</code>.\n\nIn prior versions, Spark discovers DataSource table metadata from the filesystem and caches it in memory. This metadata includes the list of partitions and also file statistics within each partition. Once cached, table partitions could be pruned in memory very quickly to address incoming queries.</p></li>\n<li><p>For users coming from Apache Hive deployments, Spark SQL can also read catalog tables defined by Hive serdes. When possible, Spark transparently converts such Hive tables to DataSource format in order to take advantage of IO performance improvements in Spark SQL. Spark does this internally by reading the table and partition metadata from the Hive metastore and caching it in memory.</p></li>\n</ol>\n\n<p>While this strategy provides optimal performance once table metadata is cached in memory, it also has two downsides: First, the initial query over the table is blocked until Spark loads all the table partitions\u2019 metadata. For a large partitioned table, recursively scanning the filesystem to discover file metadata for the initial query can take many minutes, especially when data is stored in cloud storage such as S3. Second, all the metadata for a table needs to be materialized in-memory on the driver process and increases memory pressure.\n\nWe have seen this issue coming up a lot from our customers and other large-scale Spark users. While it is sometimes possible to avoid the initial query latency by reading files directly with other Spark APIs, we wanted table performance to scale without workarounds. For Spark 2.1, Databricks collaborated with VideoAmp to eliminate this overhead and unify management of DataSource and Hive format tables.\n\nVideoAmp has been using Spark SQL from the inception of its data platform, starting with Spark 1.1. As a demand side platform in the real-time digital advertising marketplace, they receive and warehouse billions of events per day. VideoAmp now has tables with tens of thousands of partitions.\n\nMichael Allman (VideoAmp) describes their involvement in this project:\n\n<blockquote>\n  Prior to Spark 2.1, fetching the metadata of our largest table took minutes and had to be redone every time a new partition was added. Shortly after the release of Spark 2.0, we began to prototype a new approach based on deferred partition metadata loading. At the same time, we approached the <a href=\"http://apache-spark-developers-list.1001551.n3.nabble.com/\">Spark developer community</a> to sound out our ideas. We submitted one of our prototypes as a pull request to the Spark source repo, and began our collaboration to bring it to a production level of reliability and performance.\n</blockquote>\n\n<h2>Performance Benchmark</h2>\n\nBefore diving into the technical details, first we showcase query latency improvements over one of our internal metrics tables, which has over 50,000 partitions (at Databricks, we believe in eating our own dog food). The table is partitioned by date and metric type, roughly as follows:\n\n<pre class=\"lang-sql\">CREATE TABLE metricData (\n    value BIGINT,\n    dimensions struct&lt;...&gt;,\n    date STRING,\n    metric STRING)\nUSING parquet\nOPTIONS (path 'dbfs:/mnt/path/to/data-root')\nPARTITIONED BY (date, metric)\n</pre>\n\nWe use a small Databricks cluster with 8 workers, 32 cores, and 250GB of memory. We run a simple aggregation query over a day, a week, and a month\u2019s worth of data, and evaluate the <em>time to first result</em> on a newly launched Spark cluster:\n\n<pre class=\"lang-sql\">SELECT metric, avg(value)\nFROM metricData\nWHERE date &gt;= \"2016-11-01\" AND date &lt;= \"2016-11-07\"  // ex. 1 week\nGROUP BY metric\n</pre>\n\nWhen scalable partition management is enabled in Spark 2.1, reading a day\u2019s worth of data takes a little over 10 seconds. This time scales linearly with the number of partitions touched in the query. To understand what contributes to this time, we break it down between time spent during query planning and Spark job execution:\n\n<img src=\"https://databricks.com/wp-content/uploads/2016/12/spark-2-1-scalable-partition-management-comparison.png\" alt=\"Diagram showing performance improvements with scalable partition management in spark 2.1.\" width=\"584\" height=\"675\" class=\"aligncenter size-full wp-image-9845\" />\n\nWe see that query planning time (red bar) grows proportionally with the execution time (blue bar), since it is scanning more data. If the query is run again, the planning time becomes negligible (too small to graph) thanks to metadata caching.\n\nHowever with the improvements flag disabled, we see that there is a large constant factor of nearly 200 seconds for the first query against the table, regardless of the query selectivity.\n\nWe expect these improvements to be most visible in cloud-native Spark deployments, which typically experience slower file-system metadata performance, and require frequent re-caching of metadata due to the use of short-lived Spark clusters. However even HDFS users will see benefits for very large partitioned tables.\n\n<h3>VideoAmp Production Benchmarks</h3>\n\nWe also show that these improvements substantially impact production queries in workloads used by VideoAmp. They run complex multi-stage queries with dozens of columns, multiple aggregations and unions over regularly updated tables tens of thousands of partitions in size.\n\nVideoAmp measured the fraction of time spent in the planner for several of their day-to-day queries, comparing performance between Spark 2.0 and 2.1. They found significant\u2014sometimes dramatic\u2014improvements across the board:\n\n<img src=\"https://databricks.com/wp-content/uploads/2016/12/videoamp-production-queries-planning-time.png\" alt=\"Diagram showing time spent planning production queries between spark 2.0 and spark 2.1\" width=\"600\" height=\"371\" class=\"aligncenter size-full wp-image-9846\" />\n\n<h2>Implementation</h2>\n\nThese benefits were enabled by two significant changes to Spark SQL internals.\n\n<ol>\n<li>Spark now persists table partition metadata in the system catalog (a.k.a. Hive metastore) for both Hive and DataSource tables. With the new <a href=\"https://github.com/apache/spark/blob/branch-2.1/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala\">PruneFileSourcePartitions</a> rule, the Catalyst optimizer uses the catalog to prune partitions during <a href=\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\">logical planning</a>, before metadata is ever read from the filesystem. This avoids needing to locate files from partitions that are not used.</p></li>\n<li><p>File statistics can now be cached incrementally and partially during planning, instead of all upfront. Spark needs to know the size of files in order to divide them among read tasks during <a href=\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\">physical planning</a>. Rather than eagerly cache all table files statistics in memory, tables now share a <a href=\"https://github.com/apache/spark/blob/56a503df5ccbb233ad6569e22002cc989e676337/sql/core/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala#L281\">fixed-size cache of (configurable) 250MB</a> to robustly speed up repeated queries without risking out of memory errors.</p></li>\n</ol>\n\n<p>In combination, these changes mean queries are faster from a cold start of Spark. Thanks to the incremental file statistics cache, there is also close to no performance penalty for repeated queries compared to the old partition management strategy.\n\n<h2>Newly supported partition DDLs</h2>\n\nAnother benefit of these changes is DataSource table support for several DDL commands previously only available for Hive tables. These DDLs allow the location of files for a partition to be changed from the default layout, e.g. partition <code>(date='2016-11-01', metric='m1')</code> can be placed at arbitrary filesystem locations, not only <code>/date=2016-11-01/metric=m1</code>.\n\n<pre class=\"lang-sql\">ALTER TABLE table_name ADD [IF NOT EXISTS]\n    (PARTITION part_spec [LOCATION path], ...)\nALTER TABLE table_name DROP [IF EXISTS] (PARTITION part_spec, ...)\nALTER TABLE table_name PARTITION part_spec SET LOCATION path\nSHOW PARTITIONS [db_name.]table_name [PARTITION part_spec]\n</pre>\n\nOf course, you can still use native DataFrame APIs such as <code>df.write.insertInto</code> and <code>df.write.saveAsTable</code> to append to partitioned tables. For more information about supported DDLs in Databricks, see the <a href=\"https://docs.databricks.com/spark/latest/spark-sql/index.html\">language manual</a>.\n\n<h2>Migration Tips</h2>\n\nWhile new DataSource tables created by Spark 2.1 will use the new scalable partition management strategy by default, for backwards compatibility this is not the case for existing tables. To take advantage of these improvements for existing DataSource tables, you can use the MSCK command to convert an existing table using the old partition management strategy to using the new approach:\n\n<pre>MSCK REPAIR TABLE table_name;</pre>\n\nYou will also need to issue <code>MSCK REPAIR TABLE</code> when creating a new table over existing files.\n\nNote that this can potentially be a backwards-incompatible change, since direct writes to the table\u2019s underlying files will no longer be reflected in the table until the catalog is also updated. This syncing is done automatically by Spark 2.1, but writes from older Spark versions, external systems, or outside of the Spark\u2019s table APIs will require <code>MSCK REPAIR TABLE</code> to be called again.\n\nHow can you tell if catalog partition management is enabled for a table? Issue a <code>DESCRIBE FORMATTED table_name</code> command, and check for <code>PartitionProvider: Catalog</code> in the output:\n\n<pre>scala&gt; sql(\"describe formatted test_table\")\n.filter(\"col_name like '%Partition Provider%'\").show\n+-------------------+---------+-------+\n|           col_name|data_type|comment|\n+-------------------+---------+-------+\n|Partition Provider:|  Catalog|       |\n+-------------------+---------+-------+\n</pre>\n\n<h2>Conclusion</h2>\n\nAll of the work described in this blog post is included in Apache Spark\u2019s 2.1 release. The JIRA ticket covering the major items can be found at <a href=\"https://issues.apache.org/jira/browse/SPARK-17861\">SPARK-17861</a>.\n\nWe are excited about these changes, and look forward to building upon them for further performance improvements. To try out some queries using Spark SQL for free, <a href=\"https://databricks.com/try-databricks\">sign up for an account</a> on Databricks Community Edition."}
{"status": "publish", "description": null, "creator": "joseph", "link": "https://databricks.com/blog/2016/12/28/10-things-i-wish-i-knew-before-using-apache-sparkr.html", "authors": null, "id": 9599, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2016-12-28", "tz": "UTC", "createdOn": "2016-12-28"}, "title": "10 Things I Wish I Knew Before Using Apache SparkR", "slug": "10-things-i-wish-i-knew-before-using-apache-sparkr", "content": "<em>This is a guest post from <a href=\"https://www.linkedin.com/in/ndewar\" target=\"_blank\">Neil Dewar</a>, a senior data science manager at a global asset management firm. In this blog, Neil shares lessons learned using R and Apache Spark.</em>\n\nIf you know how to use R and are learning Apache Spark, then this blog post and notebook contain key tips to smooth out your road ahead.\n\n<strong><a href=\"http://dbricks.co/2iF3JEO\" target=\"_blank\">Try this notebook in Databricks</a></strong>\n\nAs the notebook explains:\n<blockquote>I\u2019m an R user. Certainly not an object oriented programmer, and no experience of distributed computing. As my team starts to explore options for distributed processing of big data, I took the task to evaluate SparkR.\n\nAfter much exploration, I eventually figured out that what's missing is the contextual advice for people who already know R, to help them understand what's different about SparkR and how to adapt your thinking to make best use of it.  That's the purpose of this blog and notebook -- to document the \"aha!\" moments in a journey from R to SparkR. I hope my hard-earned discovery helps you get there faster!</blockquote>\n\nThe notebook lists 10 key pieces of knowledge, with code snippets and explanations, tailored for R users.  Here is the list in brief; check out the <a href=\"http://dbricks.co/2iF3JEO\" target=\"_blank\">notebook</a> to learn more!\n\n[btn href=\"http://dbricks.co/2iF3JEO\" target=\"_blank\"]View this Notebook[/btn]\n\n<br class=\"\">\n<ol>\n\t<li><strong>Apache Spark Building Blocks</strong>. A high-level overview of Spark describes what is available for the R user.</li>\n\t<li><strong>SparkContext, SQLContext, and SparkSession</strong>. In Spark 1.x, SparkContext and SQLContext let you access Spark. In Spark 2.x, SparkSession becomes the primary method.</li>\n\t<li><strong>A DataFrame or a data.frame?</strong> Spark\u2019s distributed DataFrame is different from R\u2019s local data.frame. Knowing the differences lets you avoid simple mistakes.</li>\n\t<li><strong>Distributed Processing 101</strong>. Understanding the mechanics of Big Data processing helps you write efficient code\u2014and not blow up your cluster\u2019s master node.</li>\n\t<li><strong>Function Masking</strong>. Like all R libraries, SparkR masks some functions.</li>\n\t<li><strong>Specifying Rows</strong>. With Big Data and Spark, you generally select rows in DataFrames differently than in local R data.frames.</li>\n\t<li><strong>Sampling</strong>. Sample data in the right way, and use it as a tool for converting between big and small data.</li>\n\t<li><strong>Machine Learning</strong>. SparkR has a growing library of distributed ML algorithms.</li>\n\t<li><strong>Visualization</strong>.It can be hard to visualize big data, but there are tricks and tools which help.</li>\n\t<li><strong>Understanding Error Messages</strong>. For R users, Spark error messages can be daunting.  Knowing how to parse them helps you find the relevant parts.</li>\n</ol>"}
{"status": "publish", "description": null, "creator": "joseph", "link": "https://databricks.com/blog/2016/12/21/deep-learning-on-databricks.html", "authors": null, "id": 9864, "categories": ["Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2016-12-21", "tz": "UTC", "createdOn": "2016-12-21"}, "title": "Deep Learning on Databricks", "slug": "deep-learning-on-databricks", "content": "We are excited to announce the general availability of Graphic Processing Unit (GPU) and deep learning support on Databricks! This blog post will help users get started via a tutorial with helpful tips and resources, aimed at data scientists and engineers who need to run deep learning applications at scale.\n\n<h2>What\u2019s new?</h2>\n\nDatabricks now offers a simple way to leverage GPUs to power image processing, text analysis, and other machine learning tasks. Users can create GPU-enabled clusters with <a href=\"https://aws.amazon.com/ec2/instance-types/p2/\">EC2 P2</a> instance types. Databricks includes pre-installed NVIDIA drivers and libraries, Apache Spark deployments configured for GPUs, and material for getting started with several popular deep learning libraries.\n\nOur previous blog post on <a href=\"http://databricks.com/blog/2016/10/27/gpu-acceleration-in-databricks.html\">GPU Acceleration in Databricks</a> provides more technical details on our GPU offering. It also contains example benchmarks showing how GPUs can be very cost-effective for machine learning, especially for the expensive computations required for deep learning.\n\nThis blog post provides a tutorial on how to get started using GPUs and deep learning in Databricks. We will walk through an example task of integrating Spark with TensorFlow in which we will deploy a Deep Neural Network to identify objects and animals in images.\n\n<h2>Using deep learning with Apache Spark</h2>\n\nBefore diving into our tutorial, let\u2019s discuss how users can take advantage of GPU instances and apply deep learning libraries on Databricks. Common workflows include:\n\n<ul>\n<li><strong>Deploying models at scale:</strong> Deploy trained models to make predictions on data stored in Spark RDDs or DataFrames. In this blog post, we will deploy the famous <em><a href=\"https://arxiv.org/pdf/1512.00567v3.pdf\">Inception</a></em> model for computer vision.</li>\n<li><strong>Distributed model training:</strong> Use deep learning libraries like TensorFlow on each worker to test different model hyperparameters, speeding up this time-intensive task with Spark. Check out the example in our previous blog post on <a href=\"http://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html\">Deep Learning with Apache Spark and TensorFlow</a>.</li>\n<li><strong>GPU workstation:</strong> If your data fit onto a single machine, it can be cost-effective to create a Driver-only cluster (0 Workers) and use deep learning libraries on the GPU-powered driver.</li>\n</ul>\n\nNote that deep learning does not require GPUs. These deep learning libraries will all run on CPUs, especially if used with compute-optimized instance types.\n\n<h2>Tutorial: Deploying a deep learning model at scale</h2>\n\nWe give a brief walkthrough on how to deploy a pre-trained deep learning model on Apache Spark. For the full code, refer to the <a href=\"https://docs.databricks.com/applications/deep-learning/spark-integration.html\">example notebook</a> in the Databricks Guide.\n\n<h3>Understanding the task</h3>\n\nIn this example, we take an existing single-machine workflow and modify it to run on top of Apache Spark. We use the <a href=\"https://github.com/tensorflow/tensorflow/blob/6dc8deaed8d8bd9cc6d52a03474d0b82891c8b86/tensorflow/models/image/imagenet/classify_image.py\">\u201cSimple image classification with Inception\u201d example</a> from TensorFlow, which applies the Inception model to predict the contents of a set of images.\n\nFor example, given an image like this:\n\n<img class=\"aligncenter size-full wp-image-9868\" src=\"https://databricks.com/wp-content/uploads/2016/12/photo-of-scuba-divers.jpg\" alt=\"Photo of two scuba divers\" width=\"500\" height=\"375\" />\n\nThe Inception model will tell us the contents of the image, in this case:\n\n<pre><code>('scuba diver', 0.88708681),\n('electric ray, crampfish, numbfish, torpedo', 0.012277877),\n('sea snake', 0.005639134),\n('tiger shark, Galeocerdo cuvieri', 0.0051873429),\n('reel', 0.0044495272)\n</code></pre>\n\nEach of the lines above represents a \u201csynset,\u201d or a set of synonymous terms representing a concept. The weight given to each synset represents a confidence in how applicable the synset is to the image. In this case, \u201cscuba diver\u201d is pretty accurate!\n\nMaking predictions with Inception-v3 is expensive: each prediction requires about 4.8 billion operations (<a href=\"https://arxiv.org/pdf/1512.00567v3.pdf\">Szegedy et al., 2015</a>). Even with smaller datasets, it can be worthwhile to parallelize this computation. We will distribute these costly predictions using Spark.\n\n<h3>Creating a GPU-enabled cluster</h3>\n\nUsers can create a GPU-enabled cluster on Databricks almost like any other cluster. We will point out the key differences.\n\nSelect an \u201cApache Spark Version\u201d which is GPU-enabled. Selecting a GPU-enabled version makes sure that the NVIDIA CUDA and cuDNN libraries are pre-installed on your cluster. We avoid installing these libraries on non-GPU clusters since they take extra resources.\n\n<img class=\"aligncenter size-full wp-image-9866\" src=\"https://databricks.com/wp-content/uploads/2016/12/databricks-creating-gpu-accelerated-spark-cluster-example.png\" alt=\"Creating a GPU-accelerated Apache Spark 2.0 cluster in Databricks\" width=\"413\" height=\"325\" />\n\nBy selecting a GPU-enabled Spark version, you agree to the <a href=\"http://go.databricks.com/hubfs/docs/nvidia-license-via-compute-saas-provider-eula-11022016.pdf\">NVIDIA EULA</a>.\n\n<img class=\"aligncenter size-full wp-image-9867\" src=\"https://databricks.com/wp-content/uploads/2016/12/nvidia-eula-agreement-databricks.png\" alt=\"Screenshot showing that the user accepts the NVIDIA EULA when creating a GPU accelerated cluster\" width=\"455\" height=\"140\" />\n\nAt this date, GPU spot instances can be hard to acquire on EC2, so we recommend using on-demand instances for stability.\n\nSelect GPU instances types for your cluster Workers and Driver. Databricks currently supports all P2 instances types: p2.xlarge (1 GPU), p2.8xlarge (8 GPUs), and p2.16xlarge (16 GPUs). <em>(UPDATE: P2 Instances are available in three AWS regions: US East (N. Virginia), US West (Oregon), and EU (Ireland). Your Databricks deployment must reside in a supported region to create a GPU-enabled cluster.)</em> Currently, AWS default limits on P2 instance types are <a href=\"https://aws.amazon.com/ec2/faqs/#How_many_instances_can_I_run_in_Amazon_EC2\">1 instance only</a>, so you may need to <a href=\"http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html\">request a limit increase from AWS</a>.\n\n<blockquote>\n  <strong>Note:</strong> Our original blog post discussed G2 instance types, an older EC2 GPU-enabled type. We are releasing P2 instance support instead since P2 offer significantly more memory and GPU cores per dollar.\n</blockquote>\n\n<img class=\"aligncenter size-full wp-image-9869\" src=\"https://databricks.com/wp-content/uploads/2016/12/select-gpu-accelerated-nodes.png\" alt=\"Screenshot showing the various AWS instances Databricks offers\" width=\"396\" height=\"461\" />\n\nFinally, P2 instance types do not come with built-in SSD storage, so users must add storage.\n\n<img class=\"aligncenter size-full wp-image-9865\" src=\"https://databricks.com/wp-content/uploads/2016/12/add-ssd-to-databricks-cluster.png\" alt=\"Screenshot showing how to add SSD storage to your GPU-accelerated instance\" width=\"571\" height=\"62\" />\n\nYou can run this small example using a single p2.xlarge worker and a single EBS volume. For a larger dataset, using more workers would provide near-linear speedups from distributing the computation. More details on creating a cluster, as well as NVIDIA driver and library versions, are given in the <a href=\"https://docs.databricks.com/user-guide/clusters/gpu.html\">Databricks Guide on GPU Clusters</a>.\n\n<h3>Installing TensorFlow</h3>\n\nDatabricks provides Init Scripts for installing deep learning libraries on clusters. Because these libraries are often built from source and need custom configurations, we recommend installing these libraries via customizable scripts. You only need to create the Init Script once, and this creation can be done using any cluster (without GPUs).\n\nClone the TensorFlow Init Script notebook](https://docs.databricks.com/applications/deep-learning/tensorflow.html#install-tensorflow-using-an-init-script), and modify the cluster name in the script to match your GPU cluster (or the name of a GPU cluster to be created in the future):\n\n<pre><code class=\"python\"># The name of the cluster on which to install TensorFlow:\nclusterName = \"tensorflow-gpu\"\n</code></pre>\n\nRun the notebook to install the Init Script. The next time you create a cluster with this name, it will pick up the Init Script and pre-install TensorFlow on the Spark driver and workers. If you have a cluster by this name already running, then restart it to install TensorFlow.\n\n<h3>Deploying a TensorFlow model on Spark</h3>\n\nNow that we have a GPU-enabled cluster running with TensorFlow installed, we can deploy a deep learning model. For the full workflow, <a href=\"https://docs.databricks.com/applications/deep-learning/spark-integration.html#deep-learning-in-spark-jobs\">refer to the full notebook</a> in the Databricks Guide. We will cover the key points here.\n\nWe begin by downloading the Inception-v3 model, which is hosted by TensorFlow:\n\n<pre><code class=\"python\">MODEL_URL = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\n</code></pre>\n\nWe broadcast the Inception model to communicate it efficiently to the workers in our Spark cluster:\n\n<pre><code class=\"python\">model_data_bc = sc.broadcast(model_data)\n</code></pre>\n\nIf you read the full notebook, you will see we also broadcast other helpful metadata, such as <code>node_lookup</code> information for mapping Inception\u2019s predictions into human-readable results.\n\nWe then read a list of image file URLs and parallelize that list in a Spark RDD:\n\n<pre><code class=\"python\">batched_data = read_file_index()\nurls = sc.parallelize(batched_data)   # later in this workflow\n</code></pre>\n\nThis gives us a distributed set of images to process. The remaining work will be done in a distributed fashion on Spark workers, where each worker will process a subset of these images.\n\nWe define a function which processes a single image:\n\n<pre><code class=\"python\">run_inference_on_image(sess, img_id, img_url, node_lookup)\n</code></pre>\n\nThis function does the following:\n\n<ol>\n<li>Download the image:\n<code>image_data = urllib.request.urlopen(img_url, timeout=1.0).read()</code></li>\n<li>Make predictions using TensorFlow:\n<code>predictions = sess.run(softmax_tensor, {'DecodeJpeg/contents:0': image_data})</code></li>\n<li>Convert the results to a human-readable format.</li>\n</ol>\n\nFor efficiency, we have batched computation so that TensorFlow handles multiple images at once. The function <code>apply_inference_on_batch(batch)</code> handles an entire batch.\n\nBoth of these functions could be used outside of Spark. It can be helpful to develop a deep learning workflow on a single machine for simplicity, before actually parallelizing parts of the workflow.\n\nHere, we do not explicitly tell TensorFlow to use the GPU, so TensorFlow chooses which device to use automatically. When we run this notebook with one p2.xlarge worker and log device usage, TensorFlow uses the GPU for the bulk of the computation.\n\nFinally, we apply the inference functions by mapping them over our distributed set of images:\n\n<pre><code class=\"python\">labeled_images = urls.flatMap(apply_inference_on_batch)\n</code></pre>\n\nAnd that\u2019s it! This workflow provides a nice template from which you can build your customized deep learning + Apache Spark integration. Key points include:\n\n<ul>\n<li>Try to develop the deep learning logic on a single machine before parallelizing parts of the workflow.</li>\n<li>Know which tasks are efficient to distribute. Prediction (inference) and model tuning are often good candidates.</li>\n<li>Use Spark efficiently: Broadcast big objects like model data, parallelize expensive parts of the workflow, and minimize communication between Spark workers.</li>\n</ul>\n\n<h2>Getting started</h2>\n\nThis tutorial covered only TensorFlow, but we provide material for other popular deep learning libraries as well. Several of these libraries have more complex installation requirements, so the Init Scripts we provide can be very useful. The <a href=\"https://docs.databricks.com/applications/deep-learning/index.html\">Databricks Guide section on Deep Learning</a> currently covers:\n\n<ul>\n<li><a href=\"https://docs.databricks.com/applications/deep-learning/tensorflow.html\">TensorFlow</a></li>\n<li><a href=\"https://docs.databricks.com/applications/deep-learning/caffe.html\">Caffe</a></li>\n<li><a href=\"https://docs.databricks.com/applications/deep-learning/mxnet.html\">MXNet</a></li>\n<li><a href=\"https://docs.databricks.com/applications/deep-learning/theano.html\">Theano</a></li>\n</ul>\n\nEach library page includes an Init Script for installation and a short example of using the library within Databricks. The Databricks Guide also contains a <a href=\"https://docs.databricks.com/user-guide/clusters/gpu.html\">page on creating GPU clusters</a>. Expect more guides in the near future! To get started today, <a href=\"http://databricks.com/try\">sign-up for a free trial</a>."}
{"status": "publish", "description": null, "creator": "jules_damji", "link": "https://databricks.com/blog/2016/12/30/top-10-apache-spark-blog-posts-from-2016.html", "authors": null, "id": 9894, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2016-12-30", "tz": "UTC", "createdOn": "2016-12-30"}, "title": "Top 10 Apache Spark Blog Posts from 2016", "slug": "top-10-apache-spark-blog-posts-from-2016", "content": "<em>Spark Summit will be held in Boston on Feb 7-9, 2017. Check out the <a href=\"https://spark-summit.org/east-2017/schedule/\" target=\"_blank\">full agenda</a> and <a href=\"https://r.online-reg.com/Spark_Summit_East_2017/register\" target=\"_blank\">get your ticket</a> before it sells out!</em>\n\n<hr/>\n\nHere\u2019s our recap of what has transpired with Apache Spark since our <a href=\"https://databricks.com/blog/2016/11/16/databricks-bi-weekly-apache-spark-digest-111616.html\" target=\"_blank\">previous digest</a>. This digest includes Apache Spark\u2019s top ten 2016 blogs, along with release announcements and other noteworthy events.\n\n<h2>Top Ten Apache Spark Blogs</h2>\n\n<ol>\n    <li><a href=\"https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html\" target=\"_blank\">Apache Spark as a Compiler: Joining a Billion Rows per Second on a Laptop</a></li>\n    <li><a href=\"https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html\" target=\"_blank\">A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets</a></li>\n    <li><a href=\"https://databricks.com/blog/2016/01/04/introducing-apache-spark-datasets.html\" target=\"_blank\">Introducing Apache Spark Datasets</a></li>\n    <li><a href=\"https://databricks.com/blog/2016/03/03/introducing-graphframes.html\" target=\"_blank\">Introducing GraphFrames</a></li>\n    <li><a href=\"https://databricks.com/blog/2016/07/26/introducing-apache-spark-2-0.html\" target=\"_blank\">Introducing Apache Spark 2.0</a></li>\n    <li><a href=\"https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html\" target=\"_blank\">Structured Streaming In Apache Spark</a></li>\n    <li><a href=\"https://databricks.com/blog/2016/05/31/apache-spark-2-0-preview-machine-learning-model-persistence.html\" target=\"_blank\">Apache Spark 2.0 Preview: Machine Learning Model Persistence</a></li>\n    <li><a href=\"https://databricks.com/blog/2016/08/31/apache-spark-scale-a-60-tb-production-use-case.html\" target=\"_blank\">Apache Spark @Scale: A 60 TB+ production use case from Facebook</a></li>\n    <li><a href=\"https://databricks.com/blog/2016/12/15/scalable-partition-handling-for-cloud-native-architecture-in-apache-spark-2-1.html\" target=\"_blank\">Scalable Partition Handling for Cloud-Native Architecture in Apache Spark 2.1</a></li>\n    <li><a href=\"https://databricks.com/blog/2016/12/21/deep-learning-on-databricks.html\" target=\"_blank\">Deep Learning on Databricks</a></li>\n</ol>\n\n<h2>Releases</h2>\n\n<ul>\n\n    <li><a href=\"https://databricks.com/blog/2016/12/29/introducing-apache-spark-2-1.html\" target=\"_blank\">Apache Spark 2.1.0 released</a>, with additional support for Structured Streaming and Apache Kafka 0.10.0. <a href=\"http://databricks.com/try\" target=\"_blank\">Try it on Databricks</a>.</li>\n</ul>\n\n<h2>Webinar</h2>\n\n<ul>\n    <li>Joseph Bradley and I presented <a href=\"http://go.databricks.com/apache-spark-mllib-migrating-ml-workloads-to-dataframes-webinar\" target=\"_blank\">Apache Spark MLlib 2.x: Migrating ML Workloads to DataFrames</a>, and posted the follow up <a href=\"https://databricks.com/blog/2016/12/14/demand-webinar-faq-apache-spark-mllib-2-x-migrating-ml-workloads-dataframes.html\" target=\"_blank\">questions & answers blog</a>.</li>\n</ul>\n\n<h2>Events</h2>\n\n<ul>\n    <li>Tathagata Das of Databricks presented \u201cDeep Dive in Structured Streaming\u201d at <a href=\"https://www.meetup.com/Apache-Spark-Lisbon-Meetup/events/235359131/\" target=\"_blank\">Apache Spark Meetup Lisbon</a>. </li>\n</ul>\n\n<h2>What\u2019s Next</h2>\n\nTo stay abreast with what\u2019s happening with Apache Spark, follow us on Twitter <a href=\"https://twitter.com/databricks\" target=\"_blank\">@databricks</a> and visit <a href=\"http://sparkhub.databricks.com\" target=\"_blank\">SparkHub</a>."}
{"status": "publish", "description": null, "creator": "rxin", "link": "https://databricks.com/blog/2016/12/29/introducing-apache-spark-2-1.html", "authors": null, "id": 9917, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2016-12-29", "tz": "UTC", "createdOn": "2016-12-29"}, "title": "Introducing Apache Spark 2.1", "slug": "introducing-apache-spark-2-1", "content": "[sidenote]Spark Summit will be held in Boston on Feb 7-9, 2017. Check out the <a href=\"https://spark-summit.org/east-2017/schedule/\" target=\"_blank\">full agenda</a> and <a href=\"https://r.online-reg.com/Spark_Summit_East_2017/register\" target=\"_blank\">get your ticket</a> before it sells out![/sidenote]\n\n<hr/>\n\nToday we are happy to announce the availability of <a href=\"https://spark.apache.org/releases/spark-release-2-1-0.html\" target=\"_blank\">Apache Spark 2.1.0</a>.\n\nThis release makes measurable strides in the production readiness of Structured Streaming, with added support for event time watermarks and Apache Kafka 0.10 support. In addition, the release focuses more on usability, stability, and refinement, resolving over 1200 tickets, than previous Spark releases.\n\nThis blog post discusses some of the high-level changes to help you navigate the 1200+ improvements and bug fixes:\n\n<ul>\n<li>Production readiness of Structured Streaming</li>\n<li>Expanding SQL functionalities</li>\n<li>New distributed machine learning algorithms in R</li>\n</ul>\n\n<h2>Structured Streaming</h2>\n\nIntroduced in Spark 2.0, <a href=\"https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html\">Structured Streaming</a> is a high-level API for building <a href=\"https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html\">continuous applications</a>. The main goal is to make it easier to build end-to-end streaming applications, which integrate with storage, serving systems, and batch jobs in a consistent and fault-tolerant way.\n\n<ul>\n<li><strong>Event-time watermarks:</strong> This change lets applications hint to the system when events are considered \u201ctoo late\u201d and allows the system to bound internal state tracking late events.</li>\n<li><strong>Support for all file-based formats and all file-based features:</strong> With these improvements, Structured Streaming can read and write all file-based formats, e.g. JSON, text, Avro, CSV. In addition, all file-based features\u2014e.g. partitioned files and bucketing\u2014are supported on all formats.</li>\n<li><strong>Apache Kafka 0.10:</strong> This adds native support for Kafka 0.10, including manual assignment of starting offsets and rate limiting.</li>\n</ul>\n\nStreaming applications run 24/7 continuously and put stringent requirements on the visibility and manageability of the underlying system. To that end, Spark 2.1 adds the following features:\n\n<ul>\n<li><strong>GUID:</strong> The addition of a GUID that can be used to identify a streaming query across restarts.</li>\n<li><strong>Forward compatible & human readable checkpoint logs:</strong> A stable JSON format is now used for all checkpoint logs, which allows users to upgrade a streaming query from Spark 2.1 to future versions of Spark. In addition, the log format is designed so it can be inspected readily by a human, to gain visibility into the running systems.</li>\n<li><strong>Improved reporting of query status:</strong> The query status API has been updated to include more information based on our own production experience, for both current query status as well as historical progress.</li>\n</ul>\n\nAt Databricks, we religiously believe in dogfooding. Using a release candidate version of Spark 2.1, we have ported some of our internal data pipelines as well as worked with some of our customers to port their production pipelines using Structured Streaming. In coming weeks, we will be publishing a series of blog posts on various aspects of Structured Streaming as well as our experience with it. Stay tuned for more deep dives.\n\n<h2>SQL and Core APIs</h2>\n\nSince Spark 2.0 release, Spark is now one of the most feature-rich and standard-compliant SQL query engine in the Big Data space. It can connect to a variety of data sources and perform SQL-2003 feature sets such as <a href=\"https://databricks.com/blog/2016/06/17/sql-subqueries-in-apache-spark-2-0.html\">analytic functions and subqueries</a>. Spark 2.1 adds a number of SQL functionalities:\n\n<ul>\n<li><strong>Table-valued functions:</strong> Spark 2.1 introduces the concept of table-valued functions, or TVF, a function that returns a relation, or a set of rows. The first built-in table-valued functions is \u201crange\u201d, a TVF that returns a range of rows. As an example, \u201cSELECT count(*) FROM range(1000)\u201d would return 1000.</li>\n<li><strong>Enhanced partition column inference:</strong> Added support for inferring date, timestamp, and decimal type for partition columns.</li>\n<li><strong>Enhanced inline tables:</strong> While Spark 2.0 added support for inline tables, Spark 2.1 enhanced inline tables to support specifying values using any foldable expressions and also automatically coerced types. As an example, \u201cSELECT * FROM VALUES (1, \u201cone\u201d), (1 + 1, \u201ctwo\u201d)\u201d selects from a table with 2 rows.</li>\n<li><strong>Null ordering:</strong> Users can now specify how to order nulls, e.g. NULLS FIRST or NULLS LAST in ORDER BY clause.</li>\n<li><strong>Binary literals:</strong> X'1C7' would mean a binary literal (byte array) 0x1c7.</li>\n<li><strong>MINUS:</strong> Added support for MINUS set operation, which is the equivalent of EXCEPT DISTINCT.</li>\n<li><strong>to_json and from_json functions:</strong> All along Spark automatically infers types for JSON datasets. We have also seen a lot of datasets in which one or two string columns are JSON encoded. Two new functions work with JSON columns.</li>\n<li><strong>Cross join hint:</strong> When working with a large amount of data, a cross join can be extremely expensive and users often don\u2019t want to actually perform a cross join. Spark 2.1 out of the box disables cross join support, unless users explicitly issue a query with \u201cCROSS JOIN\u201d syntax. That is to say, Spark 2.1 will reject \u201cSELECT * FROM a JOIN b\u201d, but allow \u201cSELECT * FROM a CROSS JOIN b.\u201d This way Spark prevents users from shooting themselves in the shoot. To disable this behavior, change \u201cspark.sql.crossJoin.enabled\u201d to \u201ctrue\u201d.</li>\n</ul>\n\nSpark 2.1 also adds a number of improvements to the core Dataset/DataFrame API, mostly in the typed API:\n\n<ul>\n<li><strong>KeyValueGroupedDataset.mapValues:</strong> Users can now map over the values on a KeyValueGroupedDataset, without modifying the keys.</li>\n<li><strong>Partial aggregation for KeyValueGroupedDataset.reduceGroups:</strong> reduceGroups now supports partial aggregation to reduce the amount of data shuffled across the network.</li>\n<li><strong>Encoder for java.util.Map:</strong> java.util.Map types can be automatically inferred as a Spark map type.</li>\n</ul>\n\n<h2>MLlib and SparkR</h2>\n\nThe last major set of changes in Spark 2.1 focuses on advanced analytics. The following new algorithms were added to MLlib and GraphX:\n\n<ul>\n<li>Locality Sensitive Hashing</li>\n<li>Multiclass Logistic Regression</li>\n<li>Personalized PageRank</li>\n</ul>\n\nSpark 2.1 also adds support for the following distributed algorithms in SparkR: \n\n<ul>\n<li>ALS</li>\n<li>Isotonic Regression</li>\n<li>Multilayer Perceptron Classifier</li>\n<li>Random Forest</li>\n<li>Gaussian Mixture Model</li>\n<li>LDA</li>\n<li>Multiclass Logistic Regression</li>\n<li>Gradient Boosted Trees</li>\n</ul>\n\nWith the addition of these algorithms, SparkR has become the most comprehensive library for distributed machine learning on R.\n\nThis blog post only covered some of the major features in this release. You can head over to the <a href=\"https://spark.apache.org/releases/spark-release-2-1-0.html\" target=\"_blank\">official release notes</a> to see the complete list of changes.\n\nWe will be posting more details about some of these new features in the coming weeks. Stay tuned to the Databricks blog to learn more about Spark 2.1. If you want to try out these new features, you can already use Spark 2.1 in Databricks, alongside older versions of Spark. <a href=\"https://databricks.com/try\">Sign up for a free trial account here.</a>"}
{"status": "publish", "description": null, "creator": "rxin", "link": "https://databricks.com/blog/2017/01/04/databricks-and-apache-spark-year-in-review.html", "authors": null, "id": 9925, "categories": ["Company Blog"], "dates": {"publishedOn": "2017-01-04", "tz": "UTC", "createdOn": "2017-01-04"}, "title": "Databricks and Apache Spark 2016 Year in\u00a0Review", "slug": "databricks-and-apache-spark-year-in-review", "content": "[sidenote]Spark Summit will be held in Boston on Feb 7-9, 2017. Check out the <a href=\"https://spark-summit.org/east-2017/schedule/\" target=\"_blank\" rel=\"noopener\">full agenda</a> and <a href=\"https://r.online-reg.com/Spark_Summit_East_2017/register\" target=\"_blank\" rel=\"noopener\">get your ticket</a> before it sells out![/sidenote]\n\n<hr />\n\nIn 2016, Apache Spark released its <a href=\"https://databricks.com/blog/2016/07/26/introducing-apache-spark-2-0.html\">second major version 2.0</a> and outgrew our wildest expectations: 4X growth in meetup members reaching 240,000 globally, and 2X growth in code contributors reaching 1000.\n\nIn addition to contributing to the success of Spark, Databricks also had a phenomenal year. We have rolled out a large number of features such as end-to-end security, seen a 40X increase in our number of active users, enterprise thought leaders adopting Databricks across industries, and last but not least our series C funding enabling us to expand and broaden the horizon of big data.\n\nIn the remainder of this blog post, we want to reflect on the major milestones we achieved in this short year\u2014and to look forward to the next year:\n\n<ul>\n    <li>SQL-2003: the most advanced and standard-compliant Big Data SQL engine</li>\n    <li>CloudSort Record: Spark and Databricks as the most efficient platform</li>\n    <li>Structured Streaming: dramatically simpler streaming</li>\n    <li>Deep Learning and GPUs on Databricks</li>\n    <li>DBES, HIPAA, GovCloud, and SOC 2: End-to-end Security for Spark</li>\n    <li>Databricks Community Edition: Best Place to Learn Spark</li>\n</ul>\n\n<h2>SQL-2003 and Advanced SQL</h2>\n\nSQL is a universal language used by data engineers, scientists, analysts for data big and small. As of Spark 2.0, Spark SQL has become one of the most feature-rich and standard-compliant SQL query engines in the Big Data space.\n\nTo build a high-performing SQL engine, we first introduced Spark SQL with a query optimizer called <a href=\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\">Catalyst</a> in 2014. Leveraging advanced programming language features, Catalyst is the most flexible and advanced production-grade query optimizer. With Catalyst, we were able to quickly implement functionalities that used to take years to implement in traditional MPP SQL query engines.\n\nIn May, we announced that <a href=\"https://databricks.com/blog/2016/05/11/apache-spark-2-0-technical-preview-easier-faster-and-smarter.html\">Spark SQL could run all 99 TPC-DS queries without modifications</a>. TPC-DS is a standard benchmark for SQL analytic workloads and it poses great challenges for query engines due to its use of complex features such as subqueries and analytic functions. Most of the Big Data SQL query engines that you have heard of, such as Apache Hive, Apache Impala and Presto, are not capable of running all the queries in this benchmark.\n\nAt the same time, we announced phase two of <a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\">Project Tungsten</a>, inspired by ideas in state-of-the-art research in both database systems and modern compilers. Tungsten is capable of running modern database operations (e.g. filter, hash join) in roughly one nanosecond per simple row, making it <a href=\"https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html\">the most advanced execution engine you can find in open source databases</a>.\n\n<img class=\"aligncenter size-full wp-image-7218\" src=\"https://databricks.com/wp-content/uploads/2016/05/preliminary-tpc-ds-spark-2-0-vs-1-6.png\" alt=\"Preliminary TPC-DS Spark 2.0 vs 1.6\" width=\"703\" height=\"380\" />\n\nIn 2016, Spark SQL emerged as one of the most feature-rich, standard-compliant, and performant Big Data SQL query engines. This investment reduces the friction for our customers to scale out their workloads over Spark SQL.\n\n<h2>CloudSort Record: The Most Efficient Platform</h2>\n\nApache Spark is known for its speed, and we have been using various third-party, independent benchmarks in order to evaluate the progress made in this area. In 2014, Databricks entered the Sort Benchmark and set a <a href=\"https://databricks.com/blog/2014/10/10/spark-petabyte-sort.html\">world record in GraySort</a> for the fastest time to sort 100TB of data using Spark.\n\nEarlier this year, in order to validate our ability to architect the most performant and cost-efficient cloud data platform, we again entered the competition in <a href=\"https://databricks.com/blog/2016/11/14/setting-new-world-record-apache-spark.html\">the CloudSort category</a>, which picks the winner with the lowest cost, measured by public cloud pricing. This benchmark effectively measures the efficiency, i.e. ratio of performance to cost, of the cloud architecture (combination of software stack, hardware stack, and tuning).\n\nAs joint effort with Nanjing University, Alibaba Group, and Databricks, we architected the most efficient way to sort 100 TB of data, beating out all other competitors using only $144.22 USD worth of cloud resources. Through this benchmark, Databricks demonstrated it offers the most efficient way to process data in the cloud.\n\n<img class=\"aligncenter size-full wp-image-9651\" src=\"https://databricks.com/wp-content/uploads/2016/11/CloudSort-Benchmark-Image.jpg\" alt=\"Databricks, in collaboration with the Alibaba Group and Nanjing University, set a new CloudSort benchmark of $1.44 per terabyte.\" width=\"1304\" height=\"670\" />\n\n<h2>Structured Streaming: Dramatically Simpler Streaming</h2>\n\nSpark 2.0 adds the first version of a new higher-level API, <a href=\"https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html\">Structured Streaming</a>, for building <a href=\"https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html\">continuous applications</a>. The main goal is to make it easier to build end-to-end real-time applications, which integrate with storage, serving systems, and batch jobs in a consistent and fault-tolerant way.\n\nHaving learned from hundreds of Spark users, we designed Structured Streaming with a number of unique properties that are extremely important to real-life applications:\n\n<ul>\n    <li><strong>Same programming model and API as batch:</strong> Unlike other streaming engines that have separate APIs for batch and streaming, Structured Streaming simply uses the same programming model for both, making streaming easy to reason about.</li>\n    <li><strong>Transactional semantics and exactly-once processing:</strong> We understand that insights from data is only as good as the quality of the data itself, so we have built Structured Streaming to be transactional and exactly-once from the beginning. Users can be sure of the correctness of the results they get from the engine.</li>\n    <li><strong>Interactive queries:</strong> Users can use the batch API to perform interactive queries directly on live streams.</li>\n    <li><strong>SQL support</strong>: Thanks to the programming model, SQL support is built into the engine.</li>\n</ul>\n\nThe recently <a href=\"https://databricks.com/blog/2016/12/29/introducing-apache-spark-2-1.html\">released Spark 2.1</a> made measurable strides in the production readiness of Structured Streaming, with added support for event time watermarks and Apache Kafka 0.10.\n\nAt Databricks, we religiously believe in dogfooding. We have ported many of our internal data pipelines as well as worked with some of our customers to port their production pipelines using Structured Streaming. In coming weeks, we will be publishing a series of blog posts on various aspects of Structured Streaming as well as our experience with it. Stay tuned for more deep dives.\n\n<h2>Deep Learning and GPUs: Boris Brexit Johnson Daydreamed on Databricks</h2>\n\nNeural networks have seen spectacular progress during the last few years and they are now the state of the art in image recognition and automated translation. With our customer-first mantra, and by listening to the needs of our customers from a wide variety of industries, we worked on <a href=\"https://databricks.com/blog/2016/12/21/deep-learning-on-databricks.html\">adding deep learning functionalities to the Databricks platform</a> as well as releasing an open source library called <a href=\"https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html\">TensorFrames</a> that integrates Apache Spark and TensorFlow.\n\nDatabricks now offers a simple way to leverage GPUs to power image processing, text analysis, and other machine learning tasks. Users can create GPU-enabled clusters with EC2 P2 instance types. Databricks includes pre-installed NVIDIA drivers and libraries, Apache Spark deployments configured for GPUs, and material for getting started with several popular deep learning libraries.\n\nAt Spark Summit Europe, we conducted a live demo applying a deep learning model called \u201cDeep Dream\u201d on Boris Johnson:\n\n<img class=\"aligncenter size-full wp-image-9961\" src=\"https://databricks.com/wp-content/uploads/2017/01/deep-dream-boris-johnson-databricks.jpg\" alt=\"\" width=\"767\" height=\"511\" />\n\n<a href=\"https://databricks.com/blog/2016/12/21/deep-learning-on-databricks.html\">This blog post</a> covers how to use popular deep learning libraries such as TensorFlow, Caffe, MXNet, and Theano on Databricks, with GPUs ready to go.\n\n<h2>DBES, HIPAA, GovCloud, and SOC 2: End-to-end Security for Apache Spark</h2>\n\nEnterprise-grade data platforms must be secure, easy-to-use, and performant. Historically, new shiny technologies often focus more on performance and capabilities while lagging behind in security, and as a result enterprises have to choose among two out of the three properties they desire.\n\nThat\u2019s not the case with Apache Spark on Databricks. <a href=\"http://www.marketwired.com/press-release/databricks-empowers-enterprises-to-secure-their-apache-spark-workloads-2132605.htm\">Databricks Enterprise Security (DBES)</a> is a suite of security features that includes SAML 2.0 compatibility, role-based access control, end-to-end encryption, and comprehensive audit logs. DBES will provide holistic security in every aspect of the entire big data lifecycle.\n\nFor customers in regulated industries such as the public sector and healthcare, <a href=\"http://www.marketwired.com/press-release/databricks-launches-industrys-first-fully-managed-just-time-apache-spark-platform-on-2135654.htm\">we also expanded into the AWS GovCloud (US)</a> and <a href=\"http://www.marketwired.com/press-release/databricks-announces-hipaa-compliance-with-apache-spark-based-platform-achieves-aws-2179354.htm\">created a HIPAA-compliant offering</a>. These enhancements enable IT and platform technology teams to focus on architecture and administer corporate policy, bypassing the difficult process of building, configuring, and maintaining Spark infrastructure. To bolster end-to-end enterprise security, <a href=\"https://databricks.com/blog/2016/10/04/databricks-completes-soc-2-type-1-certification.html\">we achieved SOC 2 Type 1 certification</a>, where auditors endorsed Databricks' platform as architected, from the ground up, according to security best practices.\n\nAt Databricks, we are proud to be the only Apache Spark platform with all these capabilities.\n\n<h2>Databricks Community Edition: Best Place to Learn Spark</h2>\n\nWhen we founded Databricks, we recognized that one of the largest challenges to Big Data is not in the technologies themselves, but in the talent gap. The number of engineers, data scientists, and business analysts that can understand and manipulate Big Data is too small. To address this challenge, we create tools that are simpler to use and build training curriculums and platforms to educate the next generation.\n\n<a href=\"https://databricks.com/blog/2016/02/17/introducing-databricks-community-edition-apache-spark-for-all.html\">Launched</a> in February, <a href=\"https://community.cloud.databricks.com/\">Databricks Community Edition</a> is designed for developers, data scientists, engineers and anyone who wants to learn Spark. On this free platform, users have access to a micro-cluster, a cluster manager, and the notebook environment to prototype simple applications. All users can share their notebooks and host them free of charge with Databricks.\n\n<img class=\"aligncenter size-full wp-image-6411\" src=\"https://databricks.com/wp-content/uploads/2016/02/Databricks-Community-Edition-thumbnail.png\" alt=\"Databricks Community Edition\" width=\"600\" height=\"315\" />\n\nIn addition to the platform itself, Databricks Community Edition comes with a rich portfolio of Spark training resources, including our award-winning <a href=\"https://databricks.com/training\">Massive Open Online Courses</a>, e.g. \u201cIntroduction to Big Data with Apache Spark.\u201d\n\nToday, Databricks Community Edition is used as a default platform for learning at MOOCs, EdX courses. Numerous universities such as UC Berkeley have also started computer science courses using this platform. At Apache Spark meetups and workshops, presenters use Databricks to demonstrate notebooks and attendees follow presenter-led workshops. We have trained over 50,000 Spark users across our various training platforms.\n\n<h2>In Closing</h2>\n\nWith this rapid pace of development, we are also happy to see how quickly users adopt new versions of Spark. For example, the graph below shows the Spark versions run by over 500 customers at Databricks (note that a single customer can also run multiple Spark versions). The majority of the clusters are now running Spark 2.0 and higher.\n\n<img class=\"aligncenter size-full wp-image-9968\" src=\"https://databricks.com/wp-content/uploads/2017/01/percent-of-clusters-by-spark-version-2.1.jpg\" alt=\"Percent of clusters by Spark Version on Databricks\" width=\"1200\" height=\"600\" />\n\n2016 is the year Spark reached ubiquity in both the real world and the digital world: We have seen Boris \u201cBrexit\u201d Johnson \u201cdaydreamed\u201d using Deep Learning and Spark, Spark t-shirts worn at various gyms across the world (see below), and last but not least, thanks to IBM\u2019s investment, Spark being able to run on mainframe computers in addition to servers.\n\n<img class=\"aligncenter size-full wp-image-9962\" src=\"https://databricks.com/wp-content/uploads/2017/01/databricks-spark-side.jpg\" alt=\"\" width=\"505\" height=\"426\" />\n\nThrough our hard work, the Databricks team has also established ourselves as a trusted partner among many major industry leaders ranging from financial tech, healthcare, advertising tech, media, energy and the public sector. To name a few, <a href=\"https://databricks.com/customers\">DNV GL</a> (the largest technical consultancy for the energy industry in the world), <a href=\"https://databricks.com/customers\">American Diabetes Association</a>, and <a href=\"https://databricks.com/customers\">Viacom</a> all presented about their use cases with Databricks this year at Spark Summit.\n\n<img class=\"aligncenter wp-image-9934 \" src=\"https://databricks.com/wp-content/uploads/2016/12/capital-one-american-diabetes-association-dnvgl-logos-e1498238300414.png\" alt=\"\" width=\"485\" height=\"145\" />\n\nIn December, <a href=\"http://www.marketwired.com/press-release/databricks-raises-60-million-series-c-funding-advance-data-science-engineering-scale-2183541.htm\">we raised $60MM in series C</a> to fuel our mission, to take us forward, to expand and to broaden the horizon of big data into newer Spark applications across a broad range of vertical industries. We will deepen our commitment to open source Apache Spark by investing more engineering resources to contribute code and engage the community. At the same time, we will also continue to expand our platform to make data science and engineering at scale with Spark even easier and faster.\n\nThat together we\u2019ve achieved so much this past year is a testament that innovation happens in collaboration and not in isolation.\n\nWe want to thank the Spark community for all their hard work in evangelizing Spark at meetups and Spark Summits, sharing best practices, and contributing to Spark\u2019s success. The immense code contributions all over the world are what make the project great. Additionally, we want to thank our customers for providing invaluable feedback.\n\nOur goal at Databricks is to make Big Data simple, and we are only at the beginning of our journey. In 2017, expect tremendous innovations coming from us to make Spark and Databricks more secure, robust, performant, and easier to use, and working with our customers to expand the use of big data across even more industry verticals."}
{"status": "publish", "description": null, "creator": "Wayne Chan", "link": "https://databricks.com/blog/2017/01/03/spark-live-2016-tour-recap.html", "authors": null, "id": 9956, "categories": ["Company Blog", "Events", "Product"], "dates": {"publishedOn": "2017-01-03", "tz": "UTC", "createdOn": "2017-01-03"}, "title": "Spark Live 2016 Tour Recap", "slug": "spark-live-2016-tour-recap", "content": "The Apache Spark community had quite the year in 2016. It has maintained its billing as the largest and most active open source community in big data, with over 1300 contributors from 250+ organizations. And Spark Summit, the largest gathering of Spark enthusiasts, drew over 6000 attendees across three events. To satisfy this ongoing thirst to learn more about Spark, we hit the road with an eight city road show called Spark Live \u2014 designed exclusively for data analytics, technology, and business professionals who want to learn how to leverage the power of Apache Spark and Databricks to simplify data processing and make their transformative use cases a reality.\n\nIn partnership with premier sponsor Intel and regional sponsors such as Redis Labs, Basho, Lockheed Martin, and AWS; we shared industry insights, discussions on how to deploy Spark in the enterprise, the roadmap for both Spark and Databricks, live product demos and a full afternoon of hands-on training.\n\n<img class=\"aligncenter size-full wp-image-9958\" src=\"https://databricks.com/wp-content/uploads/2017/01/spark-live-packed-house.jpg\" alt=\"Packed house at Spark Live!\" width=\"1024\" height=\"768\" />\n\nThe level of interest was phenomenal as we made our way across the country hitting Hanover, Los Angeles, Reston, Austin, New York, Boston, Chicago, and Seattle. Across the eight cities, <strong>we had over 1200 attendees</strong> across a range of roles including data scientists, engineers, architects, and executives. Out of those 1200 attendees, <strong>we delivered free Spark training to over 1000 people</strong> from some of the largest and innovative organizations in the world including Salesforce.com, Amgen, Disney, and the National Football League (NFL) just to name a few.\n\n<h2>What\u2019s Next for 2017?</h2>\n\n<img class=\"aligncenter size-full wp-image-9954\" src=\"https://databricks.com/wp-content/uploads/2016/04/spark-live-og.jpg\" alt=\"Databricks Spark Live 2017 Tour\" width=\"1800\" height=\"945\" />\n\nDue to the enthusiasm and positive feedback from this year\u2019s Spark Live tour, we will be hitting the road again in 2017 to continue our mission of bringing Spark to the masses. This time, we\u2019ll be hitting more cities around the world including Atlanta, Austin, Hanover, London, Los Angeles, New York, Reston, San Jose, Seattle, Sydney, and Toronto! We\u2019ll also be improving the agenda with more hands-on training with new courseware.\n\n<h3>Interested in attending a Spark Live in 2017?</h3>\n\n<a href=\"http://go.databricks.com/spark-live/request-your-spot\" target=\"_blank\" rel=\"noopener\">Sign up for updates</a> on the road show schedule.\n\n<h3>Interesting in sponsoring Spark Live?</h3>\n\nPlease send inquiries to <a href=\"mailto:events@databricks.com\">events@databricks.com</a>."}
{"status": "publish", "description": null, "creator": "Wayne Chan", "link": "https://databricks.com/blog/2017/01/09/5-cant-miss-talks-at-spark-summit-east-2017.html", "authors": null, "id": 10024, "categories": ["Company Blog", "Events"], "dates": {"publishedOn": "2017-01-09", "tz": "UTC", "createdOn": "2017-01-09"}, "title": "5 Can\u2019t Miss Talks at Spark Summit East 2017", "slug": "5-cant-miss-talks-at-spark-summit-east-2017", "content": "<img class=\"aligncenter size-full wp-image-10025\" src=\"https://databricks.com/wp-content/uploads/2017/01/SSE2017-OG-Image.jpg\" alt=\"Spark Summit East 2017 kicks off February 7, 2017 in Boston, MA.\" width=\"600\" height=\"315\" />\n\nIf you haven\u2019t been to a <a href=\"https://spark-summit.org/east-2017/\">Spark Summit</a> yet, you are missing out on the biggest gathering of Apache Spark experts and enthusiasts in the world. 2017\u2019s first Spark Summit kicks off on February 7th in Boston where more than 1,500 engineers, analysts, scientists, and business professionals will gather for three days of in-depth learning and networking. With over 100 sessions across six tracks, one can feel a bit overwhelmed by all the great content to consume.\n\nHere are five talks featuring a star-studded lineup of technology leaders and innovators in the world of big data and Apache Spark you definitely want to add to your must-watch list:\n\n<ol>\n<li><strong><a href=\"https://spark-summit.org/east-2017/events/what-to-expect-for-big-data-and-apache-spark-in-2017/\">What to Expect for Big Data and Apache Spark in 2017</a></strong> - You don\u2019t want to be late for the first talk of the conference as <a href=\"https://spark-summit.org/east-2017/speakers/matei-zaharia/\">Matei Zaharia</a>, the creator of the Spark project and Co-founder/Chief Technologist at Databricks. Matei will take the stage to share his thoughts on what the future holds for Big Data and Apache Spark in 2017.</li>\n<li><strong><a href=\"https://spark-summit.org/east-2017/events/production-ready-structured-streaming/\">Production-Ready Structured Streaming</a></strong> - Earlier this year, we published the findings from our annual Spark Survey revealing streaming analytics as one of the fastest growing areas for Spark with an increase of 57% in streaming production use cases. This talk, featuring Spark PMC member and project committer <a href=\"https://spark-summit.org/east-2017/speakers/michael-armbrust/\">Michael Armbrust</a>, as he discusses the various aspects of Structured Streaming, the new streaming API initially released in Spark 2.0, and the measurable strides made in Spark 2.1 to ensure production readiness.</li>\n<li><strong><a href=\"https://spark-summit.org/east-2017/events/virtualizing-analytics-with-apache-spark/\">Virtualizing Analytics with Apache Spark</a></strong> - In his keynote, <a href=\"https://spark-summit.org/east-2017/speakers/ali-ghodsi/\">Ali Ghodsi</a>, CEO and Co-founder of Databricks, will walk us through the demand for Virtual Analytics, where the complexities of disparate data and technology silos have been abstracted away, coupled with a powerful range of analytics and processing horsepower, all in one unified data platform.</li>\n<li><strong><a href=\"https://spark-summit.org/east-2017/events/using-apache-spark-for-intelligent-services/\">Using Apache Spark for Intelligent Services</a></strong> - Salesforce is developing Einstein, an artificial intelligence (AI) capability built into the core of the Salesforce Platform. <a href=\"https://spark-summit.org/east-2017/speakers/alexis-roos/\">Alexis Roos</a>, a Senior Engineering Manager at Salesforce, will share how this tech giant is leveraging Spark and Databricks to scale data science and engineering to make this possible.</li>\n<li><strong><a href=\"https://spark-summit.org/east-2017/events/big-data-meets-learning-science/\">Big Data meets Learning Science</a></strong> - Another not-to-miss talk at Spark Summit East is by <a href=\"https://spark-summit.org/east-2017/speakers/alfred-essa/\">Alfred Essa</a>, VP of Research and Data Science at McGraw-Hill Education. This talk focuses on McGraw-Hill\u2019s efforts to leverage data science to deliver a high-quality personalized education experience with Apache Spark and Databricks, that is accessible and affordable by all.</li>\n</ol>\n\nThere are plenty of other compelling talks and sessions that might pique your interest. <a href=\"https://spark-summit.org/east-2017/schedule\">Check out the full schedule</a> to see what else Spark Summit East has to offer.\n\n<strong><a href=\"https://r.online-reg.com/Spark_Summit_East_2017/register\">Register today</a> with the discount code \u201cDatabricksBT20\u201d and receive 20% off your spot at the largest Spark event on the East Coast!</strong>"}
{"status": "publish", "description": null, "creator": "jules_damji", "link": "https://databricks.com/blog/2017/01/10/5-reasons-to-attend-spark-summit-east-2017.html", "authors": null, "id": 10069, "categories": ["Company Blog", "Events"], "dates": {"publishedOn": "2017-01-10", "tz": "UTC", "createdOn": "2017-01-10"}, "title": "5 Reasons to Attend Spark Summit East 2017", "slug": "5-reasons-to-attend-spark-summit-east-2017", "content": "[sidenote]Spark Summit East will be held in Boston on Feb 7-9, 2017. Check out the <a href=\"https://spark-summit.org/east-2017/schedule/\">full agenda</a> and <a href=\"https://r.online-reg.com/Spark_Summit_East_2017/register\">get your ticket</a> before it sells out![/sidenote]\n\n<hr />\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/01/SSE2017-OG-Image.jpg\" alt=\"Spark Summit East 2017 kicks off February 7, 2017 in Boston, MA.\" width=\"600\" height=\"315\" class=\"aligncenter size-full wp-image-10025\" />\n\n<h2>Crucibles of Spark Knowledge</h2>\n\nAt eight Spark Summits. In six cosmopolitan cities. Across two continents. With over 10,400 attendees.\n\nTo any <a href=\"http://spark.apache.org/\">Apache Spark</a> expert or enthusiast, Spark Summits, held at global cities, are crucibles of Spark knowledge. Bounded by a growing global community and drawn by an enduring belief in Spark\u2019s proven potential in the big data landscape, attendees have convened since 2013\u2014this time they will converge in Boston, Massachusetts.\n\nAs a Spark Community advocate and co-organizer of <a href=\"https://www.meetup.com/spark-users/\">Bay Area Apache Spark Meetup</a> group, there is no other summit I would rather be at to connect and converse about Spark. Like a communal thread that binds us together at these summit venues, we are drawn to hear from Spark contributors and creators: how Spark has fared, how it has evolved, what are its aspirations.\n\nFor that and much more, here are my five reasons why you should join us:\n\n<h3>1. Keynotes from Distinguished Engineers and Industry Leaders</h3>\n\n<em><a href=\"https://databricks.com/blog/2017/01/09/5-cant-miss-talks-at-spark-summit-east-2017.html\">See our list of must-see talks for Spark Summit East 2017!</a></em>\n\nDistinguished engineers (<a href=\"https://spark-summit.org/east-2017/speakers/matei-zaharia/\">Matei Zaharia</a>, <a href=\"https://spark-summit.org/east-2017/speakers/cotton-seed/\">Cotton Seed</a>, <a href=\"https://spark-summit.org/east-2017/speakers/arun-murthy/\">Arun Murthy</a>) and visionary industry leaders (<a href=\"https://spark-summit.org/east-2017/speakers/ali-ghodsi/\">Ali Ghodsi</a>, <a href=\"https://spark-summit.org/east-2017/speakers/ziya-ma/\">Zia Ma</a>, <a href=\"https://spark-summit.org/east-2017/speakers/alfred-essa/\">Alfred Essa</a>) in the Big Data industries will share their vision of where Apache Spark is heading in 2017; how Spark is solving real-world problems at scale in education, business, real-time advanced analytics, and artificial intelligence. You won\u2019t want to miss <a href=\"https://spark-summit.org/east-2017/\">two days of keynotes</a>.\n\n<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">We\u2019ve announced more must-see Keynote Speakers for Spark Summit East, including <a href=\"https://twitter.com/hashtag/ApacheSpark?src=hash\">#ApacheSpark</a> creator <a href=\"https://twitter.com/matei_zaharia\">@matei_zaharia</a>: <a href=\"https://t.co/ushieLWqve\">https://t.co/ushieLWqve</a></p>&mdash; Spark Summit (@spark_summit) <a href=\"https://twitter.com/spark_summit/status/817401282361257985\">January 6, 2017</a></blockquote>\n\n<h3>2. Developer and Enterprise Day</h3>\n\n<em><a href=\"https://spark-summit.org/east-2017/schedule/\">Check out the full Spark Summit East 2017 Agenda.</a></em>\n\nConsider the choice from over 100 sessions spanning five tracks: Data Science, Developer, Spark Ecosystem, Spark Experience &amp; Use Cases, Research, and Enterprise. You can learn about recent developments in <a href=\"http://spark.apache.org/\">Apache Spark 2.1</a>, Structured Streaming, Machine Learning and Deep Learning, and much more.\n\n<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\"><a href=\"https://twitter.com/lemire\">@lemire</a> will talk about &quot;Engineering fast indexes for big-data applications&quot; at <a href=\"https://twitter.com/spark_summit\">@spark_summit</a> Boston in Feb. <a href=\"https://t.co/j2a77T9gsF\">https://t.co/j2a77T9gsF</a></p>&mdash; Reynold Xin (@rxin) <a href=\"https://twitter.com/rxin/status/802975839679676417\">November 27, 2016</a></blockquote>\n\n<h3>3. Apache Spark Training</h3>\n\n<em><a href=\"https://spark-summit.org/east-2017/apache-spark-training/\">Learn more about Apache Spark training at Spark Summit East 2017.</a></em>\n\nGet the best training from Databricks\u2019 best trainers, who have trained close to 3,200 summit attendees. A day dedicated to training, you can choose from two technical courses and stay abreast with the latest in Spark 2.x: <a href=\"https://spark-summit.org/east-2017/events/training-data-science-with-apache-spark-2x/\">Data Science with Apache Spark 2.x</a> and <a href=\"https://spark-summit.org/east-2017/events/training-exploring-wikipedia-with-apache-spark-2x/\">Exploring Wikipedia with Apache Spark 2.x</a>.\n\n<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">I had a great training today by <a href=\"https://twitter.com/GoDataDriven\">@GoDataDriven</a>&#39;s <a href=\"https://twitter.com/asnare\">@asnare</a> on advanced <a href=\"https://twitter.com/hashtag/Spark?src=hash\">#Spark</a> topics <a href=\"https://twitter.com/spark_summit\">@spark_summit</a> <a href=\"https://twitter.com/hashtag/SparkSummit?src=hash\">#SparkSummit</a> <a href=\"https://twitter.com/hashtag/Brussels?src=hash\">#Brussels</a> <a href=\"https://t.co/cYxaW5cR0I\">pic.twitter.com/cYxaW5cR0I</a></p>&mdash; Tom Lous (@tomlous) <a href=\"https://twitter.com/tomlous/status/790942458339262464\">October 25, 2016</a></blockquote>\n\n<h3>4. The Boston Apache Spark Meetup</h3>\n\n<em><a href=\"https://www.meetup.com/Boston-Apache-Spark-User-Group/events/236737434/\">RSVP for the Meetup today!</a></em>\n\nApache Spark Meetups are reputed for tech-talks. At Spark Summits\u2019 meetups, you learn what other Spark developers from all over are up to, mingle and enjoy the beverages and camaraderie in an informal setting, and ask burning questions.\n\n<h3>5. Best Brews in Boston</h3>\n\n<em><a href=\"https://www.thrillist.com/drink/boston/best-craft-beer-bars-in-boston-meadhall-lord-hobo-tip-tap-room\">Thirsty? We found a great guide to Boston\u2019s best beer bars.</a></em>\n\nA city famed for its <a href=\"http://www.topuniversities.com/university-rankings-articles/qs-best-student-cities/boston\">reputed seats of learning</a>, <a href=\"http://boston.redsox.mlb.com/index.jsp?c_id=bos\">baseball</a> and <a href=\"http://www.nba.com/celtics/\">basketball teams</a>, Boston also shared with us an enduring and entertaining cultural gem, <a href=\"https://cheersboston.com/locations/beacon\">\u201cCheers,\u201d</a> so after long day of cerebral Spark sessions, why not chill and have the best brew at one of Boston\u2019s best pubs.\n\nAfter a three-day Spark fest in Boston, you\u2019ll have sharpened your saw with Spark knowledge; you\u2019ll have forged enduring professional relationships with Spark enthusiasts; and you\u2019ll have appreciated the best brew of Boston with friends\u2014not to mention memories of moments to share with others.\n\n<blockquote class=\"twitter-tweet\" data-partner=\"tweetdeck\"><p lang=\"en\" dir=\"ltr\">Good bye <a href=\"https://twitter.com/hashtag/SparkSummit?src=hash\">#SparkSummit</a> EU Had great moments and enjoyed the conference much. See you next time in...<a href=\"https://twitter.com/hashtag/Boston?src=hash\">#Boston</a>? <a href=\"https://twitter.com/hashtag/dreams?src=hash\">#dreams</a> <a href=\"https://t.co/eEttczkS8n\">pic.twitter.com/eEttczkS8n</a></p>&mdash; Jacek Laskowski (@jaceklaskowski) <a href=\"https://twitter.com/jaceklaskowski/status/791673926569103360\">October 27, 2016</a></blockquote>\n\nWe hope to you see you in Boston!\n\n<h2>What\u2019s Next</h2>\n\nTickets are selling fast. If you haven\u2019t yet, <strong>register today with the discount code \u201cDatabricksBT20\u201d and get 20% off.</strong>\n\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"}
{"status": "publish", "description": null, "creator": "Wayne Chan", "link": "https://databricks.com/blog/2017/01/18/demand-webinar-faq-apache-spark-unified-engine-workloads.html", "authors": null, "id": 10089, "categories": ["Company Blog", "Events", "Product"], "dates": {"publishedOn": "2017-01-18", "tz": "UTC", "createdOn": "2017-01-18"}, "title": "On-Demand Webinar and FAQ: Apache Spark - The Unified Engine for All Workloads", "slug": "demand-webinar-faq-apache-spark-unified-engine-workloads", "content": "Last week, we held a live webinar \u2014 <a href=\"http://go.databricks.com/apache-spark-unified-engine-for-all-workloads-webinar\">Apache Spark - The Unified Engine for All Workloads</a> \u2014 to explain the real-world benefits to practitioners and enterprises when they build a technology stack based on a unified approach with Apache Spark.\n\nThis webinar features Ovum analyst Tony Baer, who will explain the real-world benefits to practitioners and enterprises when they build a technology stack based on a unified approach with Apache Spark.\n\nThis webinar will cover:\n\n<ul>\n<li>Findings around the growth of Spark and diverse applications using machine learning and streaming. </li>\n<li>The advantages of using Spark to unify all workloads, rather than stitching together many specialized engines like Presto, Storm, MapReduce, Pig, and others. </li>\n<li>Use case examples that illustrate the flexibility of Spark in supporting various workloads.</li>\n</ul>\n\nThe webinar is now <a href=\"http://go.databricks.com/apache-spark-unified-engine-for-all-workloads-webinar\">accessible on-demand</a>, and the slides used in the webinar are also downloadable as attachments to the webinar.\n\nWe have also answered the common questions raised by webinar viewers below. If you have additional questions, check out the <a href=\"https://forums.databricks.com/\">Databricks Forum</a> or the <a href=\"http://docs.databricks.com\">new documentation resource</a>.\n\nIf you\u2019d like free access to Databricks, you can access the <a href=\"https://databricks.com/try\">free trial here</a>.\n\n<h2>Common webinar questions and answers</h2>\n\n<em>Click on the question to see answer</em>\n\n<ul>\n<li><a href=\"https://forums.databricks.com/questions/10729/from-webinar-apache-spark-as-a-unified-engine-are.html\">Are there any projects under way that integrate Databrick/Spark with IBM BigIntegrate?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/10730/from-webinar-apache-spark-as-a-unified-engine-what.html\">What is the best practice for storing metadata for a Spark environment?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/10731/from-webinar-apache-spark-as-a-unified-engine-for.html\">For ETL batch loads which process high data volumes, how would memory size constraints for Spark be addressed?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/10732/from-webinar-apache-spark-as-a-unified-engine-is-i.html\">Is it advisable to use Spark on the data ingestion side as a replacement to Sqoop?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/10733/from-webinar-apache-spark-as-a-unified-engine-are-1.html\">Are there any plans to create a content based filtering recommendation algorithm in MLlib?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/10734/from-webinar-apache-spark-as-a-unified-engine-when.html\">When we create an application with Spark, we must control our cluster. So what's the difference between Ambari and Yarn?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/10735/from-webinar-apache-spark-as-a-unified-engine-is-a.html\">Is Apache Spark (using Python or SparkSQL) replacing dimensional modeling process such as a star or snowflake schema?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/10736/from-webinar-apache-spark-as-a-unified-engine-what-1.html\">What is the release date for Spark 2.2.0?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/10737/from-webinar-apache-spark-as-a-unified-engine-are-2.html\">Are there any tools/utilities to convert MapReduce to Spark?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/10738/from-webinar-apache-spark-as-a-unified-engine-what-2.html\">What are the current differences between Spark available on Amazon AWS and the Databricks-provided version of Spark?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/10739/from-webinar-apache-spark-as-a-unified-engine-what-3.html\">What is the best open source front-end for Spark queries?</a></li>\n</ul>"}
{"status": "publish", "description": null, "creator": "tdas", "link": "https://databricks.com/blog/2017/01/19/real-time-streaming-etl-structured-streaming-apache-spark-2-1.html", "authors": null, "id": 10093, "categories": ["Apache Spark", "Engineering Blog", "Streaming"], "dates": {"publishedOn": "2017-01-19", "tz": "UTC", "createdOn": "2017-01-19"}, "title": "Real-time Streaming ETL with Structured Streaming in Apache Spark 2.1", "slug": "real-time-streaming-etl-structured-streaming-apache-spark-2-1", "content": "[sidenote]Spark Summit will be held in Boston on Feb 7\u20139, 2017. Check out the <a href=\"https://spark-summit.org/east-2017/schedule/\">full agenda</a> and <a href=\"https://r.online-reg.com/Spark_Summit_East_2017/register\">get your ticket</a> before it sells out![/sidenote]\n\n<hr />\n\n[dbce_cta href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/2070341989008532/3601578643761083/latest.html\"]Try this notebook in Databricks[/dbce_cta]\n\nWe are well into the Big Data era, with organizations collecting massive amounts of data on a continual basis. Yet, the value of this data deluge hinges on the ability to extract actionable insights in a timely fashion. Hence, there is an increasing need for <em><a href=\"https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html\">continuous applications</a></em> that can derive real-time actionable insights from massive data ingestion pipelines.\n\nHowever, building production-grade continuous applications can be challenging, as developers need to overcome many obstacles, including:\n\n<ul>\n<li><strong>Providing end-to-end reliability and correctness guarantees</strong> - Long running data processing systems must be resilient to failures by ensuring that outputs are consistent with results processed in batch.  Additionally, unusual activities (e.g failures in upstream components, traffic spikes, etc.) must be continuously monitored and automatically mitigated to ensure highly available insights are delivered in real-time.</li>\n<li><strong>Performing complex transformations</strong> - Data arrives in a myriad formats (CSV, JSON, Avro, etc.) that often must be restructured, transformed and augmented before being consumed. Such restructuring requires that all the traditional tools from batch processing systems are available, but without the added latencies that they typically entail.</li>\n<li><strong>Handling late or out-of-order data</strong> - When dealing with the physical world, data arriving late or out-of-order is a fact of life. As a result, aggregations and other complex computations must be continuously (and accurately) revised as new information arrives.</li>\n<li><strong>Integrating with other systems</strong> - Information originates from a variety of sources (Kafka, HDFS, S3, etc), which must be integrated to see the complete picture.</li>\n</ul>\n\n<strong>Structured Streaming in Apache Spark builds upon the strong foundation of Spark SQL, leveraging its powerful APIs to provide a seamless query interface, while simultaneously optimizing its execution engine to enable low-latency, continually updated answers.</strong> This blog post kicks off a series in which we will explore how we are using the new features of Apache Spark 2.1 to overcome the above challenges and build our own production pipelines.\n\nIn this first post, we will focus on an ETL pipeline that converts raw <a href=\"http://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html\">AWS CloudTrail audit logs</a> into a <a href=\"https://databricks.com/blog/2015/11/30/building-a-just-in-time-data-warehouse-platform-with-databricks.html\">JIT data warehouse</a> for faster ad-hoc queries. <strong>We will show how easy it is to take an existing batch ETL job and subsequently productize it as a real-time streaming pipeline using Structured Streaming in Databricks.</strong> Using this pipeline, we have converted <strong>3.8 million</strong> JSON files containing <strong>7.9 billion</strong> records into a Parquet table, which allows us to do ad-hoc queries  on updated-to-the-minute Parquet table 10x faster than those on raw JSON files.\n\n<h2>The Need for Streaming ETL</h2>\n\nExtract, Transform, and Load (ETL) pipelines prepare raw, unstructured data into a form that can be queried easily and efficiently. Specifically, they need to be able to do the following:\n\n<ul>\n<li><strong>Filter, transform, and clean up data</strong> - Raw data is naturally messy and needs to be cleaned up to fit into a well-defined structured format. For example, parsing timestamp strings to date/time types for faster comparisons, filtering corrupted data, nesting/unnesting/flattening complex structures to better organize important columns, etc. </li>\n<li><strong>Convert to a more efficient storage format</strong> - Text, JSON and CSV data are easy to generate and are human readable, but are very expensive to query.  Converting it to more efficient formats like Parquet, Avro, or ORC can reduce file size and improve processing speed. </li>\n<li><strong>Partition data by important columns</strong> - By partitioning the data based on the value of one or more columns, common queries can be answered more efficiently by reading only the relevant fraction of the total dataset.</li>\n</ul>\n\nTraditionally, ETL is performed as periodic batch jobs. For example, dump the raw data in real time, and then convert it to structured form every few hours to enable efficient queries. We had initially setup our system this way, but this technique incurred a high latency; we had to wait for few hours before getting any insights. For many use cases, this delay is unacceptable. When something suspicious is happening in an account, we need to be able to ask questions immediately. Waiting minutes to hours could result in an unreasonable delay in responding to an incident.\n\nFortunately, Structured Streaming makes it easy to convert these periodic batch jobs to a real-time data pipeline. Streaming jobs are expressed using the same APIs as batch data. Additionally, the engine provides the same fault-tolerance and data consistency guarantees as periodic batch jobs, while providing much lower end-to-end latency.\n\nIn the rest of post, we dive into the details of how we transform <a href=\"http://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html\">AWS CloudTrail audit logs</a> into an efficient, partitioned, parquet data warehouse. AWS CloudTrail allows us to track all actions performed in a variety of AWS accounts, by delivering gzipped JSON logs files to a S3 bucket. These files enable a variety of business and mission critical intelligence, such as cost attribution and security monitoring. However, in their original form, they are very costly to query, even with the capabilities of Apache Spark. To enable rapid insight, we run a Continuous Application that transforms the raw JSON logs files into an optimized Parquet table. Let's dive in and look at how to write this pipeline. If you want to see the full code, here are the <a href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/2070341989008532/3601578643761083/latest.html\">Scala</a> and <a href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/2070341989008551/3601578643761083/latest.html\">Python</a> notebooks. <a href=\"http://databricks.com/try\">Import them into Databricks</a> and run them yourselves.\n\n<h2>Transforming Raw Logs with Structured Streaming</h2>\n\nWe start by defining the schema of the JSON records based on <a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-event-reference-record-contents.html\">CloudTrail documentation</a>.\n\n<pre><code class=\"scala\">val cloudTrailSchema = new StructType()\n  .add(\"Records\", ArrayType(new StructType()\n    .add(\"additionalEventData\", StringType)\n    .add(\"apiVersion\", StringType)\n    .add(\"awsRegion\", StringType)\n    // ...\n</code></pre>\n\nSee the attached notebook for the full schema. With this, we can define a streaming DataFrame that represents the data stream from CloudTrail files that are being written in a S3 bucket.\n\n<pre><code class=\"scala\">val rawRecords = spark.readStream\n  .schema(cloudTrailSchema)\n  .json(\"s3n://mybucket/AWSLogs/*/CloudTrail/*/2017/*/*\")\n</code></pre>\n\nA good way to understand what this <code>rawRecords</code> DataFrame represents is to first understand the <a href=\"https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html\">Structured Streaming programming model</a>. The key idea is to treat any data stream as an unbounded table: new records added to the stream are like rows being appended to the table.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/01/cloudtrail-unbounded-tables.png\" alt=\"Structured Streaming Model: Treat Data Streams as Unbounded Tables\" width=\"898\" height=\"417\" class=\"aligncenter size-full wp-image-10096\" />\n\nThis allows us to treat both batch and streaming data as tables. Since tables and DataFrames/Datasets are semantically synonymous, the same batch-like DataFrame/Dataset queries can be applied to both batch and streaming data. In this case, we will transform the raw JSON data such that it\u2019s easier to query using Spark SQL's built-in support for manipulating complex nested schemas. Here is an abridged version of the transformation.\n\n<pre><code class=\"scala\">val cloudtrailEvents = rawRecords \n  .select(explode($\"records\") as 'record)\n  .select(\n    unix_timestamp(\n      $\"record.eventTime\", \n      \"yyyy-MM-dd'T'hh:mm:ss\").cast(\"timestamp\") as 'timestamp, $\"record.*\")\n</code></pre>\n\nHere, we <code>explode</code> (split) the array of records loaded from each file into separate records. We also parse the string event time string in each record to Spark\u2019s timestamp type, and flatten out the nested columns for easier querying.  Note that if <code>cloudtrailEvents</code> was a batch DataFrame on a fixed set of files, then we would have written the same query, and we would have written the results only once as <code>parsed.write.parquet(\"/cloudtrail\")</code>. Instead, we will start a <strong>StreamingQuery</strong> that runs continuously to transform new data as it arrives.\n\n<pre><code class=\"scala\">val streamingETLQuery = cloudtrailEvents\n  .withColumn(\"date\", $\"timestamp\".cast(\"date\") // derive the date\n  .writeStream\n  .trigger(ProcessingTime(\"10 seconds\")) // check for files every 10s\n  .format(\"parquet\") // write as Parquet partitioned by date\n  .partitionBy(\"date\")\n  .option(\"path\", \"/cloudtrail\")\n  .option(\"checkpointLocation\", \"/cloudtrail.checkpoint/\")\n  .start()\n</code></pre>\n\nHere we are specifying the following configurations for the StreamingQuery before starting it.\n\n<ul>\n<li>Derive the date from the timestamp column</li>\n<li>Check for new files every 10 seconds (i.e., trigger interval)</li>\n<li>Write the transformed data from parsed DataFrame as a Parquet-formatted table at the path <code>/cloudtrail</code>.</li>\n<li>Partition the Parquet table by date so that we can later efficiently query time slices of the data; a key requirement in monitoring applications.</li>\n<li>Save checkpoint information at the path <code>/checkpoints/cloudtrail</code> for fault-tolerance (explained later in the blog)</li>\n</ul>\n\nIn terms of the Structured Streaming Model, this is how the execution of this query is performed.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/01/cloudtrail-structured-streaming-model.png\" alt=\"Structured Streaming Model: Spark incrementalizes user\u2019s batch-like query to run on streams\" width=\"898\" height=\"571\" class=\"aligncenter size-full wp-image-10095\" />\n\nConceptually, the <code>rawRecords</code> DataFrame is an append-only <strong>Input Table</strong>, and the <code>cloudtrailEvents</code> DataFrame is the transformed <strong>Result Table</strong>. In other words, when new rows are appended to the input (<code>rawRecords</code>), the result table (<code>cloudtrailEvents</code>) will have new transformed rows. In this particular case, every 10 seconds, Spark SQL engine <strong>triggers</strong> a check for new files. When it finds new data (i.e., new rows in the Input Table), it transforms the data to generate new rows in the Result Table, which then get written out as Parquet files.\n\nFurthermore, while this streaming query is running, you can use Spark SQL to simultaneously query the Parquet table. The streaming query writes the Parquet data transactionally such that concurrent interactive query processing will always see a consistent view of the latest data. This strong guarantee is known as <a href=\"https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html\">prefix-integrity</a> and it makes Structured Streaming pipelines integrate nicely with the larger Continuous Application.\n\nYou can read more details about the Structured Streaming model, and its advantages over other streaming engines in our <a href=\"https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html\">previous blog</a>.\n\n<h2>Solving Production Challenges</h2>\n\nEarlier, we highlighted a number of challenges that must be solved for running a streaming ETL pipeline in production. Let\u2019s see how Structured Streaming running on the Databricks platform solves them.\n\n<h3>Recovering from Failures to get Exactly-once Fault-tolerance Guarantees</h3>\n\nLong running pipelines must be able to tolerate machine failures. With Structured Streaming, achieving fault-tolerance is as easy as specifying a checkpoint location for the query. In the earlier code snippet, we did  so in the following line.\n\n<pre><code class=\"scala\">.option(\"checkpointLocation\", \"/cloudtrail.checkpoint/\")\n</code></pre>\n\nThis checkpoint directory is per query, and while a query is active, Spark continuously writes metadata of the processed data to the checkpoint directory. Even if the entire cluster fails, the query can be restarted on a new cluster, using the same checkpoint directory, and consistently recover. More specifically, on the new cluster, Spark uses the metadata to start the new query where the failed one left off, thus ensuring end-to-end exactly-once guarantees and data consistency (see <a href=\"https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html\">Fault Recovery section of our previous blog</a>).\n\nFurthermore, this same mechanism allows you to upgrade your query between restarts, as long as the input sources and output schema remain the same. Since Spark 2.1, we encode the checkpoint data in JSON for future-proof compatibility. So you can restart your query even after updating your Spark version. In all cases, you will get the same fault-tolerance and consistency guarantees.\n\nNote that <strong>Databricks makes it very easy to set up automatic recovery</strong>, as we will show in the next section.\n\n<h3>Monitoring, Alerting and Upgrading</h3>\n\nFor a Continuous Application to run smoothly, it must be robust to individual machine or even whole cluster failures. In Databricks, we have developed tight integration with Structured Streaming that allows us continuously monitor your StreamingQueries for failures (and automatically restart them. All you have to do is create a new Job, and configure the Job retry policy. You can also configure the job to send emails to notify you of failures.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/01/cloudtrail-streaming-application-job.png\" alt=\"Databricks Jobs can automatically monitor your application and recover from failures \" width=\"817\" height=\"251\" class=\"aligncenter size-full wp-image-10097\" />\n\nApplication upgrades can be easily made by updating your code and/or Spark version and  then restarting the Job. See our <a href=\"https://docs.databricks.com/spark/latest/structured-streaming/production.html\">guide on running Structured Streaming in Production</a> for more details.\n\nMachine failures are not the only situations that we need to handle to ensure robust processing. We will discuss how to monitor for traffic spikes and upstream failures in more detail later in this series.\n\n<h3>Combining Live Data with Historical/Batch Data</h3>\n\nMany applications require historical/batch data to be combined with live data. For example, besides the incoming audit logs, we may already have a large backlog of logs waiting to be converted. Ideally, we would like to achieve both, interactively query the latest data as soon as possible, and also have access to historical data for future analysis. It is often complex to set up such a pipeline using most existing systems as you would have to set up multiples processes: a batch job to convert the historical data, a streaming pipeline to convert the live data, and maybe a another step to combine the results.\n\nStructured Streaming eliminates this challenge. You can configure the above query to prioritize the processing new data files as they arrive, while using the space cluster capacity to process the old files. First, we set the option <code>latestFirst</code> for the file source to true, so that new files are processed first. Then, we set the <code>maxFilesPerTrigger</code> to limit how many files to process every time. This tunes the query to update the downstream data warehouse more frequently, so that the latest data is made available for querying as soon as possible. Together, we can define the <code>rawLogs</code> DataFrame as follows:\n\n<pre><code class=\"scala\">val rawJson = spark.readStream\n  .schema(cloudTrailSchema)\n  .option(\"latestFirst\", \"true\")\n  .option(\"maxFilesPerTrigger\", \"20\")\n  .json(\"s3n://mybucket/AWSLogs/*/CloudTrail/*/2017/01/*\")\n</code></pre>\n\nIn this way, we can write a single query that easily combines live data with historical data, while ensuring low-latency, efficiency and data consistency.\n\n<h2>Conclusion</h2>\n\nStructured Streaming in Apache Spark is the best framework for writing your streaming ETL pipelines, and Databricks makes it easy to run them in production at scale, as we demonstrated above. We shared a high level overview of the steps\u2014extracting, transforming, loading and finally querying\u2014to set up your streaming ETL production pipeline. We also discussed and demonstrated how Structured Streaming overcomes the challenges in solving and setting up high-volume and low-latency streaming pipelines in production.\n\nIn the future blog posts in this series, we\u2019ll cover how we address other hurdles, including:\n\n<ul>\n<li><a href=\"https://databricks.com/blog/2017/02/23/working-complex-data-formats-structured-streaming-apache-spark-2-1.html\">Applying complex transformations to nested JSON data</a></li>\n<li><a href=\"https://databricks.com/blog/2017/04/26/processing-data-in-apache-kafka-with-structured-streaming-in-apache-spark-2-2.html\">Processing Data in Apache Kafka with Structured Streaming in Apache Spark 2.2</a></li>\n<li><a href=\"https://databricks.com/blog/2017/05/18/taking-apache-sparks-structured-structured-streaming-to-production.html\">Monitoring your streaming applications</a></li>\n<li><a href=\"https://databricks.com/blog/2017/04/26/processing-data-in-apache-kafka-with-structured-streaming-in-apache-spark-2-2.html\">Integrating Structured Streaming with Apache Kafka</a></li>\n<li><a href=\"https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html\">Computing event time aggregations with Structured Streaming</a></li>\n<li><a href=\"https://databricks.com/blog/2017/05/22/running-streaming-jobs-day-10x-cost-savings.html\">Running Streaming Jobs Once a Day For 10x Cost Savings</a></li>\n</ul>\n\nIf you want to learn more about the Structured Streaming, here are a few useful links.\n\n<ul>\n<li>Previous blogs posts explaining the motivation and concepts of Structured Streaming:\n\n<ul>\n<li><a href=\"https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html\">Continuous Applications: Evolving Streaming in Apache Spark 2.0</a></li>\n<li><a href=\"https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html\">Structured Streaming In Apache Spark</a></li>\n</ul></li>\n<li><a href=\"http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\">Structured Streaming Programming Guide for Apache Spark 2.1</a></li>\n<li><a href=\"https://spark-summit.org/2016/events/a-deep-dive-into-structured-streaming/\">Spark Summit 2016 Talk - A Deep Dive Into Structured Streaming</a></li>\n</ul>\n\n<h2>What\u2019s Next</h2>\n\nYou can try two notebooks with your own AWS CloudTrail Logs. <a href=\"http://databricks.com/try\">Import the notebooks into Databricks.</a>\n\n<ul>\n<li><a href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/2070341989008532/3601578643761083/latest.html\">Try the Scala Notebook</a></li>\n<li><a href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/2070341989008551/3601578643761083/latest.html\">Try the Python Notebook</a></li>\n</ul>"}
{"status": "publish", "description": null, "creator": "jakebellacera", "link": "https://databricks.com/blog/2017/01/23/integration-aws-data-pipeline-databricks-building-etl-pipelines-apache-spark.html", "authors": null, "id": 10116, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2017-01-23", "tz": "UTC", "createdOn": "2017-01-23"}, "title": "Integration of AWS Data Pipeline with Databricks: Building ETL pipelines with Apache Spark", "slug": "integration-aws-data-pipeline-databricks-building-etl-pipelines-apache-spark", "content": "[sidenote]This is one of a series of blogs on integrating Databricks with commonly used software packages. See the \u201cWhat\u2019s Next\u201d section at the end to read others in the series, which includes how-tos for AWS Lambda, Kinesis, Airflow and more.[/sidenote]\n\n<hr />\n\nDatabricks is a fully managed Apache Spark data platform in the cloud, built to make real-time data exploration and deploying production jobs easy for data engineers, data scientists, or anyone interested in using Spark for data processing.\n\n<a href=\"https://aws.amazon.com/datapipeline/\">AWS Data Pipeline</a> is a web service that helps reliably process and move data between different AWS compute and storage services at specified intervals. AWS Data Pipeline helps users to easily create complex data processing workloads that are fault tolerant, repeatable, and highly available.\n\nDatabricks is natively deployed to our users\u2019 AWS VPC and is compatible with every tool in the AWS ecosystem. In this blog, I will demonstrate how to build an ETL pipeline using Databricks and AWS Data Pipeline.\n\n<h2>How to use Data Pipeline with Databricks</h2>\n\nThe <a href=\"https://docs.databricks.com/api/latest/index.html\">Databricks REST API</a> enables programmatic access to Databricks instead of going through the Web UI. It can automatically <a href=\"https://docs.databricks.com/user-guide/jobs.html\">create and run jobs</a>, <a href=\"https://docs.databricks.com/user-guide/notebooks/notebook-workflows.html?highlight=workflow\">productionalize a workflow</a>, and much more. It also allows us to integrate Data Pipeline with Databricks, by triggering an action based on events in other AWS services.\n\nUsing AWS Data Pipeline, you can create a pipeline by defining:\n\n<strong>The \u201cdata sources\u201d that contain your data.</strong> To ensure that data is available prior to the execution of an activity, AWS Data Pipeline allows you to optionally create data availability checks called <strong>preconditions</strong>. These checks will repeatedly attempt to verify data availability and will block any dependent activities from executing until the preconditions succeed.\n\n<strong>The \u201cactivities\u201d or business logic such as launching a Databricks job.</strong> One can use the AWS Data Pipeline object <code>ShellCommandActivity</code> to call a Linux curl command to trigger a REST API call to Databricks.\n\n<strong>The \u201cschedule\u201d on which your business logic executes.</strong> In the case of scheduling a start time in the past, Data Pipeline backfills the tasks. It also allows you to maximize the efficiency of resources by supporting different schedule periods for a resource and an associated activity. For example by reusing the same resources if the scheduling period permits.\n\nBelow is an example of setting up a Data Pipeline to process log files on a regular basis using Databricks. AWS Data Pipeline is used to orchestrate this pipeline by detecting when daily files are ready for processing and setting a \u201cprecondition\u201d for detecting the output of the daily job and sending a final email notification.\n\n<h2>Log Processing Example</h2>\n\nSetup the data pipeline:\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/01/setup-aws-data-pipeline.png\" alt=\"Setting up AWS Data Pipeline to work with Databricks\" width=\"1000\" height=\"564\" class=\"aligncenter size-full wp-image-10117\" />\n\n<em>Figure 1: ETL automation:</em> 1) Data lands in S3 from Web servers, <code>InputDataNode</code>, 2) An event is triggered and a call is made to the Databricks via the <code>ShellCommandActivity</code> 3) Databricks processes the log files and writes out Parquet data, <code>OutputDataNode</code>, 4) An SNS notification is sent once as the results of the previous step.\n\nThis pipeline has several steps:\n\n<h3>1. Input Precondition</h3>\n\nCheck that the input data exists:\n\n<pre><code>s3://&lt;bucket-name&gt;/input/\n</code></pre>\n\n<h3>2. Input Data Node</h3>\n\nConfigure the input as the source for the next step which is the shell script for calling the REST API.\n\n<h3>3. Invoke Databricks REST API</h3>\n\nInvoke the <code>ShellCommandActivity</code> operator to call the Databricks REST API with the file input and output arguments (For the purposes of illustrating the point in this blog, we use the command below; for your workloads, there are many ways to maintain security):\n\n<pre><code>curl -X POST -u &lt;username&gt;:&lt;password&gt; https://&lt;shard-name&gt;.cloud.databricks.com/api/2.0/jobs/run-now -d'{\"job_id\":&lt;job-id&gt;, \"notebook_params\":{\"inputPath\": \"s3a://&lt;access-key&gt;:&lt;secrte-key&gt;@&lt;bucket-name&gt;/input/sample_logs/part-00001\",\"outputPath\": \"s3a://&lt;access-key&gt;:&lt;secrte-key&gt;@&lt;bucket-name&gt;/output/sample_log_parquet\"}}'\n</code></pre>\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/01/databricks-job-triggered-by-aws-data-pipeline.png\" alt=\"Example of a Databricks Job being triggered by AWS Data Pipeline\" width=\"1000\" height=\"413\" class=\"aligncenter size-full wp-image-10118\" />\n\n<em>Figure 2: A screenshot of the job within Databricks that is getting called.</em> Note that this job is not scheduled within Databricks, it is getting triggered by Data Pipeline.\n\n<h3>4. Databricks Action</h3>\n\nDatabricks Action involves reading an input log file, creating a schema and converting it into Parquet:\n\n<pre><code class=\"scala\">val inputPath = getArgument(\"inputPath\", \"default\")\n\ncase class ApacheAccessLog(ipAddress: String, clientIdentd: String,\n  userId: String, dateTime: String, method:String, endpoint: String,\n  protocol: String, responseCode: Int, contentSize: Long) {\n}\n\nval Pattern = \"\"\"^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+) (\\S+)\" (\\d{3}) (\\d+)\"\"\".r\n\ndef parseLogLine(log: String): ApacheAccessLog = {\n  val res = Pattern.findFirstMatchIn(log)\n  if (res.isEmpty) {\n    throw new RuntimeException(\"Cannot parse log line: \" + log)\n  }\n  val m = res.get\n  ApacheAccessLog(m.group(1), m.group(2), m.group(3), m.group(4),\n    m.group(5), m.group(6), m.group(7), m.group(8).toInt, m.group(9).toLong)\n}\n\nval accessLogs = (sc.textFile(inputPath).map(parseLogLine)).toDF().cache()\n\n// additional processing on \u2018accessLogs\u2019 dataframe\nval outPath = getArgument(\"outputPath\", \"default\")\naccessLogs.write.format(\"parquet\").mode(\"overwrite\").save(outPath)\n</code></pre>\n\n<h3>5. Output Precondition</h3>\n\nCheck that the output data exists:\n\n<pre><code>s3://&lt;bucket-name&gt;/output/sample_log_parquet\n</code></pre>\n\n<h3>6. Send Amazon SNS Alarm</h3>\n\nAs the final step, send out an Amazon SNS alarm when the job is successful. You can subscribe to SNS messages in a variety of ways, e.g. email or text notification push.\n\n<h2>What\u2019s Next</h2>\n\nWe hope this simple example shows how you can use Amazon Data Pipeline and the Databricks API to solve your data processing problems. Data Pipeline integrates with a variety of storage layers in AWS and using <code>ShellCommandActivity</code> it can integrate with the Databricks REST API and parameter arguments can be passed to Databricks\u2019 notebooks or libraries dynamically. To try Databricks, <a href=\"http://databricks.com/try\">sign-up for a free trial</a> or <a href=\"http://go.databricks.com/contact-databricks\">contact us</a>.\n\nRead other blogs in the series to learn how to integrate Databricks with your existing architecture:\n\n<ul>\n<li><a href=\"https://databricks.com/blog/2016/12/08/integrating-apache-airflow-databricks-building-etl-pipelines-apache-spark.html\">Integrating Apache Airflow and Databricks</a></li>\n<li><a href=\"https://databricks.com/blog/2016/11/16/oil-gas-asset-optimization-aws-kinesis-rds-databricks.html\">Oil and Gas Asset Optimization with AWS Kinesis, RDS, and Databricks</a></li>\n<li><a href=\"https://databricks.com/blog/2016/10/11/using-aws-lambda-with-databricks-for-etl-automation-and-ml-model-serving.html\">Using AWS Lambda with Databricks for ETL Automation and ML Model Serving</a></li>\n</ul>\n\n<h2>References</h2>\n\n<ul>\n<li><a href=\"https://aws.amazon.com/datapipeline/\">AWS Data Pipeline</a></li>\n<li><a href=\"https://aws.amazon.com/datapipeline/\">AWS Data Pipeline Developer Guide</a></li>\n<li><a href=\"https://aws.amazon.com/datapipeline/\">AWS Data Pipeline Preconditions</a></li>\n<li><a href=\"https://aws.amazon.com/datapipeline/\">AWS Data Pipeline ShellCommandActivity</a></li>\n<li><a href=\"https://aws.amazon.com/datapipeline/\">AWS Data Pipeline Amazon SNS notification</a></li>\n<li><a href=\"https://aws.amazon.com/datapipeline/\">Amazon Simple Notification Simple (SNS)</a></li>\n<li><a href=\"https://aws.amazon.com/datapipeline/\">Databricks REST API</a></li>\n</ul>"}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2017/01/25/delivering-exceptional-care-data-driven-medicine.html", "authors": null, "id": 10127, "categories": ["Company Blog", "Customers", "Product"], "dates": {"publishedOn": "2017-01-25", "tz": "UTC", "createdOn": "2017-01-25"}, "title": "Delivering Exceptional Care Through Data-Driven Medicine", "slug": "delivering-exceptional-care-data-driven-medicine", "content": "[sidenote]This is a guest blog from our friends at <a href=\"https://www.distal.co/\" target=\"_blank\">Distal</a>.[/sidenote]\n\nToday, 96% of U.S. health care providers use electronic health records (EHRs) - up from 10% in 2008. When I was a clinical intern, paper charts were the norm. Today, we practice medicine in a data-driven world. Large payers, such as Medicare and health insurance companies, would like to see providers leverage clinical data in ways that improve health outcomes, lower costs, and improve the patient experience.\n\nIn order to effect such change, payers have begun to shift some of the costs associated with suboptimal care to providers; this is a fundamental shift in the healthcare industry. Just a few years go, healthcare claims were paid without regard to clinical outcomes. Today, over 30% of all Medicare payments ($117B annually) are linked to clinical quality through a number of \u201cvalue-based payment\u201d mechanisms. Beginning this year, healthcare providers that fail to demonstrate progress toward data-driven quality improvement will face substantial financial penalties.\n\nProviders have responded by assembling teams of clinical quality managers, informaticians, and health IT experts to navigate the transition to value-based payment. These clinical and technical leaders must overcome a number of technical challenges, and it remains to be seen whether all providers will ultimately succeed. An emerging theme among providers that are reporting early wins is the central role of big data technologies, such as Apache Spark.\n\n<h2>Healthcare providers have a big data problem</h2>\n\n<ul>\n<li><strong>Massive volume and overwhelming variety:</strong> Collectively, U.S. hospitals generate over 20 petabytes of data each year. This figure doesn\u2019t include data collected in outpatient medical offices, pharmacies, third-party clinical labs, consumer wearables, or portable monitors. </li>\n<li><strong>Disparate systems:</strong> Clinical data doesn't reside exclusively in electronic health records (EHRs). It is common to have clinical data scattered across radiology, lab, and pharmacy information systems (to name a few).</li>\n<li><strong>Siloed data stores:</strong> Sometimes multiple EHR instances are deployed at a single physical location. For example: <em>Stanford Hospital</em> and <em>Lucile Packard Children's Hospital at Stanford</em> both use the same EHR software but they operate separate instances. As a result, there is no way to retrieve a complete patient record from the adult hospital through the children's hospital EHR (and vice versa). This is a big problem in the setting of medical emergencies where patient care teams are comprised of staff from both hospitals (e.g. STAT C-sections or pediatric trauma). Poor data interoperability in healthcare creates information gaps that <a href=\"http://www.nationalacademies.org/hmd/Reports/2012/Best-Care-at-Lower-Cost-The-Path-to-Continuously-Learning-Health-Care-in-America.aspx\">expose patients to harm and contribute billions to wasteful spending</a>.</li>\n<li><strong>Inconsistent data schemas:</strong> The RDBMS representation of raw clinical data consists of hundreds of tables that relate to one another in ways that are poorly documented (if at all). That's because the underlying schemas evolve as customizations are made to the front-end. This also means that Stanford's data schema is very different from that of Kaiser's, Sutter's, and UCSF - even though they all license their EHR software from the same vendor.</li>\n</ul>\n\n<h2>How Distal tames healthcare big data with Databricks</h2>\n\n<a href=\"https://www.distal.co\">Distal</a> offers a turnkey software solution that connects healthcare providers to the information they need to deliver exceptional care. Our clinical intelligence platform delivers clinically-meaningful insights through an easy-to-use web application. We use Databricks to build ETL pipelines that blend multiple data sources into a canonical source of truth. We also combine Distal\u2019s proprietary algorithm with MLlib's DataFrame-based pipelines to automate data modeling and clinical concept mapping.\n\nDatabricks has been key to building a cost-effective software solution that enables clinicians to engage in data-driven quality improvement. Databricks allows us to leverage the power of Apache Spark to:\n\n<ul>\n<li>ingest massive volumes of structured, semi-structured, and unstructured data from a variety of sources,</li>\n<li>apply a common semantic layer to raw data,</li>\n<li>integrate and deduplicate data elements,</li>\n<li>analyze the data,</li>\n<li>map the results to a common set of API resources,</li>\n<li>surface the insights to the end-user, and</li>\n<li>protect patient privacy through encryption, granular access controls, and detailed audit trails.</li>\n</ul>\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/01/distal-flow.png\" alt=\"How Databricks allows Distal to leverage the power of Apache Spark.\" width=\"1344\" height=\"994\" class=\"aligncenter size-full wp-image-10128\" />\n\nDatabricks components that play particularly important roles include:\n\n<ul>\n<li><strong>Notebooks and collaboration features:</strong> Databricks notebooks allowed us to quickly iterate on new ETL components and to test how they fit into complementary pipelines. We began with a single notebook that included all of the code to get the job done. At this stage, there were snippets of Java, Scala, and Python in a single notebook. Once we had a working concept, we broke out functional code blocks into separate notebooks in order to run them as independent jobs. Throughout the process, we relied on Databricks\u2019 collaboration features to a) fix bugs, b) track changes, and c) share production-ready libraries internally.</li>\n<li><strong>Databricks library manager:</strong> We felt the need to move quickly, so we incorporated open source libraries/packages into our ETL processes whenever possible. Very early on, we used Databricks to search through Spark Packages and Maven Central for the most stable, best-supported libraries. Once the pipelines began to take shape, we used the Databricks package manager to automatically attach required libraries to new clusters. </li>\n<li><strong>Integration with Amazon Web Services:</strong> The fact that Databricks is built on Amazon Web Services (AWS) allowed us to seamlessly integrate with the full spectrum of AWS services while adhering to security best practices. Rather than relying exclusively on AWS access keys, which can be lost or stolen, Databricks enabled us to use IAM roles to restrict read/write access to S3 buckets. Furthermore, we restricted access to pipelines that handle sensitive data by applying user-level access controls through the Databricks admin panel. </li>\n</ul>\n\n<h2>A hint of things to come</h2>\n\nWith Apache Spark serving as the cornerstone of our ETL and analytics stack, we are confident in our ability to develop new features and products. In the near term, we are looking to augment our ETL pipelines and knowledge representation by developing novel ML algorithms and borrowing from <a href=\"https://spark-packages.org/?q=tags%3A%22Machine%20Learning%22\">existing methods</a>. We are also looking grow our team. If you love Databricks and would like to help us build products that have a measurable impact on millions of people, please introduce yourself by sending a quick note to <a href=\"mailto:team@distal.co\">team@distal.co</a>."}
{"status": "publish", "description": null, "creator": "jakebellacera", "link": "https://databricks.com/blog/2017/01/30/integrating-central-hive-metastore-apache-spark-databricks.html", "authors": null, "id": 10180, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2017-01-30", "tz": "UTC", "createdOn": "2017-01-30"}, "title": "Integrating Your Central Apache Hive Metastore with Apache Spark on Databricks", "slug": "integrating-central-hive-metastore-apache-spark-databricks", "content": "Databricks provides a managed Apache Spark platform to simplify running production applications, real-time data exploration, and infrastructure complexity. A key piece of the infrastructure is the Apache Hive Metastore, which acts as a data catalog that abstracts away the schema and table properties to allow users to quickly access the data.\n\nThe Databricks platform provides a fully managed Hive Metastore that allows users to share a data catalog across multiple Spark clusters. We realize that users may already have a Hive Metastore that they would like to integrate with Databricks, so we also support the seamless integration with your existing Hive Metastore. This allows Databricks to integrate with existing systems such as EMR, Cloudera, or any system running a Hive Metastore. This blog outlines the technical details.\n\n<h2>Apache Hive Metastore Background</h2>\n\nHive is a component that was added on top of Hadoop to provide SQL capabilities to the big data environment. It began with a Hive client and a Hive Metastore. Users would execute queries that were compiled into a MapReduce job. To abstract the underlying data structures and file locations, Hive applied a table schema on top of the data. This schema is placed in a database and managed by a metastore process.\n\n<a href=\"https://cwiki.apache.org/confluence/display/Hive/CSV+Serde\">Example CSV Table</a>:\n\n<pre><code class=\"sql\">CREATE EXTERNAL TABLE users (name STRING, age INT)\nROW FORMAT DELIMITED FIELDS TERMINATED BY ','\nLOCATION 's3a://my-bucket/users-csv-table/';\n</code></pre>\n\n<a href=\"https://cwiki.apache.org/confluence/display/Hive/Parquet\">Example Parquet table</a>:\n\n<pre><code class=\"sql\">CREATE EXTERNAL TABLE users_parquet (name STRING, age INT)\nSTORED AS PARQUET\nLOCATION 's3a://my-bucket/users-parquet-table/';\n</code></pre>\n\nThe above examples show that the metastore abstracts the underlying file types, and allows the users to interact with the data at a higher level.\n\nTraditionally, Hadoop clusters will have one Hive Metastore per cluster. A cluster would be composed of Apache HDFS, Yarn, Hive, Spark. The Hive Metastore has a metastore proxy service that users connect to, and the data is stored in a relational database. <a href=\"https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin\">Hive supports a variety of backend databases to host the defined schema</a>, including MySql, Postgres, Oracle.\n\n[caption id=\"attachment_10182\" align=\"aligncenter\" width=\"1690\"]<img src=\"https://databricks.com/wp-content/uploads/2017/01/example-aws-to-databricks-hive-metastore.png\" alt=\"AWS to Databricks Hive metastore diagram\" width=\"1690\" height=\"662\" class=\"size-full wp-image-10182\" /> Example of AWS Infrastructure to connect Databricks to a Central Hive Metastore in\u00a0a\u00a0peer\u00a0VPC.[/caption]\n\n<h2>Supported Apache Hive Versions</h2>\n\n<a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#interacting-with-different-versions-of-hive-metastore\">Apache Spark supports multiple versions of Hive, from 0.12 up to 1.2.1.</a> This allows users to connect to the metastore to access table definitions. Configurations for setting up a central Hive Metastore can be challenging to verify that the corrects jars are loaded, the correction configurations are applied, and the proper versions are supported.\n\n<h2>Integration How-To</h2>\n\nIn the cloud, clusters are viewed as transient compute resources. Customers want the flexibility and elasticity of a cloud environment by leveraging the fact that compute resources can be shut down. One item that needs to be highly available is the Hive Metastore process. There are two ways to integrate with the Hive Metastore process.\n\n<ol>\n<li>Connect directly to the backend database</li>\n<li>Configure clusters to connect to the Hive Metastore proxy server</li>\n</ol>\n\nUsers follow option #2 if they need to integrate with a legacy system. Note that this has additional costs in the cloud as the proxy service needs to run 24x7 and only acts as a proxy. Option #1 removes the overhead of proxy services and allows a user to connect directly to the backend database. We can leverage AWS\u2019 hosting capabilities to maintain this as an RDS instance. Option #1 is recommended and discussed below. For instructions on option #2, read our documentation link at the end.\n\n<h2>Proposed Configuration Docs</h2>\n\nThe configuration process will use Databricks specific tools called the <a href=\"https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html\">Databricks File System APIs</a>. The tools allow you to create bootstrap scripts for your cluster, read and write to the underlying S3 filesystem, etc. Below is the configuration guidelines to help integrate the Databricks environment with your existing Hive Metastore.\n\nThe configurations below use a <a href=\"https://docs.databricks.com/user-guide/advanced/init-scripts.html\">bootstrap script</a> to install the Hive Metastore configuration to a specific cluster name, e.g. replace <code>${cluster-name}</code> with <code>hive-test</code> to test central metastore connectivity.\n\nOnce tested, you can deploy the init script in the root directory to be configured for every cluster.\n\n<h3>Configuration</h3>\n\n<pre><code>%scala\n\ndbutils.fs.put(\n  \"/databricks/init/${cluster-name}/external-metastore.sh\",\n  \"\"\"#!/bin/sh\n    |\n    |# Quoting the label (i.e. EOF) with single quotes to disable variable interpolation.\n    |cat &lt;&lt; 'EOF' &gt; /databricks/driver/conf/00-custom-spark.conf\n    |[driver] {\n    |    # Hive specific configuration options for metastores in the local mode.\n    |    # spark.hadoop prefix is added to make sure these Hive specific options will propagate to the metastore client.\n    |    \"spark.hadoop.javax.jdo.option.ConnectionURL\" = \"jdbc:mysql://${mysql-host}:${mysql-port}/${metastore-db}\"\n    |    \"spark.hadoop.javax.jdo.option.ConnectionDriverName\" = \"com.mysql.jdbc.Driver\"\n    |    \"spark.hadoop.javax.jdo.option.ConnectionUserName\" = \"${mysql-username}\"\n    |    \"spark.hadoop.javax.jdo.option.ConnectionPassword\" = \"${mysql-password}\"\n    |\n    |    # Spark specific configuration options\n    |    \"spark.sql.hive.metastore.version\" = \"${hive-version}\"\n    |    # Skip this one if ${hive-version} is 0.13.x.\n    |    \"spark.sql.hive.metastore.jars\" = \"${hive-jar-source}\"\n    |\n    |    # If any of your table or database use s3 as the file system scheme,\n    |    # please uncomment the next line to set the s3:// URL scheme to S3A file system.\n    |    # spark.hadoop prefix is added to make sure these file system options will\n    |    # propagate to the metastore client and Hadoop configuration.\n    |    # \"spark.hadoop.fs.s3.impl\" = \"com.databricks.s3a.S3AFileSystem\"\n    |\n    |    # If you need to use AssumeRole, please uncomment the following settings.\n    |    # \"spark.hadoop.fs.s3a.impl\" = \"com.databricks.s3a.S3AFileSystem\"\n    |    # \"spark.hadoop.fs.s3n.impl\" = \"com.databricks.s3a.S3AFileSystem\"\n    |    # \"spark.hadoop.fs.s3a.credentialsType\" = \"AssumeRole\"\n    |    # \"spark.hadoop.fs.s3a.stsAssumeRole.arn\" = \"${sts-arn}\"\n    |}\n    |EOF\n    |\"\"\".stripMargin,\n  overwrite = true\n)\n</code></pre>\n\n<h2>A Word of Caution</h2>\n\nThe configurations help bootstrap all clusters in the environment, which can lead to an outage if changes are made to the metastore environment.\n\n<ul>\n<li>Broken network connectivity</li>\n<li>Changes to the underlying Hive version</li>\n<li>Changes to metastore credentials </li>\n<li>Firewall rules added to the metastore</li>\n</ul>\n\nTo temporarily resolve the issue, you can use the <a href=\"https://docs.databricks.com/api/latest/dbfs.html\">Databricks Rest API</a> to collect the current contents of the bootstrap script and remove it while you work on fixing the issue.\n\n<pre><code class=\"py\"># Returns a tuple of credentials for the rest api calls\ntCreds = ('myuser@example.com', 'myStrongPassword')\n\nimport json, pprint, requests\n\npath_to_script = '/databricks/init/{clusterName}/external-metastore.sh'\nenv_url = 'https://yourenv.cloud.databricks.com'\n\n# Helper to pretty print json\ndef pprint_j(i):\n  print json.dumps(json.loads(i), indent=4, sort_keys=True)\n\n\n# Read Example\nread_payload = {\n  'path' : path_to_script\n}\n\nresp = requests.get(env_url + '/api/2.0/dbfs/read', auth=tCreds, json = read_payload)\nresults = resp.content\npprint_j(results)\nprint resp.status_code\n# Decode the base64 binary strings \nprint json.loads(results)['data'].decode('base64')\n\n# Delete example\ndelete_payload = {\n  'path' : path_to_script,\n  'recursive' : 'false'\n}\n\nresp = requests.post(env_url + '/api/2.0/dbfs/delete', auth=tCreds, json = delete_payload)\nresults = resp.content\npprint_j(results)\nprint resp.status_code\n</code></pre>\n\nIf you want to configure clusters to connect to the Hive Metastore proxy server, you can find instructions in our <a href=\"https://docs.databricks.com/user-guide/advanced/external-hive-metastore.html\">Hive Metastore online guide</a>.\n\n<h2>What\u2019s Next</h2>\n\nDatabricks is a very fast and easy way to start coding and analyzing data on Apache Spark. If you would like to get started with Spark and have an existing Hive Metastore you need help to integrate, you can get in touch with one of the solution architects through our <a href=\"http://go.databricks.com/contact-databricks\">Contact Us</a> page. To get a head start, <a href=\"https://databricks.com/try\">sign up for a free trial of Databricks</a> and try out some of the live exercises we have in the <a href=\"https://docs.databricks.com/\">documentation</a>.\n\nIf you need help with configuring VPC peering within Databricks, check out our <a href=\"https://docs.databricks.com/user-guide/cloud-configurations/aws/vpc-peering.html\">VPC peering documentation</a>."}
{"status": "publish", "description": null, "creator": "Wayne Chan", "link": "https://databricks.com/blog/2017/01/31/announcing-spark-live-2017-world-tour.html", "authors": null, "id": 10189, "categories": ["Company Blog", "Events", "Product"], "dates": {"publishedOn": "2017-01-31", "tz": "UTC", "createdOn": "2017-01-31"}, "title": "Announcing the Spark Live 2017 World Tour", "slug": "announcing-spark-live-2017-world-tour", "content": "<img src=\"https://databricks.com/wp-content/uploads/2016/04/spark-live-og.jpg\" alt=\"Databricks Spark Live 2017 Tour\" width=\"1800\" height=\"945\" class=\"aligncenter size-full wp-image-9954\" />\n\nDue to the <a href=\"https://databricks.com/blog/2017/01/03/spark-live-2016-tour-recap.html\">enthusiasm and positive feedback</a> from last year\u2019s Spark Live tour, we will be hitting the road again in 2017 to continue our mission of bringing Apache Spark and Databricks to the masses. This time, we\u2019ll be visiting more cities and expanding our reach across the globe including Atlanta, Austin, DC, London, Los Angeles, New York, Reston, San Jose, Seattle, Sydney, and Toronto!\n\nIf you weren\u2019t one of the 1000+ attendees of Spark Live from 2016, this is an event you won\u2019t want to miss. Spark Live is a complimentary one-day workshop for data professionals and IT leaders who want to learn how to leverage the power of open source Apache Spark and Databricks to make transformative use cases a reality.\n\nOur agenda will be packed with valuable insights from the creators of Spark and Databricks technical experts, discussions on how to deploy Spark in the enterprise, demos from our customers, hands-on labs, and technical training from the foremost Spark experts in the world. Whether you are new to Spark or are a seasoned veteran wanting to increase your expertise, Spark Live will have something for everyone.\n\nFurthermore, all attendees will receive free ongoing access to Databricks Community Edition after the event\u2014providing you with the ideal platform to continue your journey with open source Apache Spark for as long as you want.\n\n<strong>Interested in attending a Spark Live in 2017?</strong>\n<a href=\"https://databricks.com/spark-live\">Review the schedule</a> and request your spot today!\n\n<strong>Interesting in sponsoring Spark Live?</strong>\nPlease send inquiries to <a href=\"mailto:events@databricks.com\">events@databricks.com</a>."}
{"status": "publish", "description": null, "creator": "joseph", "link": "https://databricks.com/blog/2017/02/09/intels-bigdl-databricks.html", "authors": null, "id": 10228, "categories": ["Apache Spark", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2017-02-09", "tz": "UTC", "createdOn": "2017-02-09"}, "title": "Intel\u2019s BigDL on Databricks", "slug": "intels-bigdl-databricks", "content": "[dbce_cta href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/5669198905533692/2626293254583012/3983381308530741/latest.html\"]Try this notebook on Databricks[/dbce_cta]\n\nIntel recently released its <a href=\"https://software.intel.com/en-us/articles/bigdl-distributed-deep-learning-on-apache-spark\">BigDL project</a> for distributed deep learning on Apache Spark. BigDL has native Spark integration, allowing it to leverage Spark during model training, prediction, and tuning. This blog post gives highlights of BigDL and a tutorial showing how to get started with BigDL on Databricks.\n\n<h2>Intel\u2019s BigDL project</h2>\n\nBigDL is an open source deep learning library from Intel.  Modeled after Torch, BigDL provides functionality both for low-level numeric computing and high-level neural networks.  BigDL is written on top of Apache Spark, allowing easy scale-out computing.\n\n<h3>Native Apache Spark integration</h3>\n\nBigDL\u2019s native Spark integration separates BigDL from many other deep learning projects.  All major deep learning libraries can be integrated with Spark (see <a href=\"https://databricks.com/blog/2016/12/21/deep-learning-on-databricks.html\">our previous blog post</a>), but many libraries are best integrated by running  the libraries separately on each Spark worker.  This setup makes it easy to use Spark to distribute certain tasks, such as prediction and model tuning, but harder to distribute model training.\n\nSince BigDL is built on top of Spark, it makes it easy to distribute model training, one of the most computationally intensive parts of deep learning.  The user does not have to handle distributing computation explicitly. Instead, BigDL automatically spreads the work across a Spark cluster.\n\n<h3>Leveraging recent CPU architectures</h3>\n\nCompared with other deep learning frameworks running on CPUs, BigDL also achieves speedups by leveraging the latest Intel architecture. In particular, it ships with the Intel Math Kernel Library (MKL), which can accelerate the heavy numerical computations required for deep learning. Check out <a href=\"https://software.intel.com/en-us/articles/bigdl-distributed-deep-learning-on-apache-spark\">Intel\u2019s BigDL article</a> and the <a href=\"https://github.com/intel-analytics/BigDL\">BigDL GitHub page</a> for details.\n\n<h2>Tutorial: using BigDL on Databricks</h2>\n\nIn the rest of this blog post, we will walk through an example of training a deep neural network using BigDL on Databricks. Our application is a classic handwritten digit recognition problem using the MNIST dataset.\n\n<img class=\"aligncenter size-full wp-image-7795\" src=\"https://databricks.com/wp-content/uploads/2016/05/handwritten-digits.jpg\" alt=\"Screenshot of handwritten digits.\" width=\"438\" height=\"145\">\n\nGiven a dataset of handwritten digits, plus the true labels (0-9), we will use BigDL to train the <a href=\"http://yann.lecun.com/exdb/lenet/\">LeNet 5 network model</a>. Our trained model will be able to take new images and infer their digits.  This blog post gives a high-level description of the workflow, and you can check out the companion <a href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/5669198905533692/2626293254583012/3983381308530741/latest.html\">Databricks notebook</a> for the full details. The material largely comes from <a href=\"https://github.com/intel-analytics/BigDL/wiki/Tutorials\">this BigDL tutorial</a> and the <a href=\"https://github.com/intel-analytics/BigDL/tree/master/dl/src/main/scala/com/intel/analytics/bigdl/models/lenet\">BigDL example code</a>.\n\n<h3>Set up a Spark cluster</h3>\n\nTo set up BigDL on a Spark cluster, you will need to:\n\n<ul>\n<li>Build BigDL</li>\n<li>Create and configure your cluster --- This is a critical step in getting the best performance from BigDL.</li>\n<li>Import BigDL and attach the library to your cluster</li>\n</ul>\n\nRefer to the <a href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/5669198905533692/2626293254583012/3983381308530741/latest.html\">companion notebook</a> for details on building BigDL and configuring your cluster.\n\n<h3>Initialize BigDL</h3>\n\nBigDL takes some special configurations telling it the dimensions of your Spark cluster. Given these dimensions, BigDL will figure out how to split tasks across the workers.  Key configurations include:\n\n<ul>\n<li><code>nodeNumber</code>: the number of Spark executor nodes</li>\n<li><code>coreNumber</code>: the number of cores per executor node</li>\n<li><code>batchSize</code>: the number of data rows to be processed per iteration</li>\n</ul>\n\nLook at the <code>Engine.init</code> call in the <a href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/5669198905533692/2626293254583012/3983381308530741/latest.html\">companion notebook</a> to see how initialization is done.\n\n<h3>Download and prepare the MNIST data</h3>\n\nTo help you get started, we provide a script for downloading the MNIST byte data files, as well as methods for loading those byte files. Finally, we use those images to load our training and validation sets. For example, the snippet below loads the raw byte data, converts it to grayscale images, normalizes the image features, and groups the images into batches for training.\n\n<pre><code class=\"scala\">val trainSet = DataSet.array(load(trainData, trainLabel), sc) -&gt; \n  BytesToGreyImg(28, 28) -&gt;\n  GreyImgNormalizer(trainMean, trainStd) -&gt;\n  GreyImgToBatch(param.batchSize)\n</code></pre>\n\n<h3>Train the model</h3>\n\nWith the data loaded, we can now train our LeNet model to learn the network parameters.  BigDL is using Stochastic Gradient Descent (SGD) for learning, so key learning parameters to tweak are:\n\n<ul>\n<li><code>learningRate</code>: \u201cspeed\u201d at which to learn, where smaller values help avoid local optima but larger values produce more progress on each iteration</li>\n<li><code>maxEpoch</code>: max number of outermost epochs, or iterations, for training</li>\n</ul>\n\nWe create an initial model using the provided LeNet 5 network structure, specifying that we are predicting 10 classes (digits 0-9):\n\n<pre><code class=\"scala\">val initialModel = LeNet5(10)\n</code></pre>\n\nWe next specify the optimizer, which includes the learning criterion we want to optimize.  In this case, we minimize the class negative log likelihood criterion.\n\n<pre><code class=\"scala\">val optimizer = Optimizer(\n  model = initialModel,\n  dataset = trainSet,\n  criterion = ClassNLLCriterion[Float]())\n</code></pre>\n\nFinally, we specify a validation criterion (<code>Top1Accuracy</code>) for tracking accuracy on our test set, as well as a termination criterion (<code>Trigger.maxEpoch</code>) for deciding when to stop training.  We call <code>optimize()</code>, and BigDL starts training!\n\n<h3>Make predictions and evaluate</h3>\n\nWith our <code>trainedModel</code>, we can now make predictions and evaluate accuracy on new data.  BigDL provides a set of evaluation metrics via Validator and ValidationMethod classes.\n\n<pre><code class=\"scala\">val validator = Validator(trainedModel, validationSet)\nval result = validator.test(Array(new Top1Accuracy[Float]))\n</code></pre>\n\nInspecting <code>result</code>, we can see that our model achieved about 99% accuracy, making correct predictions on 9884 out of 10,000 validation examples.\n\n<h2>Next steps</h2>\n\nWe have given a quick overview of BigDL and how to get started. To learn more about BigDL, we recommend checking out the <a href=\"https://github.com/intel-analytics/BigDL\">BigDL GitHub page</a> and the <a href=\"https://spark-summit.org/east-2017/events/bigdl-a-distributed-deep-learning-library-on-spark/\">BigDL talk</a> at <a href=\"https://spark-summit.org/east-2017/\">Spark Summit Boston 2017</a> (to be posted online at this link soon after the summit).\n\nTo get started with BigDL in Databricks, try out the <a href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/5669198905533692/2626293254583012/3983381308530741/latest.html\">companion notebook</a> for free on <a href=\"https://databricks.com/try\">Databricks Community Edition</a>. The notebook provides a much more detailed walkthrough. Try tuning the model or learning settings to achieve even higher accuracy!"}
{"status": "publish", "description": null, "creator": "jules_damji", "link": "https://databricks.com/blog/2017/02/09/spark-summit-east-2017-another-record-setting-spark-summit.html", "authors": null, "id": 10242, "categories": ["Company Blog", "Events"], "dates": {"publishedOn": "2017-02-10", "tz": "UTC", "createdOn": "2017-02-10"}, "title": "Spark Summit East 2017: Another Record-Setting Spark Summit", "slug": "spark-summit-east-2017-another-record-setting-spark-summit", "content": "<img class=\"aligncenter size-full wp-image-10253\" src=\"https://databricks.com/wp-content/uploads/2017/02/recap-hero.jpg\" alt=\"\" width=\"700\" height=\"525\" />\n\nWe\u2019ve put together a short recap of the keynotes and highlights from Databricks\u2019 speakers for Apache Spark enthusiasts who could not attend the summit\u2014to enjoy the Patriots\u2019 Super-Bowl victory euphoria that suffused the Spark Summit attendees: the snowstorm outside did not dampen the high spirits inside.\n<h2>Day One: Voices of Databricks Speakers</h2>\n<h3>What to Expect for Big Data and Apache Spark in 2017</h3>\nMatei Zaharia led us through the journey of how Spark evolved from its early days, advancing in lockstep and taking advantage of (or overcoming shortcomings) three particular evolutionary trends in the industry: Hardware, Users, and Applications. Watch his keynote where he expounds on these three trends.\n\n<a href=\"http://www.slideshare.net/databricks/spark-summit-east-2017-matei-zaharia-keynote-trends-for-big-data-and-apache-spark-in-2017\">View the slides for this talk.</a>\n\n<iframe style=\"margin-left: auto; margin-right: auto;\" src=\"https://www.youtube.com/embed/vtxwXSGl9V8?list=PLTPXxbhUt-YVEyOqTmZ_X_tpzOlJLiU2k\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n<h3>Insights Without Tradeoffs: Using Structured Streaming in Apache Spark</h3>\nEchoing what Matei referred to as big data trends moving to production use, Michael Armbrust demonstrated how structured streaming allows developers to derive valuable insights without tradeoffs. All this is possible because, though streaming in general demands parallelism and complexity, structure streaming alleviates the burden of fault-tolerance, exactly-once semantics, data consistency, and integrity. In this talk and demo, Michael articulates Matei\u2019s vision of continuous applications and demonstrates the ease of use of Structured Streaming APIs in a live demo.\n\n<a href=\"http://www.slideshare.net/databricks/spark-summit-east-2017-michael-armbrust-keynote-insights-without-tradeoffs-using-structured-streaming\">View the slides for this talk.</a>\n\n<iframe style=\"margin-left: auto; margin-right: auto;\" src=\"https://www.youtube.com/embed/IJmFTXvUZgY?list=PLTPXxbhUt-YVEyOqTmZ_X_tpzOlJLiU2k\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n<h3>RISELab: Enabling Intelligent Real-Time Decisions</h3>\nLate last year, the <a href=\"https://www.youtube.com/user/BerkeleyAMPLab\">UC Berkeley AMPLab</a> shut down after producing notable projects such as Apache Spark, Apache Mesos, Alluxio, BlinkDB, and others. To continue its transformative contributions to research and industry, AMPLab transitioned into a new phase of innovation: RISELab (Real-time Intelligent Secure Execution).\n\nToday in his keynote Ion Stoica shared with the community his vision and goal of RISELab. In particular, he stressed researchers will work on AI, robotics, security, and fast distributed data systems at scale.\n\nWhile AMPLab projects enabled batch data processing to advanced analytics, RISELab projects will engender and enable live data to real-time decisions, Stoica said.\n\n<img class=\"aligncenter size-full wp-image-10243\" src=\"https://databricks.com/wp-content/uploads/2017/02/amplab-to-riselab-slide.png\" alt=\"The RISELab will focus on harnessing live data from real-time decisions.\" width=\"700\" height=\"392\" />\n\nCommitted to the goal of building open-source frameworks, tools, and algorithms that make building real-time applications decisions on live data with stronger security, this new phase is set to innovate and enhance Spark with two projects\u2014Drizzle and Opaque\u2014Stoica said.\n\nWhile Drizzle reduces Apache Spark\u2019s streaming latency by a factor of ten and bolsters fault-tolerance, Opaque enhances Spark\u2019s data encryption-at-rest or in-motion, offering stronger protection in cloud or on-premise.\n<h3>Making Structured Streaming Ready for Production - Updates and Future Directions</h3>\nFor much of Spark\u2019s short history, Spark streaming has continued to evolve\u2014all to simplify writing streaming applications. Today, developers need more than just a streaming programming model to transform elements in a stream. Instead, they need a streaming model that supports end-to-end applications that continuously react to data in real-time.\n\n<img class=\"aligncenter size-full wp-image-10244\" src=\"https://databricks.com/wp-content/uploads/2017/02/tathagata-das-spark-summit-east-2017.jpg\" alt=\"Tathagata Das speaking at Spark Summit East 2017.\" width=\"700\" height=\"399\" />\n\nTathagata Das (TD) shared how Structured Streaming, with new features introduced in Apache Spark 2.x, allow developers to write a fault-tolerant, end-to-end streaming applications, using the unified DataFrame/Dataset APIs built atop the SparkSQL engine. Whether you doing a streaming ETL, joining real-time data from Kafka with static data, or aggregating data and updating your sinks, Structured Streaming, TD emphasized, ensures all aspects of reliability and repeatability. Looking ahead, he hinted new features, such as sessionization, support for Apache Kafka, and more windowing operations\u2014planned for future releases.\n\n<a href=\"https://www.slideshare.net/databricks/making-structured-streaming-ready-for-production\">View the slides for this talk.</a>\n<h3>Optimizing Apache Spark SQL Joins</h3>\nVida Ha takes on one of the primary sources of performance problems in Apache Spark - Spark SQL joins. She demystified how joins work in Spark, and described best practices to debug and optimize performance.\n\n<a href=\"https://www.slideshare.net/databricks/optimizing-apache-spark-sql-joins\">View the slides for this talk.</a>\n<h3>Women In Big Data</h3>\nSeveral inspiring female leaders in big data came together for a luncheon and panel discussion hosted by the Women in Big Data Forum. Their goal is to strengthen diversity in the industry by helping female talent connect, engage, and grow in the field of big data and analytics. The event kicked off with a keynote presentation by Kavitha Mariappan, VP of marketing at Databricks. Kavitha spoke to the \u201cleaky pipeline\u201d problem facing our industry and gave actionable advice on how to make a mark as a woman in the world of big data.\n\n<img class=\"aligncenter size-full wp-image-10245\" src=\"https://databricks.com/wp-content/uploads/2017/02/women-in-big-data.jpg\" alt=\"Kavitha Mariappan gave a keynote at the Women in Big Data panel.\" width=\"700\" height=\"280\" />\n\nFollowing the keynote was a panel of esteemed technology leaders including Ziya Ma, VP of Big Data at Intel, Business Unit Executive at IBM, Julie Greenway, Head of Big Data Analytics at Bose, and Gunjan Sharma, Head of Big Data Analytics at Capital One. Moderated by Donna Fernandez, COO of MetiStream, the panel debated and weighed in on why this dilemma exists and how as female leaders and role models, they need to continue to work towards a more equitable work environment.\n\n<a href=\"http://www.slideshare.net/databricks/the-leaky-pipeline-problem-making-your-mark-as-a-woman-in-big-data\">View the slides for this talk.</a>\n<h3>Tuning and Monitoring Deep Learning on Apache Spark</h3>\nThe increasing rapid growth of deep learning in data analytics, cyber security, fraud detection, and database systems have huge potential for impact in research and across industries. Tim Hunter, software engineer at Databricks and contributor to the <a href=\"http://spark.apache.org/mllib/\">Apache Spark MLlib project</a> and leading deep learning expert, took the stage and dove into a number of common challenges users face when integrating deep learning libraries with Spark such as needing to optimize cluster setup and data ingest, tuning the cluster, and monitoring long-running jobs. He then demonstrated various tuning techniques using the popular TensorFlow library from Google and shared best practices for building deep learning pipelines with Spark.\n\n<a href=\"http://www.slideshare.net/databricks/17-0207-spark-summit-east-tuning-and-monitoring-deep-learning\">View the slides for this talk.</a>\n\n<img class=\"aligncenter size-full wp-image-10246\" src=\"https://databricks.com/wp-content/uploads/2017/02/tim-hunter-spark-summit-east-2017.jpg\" alt=\"Tim Hunter speaking at Spark Summit East 2017\" width=\"700\" height=\"358\" />\n<h2>Day Two: Voices of Databricks Speakers</h2>\n<h3>Virtualizing Analytics with Apache Spark</h3>\nHow can enterprises use more of their data to make decisions and act with more intelligence? This question kicked off the keynote on day 2. Databricks\u2019 VP of Field Engineering, Arsalan Tavakoli-Shiraji, described how increasing diversity across the data, analytics, and end-user groups is creating a multi-faceted challenge for enterprises today.\n\n<img class=\"aligncenter size-full wp-image-10247\" src=\"https://databricks.com/wp-content/uploads/2017/02/arsalan-spark-summit-east-2017.jpg\" alt=\"Arsalan Tavakoli-Shiraji gave a keynote on day 3 of Spark Summit East 2017\" width=\"700\" height=\"418\" />\n\nThe solution, observed Arsalan, lies not in traditional data warehouses nor Hadoop data lakes, which are artifacts from an era with much more limited problems. Rather, this new challenge requires an entirely different paradigm based on four key principles:\n<ol>\n \t<li>Decoupled compute and storage</li>\n \t<li>Uniform data management and security model</li>\n \t<li>Unified analytics engine</li>\n \t<li>Enterprise-wide collaboration</li>\n</ol>\n<iframe style=\"margin-left: auto; margin-right: auto;\" src=\"https://www.youtube.com/embed/U3Jx21QNInc?list=PLTPXxbhUt-YVEyOqTmZ_X_tpzOlJLiU2k\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\nArsalan noted that while Spark is the perfect engine to power this new paradigm, there are many operational needs around the engine in an enterprise setting. These enterprise requirements ultimately drove Databricks to build a platform around Spark that enables them to reap the benefits of Spark faster, easier, and cheaper.\n<h3>Exceptions are the Norm: Dealing with Bad Actors in ETL</h3>\nBig Data is messy; data is deeply nested; and data is often \u201cincorrect, incomplete or inconsistent,\u201d explained Sameer Agarwal, as he opened as the first talk of the Developer track after the keynotes. He explained how the new features in Apache Spark 2.x are able to handle nested types, isolate input records for informative diagnostics, and improve the robustness of your ETLs.\n\nHe suggested techniques in how to deal and handle these \u201cbad actors\u201d in ETL.\n\n<a href=\"http://www.slideshare.net/databricks/exceptions-are-the-norm-dealing-with-bad-actors-in-etl\">View the slides for this talk.</a>\n\n<img class=\"aligncenter size-full wp-image-10248\" src=\"https://databricks.com/wp-content/uploads/2017/02/sameer-agarwal-speaking-at-spark-summit-east-2017.jpg\" alt=\"Sameer Agarwal speaking at Spark Summit East 2017.\" width=\"700\" height=\"319\" />\n<h3>Robust and Scalable ETL Over Cloud Storage with Spark</h3>\nWhen it comes to data processing pains, ETL tends to be near the top of the list for most of them. As data continues to make its move into the cloud, in order to optimize for performance, it is preferable for Spark to access data directly from services such as Amazon S3, thereby decoupling storage and compute. However, there are some limitations to object stores such as S3 that can impact ETL jobs performance. Eric Liang, a Databricks software engineer, exposed these issues in great detail and provides a path forward with Databricks, which provides a highly optimized S3 access layer that gives you blazing fast read/write performance.\n\n<a href=\"http://www.slideshare.net/databricks/robust-and-scalable-etl-over-cloud-storage-with-apache-spark\">View the slides for this talk.</a>\n\n<img class=\"aligncenter size-full wp-image-10249\" src=\"https://databricks.com/wp-content/uploads/2017/02/eric-liang-spark-summit-east-2017.jpg\" alt=\"Eric Liang speaking at Spark Summit East 2017\" width=\"700\" height=\"519\" />\n<h3>SparkSQL: A Compiler from Queries to RDDs</h3>\nTo a packed room\u2014even though it was the last talk on the last day\u2014Sameer Agrawal led us on an incisive journey and a peek under the hood, tracing a lifecycle a query (or high-level computation) undergoes with the SparkSQL engine. Using a compiler analogy, he outlined how SparkSQL parses, analyzes, optimizes, plans, and executes a user\u2019s query\u2014all along its phases taking advantage of the optimal benefits and techniques of the Catalyst Optimizer and Tungsten\u2019 whole-stage code generation.\n\n<a href=\"http://www.slideshare.net/databricks/sparksql-a-compiler-from-queries-to-rdds\">View the slides for this talk.</a>\n\n<img class=\"aligncenter size-full wp-image-10250\" src=\"https://databricks.com/wp-content/uploads/2017/02/sameer-blog-second-talk.png\" alt=\"An overview of how Catalyst works\" width=\"700\" height=\"392\" />\n<h3>Parallelizing Existing R Packages with SparkR</h3>\nApache Spark 2.0 is a big step forward for SparkR. Hossein Falaki \u2013 one of the major contributors to SparkR - provided an overview of the new SparkR API. His talk described how to use this API to parallelize existing R packages, with special considerations for performance and correctness.\n\n<a href=\"http://www.slideshare.net/databricks/parallelizing-existing-r-packages-with-sparkr\">View the slides for this talk.</a>\n\n<img class=\"aligncenter size-full wp-image-10251\" src=\"https://databricks.com/wp-content/uploads/2017/02/hossein-spark-summit-east-2017.jpg\" alt=\"Hossein Falaki speaking at Spark Summit East 2017\" width=\"700\" height=\"430\" />\n<h3>Keeping Spark on Track: Productionizing Apache Spark for ETL</h3>\nBuilding production-quality ETL pipelines is one of the most common use cases for Apache Spark. Databricks solution engineers Kyle Pistor and Miklos Christine took the stage to share a wealth of their accumulated knowledge on how to build fast and robust ETL pipelines with Spark, based on their extensive experience working with Databricks customers.\n\n<a href=\"http://www.slideshare.net/databricks/keeping-spark-on-track-productionizing-spark-for-etl\">View the slides for this talk.</a>\n\n<img class=\"aligncenter size-full wp-image-10252\" src=\"https://databricks.com/wp-content/uploads/2017/02/kyle-miklos-spark-summit-east-2017.jpg\" alt=\"Kyle Pistor and Miklos Christine speaking at Spark Summit East 2017\" width=\"700\" height=\"366\" />\n<h2>What\u2019s Next</h2>\nVideos and slides of every talk will be posted on the <a href=\"https://spark-summit.org/east-2017/schedule/\">Spark Summit website</a> in two weeks. <a href=\"https://twitter.com/databricks\">Follow Databricks on Twitter</a> or <a href=\"http://go.databricks.com/newsletter-registration\">subscribe to our newsletter</a> to get notified when the new content becomes available.\n\nIn the meantime, learn Apache Spark on the Community Edition or build a production Spark application with a <a href=\"https://databricks.com/try\">free trial of Databricks today</a>!"}
{"status": "publish", "description": null, "creator": "jakebellacera", "link": "https://databricks.com/blog/2017/02/13/anonymizing-datasets-at-scale-leveraging-databricks-interoperability.html", "authors": null, "id": 10259, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2017-02-13", "tz": "UTC", "createdOn": "2017-02-13"}, "title": "Anonymizing Datasets at Scale Leveraging Databricks Interoperability", "slug": "anonymizing-datasets-at-scale-leveraging-databricks-interoperability", "content": "A key challenge for data-driven companies across a wide range of industries is how to leverage the benefits of analytics at scale when working with Personally Identifiable Information (PII). In this blog, we walk through how to leverage Databricks and the 3rd party Faker library to anonymize sensitive datasets in the context of broader analytical pipelines.\n\n<h2>Problem Overview</h2>\n\nData anonymization is often the first step performed when preparing data for analysis.  It is important that anonymization preserves the integrity of the data.  This blog focuses on a simple example that requires the anonymization of four fields: name, email, social security number and phone number. This sounds easy, right? The challenge is we must preserve the semantic relationships and distributions from the source dataset in our target dataset.  This allows us to later analyze and mine the data for important patterns related to the semantic relationships and distribution. So, what happens if there are multiple rows per user? Real data often contains redundant data like rows with repeated full names, emails etcetera that may or may not have different features associated with them.  Therefore, we will need to maintain a mapping of profile information.\n\nWe want to anonymize the data but maintain the relationship between a given person and their features.  So, if John Doe appears in our data set multiple times, we want to anonymize all of John Doe\u2019s PII data while maintaining the relationship between John Doe and the different feature sets associated with John.  However, we also must guard against someone reverse engineering our anonymization.  To do this, we will use a Python package, known as Faker, designed to generate fake data.   Each call to the method <code>fake.name()</code> yields a different random result.  This means that the same source file will yield a different set of anonymized data in the target file each time Faker is run against the source file.  However, the semantic relationships and distributions will be maintained.\n\n[caption id=\"attachment_10260\" align=\"aligncenter\" width=\"916\"]<img src=\"https://databricks.com/wp-content/uploads/2017/02/don-healthcare-fig1.png\" alt=\"\" width=\"916\" height=\"483\" class=\"size-full wp-image-10260\" /> Figure 1 - Source[/caption]\n\n[caption id=\"attachment_10261\" align=\"aligncenter\" width=\"955\"]<img src=\"https://databricks.com/wp-content/uploads/2017/02/don-healthcare-fig2.png\" alt=\"\" width=\"955\" height=\"487\" class=\"size-full wp-image-10261\" /> Figure 2 - Target[/caption]\n\n<h2>The Solution: Anonymizing Data While Maintaining Semantic Relationships and Distributions.</h2>\n\nFirst, let\u2019s install Faker for anonymization and <code>unicodecsv</code> so we can handle unicode strings without a hassle.\n\n<pre><code>%sh pip install Faker unicodecsv\n</code></pre>\n\nSecond, let\u2019s import our packages into the Databricks Notebook.  The unicodecsv is a drop-in replacement for Python 2.7's csv module which supports unicode strings without a hassle.  Faker is a Python package that generates fake data.\n\n<pre><code class=\"py\">import unicodecsv as csv\nfrom faker import Factory\n</code></pre>\n\nThird, let\u2019s build the function that will anonymize the data\n\n<pre><code class=\"py\">def anonymize_rows(rows):\n    \"\"\"\n    Rows is an iterable of dictionaries that contain name and\n    email fields that need to be anonymized.\n    \"\"\"\n    # Load faker\n    faker  = Factory.create()\n\n    # Create mappings of names, emails, social security numbers, and phone numbers to faked names &amp; emails.\n    names  = defaultdict(faker.name)\n    emails = defaultdict(faker.email)\n    ssns = defaultdict(faker.ssn)\n    phone_numbers = defaultdict(faker.phone_number)\n\n    # Iterate over the rows from the file and yield anonymized rows.\n    for row in rows:\n        # Replace name and email fields with faked fields.\n        row[\"name\"]  = names[row[\"name\"]]\n        row[\"email\"] = emails[row[\"email\"]]\n        row[\"ssn\"] = ssns[row[\"ssn\"]]\n        row[\"phone_number\"] = phone_numbers[row[\"phone_number\"]]\n\n\n        # Yield the row back to the caller\n        yield row\n</code></pre>\n\nNext, let\u2019s build the function that takes our source file and outputs our target file:\n\n<pre><code class=\"py\">def anonymize(source, target):\n    \"\"\"\n    The source argument is a path to a CSV file containing data to anonymize,\n    while target is a path to write the anonymized CSV data to.\n    \"\"\"\n    with open(source, 'rU') as f:\n        with open(target, 'w') as o:\n            # Use the DictReader to easily extract fields\n            reader = csv.DictReader(f)\n            writer = csv.DictWriter(o, reader.fieldnames)\n\n            # Read and anonymize data, writing to target file.\n            for row in anonymize_rows(reader):\n                writer.writerow(row)\n</code></pre>\n\nFinally, we will pass our source file to the <em>anonymize</em> function and provide the name and location of the target file we would like it to create. Here we are using the DBFS functionality of Databricks, <a href=\"https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html\">see the DBFS documentation</a> to learn more about how it works.\n\n<pre><code class=\"py\">anonymize(\"/dbfs/databricks/mydir/test.txt\", \"/dbfs/databricks/mydir/anonymized.txt\")\n</code></pre>\n\nYou can see all the code in a Databricks notebook <a href=\"http://go.databricks.com/hubfs/notebooks/blogs/Healthcare%20PII%20anonymization/Healthcare%20PII%20anonymization%20example.html\">here</a>.\n\nOur source data, shown in <strong>Figure 3</strong>, has the same patients, highlighted in green appearing multiple times, with different types of care, highlighted in red.\n\n[caption id=\"attachment_10262\" align=\"aligncenter\" width=\"916\"]<img src=\"https://databricks.com/wp-content/uploads/2017/02/don-healthcare-fig3.png\" alt=\"\" width=\"916\" height=\"605\" class=\"size-full wp-image-10262\" /> Figure 3[/caption]\n\nOur anonymization process should anonymize the patient\u2019s PII while maintaining the semantic relationships and distributions.  <strong>Figure 4</strong> demonstrates that the relationships and distributions are maintained while the PII is completely anonymized.\n\n[caption id=\"attachment_10263\" align=\"aligncenter\" width=\"929\"]<img src=\"https://databricks.com/wp-content/uploads/2017/02/don-healthcare-fig4.png\" alt=\"\" width=\"929\" height=\"606\" class=\"size-full wp-image-10263\" /> Figure 4[/caption]\n\n<strong>Figure 5</strong> demonstrates that our anonymization is completely random because when we run the source dataset against Faker again we get a completely different set of anonymized data while still maintaining relationships and distributions.\n\n[caption id=\"attachment_10264\" align=\"aligncenter\" width=\"927\"]<img src=\"https://databricks.com/wp-content/uploads/2017/02/don-healthcare-fig5.png\" alt=\"\" width=\"927\" height=\"610\" class=\"size-full wp-image-10264\" /> Figure 5[/caption]\n\n<h2>What\u2019s next</h2>\n\nThe example code we used in this blog is available as a <a href=\"http://go.databricks.com/hubfs/notebooks/blogs/Healthcare%20PII%20anonymization/Healthcare%20PII%20anonymization%20example.html\">Databricks notebook</a>, you can try it with a free trial of Databricks. If you are interested in more examples of Apache Spark in action, <a href=\"https://databricks.com/blog/2016/11/16/oil-gas-asset-optimization-aws-kinesis-rds-databricks.html\">check out my earlier blog on asset optimization in the oil &amp; gas industry</a>.\n\nThis just one of many examples of how Databricks can seamlessly enable innovation in the healthcare industry. In the next installation of this blog, we will build a predictive model using this anonymized data.  If you want to get started with Databricks, <a href=\"https://databricks.com/try\">sign-up for a free trial</a> or <a href=\"http://go.databricks.com/contact-databricks\">contact us</a>."}
{"status": "publish", "description": null, "creator": "rxin", "link": "https://databricks.com/blog/2017/02/16/processing-trillion-rows-per-second-single-machine-can-nested-loop-joins-fast.html", "authors": null, "id": 10273, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2017-02-16", "tz": "UTC", "createdOn": "2017-02-16"}, "title": "Processing a Trillion Rows Per Second on a Single Machine: How Can Nested Loop Joins be this Fast?", "slug": "processing-trillion-rows-per-second-single-machine-can-nested-loop-joins-fast", "content": "This blog post describes our experience debugging a failing test case caused by a cross join query running \u201ctoo fast.\u201d Because the root cause of fail test case spans across multiple layers\u2014from Apache Spark to the JVM JIT compiler\u2014 we wanted to share our analysis in this post.\n\n<h2>Spark as a compiler</h2>\n\nThe vast majority of big data SQL or MPP engines follow the Volcano iterator architecture that is inefficient for analytical workloads. Since Spark 2.0 release, the new Tungsten execution engine in Apache Spark implements whole-stage code generation, a technique inspired by modern compilers to collapse the entire query into a single function. This JIT compiler approach is a far superior architecture than the row-at-a-time processing or code generation model employed by other engines, making Spark one of the most efficient in the market. Our earlier blog post demonstrated that Spark 2.0 was capable of <a href=\"https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html\">producing a billion records a second on a laptop</a> using its broadcast hash join operator.\n\nSpark 2.0 implemented whole-stage code generation for most of the essential SQL operators, such as scan, filter, aggregate, hash join. Based on our customers\u2019 feedback, we recently implemented whole-stage code generation for broadcast nested loop joins in Databricks, and gained 2 to 10X improvement.\n\n<h2>Mystery of a failing test case</h2>\n\nWhile we were pretty happy with the improvement, we noticed that one of the test cases in Databricks started failing. To simulate a hanging query, the test case performed a cross join to produce 1 trillion rows.\n\n<pre><code class=\"java\">spark.range(1000 * 1000).crossJoin(spark.range(1000 * 1000)).count()\n</code></pre>\n\nOn a single node, we expected this query would run infinitely or \"hang.\" To our surprise, we started seeing this test case failing nondeterministically because sometimes it completed on our Jenkins infrastructure in less than one second, the time limit we put on this query.\n\nWe noticed that in one of the failing instances, the query was performing a broadcast nested loop join using 40 cores, as shown below. That is to say, each core was able to process 25 billion rows per second. As much as we enjoyed the performance improvements, something was off: the CPU was running at less than 4 GHz, so how could a core process more than 6 rows per cycle in joins?\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/02/trillion-rows-per-second-dag.png\" alt=\"apache spark 25 billion rows per second\" width=\"608\" height=\"697\" class=\"aligncenter size-full wp-image-10277\" />\n\n<h2>Life of a Spark query</h2>\n\nBefore revealing the cause, let\u2019s walk through how Spark\u2019s query execution works.\n\nSpark internally represents a query or a DataFrame as a logical plan. The Catalyst optimizer applies both rule-based peephole optimizations as well as cost-based optimizations on logical plans. After logical query optimization, Catalyst transforms the logical plan into a physical plan, which contains more information about how the query should be executed. As an example, \u201cjoin\u201d is a logical plan node, which doesn\u2019t dictate how the join should be physically executed. By contrast, \u201chash join\u201d or \u201cnested loop join\u201d would be a physical plan node, as it specifies how the query should be executed.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/02/life-of-a-spark-query.png\" alt=\"Life of an Apache Spark query\" width=\"623\" height=\"121\" class=\"aligncenter size-full wp-image-10279\" />\n\nPrior to whole-stage code generation, each physical plan is a class with code defining the execution. With whole-stage code generation, all the physical plan nodes in a plan tree work together to generate Java code in a single function for execution. This Java code is then turned into JVM bytecode using Janino, a fast Java compiler. Then JVM JIT kicks in to optimize the bytecode further and eventually compiles them into machine instructions.\n\nIn this case, the generated Java code looked like the following (simplified for illustration):\n\n<pre><code class=\"java\">long agg_value1 = 0L;\nwhile (range_number != range_batchEnd) {\n  range_number += 1L;\n\n  for(int bnlj_index = 0; bnlj_index &lt; broadcast.length; ++bnlj_index) {\n    InternalRow row = broadcast[bnlj_index];\n    agg_value1 += 1L;\n  }\n}\n</code></pre>\n\nOur first guess was that JVM JIT was smart enough to eliminate the inner loop, because JIT analyzed the bytecode and found the inner loop had no side effect other than incrementing some counters. In that case, JIT would rewrite the code into the following:\n\n<pre><code class=\"java\">long agg_value1 = 0L;\nwhile (range_number != range_batchEnd) {\n  range_number += 1L;\n  agg_value1 += broadcast.length;\n}\n</code></pre>\n\nThis would turn an operation that is <strong>O(outer * inner)</strong> to just <strong>O(outer)</strong>. To verify this, we used the flag <code>-XX:PrintAssembly</code> to dump the assembly code by JIT, and inspected the assembly code. A shortened version of the generated assembly looks like the following (annotation added by us; you can <a href=\"https://gist.github.com/rxin/74d65a166b2f48773e0f61f923c0caa2\">find the full version here</a>):\n\n<pre><code>0x00007f4d0510fb6f: jne    0x00007f4d0510fc85\n&lt;strong&gt;0x00007f4d0510fb75: mov    r10d,DWORD PTR [rbx+0xc]  ;r10d \u2190 bnlj_broadcast.length\n0x00007f4d0510fb79: mov    r9d,r10d\n&lt;strong&gt;0x00007f4d0510fb7c: add    r9d,0xfffffff1 ; r9d \u2190 bnlj_broadcast.length - 15\n0x00007f4d0510fb80: xor    r8d,r8d\n...\n0x00007f4d0510fba6: je     0x00007f4d0510fc4e\n0x00007f4d0510fbac: add    r13,0x1    ; outer loop increment\n0x00007f4d0510fbb0: xor    ebp,ebp\n0x00007f4d0510fbb2: cmp    ebp,r10d   ; inner loop condition\n0x00007f4d0510fbb5: jge    0x00007f4d0510fb9b \n...\n0x00007f4d0510fbcd: mov    r11d,ebp\n0x00007f4d0510fbd0: inc    r11d               ; inner loop increment by 1\n0x00007f4d0510fbd3: add    r14,0x1            ; agg_value1 increment by 1\n\n0x00007f4d0510fbd7: cmp    r11d,r10d        ; inner loop condition\n0x00007f4d0510fbda: jge    0x00007f4d0510fb9b \n...\n0x00007f4d0510fc14: mov    QWORD PTR [rcx+0x68],rdi\n0x00007f4d0510fc18: add    r14,0x10           ; agg_value1 += 16\n0x00007f4d0510fc1c: add    r11d,0x10          ; inner loop increment by 16\n0x00007f4d0510fc20: cmp    r11d,r9d   ; inner loop condition\n; (bnlj_index &lt; bnlj_broadcast.length - 15\n0x00007f4d0510fc23: jl     0x00007f4d0510fc10 \n0x00007f4d0510fc25: cmp    r11d,r10d  ; inner loop condition\n; (bnlj_index &lt; bnlj_broadcast.length\n0x00007f4d0510fc28: jge    0x00007f4d0510fb9b\n0x00007f4d0510fc2e: xchg   ax,ax\n</code></pre>\n\nThe assembly is a bit verbose, but it is easy to notice that the agg_value1 += 1 instruction was implemented using both add 0x01 and add 0x10 assembly instructions. This suggests the inner loop was unrolled with a factor of 16, after which further optimizations were possible. Since bnlj_broadcast.length might not be a multiple of 16, add 0x01 instructions are still needed to finish the loop.\n\nSo what really happened was that the nested loops were rewritten as following:\n\n<pre><code class=\"java\">long agg_value1 = 0L;\nwhile (range_number != range_batchEnd) {\n  range_number += 1L;\n\n  for(int bnlj_index = 0; bnlj_index &lt; broadcast.length; bnlj_index += 16) {\n    agg_value1 += 16L;\n  }\n  ...\n}\n</code></pre>\n\n<h2>What we learned and our takeaways</h2>\n\nMystery solved. <strong>Would this particular optimization matter in practice? Probably not</strong>, unless you are running a query that counts the output of cross joins.\n\nHowever, we found the experience and cause fascinating and wanted to share with the curious. Without implementing a specific optimization rule to unroll the inner loop, we gained this optimization because it existed in another layer of abstraction, namely JVM JIT. Another interesting takeaway is that with multiple layers of optimizations, performance engineering can be quite challenging, and extreme care must be taken when designing benchmarks to measure the intended optimization, as optimizations in other layers might bring unexpected speedups and lead to incorrect conclusions.\n\nThe broadcast nested loop join improvement is, nonetheless, generally applicable, and all Databricks customers will automatically get this optimization in the next software update."}
{"status": "publish", "description": null, "creator": "michael", "link": "https://databricks.com/blog/2017/02/23/working-complex-data-formats-structured-streaming-apache-spark-2-1.html", "authors": null, "id": 10297, "categories": ["Apache Spark", "Engineering Blog", "Streaming"], "dates": {"publishedOn": "2017-02-23", "tz": "UTC", "createdOn": "2017-02-23"}, "title": "Working with Complex Data Formats with Structured Streaming in Apache Spark 2.1", "slug": "working-complex-data-formats-structured-streaming-apache-spark-2-1", "content": "In <a href=\"https://databricks.com/blog/2017/01/19/real-time-streaming-etl-structured-streaming-apache-spark-2-1.html\">part 1</a> of this series on Structured Streaming blog posts, we demonstrated how easy it is to write an end-to-end streaming ETL pipeline using Structured Streaming that converts JSON CloudTrail logs into a Parquet table. The blog highlighted that one of the major challenges in building such pipelines is to read and transform data from various sources and complex formats. In this blog post, we are going to examine this problem in further detail, and show how Apache Spark SQL\u2019s built-in functions can be used to solve all your data transformation challenges.\n\nSpecifically, we are going to discuss the following:\n\n<ul>\n<li>What are the different data formats and their tradeoffs</li>\n<li>How to work with them easily using Spark SQL</li>\n<li>How to choose the right final format for your use case</li>\n</ul>\n\n<h2>Data sources and formats</h2>\n\nData is available in a myriad of different formats. Spreadsheets can be expressed in XML, CSV, TSV; application metrics can be written out in raw text or JSON. Every use case has a particular data format tailored for it. In the world of Big Data, we commonly come across formats like Parquet, ORC, Avro, JSON, CSV, SQL and NoSQL data sources, and plain text files. We can broadly classify these data formats into three categories: structured, semi-structured, and unstructured data. Let\u2019s try to understand the benefits and shortcomings of each category.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/02/blog-illustration-01.png\" alt=\"Diagram showing the breakdown of various types of data sources and formats\" width=\"700\" height=\"407\" class=\"aligncenter size-full wp-image-10303\" />\n\n<h3>Structured data</h3>\n\nStructured data sources define a schema on the data. With this extra bit of information about the underlying data, structured data sources provide efficient storage and performance. For example, columnar formats such as Parquet and ORC make it much easier to extract values from a subset of columns. Reading each record row by row first, then extracting the values from the specific columns of interest can read much more data than what is necessary when a query is only interested in a small fraction of the columns. A row-based storage format such as Avro efficiently serializes and stores data providing storage benefits. However, these advantages often come at the cost of flexibility. For example, because of rigidity in structure, evolving a schema can be challenging.\n\n<h3>Unstructured data</h3>\n\nBy contrast, unstructured data sources are generally free-form text or binary objects that contain no markup, or metadata (e.g., commas in CSV files), to define the organization of data. Newspaper articles, medical records, image blobs, application logs are often treated as unstructured data. These sorts of sources generally require context around the data to be parseable. That is, you need to know that the file is an image or is a newspaper article. Most sources of data are unstructured. The cost of having unstructured formats is that it becomes cumbersome to extract value out of these data sources as many transformations and feature extraction techniques are required to interpret these datasets.\n\n<h3>Semi-structured data</h3>\n\nSemi-structured data sources are structured per record but don\u2019t necessarily have a well-defined global schema spanning all records. As a result, each data record is augmented with its schema information. JSON and XML are popular examples. The benefits of semi-structured data formats are that they provide the most flexibility in expressing your data as each record is self-describing. These formats are very common across many applications as many lightweight parsers exist for dealing with these records, and they also have the benefit of being human readable. However, the main drawback for these formats is that they incur extra parsing overheads, and are not particularly built for ad-hoc querying.\n\n<h2>Interchanging data formats with Spark SQL</h2>\n\nIn our <a href=\"https://databricks.com/blog/2017/01/19/real-time-streaming-etl-structured-streaming-apache-spark-2-1.html\">previous blog post</a>, we discussed how transforming Cloudtrail Logs from JSON into Parquet shortened the runtime of our ad-hoc queries by 10x. Spark SQL allows users to ingest data from these classes of data sources, both in batch and streaming queries. It natively supports reading and writing data in Parquet, ORC, JSON, CSV, and text format and a plethora of other connectors exist on <a href=\"https://spark-packages.org/?q=tags%3A%22Data%20Sources%22\">Spark Packages</a>. You may also connect to SQL databases using the JDBC DataSource.\n\nApache Spark can be used to interchange data formats as easily as:\n\n<pre><code class=\"python\">events = spark.readStream \\\n  .format(\"json\") \\           # or parquet, kafka, orc...\n  .option() \\                 # format specific options\n  .schema(my_schema) \\        # required\n  .load(\"path/to/data\")\n\noutput = \u2026                   # perform your transformations\n\noutput.writeStream \\          # write out your data \n  .format(\"parquet\") \\\n  .start(\"path/to/write\")\n</code></pre>\n\nWhether batch or streaming data, we know how to read and write to different data sources and formats, but different sources support different kinds of schema and data types. Traditional databases only support primitive data types, whereas formats like JSON allow users to nest objects within columns, have an array of values or represent a set of key-value pairs. Users will generally have to go in-between these data types to efficiently store and represent their data. Fortunately, Spark SQL makes it easy to handle both primitive and complex data types. Let\u2019s now dive into a quick overview of how we can go from complex data types to primitive data types and vice-a-versa.\n\n<h2>Transforming complex data types</h2>\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/02/various-data-types.png\" alt=\"Example of various different types of data\" width=\"550\" height=\"126\" class=\"aligncenter size-full wp-image-10305\" />\n\nIt is common to have complex data types such as structs, maps, and arrays when working with semi-structured formats. For example, you may be logging API requests to your web server. This API request will contain HTTP Headers, which would be a string-string map. The request payload may contain form-data in the form of JSON, which may contain nested fields or arrays. Some sources or formats may or may not support complex data types. Some formats may provide performance benefits when storing the data in a specific data type. For example, when using Parquet, all struct columns will receive the same treatment as top-level columns. Therefore, if you have filters on a nested field, you will get the same benefits as a top-level column. However, maps are treated as two array columns, hence you wouldn\u2019t receive efficient filtering semantics.\n\nLet\u2019s look at some examples on how Spark SQL allows you to shape your data ad libitum with some data transformation techniques.\n\n<h4>Selecting from nested columns</h4>\n\nDots (<code>.</code>) can be used to access nested columns for structs and maps.\n\n<pre><code class=\"scala\">// input\n{\n  \"a\": {\n     \"b\": 1\n  }\n}\n\nPython: events.select(\"a.b\")\n Scala: events.select(\"a.b\")\n   SQL: select a.b from events\n\n// output\n{\n  \"b\": 1\n}\n</code></pre>\n\n<h4>Flattening structs</h4>\n\nA star (<code>*</code>) can be used to select all of the subfields in a struct.\n\n<pre><code class=\"scala\">// input\n{\n  \"a\": {\n     \"b\": 1,\n     \"c\": 2\n  }\n}\n\nPython:  events.select(\"a.*\")\n Scala:  events.select(\"a.*\")\n   SQL:  select a.* from events\n\n// output\n{\n  \"b\": 1,\n  \"c\": 2\n}\n</code></pre>\n\n<h4>Nesting columns</h4>\n\nThe struct function or just parentheses in SQL can be used to create a new struct.\n\n<pre><code class=\"scala\">// input\n{\n  \"a\": 1,\n  \"b\": 2,\n  \"c\": 3\n}\n\nPython: events.select(struct(col(\"a\").alias(\"y\")).alias(\"x\"))\n Scala: events.select(struct('a as 'y) as 'x)\n   SQL: select named_struct(\"y\", a) as x from events\n\n// output\n{\n  \"x\": {\n    \"y\": 1\n  }\n}\n</code></pre>\n\n<h4>Nesting all columns</h4>\n\nThe star (<code>*</code>) can also be used to include all columns in a nested struct.\n\n<pre><code class=\"scala\">// input\n{\n  \"a\": 1,\n  \"b\": 2\n}\n\nPython: events.select(struct(\"*\").alias(\"x\"))\n Scala: events.select(struct(\"*\") as 'x)\n   SQL: select struct(*) as x from events\n\n// output\n{\n  \"x\": {\n    \"a\": 1,\n    \"b\": 2\n  }\n}\n</code></pre>\n\n<h4>Selecting a single array or map element</h4>\n\n<code>getItem()</code> or square brackets (i.e. <code>[ ]</code>) can be used to select a single element out of an array or a map.\n\n<pre><code class=\"scala\">// input\n{\n  \"a\": [1, 2]\n}\n\nPython: events.select(col(\"a\").getItem(0).alias(\"x\"))\n Scala: events.select('a.getItem(0) as 'x)\n   SQL: select a[0] as x from events\n\n// output\n{ \"x\": 1 }\n</code></pre>\n\n<pre><code class=\"scala\">// input\n{\n  \"a\": {\n    \"b\": 1\n  }\n}\n\nPython: events.select(col(\"a\").getItem(\"b\").alias(\"x\"))\n Scala: events.select('a.getItem(\"b\") as 'x)\n   SQL: select a['b'] as x from events\n\n// output\n{ \"x\": 1 }\n</code></pre>\n\n<h4>Creating a row for each array or map element</h4>\n\n<code>explode()</code> can be used to create a new row for each element in an array or each key-value pair. This is similar to LATERAL VIEW EXPLODE in HiveQL.\n\n<pre><code class=\"scala\">// input\n{\n  \"a\": [1, 2]\n}\n\nPython: events.select(explode(\"a\").alias(\"x\"))\n Scala: events.select(explode('a) as 'x)\n   SQL: select explode(a) as x from events\n\n// output\n[{ \"x\": 1 }, { \"x\": 2 }]\n</code></pre>\n\n<pre><code class=\"scala\">// input\n{\n  \"a\": {\n    \"b\": 1,\n    \"c\": 2\n  }\n}\n\nPython: events.select(explode(\"a\").alias(\"x\", \"y\"))\n Scala: events.select(explode('a) as Seq(\"x\", \"y\"))\n   SQL: select explode(a) as (x, y) from events\n\n// output\n[{ \"x\": \"b\", \"y\": 1 }, { \"x\": \"c\", \"y\": 2 }]\n</code></pre>\n\n<h4>Collecting multiple rows into an array</h4>\n\n<code>collect_list()</code> and <code>collect_set()</code> can be used to aggregate items into an array.\n\n<pre><code class=\"scala\">// input\n[{ \"x\": 1 }, { \"x\": 2 }]\n\nPython: events.select(collect_list(\"x\").alias(\"x\"))\n Scala: events.select(collect_list('x) as 'x)\n   SQL: select collect_list(x) as x from events\n\n// output\n{ \"x\": [1, 2] }\n</code></pre>\n\n<pre><code class=\"scala\">// input\n[{ \"x\": 1, \"y\": \"a\" }, { \"x\": 2, \"y\": \"b\" }]\n\nPython: events.groupBy(\"y\").agg(collect_list(\"x\").alias(\"x\"))\n Scala: events.groupBy(\"y\").agg(collect_list('x) as 'x)\n   SQL: select y, collect_list(x) as x from events group by y\n\n// output\n[{ \"y\": \"a\", \"x\": [1]}, { \"y\": \"b\", \"x\": [2]}]\n</code></pre>\n\n<h4>Selecting one field from each item in an array</h4>\n\nWhen you use dot notation on an array we return a new array where that field has been selected from each array element.\n\n<pre><code class=\"scala\">// input\n{\n  \"a\": [\n    {\"b\": 1},\n    {\"b\": 2}\n  ]\n}\n\nPython: events.select(\"a.b\")\n Scala: events.select(\"a.b\")\n   SQL: select a.b from events\n\n// output\n{\n  \"b\": [1, 2]\n}\n</code></pre>\n\n<h3>Power of to_json() and from_json()</h3>\n\nWhat if you really want to preserve your column\u2019s complex structure but you need it to be encoded as a string to store it? Are you doomed? Of course not! Spark SQL provides functions like <code>to_json()</code> to encode a struct as a string and <code>from_json()</code> to retrieve the struct as a complex type. Using JSON strings as columns are useful when reading from or writing to a streaming source like Kafka. Each Kafka key-value record will be augmented with some metadata, such as the ingestion timestamp into Kafka, the offset in Kafka, etc. If the \u201cvalue\u201d field that contains your data is in JSON, you could use <code>from_json()</code> to extract your data, enrich it, clean it, and then push it downstream to Kafka again or write it out to a file.\n\n<h4>Encode a struct as json</h4>\n\n<code>to_json()</code> can be used to turn structs into JSON strings. This method is particularly useful when you would like to re-encode multiple columns into a single one when writing data out to Kafka. This method is not presently available in SQL.\n\n<pre><code class=\"scala\">// input\n{\n  \"a\": {\n    \"b\": 1\n  }\n}\n\nPython: events.select(to_json(\"a\").alias(\"c\"))\n Scala: events.select(to_json('a) as 'c)\n\n// output\n{\n  \"c\": \"{\\\"b\\\":1}\"\n}\n</code></pre>\n\n<h4>Decode json column as a struct</h4>\n\n<code>from_json()</code> can be used to turn a string column with JSON data into a struct. Then you may flatten the struct as described above to have individual columns. This method is not presently available in SQL.\n\n<pre><code class=\"scala\">// input\n{\n  \"a\": \"{\\\"b\\\":1}\"\n}\n\nPython: \n  schema = StructType().add(\"b\", IntegerType())\n  events.select(from_json(\"a\", schema).alias(\"c\"))\nScala:\n  val schema = new StructType().add(\"b\", IntegerType)\n  events.select(from_json('a, schema) as 'c)\n\n// output\n{\n  \"c\": {\n    \"b\": 1\n  }\n}\n</code></pre>\n\nSometimes you may want to leave a part of the JSON string still as JSON to avoid too much complexity in your schema.\n\n<pre><code class=\"scala\">// input\n{\n  \"a\": \"{\\\"b\\\":{\\\"x\\\":1,\\\"y\\\":{\\\"z\\\":2}}}\"\n}\n\nPython: \n  schema = StructType().add(\"b\", StructType().add(\"x\", IntegerType())\n                              .add(\"y\", StringType()))\n  events.select(from_json(\"a\", schema).alias(\"c\"))\nScala:\n  val schema = new StructType().add(\"b\", new StructType().add(\"x\", IntegerType)\n    .add(\"y\", StringType))\n  events.select(from_json('a, schema) as 'c)\n\n// output\n{\n  \"c\": {\n    \"b\": {\n      \"x\": 1,\n      \"y\": \"{\\\"z\\\":2}\"\n    }\n  }\n}\n</code></pre>\n\n<h4>Parse a set of fields from a column containing JSON</h4>\n\n<code>json_tuple()</code> can be used to extract fields available in a string column with JSON data.\n\n<pre><code class=\"scala\">// input\n{\n  \"a\": \"{\\\"b\\\":1}\"\n}\n\nPython: events.select(json_tuple(\"a\", \"b\").alias(\"c\"))\nScala:  events.select(json_tuple('a, \"b\") as 'c)\nSQL:    select json_tuple(a, \"b\") as c from events\n\n// output\n{ \"c\": 1 }\n</code></pre>\n\nSometimes a string column may not be self-describing as JSON, but may still have a well-formed structure. For example, it could be a log message generated using a specific Log4j format. Spark SQL can be used to structure those strings for you with ease!\n\n<h4>Parse a well-formed string column</h4>\n\n<code>regexp_extract()</code> can be used to parse strings using regular expressions.\n\n<pre><code class=\"scala\">// input\n[{ \"a\": \"x: 1\" }, { \"a\": \"y: 2\" }]\n\nPython: events.select(regexp_extract(\"a\", \"([a-z]):\", 1).alias(\"c\"))\nScala:  events.select(regexp_extract('a, \"([a-z]):\", 1) as 'c)\nSQL:    select regexp_extract(a, \"([a-z]):\", 1) as c from events\n\n// output\n[{ \"c\": \"x\" }, { \"c\": \"y\" }]\n</code></pre>\n\nThat\u2019s a lot of transformations! Let\u2019s now look at some real life use cases to put all of these data formats, and data manipulation capabilities to good use.\n\n<h2>Harnessing all of this power</h2>\n\nAt Databricks, we collect logs from our services and use them to perform real-time monitoring to detect issues, before our customers are affected. Log files are unstructured files, but they are parseable because they have a well-defined Log4j format. We run a log collector service that sends each log entry and additional metadata about the entry (e.g. source) in JSON to Kinesis. These JSON records are then batch-uploaded to S3 as files. Querying these JSON logs to answer any question is tedious: these files contain duplicates, and for answering any query, even if it involves a single column, the whole JSON record may require deserialization.\n\nTo address this issue, we run a pipeline that reads these JSON records and performs de-duplication on the metadata. Now we are left with the original log record, which may be in JSON format or as unstructured text. If we\u2019re dealing with JSON, we use <code>from_json()</code> and several of the transformations described above to format our data. If it is text, we use methods such as <code>regexp_extract()</code> to parse our Log4j format into a more structured form. Once we are done with all of our transformations and restructuring, we save the records in Parquet partitioned by date. This gives us 10-100x speed-up when answering questions like \u201dhow many ERROR messages did we see between 10:00-10:30 for this specific service\u201d? The speed-ups can be attributed to:\n\n<ul>\n<li>We no longer pay the price of deserializing JSON records</li>\n<li>We don\u2019t have to perform complex string comparisons on the original log message</li>\n<li>We only have to extract two columns in our query: the time, and the log level</li>\n</ul>\n\nHere are a few more common use cases that we have seen among our customers:\n\n<blockquote>\n  \u201cI would like to run a Machine Learning pipeline with my data. My data is already pre-processed, and I will use all my features throughout the pipeline.\u201d\n</blockquote>\n\nAvro is a good choice when you will access the whole row of data.\n\n<blockquote>\n  \u201cI have an IoT use case where my sensors send me events. For each event the metadata that matters is different.\u201d\n</blockquote>\n\nIn cases where you would like flexibility in your schema, you may consider using JSON to store your data.\n\n<blockquote>\n  \u201cI would like to train a speech recognition algorithm on newspaper articles or sentiment analysis on product comments.\u201d\n</blockquote>\n\nIn cases where your data may not have a fixed schema, nor a fixed pattern/structure, it may just be easier to store it as plain text files. You may also have a pipeline that performs feature extraction on this unstructured data and stores it as Avro in preparation for your Machine Learning pipeline.\n\n<h2>Conclusion</h2>\n\nIn this blog post, we discussed how Spark SQL allows you to consume data from many sources and formats, and easily perform transformations and interchange between these data formats. We shared how we curate our data at Databricks, and considered other production use cases where you may want to do things differently.\n\nSpark SQL provides you with the necessary tools to access your data wherever it may be, in whatever format it may be in and prepare it for downstream applications either with low latency on streaming data or high throughput on old historical data!\n\nIn the future blog posts in this series, we\u2019ll cover more on:\n\n<ul>\n<li>Monitoring your streaming applications</li>\n<li>Integrating Structured Streaming with Apache Kafka</li>\n<li>Computing event time aggregations with Structured Streaming</li>\n</ul>\n\nIf you want to learn more about the Structured Streaming, here are a few useful links.\n\n<ul>\n<li>Previous blogs posts explaining the motivation and concepts of Structured Streaming:\n\n<ul>\n<li><a href=\"https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html\">Continuous Applications: Evolving Streaming in Apache Spark 2.0</a></li>\n<li><a href=\"https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html\">Structured Streaming In Apache Spark</a></li>\n</ul></li>\n<li><a href=\"https://databricks.com/blog/2017/04/26/processing-data-in-apache-kafka-with-structured-streaming-in-apache-spark-2-2.html\">Processing Data in Apache Kafka with Structured Streaming in Apache Spark 2.2</a>\n\n<ul>\n<li><a href=\"https://databricks.com/blog/2017/01/19/real-time-streaming-etl-structured-streaming-apache-spark-2-1.html\">Real-time Streaming ETL with Structured Streaming in Apache Spark 2.1</a></li>\n</ul></li>\n<li><a href=\"http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\">Structured Streaming Programming Guide</a></li>\n<li><a href=\"https://spark-summit.org/east-2017/events/making-structured-streaming-ready-for-production-updates-and-future-directions/\">Talk at Spark Summit 2017 East - Making Structured Streaming Ready for Production and Future Directions</a></li>\n</ul>\n\nFinally, try our example notebooks that demonstrate transforming complex data types in Python, Scala, or SQL in <a href=\"http://databricks.com/try\">Databricks</a>:\n\n<ul>\n<li><a href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/3786892947974366/3601578643761083/latest.html\">Python Notebook</a></li>\n<li><a href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/3786892947974332/3601578643761083/latest.html\">Scala Notebook</a></li>\n<li><a href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/3786892947974290/3601578643761083/latest.html\">SQL Notebook</a></li>\n</ul>"}
{"status": "publish", "description": null, "creator": "dave_wang", "link": "https://databricks.com/blog/2017/02/27/apache-spark-databricks-taming-wild-west-wi-fi.html", "authors": null, "id": 10320, "categories": ["Company Blog", "Customers", "Product"], "dates": {"publishedOn": "2017-02-27", "tz": "UTC", "createdOn": "2017-02-27"}, "title": "How Apache Spark on Databricks is Taming the Wild West of Wi-Fi", "slug": "apache-spark-databricks-taming-wild-west-wi-fi", "content": "<img src=\"https://databricks.com/wp-content/uploads/2017/02/ipass-and-databricks.png\" alt=\"iPass and Databricks\" width=\"600\" height=\"315\" class=\"aligncenter size-full wp-image-10321\" />\n\niPass is the world\u2019s largest Wi-Fi provider, yet we don\u2019t own a single hotspot. You can think of us as the Uber of Wi-Fi. And though it may sound simple, it is actually quite complicated. That is because integrating more than 160 commercial Wi-Fi providers from around the world into a single network is difficult. Not only do hotspots require constant monitoring to ensure a consistent user experience, but as a technology, Wi-Fi is also more fragile than most people think. You can expect a consistent user experience from your home Wi-Fi network, but commercial-grade hotspots do not share the same characteristics.\n\nSignal strength at home is usually good. Bandwidth is in the tens of megabits, with only a handful of devices usually attaching to the home network. Single access point architecture prevails, and there is no provisioning requirement beyond the access key.\n\nPublic and commercial hotspots are usually farther away from the user, their signal strength is worse, bandwidth more limited and shared across many devices. Often, the hotspots themselves break and need to be replaced, or at least regularly maintained. Add community hotspots to the mix, and this Wi-Fi ecosystem becomes fairly unmanageable.\n\nAlthough Wi-Fi can be unpredictable, here at iPass, we had a big idea. What if we could monitor every single, solitary hotspot in the world? Measuring speed, availability, performance and location so that we could recommend which hotspot a device should connect to and which one it should not. I'm saying a device, not a user. Think about cellular networks for a second; you as the user do not decide which cell tower to attach to, the device does. In terms of the user experience, it just happens.\n\nLuckily for us, our years of connectivity experience and our own cutting-edge technology came together. iPass industry knowledge and experience met \u201cbig data\u201d and our Wi-Fi service platform, iPass SmartConnect\u2122, was born.\n\nWith iPass SmartConnect, we aimed to keep our users always best connected. Not only to Wi-Fi but to the best available network, Wi-Fi or cellular. That was the company\u2019s founding mission, after all. Our backend measures and analyses connectivity and quality data to ensure a device is connected to the best network every time. Our iPass SmartConnect SDK and iPass SmartConnect backend can make the same recommendation for IoT devices, ensuring they are also always best connected.\n\nFirst off, we needed to group and categorize individual access points. For example, hotels can have many access points that share common infrastructure. So, instead of learning about every hotspot separately, we wanted to group them into logical \u201cvenues\u201d and monitor them as an entity.\n\nEasier said than done, though, especially if you don't actually know where the venue is or how many hotspots there are at any given location.\n\nMoreover, from experience, we knew that many connection failures are the result of a user simply walking away from an access point. Here in the Bay Area, for instance, you\u2019ll find access points at virtually every traffic light. Once you connect to one of them, your device will remember it and try to connect you every time you are nearby.\n\nSo here you are, driving along, listening to Pandora or talking on Skype, while your phone decides to jump onto a hotspot. But in the meantime, you pull away. Wi-Fi connection will fail and cellular interface will need to renegotiate data connection.\n\nNowadays, your device knows when you\u2019re on the move, as your device has an accelerometer. But your device can\u2019t tell whether the hotspot is moving as well. If iPass SmartConnect can detect motion patterns and tag hotspots as moving, we can recommend hotspots that are moving with you, like hotspots on a plane or train.\n\nThat was our plan, technically challenging and innovative. And in early 2016, iPass asked me to relocate to headquarters in Redwood Shores, California to lead this exciting project.\n\nWe had a ton of technical debt and the company was not architected with big data in mind. We were a hardcore RDBMS shop.\n\nThe team started small, just an engineer and myself. So in the spirit of a true startup, we rolled up our sleeves and got to work.\n\nWe had some previous experience playing with Kafka, Storm, Hadoop and Hbase, but being a team of two, we did not have the time to build an in-house big data platform.\n\nIt was obvious to us that in order to build and grow our platform, we had to move out of our self-managed data center and into the cloud. So in order to allow for rapid development and on-demand scaling, I started vetting cloud big data vendors.\n\nAt that time, iPass was already using AWS for newly released products and services. \nBeing small and smart, we did not see a lot of value in managing two \u201cclouds.\u201d\n\nPlatform-wise, one solution stood out above all the others: Apache Spark. It can batch, it can stream, it is mostly actively developed, and their offices are close by. That comes in handy. And it leads the way in efficiency among all other big data technologies.\n\nSo Apache Spark it was.\n\nA new wave of business meant we needed to turn iPass SmartConnect from concept to reality and quick. We needed to start developing, right away.\n\nWe could not afford to build and maintain Spark clusters, or manage EMR. Instead, we needed to focus on writing business logic. So we scheduled our first call with Databricks.\n\nThe Databricks platform was very promising - almost too good to be true. Full separation of storage and computation, easy to use, high-level API, each job creates its own cluster, no more library dependency conflicts, various cluster sizes and instance types, all Spark versions, web-based development and a fantastic team of very smart people to help us out.\n\nWe signed on the dotted line. And shortly thereafter, we got our Databricks instance up and running.\n\nIt took us a few weeks to write the first iteration of the tracking and ranking job. iPass SmartConnect was launched.\n\nSince then, we\u2019ve developed all of the features mentioned above. Some of them are in beta, but the speed of development has been like nothing we had experienced or managed before.\n\nApart from iPass SmartConnect, we also use Databricks for ETLs, streaming, analytics, machine learning, data lake processing and reporting. It has become a central point of our entire data flow.\n\nThe team has grown, and we are now delivering new data products in very agile, short sprint cycles. We have accomplished all of this with a small team. And we no longer worry about hardware, maintenance, scalability or (lack of) resource challenges, like maintaining and expanding old infrastructure in our own datacenter, not to mention hardware and licensing costs.\n\nOur success story would not be complete if I did not mention the tremendous help we received from the engineering team at Databricks. We have been using Spark in a fully mature production environment, and the team\u2019s response was truly incredible. We are very happy to have the brainpower and dedication of Databricks\u2019 engineers on our side.\n\nSo what have we learned? Simply this. If you have a vision and need to get to market quickly, do not try to re-invent the wheel by building everything yourself. Instead, focus on business logic, and the delivery of sellable and scalable products. And let the guys at Spark do what they do best."}
{"status": "publish", "description": null, "creator": "jakebellacera", "link": "https://databricks.com/blog/2017/02/28/voice-facebook-using-apache-spark-large-scale-language-model-training.html", "authors": null, "id": 10325, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2017-02-28", "tz": "UTC", "createdOn": "2017-02-28"}, "title": "Voice from Facebook: Using Apache Spark for Large-Scale Language Model Training", "slug": "voice-facebook-using-apache-spark-large-scale-language-model-training", "content": "[sidenote]This is a guest post from Facebook. Tejas Patil and Jing Zheng, software engineers in the Facebook engineering team, show how to use Apache Spark for large scale modeling. The blog was originally posted on the <a href=\"https://code.facebook.com/posts/678403995666478/using-apache-spark-for-large-scale-language-model-training/\">Facebook Code blog</a>.[/sidenote]\n\n<hr/>\n\nProcessing large-scale data is at the heart of what the data infrastructure group does at Facebook. Over the years we have seen tremendous growth in our analytics needs, and to satisfy those needs we either have to design and build a new system or adopt an existing open source solution and improve it so it works at our scale.\n\nFor some of our batch-processing use cases we decided to use <a href=\"http://spark.apache.org/\">Apache Spark</a>, a fast-growing open source data processing platform with the ability to scale with a large amount of data and support for custom user applications.\n\nA few months ago, <a href=\"https://databricks.com/blog/2016/08/31/apache-spark-scale-a-60-tb-production-use-case.html\">we shared one such use case</a> that leveraged Spark's declarative (SQL) support. In this post, we will describe how we used the imperative side of Spark to redesign a large-scale, complex (100+ stage) pipeline that was originally written in HQL over Hive. In particular, we will describe how to control data distribution, avoid data skew, and implement application-specific optimizations to build performant and reliable data pipelines. This new Spark-based pipeline is modular, readable, and more maintainable compared with the previous set of HQL queries. In addition to the qualitative improvements, we also observed reductions in both resource usage and data landing time.\n\n<h2>Use case: N-gram language model training</h2>\n\nNatural language processing is a field of artificial intelligence concerned with the interactions between computers and human languages. Computers can be trained to model a language, and these models are used to detect and correct spelling errors. The N-gram language model is the most widely used language modeling approach. An N-gram is usually written as an N-word phrase, with the first N-1 words as the history, and the last word predicted as a probability based on the N-1 previous words. For example, <em>\u201cCan you please come here\u201d</em> contains 5 words and is a 5-gram. Its history is <em>\u201cCan you please come.\u201d</em> Based on that history, an N-gram language model can compute a conditional probability of the word <em>\u201chere.\u201d</em>\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/02/5-gram-example.jpg\" alt=\"An example of a 5-gram phrase for natural language machine model training\" width=\"736\" height=\"231\" class=\"aligncenter size-full wp-image-10326\" />\n\nLarge-scale, higher-order N-gram language models (e.g., N=5) have proven very effective in many applications, such as automatic speech recognition and machine translation. At Facebook, for example, this is used to automatically generate captions for videos uploaded to pages, and detecting pages with potentially low quality place names (eg. \u201cHome sweet home,\u201d \u201cApt #00, Fake lane, Foo City\u201d).\n\nLanguage models trained with large datasets have better accuracy compared with ones trained with smaller datasets. The possibility of covering ample instances of infrequent words (or N-grams) increases with a larger dataset. For training with larger dataset, distributed computing frameworks (e.g. MapReduce) are generally used for better scalability and parallelizing model training.\n\n<h2>Earlier solution</h2>\n\nWe had originally developed a Hive-based solution for generating an N-gram language model. The N-gram counts were sharded by the last-two-word history, and C++ based TRANSFORMs were used to estimate partial language models and save them in Hive. Separate sub-models were built on different data sources, each triggered by a Hive query. Later, all these sub-models were combined in a final job using an interpolation algorithm using weights for each sub-model. Below is a high-level overview of the pipeline:\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/02/high-level-hive-training-job-solution.jpg\" alt=\"Diagram showing a high-level overview of the natural language model implemented in Hive.\" width=\"736\" height=\"400\" class=\"aligncenter size-full wp-image-10327\" />\n\nThe Hive-based solution obtained reasonable success in building language models: It could comfortably construct 5-gram language models when trained with a few billion N-grams. As we tried to increase the size of training dataset, the end-to-end time for running the pipeline was in an unacceptable range.\n\nHive provides a SQL-based engine for easily writing queries, which automatically get transformed into MapReduce jobs. For training language models, representing the computation as a SQL query wasn't a natural fit due to following reasons:\n\n<ul>\n<li>The pipeline code comprised several SQL queries for each sub-model training. These queries were mostly similar with minor differences. Writing a new pipeline for model training leads to duplication of these SQL queries.</li>\n<li>As more clauses are added to the query, it gets harder to understand the intent of the query.</li>\n<li>Changing a part of query requires re-running the entire pipeline to make sure that it does not cause a regression. Inability to test changes in isolation makes the development cycle take longer.</li>\n</ul>\n\nAs an alternative, writing Hadoop jobs provides more freedom to developers in terms of expressing the computation, but takes a lot more time and requires expertise in Hadoop.\n\n<h2>Spark-based solution</h2>\n\nSpark comes with a Domain Specific Language (DSL) that makes it easy to write custom applications apart from writing jobs as SQL queries. With the DSL, you can control lower-level operations (e.g., when data is shuffled) and have access to intermediate data. This helps in implementing sophisticated algorithms achieve more efficiency and stability. It also enables users to write their pipeline in a modular fashion rather than one monolithic SQL string, which improves the readability, maintainability, and testability of the pipeline. These advantages led us to experiment with Spark.\n\nRewriting the C++ logic \u2014 which had implementations for language model training algorithms \u2014 in Scala or Java would have been a significant effort, so we decided to not change that part. Just like Hive, Spark supports running custom user code, which made it easy to invoke the same C++ binaries. This allowed for a smooth transition since we didn't have to maintain two versions of the C++ logic, and the migration was transparent to users. Instead of using Spark SQL, we used the RDD interface offered by Spark because of its ability to control partitioning of the intermediate data and manage shard generation directly. Spark's <code>pipe()</code> operator was used to invoke the binaries.\n\nAt a higher level, the design of the pipeline remained the same. We continued to use Hive tables as initial input and final output for the application. The intermediate outputs were written to the local disk on cluster nodes. The entire application is approximately 1,000 lines of Scala code and can generate 100+ stages (depending on the number of sources of training data) when executed over Spark.\n\n<h2>Scalability challenges</h2>\n\nWe faced some scalability challenges when we tested Spark with a larger training dataset. In this section, we start off with describing the data distribution requirements (smoothing and sharding), followed by the challenges it led to and our solution.\n\n<h3>Smoothing</h3>\n\nN-gram models are estimated from N-gram occurrence counts in the training data. Since there can be N-grams absent in the training data, this can generalize poorly to unseen data. To address this problem, many smoothing methods are used that generally discount the observed N-gram counts to promote unseen N-gram probabilities and use lower-order models to smooth higher-order models. Because of smoothing, for an N-gram with history <em>h</em>, counts for all N-grams with the same history and all lower-order N-grams with a history as a suffix of <em>h</em> are required to estimate its probability. As an example, for the trigram <em>\u201chow are you,\u201d</em> where <em>\u201chow are\u201d</em> is the history and <em>\u201cyou\u201d</em> is the word to be predicted, in order to estimate P(<em>you</em>| <em>how are</em>), we need counts for <em>\u201chow are *,\u201d</em> <em>\u201care *,\u201d</em> and all unigrams (single-word N-grams), where * is a wildcard representing any word in the vocabulary. Frequently occurring N-grams (e.g., <em>\u201chow are *\u201d</em>) lead to data skews while processing.\n\n<h3>Sharding</h3>\n\nWith a distributed computing framework, we could partition N-gram counts into multiple shards so they could be processed by multiple machines in parallel. Sharding based on the last <em>k</em> words of an N-gram's history can guarantee that all N-grams longer than <em>k</em> words are decently balanced across the shards. This requires the counts for all the N-grams with length up to <em>k</em> to be shared across all the shards. We put all these short N-grams in a special shard called \u201c0-shard.\u201d For example, if <em>k</em> is 2, then all the unigrams and bigrams extracted from the training data are put together in a single shard (0-shard) and are accessible to all the servers doing the model training.\n\n<h3>Problem: Data skew</h3>\n\nIn the Hive-based pipeline, we two-word history sharding for the model training. Two-word history sharding means that all the N-gram counts sharing the same set of the most significant two-word histories (closest to the word being predicted) are distributed to the same node for processing. Compared with single word history, two-word sharding generally leads to more balanced data distribution, except that all nodes must share the statistics of unigram and bigrams required by smoothing algorithms, which are stored in the 0-shard. The image below illustrates the comparison between distribution of shards with one-word and two-word history.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/02/data-skew.jpg\" alt=\"Illustration of data skew\" width=\"736\" height=\"324\" class=\"aligncenter size-full wp-image-10328\" />\n\nFor large datasets, two-word history sharding can generate a large 0-shard. Having to distribute a large 0-shard to all the nodes can slow down overall computation time. It can also lead to potential job instability as it is hard to predict the memory requirements upfront, and once a job is launched it can run out of memory while in progress. While allocating more memory upfront is possible, it still does not guarantee 100% job stability, and can lead to poor memory utilization across the cluster since not all instances of the job would need more memory than the historical average.\n\nWhile trying to move the computation to Spark, the job could run with small workloads. But with larger datasets, we observed these issues:\n\n<ul>\n<li>Driver marking executor as \u201clost\u201d due to no heartbeat received from the executor for a long time</li>\n<li>Executor OOM</li>\n<li>Frequent executor GC</li>\n<li>Shuffle service OOM</li>\n<li>2GB limit in Spark for blocks</li>\n</ul>\n\nAt a high level, the root cause of all these problems could be attributed to data skew. We wanted to achieve a balanced distribution of shards, but both two-word history sharding and single-word history sharding did not help with that. So we came up with a hybrid approach: Progressive sharding and dynamically adjusting the shard size.\n\n<h3>Solution: Progressive sharding</h3>\n\nProgressive sharding uses an iterative method to address data skew. In the first iteration, we start with one-word sharding where only unigram counts need to be shared across all the shards. Unigram counts are much smaller than bigram counts. This mostly works fine except when a few shards are extremely large. For example, the shard corresponding to \u201chow to ...\u201d will be skewed. To tackle this, we check the size of each shard and only process those shards smaller than a certain threshold.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/02/progressive-sharding-iteration-1.jpg\" alt=\"Illustration of our first iteration of progressive sharding\" width=\"736\" height=\"289\" class=\"aligncenter size-full wp-image-10329\" />\n\nIn the second iteration, we use two-word sharding, which distributes N-gram counts based on two-word history. At this stage, we only need to share N-gram counts for bigrams excluding those already processed in the first iteration. These bigram counts are much smaller than the complete set of bigram counts and therefore are much faster to process. As before, we check the size of each shard, and only process those smaller than the threshold. Those left over will go to next iteration using three-word history. In most cases, three iterations are sufficient for very large datasets.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/02/progressive-sharding-iteration-2.jpg\" alt=\"Illustration of our section iteration of progressive sharding\" width=\"736\" height=\"379\" class=\"aligncenter size-full wp-image-10330\" />\n\n<h3>Dynamically adjust shard size</h3>\n\nFor the first iteration, we use a pre-specified number that is sufficiently large so that most of the shards generated are small in size. Each shard is processed by a single Spark task. Since 0-shard is very small in this iteration, having lots of small shards does not affect the efficiency. In later iterations, the number of shards are determined automatically by the number of leftover N-grams.\n\nBoth of these solutions were possible due to the flexibility offered by the Spark DSL. With Hive, developers do not have control over such lower-level operations.\n\n<h2>General purpose library for training models</h2>\n\nHow language models are used depends on the context of different applications. Each application may require different data and configurations, and therefore a different pipeline. In the Hive solution, the SQL part of the pipeline was similar across applications but had different components in several places. Instead of repeating code for each pipeline, we developed a general purpose Spark application that can be invoked from different pipelines with different data sources and configurations.\n\nWith the Spark-based solution, we could also automatically optimize the workflow of steps performed by the application based on the input configuration. For example, if the user did not specify to use entropy pruning, the application would skip model re-estimation. If the user specified count-cutoff in the configuration, the application would collapse many low-count N-grams into single one with a wildcard placeholder in order to reduce storage. These optimizations saved computational resources and produced the trained model in less time.\n\n<h2>Spark pipeline vs. Hive pipeline performance comparison</h2>\n\nWe used following performance metrics to compare the Spark pipeline against the Hive pipeline:\n\n<ul>\n<li><strong>CPU time:</strong> This is the CPU usage from the perspective of the operating system. For example, if you have a job that is running one process on a 32-core machine using 50% of all CPU for 10 seconds, then your CPU time would be 32 * 0.5 * 10 = 160 CPU seconds.</li>\n<li><strong>CPU reservation time:</strong> This is the CPU reservation from the perspective of the resource management framework. For example, if we reserve a 32-core machine for 10 seconds to run the job, the CPU reservation time is 32 * 10 = 320 CPU seconds. The ratio of CPU time to CPU reservation time reflects how well are we utilizing the reserved CPU resources on the cluster. When accurate, the reservation time is a better gauge than CPU time for comparing execution engines running the same workloads. For example, if a process requires 1 CPU second to run but must reserve 100 CPU seconds, it is less efficient by this metric than a process that requires 10 CPU seconds and reserves 10 CPU seconds to do the same amount of work.</li>\n<li><strong>Latency / wall time:</strong> End-to-end elapsed time of the job.</li>\n</ul>\n\nThe following charts summarize the performance comparison between the Spark and Hive jobs. Note that with Spark the pipeline is not a 1:1 translation of Hive pipeline. It has several customizations and optimizations that contributed to the better scalability and performance.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/02/spark-vs-hive-large-scale-language-model-training-results.jpg\" alt=\"Results of Spark implementation vs Hive: CPU time is 3.6x efficient, CPU reservation time is 15x efficient, and latency is 2.6x faster\" width=\"736\" height=\"439\" class=\"aligncenter size-full wp-image-10331\" />\n\nSpark-based pipelines can scale comfortably to process many times more input data than what Hive could handle at peak. For example, we trained a large language model on 15x more data, which generated a language model containing 19.2 billion N-grams within a few hours. Ability to train with more data and run experiments faster leads to higher-quality models. Larger language models generally lead to better results in related applications, as we observed in our own experience.\n\n<h2>Conclusion</h2>\n\nThe flexibility offered by Spark helped us to:\n\n<ul>\n<li>Express application logic in a modular way that is more readable and maintainable compared with a monolithic SQL string.</li>\n<li>Perform custom processing over data (e.g., partitioning, shuffling) at any stage in the computation.</li>\n<li>Save compute resources and experimentation time due to a performant compute engine.</li>\n<li>Train high-quality language models due to ability to scale with larger input data.</li>\n<li>Build a generic application that can be used for generating language models across different products.</li>\n<li>Migrate from our earlier solution due to support to run user binaries (like Hive's TRANSFORM) and compatibility in interacting with Hive data.</li>\n</ul>\n\nFacebook is excited to be a part of the Spark open source community and will work together to develop Spark toward its full potential."}
{"status": "publish", "description": null, "creator": "jakebellacera", "link": "https://databricks.com/blog/2017/03/31/delivering-personalized-shopping-experience-apache-spark-databricks.html", "authors": null, "id": 10382, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2017-03-31", "tz": "UTC", "createdOn": "2017-03-31"}, "title": "Delivering a Personalized Shopping Experience with Apache Spark on Databricks", "slug": "delivering-personalized-shopping-experience-apache-spark-databricks", "content": "<em>This is a guest blog from our friends at Dollar Shave Club.</em>\n\n<span style=\"font-size: 1rem;\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/DSC-databricks.png\" />Dollar Shave Club (DSC) is a men's lifestyle brand and e-commerce company on a mission to change the way men address their shaving and grooming needs. Data is perhaps the most critical asset in achieving a cutting-edge user experience. Databricks has been an important partner in our efforts to build a personalized customer experience through data. This post describes how the Databricks platform supported all stages of development and deployment of a powerful, custom machine learning pipeline that we use to deliver targeted content to our members.</span>\n\nDSC\u2019s primary offering is a monthly subscription for razor cartridges, which are shipped directly to members. Our members join and manage their account on our single-page web app or native mobile apps. During their visit, they can shop our catalogue of grooming and bathroom products\u2014we now have dozens of products organized under distinctive brands. Courtesy of the club, members and guests can enjoy <em>Original Content</em>, articles and videos created for people who enjoy our characteristic style. They can satisfy their curiosity on health and grooming topics with articles that don\u2019t remind them of their junior high health class. They can get quick tips on style, work and relationships, or they can read DSC\u2019s fun take on big questions, like \u201cHow long can civilization last on Earth?\u201d DSC also seeks to engage people on social media channels, and our members can be enthusiastic about joining in. By identifying content and offers of the most interest to each individual member, we can provide a more personalized and better membership experience.\n\n<h2>Data at Dollar Shave Club</h2>\n\nDSC\u2019s interactions with members and guests generate a mountain of data. Knowing that the data would be an asset in improving member experience, our engineering team invested early in a modern data infrastructure. Our web applications, internal services and data infrastructure are 100% hosted on AWS. A Redshift cluster serves as the central data warehouse, receiving data from various systems. Records are continuously replicated from production databases into the warehouse. Data also moves between applications and into Redshift mediated by Apache Kafka, an open-source streaming platform. We use Snowplow, a highly-customizable open-source event pipeline, to collect event data from our web and mobile clients as well as server-side applications. Clients emit detailed records of page views, link clicks, browsing activity and any number of custom events and contexts. Once data reaches Redshift, it is accessed through various analytics platforms for monitoring, visualization and insights.\n\n<img class=\"aligncenter size-full wp-image-10385\" src=\"https://databricks.com/wp-content/uploads/2017/03/dollar-shave-club-pipeline.png\" alt=\"An illustration of Dollar Shave Club's data pipeline.\" width=\"678\" height=\"515\" />\n\nWith this level of visibility, there are abundant opportunities to learn from our data and act on it. But identifying those opportunities and executing at scale requires the right tools. Apache Spark\u2014a state-of-the-art cluster computing framework with engines for ETL, stream processing and machine learning\u2014is an obvious choice. Moreover, Databricks' latest developments for data engineering make it exceedingly easy to get started with Spark, providing a platform that is apt as both an IDE and deployment pipeline. On our first day using Databricks, we were equipped to grapple with a new class of data challenges.\n\n<h2>Use Case: Recommendation Engine</h2>\n\nOne of the first projects we developed on Databricks aimed to use predictive modeling to optimize the product recommendations that we make to our members over a particular email channel. Members receive a sequence of emails in the week before their subscription box is shipped. These emails inform them about the upcoming shipment and also suggest additional products that they can include in their box. Members can elect to add a recommended product from the email with just a few clicks. Our goal was to produce, for a given member, a ranking of products that prescribes which products to promote in their monthly email and with what priority.\n\n<img class=\"aligncenter size-full wp-image-10386\" src=\"https://databricks.com/wp-content/uploads/2017/03/dollar-shave-club-recommendation-email.png\" alt=\"An example of the email Dollar Shave Club sends to users to recommend new products.\" width=\"573\" height=\"544\" />\n\nWe planned to perform an exhaustive exploration in search of behavior that tends to indicate a member\u2019s level of interest in each of our products. We would extract a variety of metrics in about a dozen segments of member data, pivot that data by hundreds of categories, actions and tags, and index event-related metrics by discretized time. In all, we included nearly 10,000 features, for a large cohort of members, in the scope of our exploration. To contend with a large, high-dimensional and sparse dataset, we decided to automate the required ETL and Data Mining techniques using Spark Core, Spark SQL and MLlib. The final product would be a collection of linear models, trained and tuned on production data, that could be combined to produce product rankings.\n\nWe set out to develop a fully automated pipeline on Spark with the following stages:\n\n<ol>\n<li>Extract data from warehouse (Redshift)</li>\n<li>Aggregate and pivot data per member</li>\n<li>Select features to include in final models</li>\n</ol>\n\n<h3>Step 1: Extract Data</h3>\n\nWe start by looking at various <em>segments</em> of data in our relational databases; groupings of records that need to be stitched together to describe a domain of events and relationships. We need to understand each data segment---how it can be interpreted and how it might need to be cleaned up---so that we can extract accurate, portable, self-describing representations of the data. This is crucial work that collects together domain expertise and institutional knowledge about the data and its life-cycle, so it is important to document and communicate what is learned in the process. Databricks provides a \"notebook\" interface to the Spark shell that makes it easy to work with data interactively, while having complete use of Spark's programming model. Spark notebooks proved to be ideal for trying out ideas and quickly sharing the results or keeping a chronicle of your work for reference later.\n\nFor each data segment, we encapsulate the specifics of cleaning and denormalizing records in an <em>extractor</em> module. In many cases, we can simply export tables from Redshift, dynamically generate SQL queries, and then let Spark SQL do all the heavy lifting. If necessary, we can cleanly introduce functional programming using Spark's <em>DataFrames</em> API. And the application of domain-specific metadata has a natural home in an extractor. Importantly, the first steps for processing a particular data segment are neatly isolated from that for other segments and from other stages of the pipeline. Extractors can be developed and tested independently. And they can be reused for other explorations or for production pipelines.\n\n<pre><code class=\"python\">def performExtraction(\nextractorClass, exportName, joinTable=None, joinKeyCol=None,\nstartCol=None, includeStartCol=True, eventStartDate=None\n):\ncustomerIdCol = extractorClass.customerIdCol\ntimestampCol = extractorClass.timestampCol\nextrArgs = extractorArgs(\ncustomerIdCol, timestampCol, joinTable, joinKeyCol,\nstartCol, includeStartCol, eventStartDate\n)\nExtractor = extractorClass(**extrArgs)\nexportPath = redshiftExportPath(exportName)\nreturn extractor.exportFromRedshift(exportPath)\n</code></pre>\n\n<strong>Example code for a data extraction pipeline.</strong> The pipeline uses the interface implemented by several extractor classes, passing arguments to customize behavior. The pipeline is agnostic to the details of each extraction.\n\n<pre><code class=\"python\">def exportFromRedshift(self, path):\nexport = self.exportDataFrame()\nwriteParquetWithRetry(export, path)\nreturn sqlContext.read.parquet(path)\n.persist(StorageLevel.MEMORY_AND_DISK)\n\ndef exportDataFrame(self):\nself.registerTempTables()\nquery = self.generateQuery()\nreturn sqlContext.sql(query)\n</code></pre>\n\n<strong>Example code for an extractor interface.</strong> In many cases, an extractor simply generates a SQL query and passes it to SparkSQL.\n\n<h3>Step 2: Aggregate and Pivot</h3>\n\nThe data extracted from the warehouse is mostly detailed information about individual events and relationships; But what we really need is an aggregated description of activity over time, so that we can effectively search for behavior that indicates interest in one product or another. Comparing specific event types second by second is not fruitful\u2014at that level of granularity, the data is much too sparse to be good fodder for machine learning. The first thing to do is to aggregate event-related data over discrete periods of time. Reducing events into counts, totals, averages, frequencies, etc., makes comparisons among members more meaningful and it makes data mining more tractable. Of course, the same set of events can be aggregated over several different dimensions, in addition to time. Any or all of these numbers could tell an interesting story.\n\nAggregating over each of several attributes in a dataset is often called <em>pivoting</em> (or <em>rolling up</em>) the data. When we group data by member and pivot by time and other interesting attributes, the data is transformed from data about individual events and relationships into a (very long) list of features that describe our members. For each data segment, we encapsulate the specific method for pivoting the data in a meaningful way in its own module. We call these modules <em>transformers</em>. Because the pivoted dataset can be extremely wide, it is often more performant to work with RDDs rather than DataFrames. We generally represent the set of pivoted features using a sparse vector format, and we use key-value RDD transformations to reduce the data. Representing member behavior as a sparse vector shrinks the size of the dataset in memory and also makes it easy to generate training sets for use with MLlib in the next stage of the pipeline.\n\n<h3>Step 3: Data Mining</h3>\n\nAt this point in the pipeline, we have an extremely large set of features for each member and we want to determine, for each of our products, which subset of those features best indicate when a member is interested in purchasing that product. This is a data mining problem. If there were fewer features to consider\u2014say, dozens instead of several thousand\u2014then there would be several reasonable ways to proceed. However, the large number of features being considered presents a particularly hard problem. Thanks to the Databricks platform, we were easily able to apply a tremendous amount of computing time to the problem. We used a method that involved training and evaluating models on relatively small, randomly sampled sets of features. Over several hundred iterations, we gradually accumulate a subset of features that each make a significant contribution to a high-performing model. Training a model and calculating the evaluation statistic for each feature in that model is computationally expensive. But we had no trouble provisioning large Spark clusters to do the work for each of our products and then terminating them when the job was done.\n\nIt is essential that we be able to monitor the progress of the data mining process. If there is a bug or data quality issue that is preventing the process from converging on the best performing model, then we need to detect it as early as possible to avoid wasting many hours of processing time. For that purpose, we developed a simple dashboard on Databricks that visualizes the evaluation statistics collected on each iteration.\n\n<h3>Final Models</h3>\n\nThe evaluation module in MLlib makes it exceedingly simple to tune the parameters of its models. Once the hard work of the ETL and data mining process is complete, producing the final models is nearly effortless. After determining the final model coefficients and parameters, we were prepared to start generating product rankings in production. We used Databricks\u2019 scheduling feature to run a daily job to produce product rankings for each of the members who will be receiving an email notification that day. To generate feature vectors for each member, we simply apply to the most recent data the same extractor and transformer modules that generated the original training data. This not only saved development time upfront, it avoids the problems of dual maintenance of exploration and production pipelines. It also ensures that the models are being applied under the most favorable conditions\u2014to features that have precisely the same meaning and context as in the training data.\n\n<h2>Future Plans with Databricks and Apache Spark</h2>\n\nThe product recommendation project turned out to be a great success that has encouraged us to take on similarly ambitious data projects at DSC. Databricks continues to play a vital role in supporting our development workflow, particularly for data products. Large-scale data mining has become an essential tool that we use to gather information to address strategically important questions; and the resulting predictive models can then be deployed to power smart features in production. In addition to machine learning, we have projects that employ stream-processing applications built on Spark Streaming; for example, we consume various event streams to unobtrusively collect and report metrics, or to replicate data across systems in near real-time. And, of course, a growing number of our ETL processes are being developed on Spark."}
{"status": "publish", "description": null, "creator": "jules_damji", "link": "https://databricks.com/blog/2017/03/27/analyse-one-year-of-radio-station-songs-aired-with-apache-spark-spark-sql-spotify-and-databricks.html", "authors": null, "id": 10534, "categories": ["Apache Spark", "Ecosystem", "Engineering Blog"], "dates": {"publishedOn": "2017-03-27", "tz": "UTC", "createdOn": "2017-03-27"}, "title": "Analyse One Year of Radio Station Songs Aired with Apache Spark, Spark SQL, Spotify, and Databricks", "slug": "analyse-one-year-of-radio-station-songs-aired-with-apache-spark-spark-sql-spotify-and-databricks", "content": "<em>This is a guest blog from <a href=\"https://www.linkedin.com/in/paleclercq/\">Paul Leclercq</a>, a data engineer, sports and music lover and was originally published on his <a href=\"https://medium.com/@polomarcus/analyze-one-year-of-radio-station-songs-aired-with-sql-spark-spotify-and-databricks-835fcf73df6\">personal blog</a>.</em>\n\n[dbce_cta href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/6937750999095841/1807085967979471/6197123402747553/latest.html\"]Try this notebook in Databricks[/dbce_cta]\n\nWhenever I drive or code, I listen to music, as this happens a lot, and in order to find new songs, I listen to the radio or I listen to Spotify\u2019s discover weekly playlist, which made me like Mondays (because they release it every Monday).\n\nA french <em>old-school</em> institute called <a href=\"http://www.mediametrie.fr/radio/\" rel=\"nofollow\">Mediam\u00e9trie</a> analyzes radio stations\u2019 songs. Since I have seen their study (which I can\u2019t find anymore) some years ago, I have been obsessed with creating my own.\n\n[caption id=\"attachment_10537\" align=\"aligncenter\" width=\"310\"]<img src=\"https://databricks.com/wp-content/uploads/2017/03/mediametrie.png\" alt=\"\" width=\"310\" height=\"167\" class=\"size-full wp-image-10537\" /> Mediametrie\u2019s image you can find online \u2192 old school[/caption]\n\nThis article will present the year 2016 for 4 main french radio stations through <em>fun</em> SQL queries, then <strong>we will connect each song to the Spotify API</strong> to create the radio stations\u2019 musical profile.\n\nWe will use the <a href=\"https://community.cloud.databricks.com/\">Databricks community version</a> to visualize our data. <strong><a href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/6937750999095841/1807085967979471/6197123402747553/latest.html\">All SQL queries and all results are available on this notebook.</a></strong> <em>It\u2019s the \u201cbackstage\u201d of this article, where the magic happens if we can say.</em>\n\n<em><strong>Protip:</strong> don\u2019t miss the bonuses at the end of the article</em>\n\n<h2>Radio stations introduction</h2>\n\nWe all have a favorite radio station; mine is <a href=\"https://en.wikipedia.org/wiki/Radio_Nova_%28France%29\" rel=\"nofollow\">Radio Nova</a> for its diversity, its humor, and as a hip hop fan, this is the only national radio where we can hear <em>listenable</em> hip hop songs.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/03/Radio-Nova.png\" alt=\"Nova Logo\" width=\"450\" height=\"189\" class=\"aligncenter size-full wp-image-10539\" />\n\nRadio nova had <a href=\"http://www.mediametrie.fr/radio/communiques/telecharger.php?f=b132ecc1609bfcf302615847c1caa69a\" rel=\"nofollow\"><strong>1,4% of the audience in September 2016</strong> (PDF to download from Mediametrie)</a>.\n\n[caption id=\"attachment_10570\" align=\"aligncenter\" width=\"755\"]<img src=\"https://databricks.com/wp-content/uploads/2017/03/top-5-broadcasted-songs-radio-nova-2016.png\" alt=\"\" width=\"755\" height=\"163\" class=\"size-full wp-image-10570\" /> Most 5 broadcasted songs on Nova in 2016[/caption]\n\nIn order to see how a radio becomes number 1, we are also going to analyze the number 1 music radio called <a href=\"http://www.nrj.fr/\" rel=\"nofollow\"><strong>NRJ</strong></a> which has 10.8% of the audience and 2 others: <strong>Virgin</strong> (5%) which, we\u2019ll see, sounds like NRJ, and <strong>Skyrock</strong> (6%), <em>don\u2019t mind the name it\u2019s a rap radio\u2026 haha</em>\n\n[caption id=\"attachment_10569\" align=\"aligncenter\" width=\"759\"]<img src=\"https://databricks.com/wp-content/uploads/2017/03/top-5-broadcasted-songs-nrj-2016.png\" alt=\"\" width=\"759\" height=\"161\" class=\"size-full wp-image-10569\" /> Most 5 broadcasted songs on NRJ in 2016[/caption]\n\nThe main question is, after we compared these radios, should we give to Radio Nova the tips of how to be the number one based on NRJ\u2019s analyze? <strong>What do you say, Nova? Learn from the best, right?!</strong>\n\n<h2>Getting the Radio\u2019s songs data</h2>\n\n[caption id=\"attachment_10576\" align=\"aligncenter\" width=\"551\"]<img src=\"https://databricks.com/wp-content/uploads/2017/03/what-was-this-title-page-radio-nova.png\" alt=\"\" width=\"551\" height=\"351\" class=\"size-full wp-image-10576\" /> \u201cWhat was this title?\u201d Nova page[/caption]\n\nIn order to extract the songs lists, artist, song title and timestamp, we are going to parse each Radio \u201cWhat was this song?\u201d HTML pages, except for Skyrock, which <a href=\"http://skyrock.fm/api/v3/sound?search_date=2016-08-15&search_hour=04:59\" rel=\"nofollow\">has a handy RESTful web service</a>.\n\nEvery song extracted will be converted into this Song class, to query them easily with (Spark) SQL:\n\n<pre><code>Song(timestamp:Int, humanDate:Long, year:Int, month:Int, day:Int, hour:Int, minute: Int, artist:String, allArtists: String, title:String)\n</code></pre>\n\nIn 2016 300K broadcasts were collected:\n\n<ul>\n<li>Nova: <strong>95K broadcasts of 5000 different songs</strong></li>\n<li>NRJ: <strong>50K broadcasts of 800 different songs</strong></li>\n<li>Virgin: <strong>60K broacasts of 1200 different songs</strong></li>\n<li>Skyrock: <strong>100K broadcasts of 1000 different songs</strong></li>\n</ul>\n\nEvery song is stored in a <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#parquet-files\">parquet format</a> to extract only once the data (you\u2019re welcome radios servers :p) and <a href=\"https://blog.cloudera.com/blog/2016/04/benchmarking-apache-parquet-the-allstate-experience/\">to speed up SparkSQL queries</a>. <em>Btw, if you are interested in the file, I can export it to you in CSV or parquet.</em>\n\nRemember that the best way to speed up (the <a href=\"http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence\">Spark doc</a> says often by more than 10x) queries, if you have to use the same SQL table (or Dataset/Dataframe) again and again, is to cache table in memory (<em>Thanks Databricks for the 6Go RAM server!</em>) with the <code>dataframe.cache()</code> method.\n\nLet\u2019s dive into our analysis now!\n\n<h2>How many songs by day?</h2>\n\n<em>Some days were not recorded by the radios\u2019 history system, so the real numbers should be a bit higher.</em>\n\n[caption id=\"attachment_10568\" align=\"aligncenter\" width=\"1682\"]<a href=\"https://databricks.com/wp-content/uploads/2017/03/station-song-broadcasts-per-day.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/station-song-broadcasts-per-day.png\" alt=\"\" width=\"1682\" height=\"430\" class=\"size-full wp-image-10568\" /></a> Songs broadcasted by day[/caption]\n\nFun to see that both radio stations broadcast more songs during summer (if we do not take in consideration the one-week bug of Radio Nova, in blue, in August), this is certainly due to summer holidays. <em>They do a good job all year long, so it\u2019s OK to take some days off, I guess!</em>\n\nWe can see that Skyrock and Nova broadcast the same number of songs each day, whereas NRJ and Virgin a bit less, certainly due to more talk shows or untracked DJs night shows.\n\n<h2>How many different songs by day?</h2>\n\nThe real difference comes from the number of different songs played; see for yourself the number of different tracks per day:\n\n[caption id=\"attachment_10557\" align=\"aligncenter\" width=\"1541\"]<a href=\"https://databricks.com/wp-content/uploads/2017/03/station-different-songs-per-day.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/station-different-songs-per-day.png\" alt=\"\" width=\"1541\" height=\"396\" class=\"size-full wp-image-10557\" /></a> Different songs by day[/caption]\n\nMore mainstream radios such as <strong>NRJ, Virgin and Skyrock top 100/120 different songs</strong> a day, whereas <strong>Nova is more about 280</strong>. If you want to discover more songs, it\u2019s clearly on Nova.\n\n<h2>How many different songs by month?</h2>\n\nIf we have a look to the monthly different songs, the gap between radios is even bigger.\n\n[caption id=\"attachment_10558\" align=\"aligncenter\" width=\"1054\"]<a href=\"https://databricks.com/wp-content/uploads/2017/03/station-different-songs-per-month.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/station-different-songs-per-month.png\" alt=\"\" width=\"1054\" height=\"351\" class=\"size-full wp-image-10558\" /></a> Different songs by month in 2016[/caption]\n\n<h2>Top 10 played titles by each radio station</h2>\n\nIt\u2019s interesting to see how \u201chits\u201d are played through the year.\n\nWe can notice summer hits: <a href=\"https://open.spotify.com/track/0EM0yABJzbFOvZQkfvuvCy\" rel=\"nofollow\">Kaytranada</a> for Nova, <a href=\"https://open.spotify.com/track/6YZdkObH88npeKrrkb8Ggf\" rel=\"nofollow\">Enrique Iglesias</a> for NRJ, <a href=\"https://open.spotify.com/track/27PmvZoffODNFW2p7ehZTQ\" rel=\"nofollow\">Kent Jones</a> and <a href=\"https://open.spotify.com/track/1xznGGDReH1oQq0xzbwXa3\" rel=\"nofollow\">Drake</a> for Skyrock and <a href=\"https://open.spotify.com/track/1ZdWNpOXCJT1nmt40UuxWS\" rel=\"nofollow\">Imany</a> and <a href=\"https://open.spotify.com/track/0cAuqPI1R8RlFsXXWWO039\" rel=\"nofollow\">Kungs</a> for Virgin. And also, most broadcasted songs are mostly aired during summer.\n\nRadio tends to broadcast more songs during summer. So artists play smart here and release their songs between February and June to have more chance to become number one, <em>or to have more people hating their music because they heard it too many times?</em>\n\n<h3>Nova</h3>\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/03/top-songs-nova.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/top-songs-nova.png\" alt=\"Top 10 songs played by Nova\" width=\"1733\" height=\"314\" class=\"aligncenter size-full wp-image-10571\" /></a>\n\n<h3>NRJ</h3>\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/03/top-songs-nrj.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/top-songs-nrj.png\" alt=\"Top 10 songs played by NRJ\" width=\"1566\" height=\"281\" class=\"aligncenter size-full wp-image-10572\" /></a>\n\n<h3>Skyrock</h3>\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/03/top-songs-skyrock.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/top-songs-skyrock.png\" alt=\"Top 10 songs played by Skyrock\" width=\"1632\" height=\"274\" class=\"aligncenter size-full wp-image-10573\" /></a>\n\n<h3>Virgin</h3>\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/03/top-songs-virgin.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/top-songs-virgin.png\" alt=\"Top 10 songs played by Virgin\" width=\"1607\" height=\"308\" class=\"aligncenter size-full wp-image-10574\" /></a>\n\n<h2>Percentage of music by day</h2>\n\nIf we take the average broadcasted songs by day and the mean duration of a song, 3.30 minutes, we can guess the percentage of music by day. The other percentage is likely to be talk shows, advertising or untracked songs.\n\n[caption id=\"attachment_10565\" align=\"aligncenter\" width=\"740\"]<img src=\"https://databricks.com/wp-content/uploads/2017/03/station-percentage-of-music-per-day.png\" alt=\"\" width=\"740\" height=\"261\" class=\"size-full wp-image-10565\" /> Percentage of music per day[/caption]\n\nTo understand more these percentages, we should see what a normal day is for our analyzed radio stations.\n\n<h2>What is a typical Monday for our radio stations ?</h2>\n\nLet\u2019s have a look to the average of number of songs for all radio stations for Mondays.\n\n[caption id=\"attachment_10551\" align=\"aligncenter\" width=\"1415\"]<a href=\"https://databricks.com/wp-content/uploads/2017/03/station-average-number-of-tracks-on-monday.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/station-average-number-of-tracks-on-monday.png\" alt=\"\" width=\"1415\" height=\"375\" class=\"size-full wp-image-10551\" /></a> Average number of songs for Mondays[/caption]\n\nWe can distinguish 2 gaps during <strong>the morning and evening shows</strong> for every radio station. <em>Amazing</em>. More seriously, no discovery here; it\u2019s a known fact that most radios have morning and evening shows during which there is less music and more talk.\n\n<h2>Advertising time</h2>\n\nIf we recalculate the average percentage of music at noon, when there are no shows for all radio stations, we can estimate the percentage of advertising by radio by hour. We estimate that the radio hosts speak 5 minutes during the whole hour. <em>We have to note that radios may advertise more during prime time when they have a larger audience.</em>\n\nFor 60 minutes, we get 7 minutes of advertising time, for Skyrock, to 15 minutes, for Virgin. In details, we have this table:\n\n[caption id=\"attachment_10555\" align=\"aligncenter\" width=\"1104\"]<a href=\"https://databricks.com/wp-content/uploads/2017/03/station-average-time-spent.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/station-average-time-spent.png\" alt=\"\" width=\"1104\" height=\"136\" class=\"size-full wp-image-10555\" /></a> Average minute of music and advertising for every Monday at noon[/caption]\n\n<h2>Radios brainwashing?</h2>\n\nAn annoying feeling we have sometimes with radios is <strong>we keep listening to the same songs over and over</strong>. As we are men and women who believe in science and not in our instinct, we are going to use basic <strong>statistics to verify this weird feeling</strong>.\n\n<h3>How many times is the same song aired on the same day?</h3>\n\n[caption id=\"attachment_10552\" align=\"aligncenter\" width=\"690\"]<a href=\"https://databricks.com/wp-content/uploads/2017/03/station-average-rebroadcast.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/station-average-rebroadcast.png\" alt=\"\" width=\"690\" height=\"237\" class=\"size-full wp-image-10552\" /></a> Average number of times the same song is broadcasted by day[/caption]\n\nThese pie charts below tell us a lot about radio stations\u2019s habits; that is, more mainstream radios such as Virgin, NRJ or Skyrock are more about to broadcasts the same songs multiple times.\n\n[caption id=\"attachment_10567\" align=\"aligncenter\" width=\"1170\"]<a href=\"https://databricks.com/wp-content/uploads/2017/03/station-song-broadcast-frequency.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/station-song-broadcast-frequency.png\" alt=\"\" width=\"1170\" height=\"758\" class=\"size-full wp-image-10567\" /></a> How many times is the same song broadcasted?[/caption]\n\n<h3>When is the next time we will listen to the same song during the same day?</h3>\n\n[caption id=\"attachment_10561\" align=\"aligncenter\" width=\"1261\"]<a href=\"https://databricks.com/wp-content/uploads/2017/03/station-hours-between-same-song.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/station-hours-between-same-song.png\" alt=\"\" width=\"1261\" height=\"830\" class=\"size-full wp-image-10561\" /></a> Minimum difference in hours between the same songs broadcasted on same day[/caption]\n\nAgain, the most mainstream radios, NRJ, Skyrock and Virgin tend to broadcast the same song most often for 2/3 hours since it was first aired. Nova\u2019s value is more about 7/8 hours.\n\nWhile we have different distribution, the average for our 4 radios is between 7 and 8 hours.\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/03/station-average-hours-between-same-song.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/station-average-hours-between-same-song.png\" alt=\"Table showing the average number of hours between rebroadcasting the same song\" width=\"811\" height=\"140\" class=\"aligncenter size-full wp-image-10549\" /></a>\n\n<h3>How many new songs are added and when?</h3>\n\n\u201cNew songs\u201d means songs that are not yet broadcasted in 2016.\n\n[caption id=\"attachment_10563\" align=\"aligncenter\" width=\"1356\"]<a href=\"https://databricks.com/wp-content/uploads/2017/03/station-new-songs-per-month-distribution.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/station-new-songs-per-month-distribution.png\" alt=\"\" width=\"1356\" height=\"258\" class=\"size-full wp-image-10563\" /></a> New songs distributed by month[/caption]\n\nIf we look at the average after April 2016, we see that Nova is ahead, but don\u2019t forget that Nova plays 2500 different songs each month, so it\u2019s normal, statistically speaking.\n\n[caption id=\"attachment_10550\" align=\"aligncenter\" width=\"639\"]<a href=\"https://databricks.com/wp-content/uploads/2017/03/station-average-new-songs-per-month.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/station-average-new-songs-per-month.png\" alt=\"\" width=\"639\" height=\"242\" class=\"size-full wp-image-10550\" /></a> Average new songs by month[/caption]\n\nNew songs are distributed equally along the week for all radios.\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/03/nrj-and-nova-new-songs-per-weekday.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/nrj-and-nova-new-songs-per-weekday.png\" alt=\"NRJ and Nova - new songs per weekday\" width=\"843\" height=\"393\" class=\"aligncenter size-full wp-image-10538\" /></a>\n\n<h2>Common songs between radio stations</h2>\n\nOn the table below, we can see NRJ has 25% of common songs with Virgin and 12% with Skyrock.\n\nVirgin has 18% with NRJ while Skyrock has 9% of common songs with NRJ.\n\n[caption id=\"attachment_10541\" align=\"aligncenter\" width=\"1375\"]<a href=\"https://databricks.com/wp-content/uploads/2017/03/similar-songs-on-stations-table.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/similar-songs-on-stations-table.png\" alt=\"\" width=\"1375\" height=\"200\" class=\"size-full wp-image-10541\" /></a> Number of same songs broadcasted between radios[/caption]\n\nNova has a few similar songs with the others radio; there are mostly legendary artists such as Bob Marley, Daft Punk, Aloe Blacc, Kavinsky, Beyonc\u00e9\u2026 <em>If you are interested by the full list look for the \u201cSimilar songs between radios\u201d cell in <a href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/6937750999095841/1807085967979471/6197123402747553/latest.html\">the \u201cbackstage\u201d AKA the blog article\u2019s Databricks notebook</a>.</em>\n\nOur 4 radio stations are different, for sure, but do they have common songs among them? Surprisingly the answer is yes.\n\n<ul>\n<li><a href=\"https://open.spotify.com/track/28S2K2KIvnZ9H6AyhRtenm\" rel=\"nofollow\">Prince\u200a\u2014\u200aKiss</a></li>\n<li><a href=\"https://open.spotify.com/track/255uSDEuvkWp1QyYnm82VJ\" rel=\"nofollow\">C2C\u200a\u2014\u200aHappy</a></li>\n<li><a href=\"https://open.spotify.com/track/4z70Px77quweOupQRiaG2Q\" rel=\"nofollow\">Stromae\u200a\u2014\u200aFormidable</a></li>\n</ul>\n\nI would classify these songs as songs that everybody likes; <em>you can play them at your party without any stress of being booed.</em>\n\nIf we use a visualization for our previous table, it will look like this: the blue bar is the similar songs, the orange and the green bar are the total of different songs.\n\n[caption id=\"attachment_10540\" align=\"aligncenter\" width=\"1387\"]<a href=\"https://databricks.com/wp-content/uploads/2017/03/similar-songs-on-stations-bar.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/similar-songs-on-stations-bar.png\" alt=\"\" width=\"1387\" height=\"417\" class=\"size-full wp-image-10540\" /></a> Similar songs[/caption]\n\n<h2>What are the secrets to be #1 ?</h2>\n\nWe have analyzed 4 radio stations based on the artist name, the title name and the day and time the songs were broadcasted. Beside letters and numbers, these 3 values mean nothing, if we want to make a deeper analysis, we have to learn more about the songs played: how popular is the song right now? what is the genre of the song? How many followers does the artist have?\n\nHopefully, <strong>by connecting each song to the Spotify API</strong> we will get a lot of data we can play with:\n\n<pre><code>https://api.spotify.com/v1/search?q=[ARTIST_TITLE]&amp;type=track&amp;limit=1\n</code></pre>\n\nIn 2016, we have collected 8000 different songs from the radios, so to <strong>get the artist, the track and the tracks\u2019 audio features</strong> from the <strong>Spotify API</strong> we have to make:\n\n<pre><code>Number of songs * (Artist + track + audiofeatures) = 24K requests\n</code></pre>\n\n[caption id=\"attachment_10536\" align=\"aligncenter\" width=\"650\"]<img src=\"https://databricks.com/wp-content/uploads/2017/03/http-error-429.jpg\" alt=\"\" width=\"650\" height=\"500\" class=\"size-full wp-image-10536\" /> From <a href=\"https://httpstatusdogs.com/\" rel=\"nofollow\">HTTP STATUS DOGS</a>[/caption]\n\n<em><strong>That\u2019s a lot</strong></em>. Plus, Spotify has a limit of request in time, so we have got to do it slowly, 20 request every 2 seconds, <em>why not you know.</em>\n\n<strong>BUT</strong>, with this slow rate one thing I didn\u2019t plan is we could see the number of followers change when we requested a song\u2019s artist, as most artists have multiple songs been broadcasted, the artist information was asked from 2 to 10 times. <em>No problemo, right?</em> No\u2026This will mess up our SQL join between artist and track data later, just because the DISTINCT on artists information <a href=\"https://www.youtube.com/watch?v=1IDF-8khS3w\" rel=\"nofollow\">were fake</a> due to <a href=\"https://developer.spotify.com/web-api/object-model/#followers-object\"><code>followers.total</code></a>\n\n<em>I have to say this led me to craziness, because I had more songs after my join than before haha</em>\n\n[caption id=\"attachment_10543\" align=\"aligncenter\" width=\"807\"]<a href=\"https://databricks.com/wp-content/uploads/2017/03/spotify-api-stats.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/spotify-api-stats.png\" alt=\"\" width=\"807\" height=\"489\" class=\"size-full wp-image-10543\" /></a> Spotify API Stats from <a href=\"https://developer.spotify.com/my-applications\" rel=\"nofollow\">https://developer.spotify.com/my-applications</a>[/caption]\n\n<h2>Songs Popularity By Radio</h2>\n\n<h3>Definition by Spotify</h3>\n\n<em>The popularity is calculated by algorithm and is based, in the most part, on the total number of plays the track has had and how recent those plays were.</em>\n\n[caption id=\"attachment_10566\" align=\"aligncenter\" width=\"1332\"]<a href=\"https://databricks.com/wp-content/uploads/2017/03/station-popularity-distribution.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/station-popularity-distribution.png\" alt=\"\" width=\"1332\" height=\"313\" class=\"size-full wp-image-10566\" /></a> Percentage of songs popularity distribution by radio[/caption]\n\nNo surprises here, mainstream radios NRJ, Virgin or Skyrock, tend to play more popular songs; <em>that\u2019s why I use the term mainstream, clever, right?</em>\n\n[caption id=\"attachment_10553\" align=\"aligncenter\" width=\"834\"]<a href=\"https://databricks.com/wp-content/uploads/2017/03/station-average-song-popularity.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/station-average-song-popularity.png\" alt=\"\" width=\"834\" height=\"218\" class=\"size-full wp-image-10553\" /></a> Popularity average[/caption]\n\n<em>But the real question is: was the song popular before it was broadcasted on the radio?</em>\n\n<h2>Audio features</h2>\n\nThe Spotify API gives <a href=\"https://developer.spotify.com/web-api/get-audio-features/#tablepress-215\">audio features extracted from the song\u2019s sound waves</a>, thanks to these we can display a musical profile of each radio:\n\n<a href=\"https://www.youtube.com/watch?v=pWdd6_ZxX8c\" rel=\"nofollow\">In my opinion</a>, the most meaningful audio features are:\n\n<ul>\n<li><strong>danceability</strong> <em>describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity.</em></li>\n<li><strong>energy</strong> <em>is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy.</em></li>\n<li><strong>valence</strong> <em>describes the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).</em></li>\n</ul>\n\nLet\u2019s see <strong>their average and their distribution</strong>, <em>as a average alone can be sometimes misleading</em>, among the radios\u2019 tracks. As Nova got more different songs than the others, we are going use percentage to compare our radios to add more context to our stats.\n\n<a href=\"https://medium.com/@polomarcus/my-facebook-interview-journey-5205e111155f#.r3w705wo3\" rel=\"nofollow\">If you have read my Facebook Interview Journey</a>, you know this is where I failed during my SQL interview, this code is specially for you, dear Mr. Interviewer, no hard feelings though :p\n\n<pre><code class=\"sql\">SELECT ROUND( (COUNT(t.*) / subTotal.total_radio * 100),2) AS percentage_of_songs,\n      subTotal.total_radio,\n      ROUND(popularity / 10) * 10 AS popularity,\n      t.radio\nFROM AudioFeatureArtistTrackRadios t\nJOIN (\n    SELECT COUNT(*) AS total_radio, radio\n    FROM AudioFeatureArtistTrackRadios\n    GROUP BY radio\n) AS subTotal\nON subTotal.radio = t.radio\nGROUP BY subTotal.total_radio, ROUND(popularity / 10) * 10, t.radio\nORDER BY popularity\n</code></pre>\n\n[caption]Brilliant SQL code (<a href=\"https://gist.github.com/polomarcus/5868f1009b8361711dd556fda1de4890#file-popularity_percentage-sql\" rel=\"nofollow\">by @polomarcus</a>)[/caption]\n\n<h3>Energy</h3>\n\n[caption id=\"attachment_10548\" align=\"aligncenter\" width=\"1551\"]<a href=\"https://databricks.com/wp-content/uploads/2017/03/station-average-energy-distribution.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/station-average-energy-distribution.png\" alt=\"\" width=\"1551\" height=\"428\" class=\"size-full wp-image-10548\" /></a> Energy by radio distribution[/caption]\n\nMainstream radios tend to play more energetic songs, <em>I guess they are easier to listen to?</em> Some examples of song with a lot of energy are <a href=\"https://open.spotify.com/track/6Unf6X6PC7KZtrRXH3dFHg\" rel=\"nofollow\">We Are Your Friends - JUSTICE</a>, <a href=\"https://open.spotify.com/track/7yI4qKGEFHNId1B893XopS\" rel=\"nofollow\">Steppin\u2019 Stone - Davy Jones</a>, and of course, the classics from <a href=\"https://open.spotify.com/track/1bx7OUl2UmAnA5oZkm9If7\" rel=\"nofollow\">Jerk It Out\u200a\u2014\u200aCaesars</a>, <em>I\u2019ve first heard it while playing <a href=\"https://www.youtube.com/watch?v=-0x3aZEPcMA\" rel=\"nofollow\">SSX3 on GameCube</a> 8)</em>\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/03/station-average-song-energy.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/station-average-song-energy.png\" alt=\"Chart showing each radio station&#039;s average song energy\" width=\"686\" height=\"243\" class=\"aligncenter size-full wp-image-10578\" /></a>\n\n<h3>Danceability</h3>\n\nThis chart tells us both radio broadcasts with the same kind of danceable songs. Some examples of danceable songs are <a href=\"https://open.spotify.com/track/3aImJnJlAgcE7bJ1NxthGt\" rel=\"nofollow\">Trick Me\u200a\u2014\u200aKelis</a>, <a href=\"https://open.spotify.com/track/1pKYYY0dkg23sQQXi0Q5zN\" rel=\"nofollow\">Around the World\u200a\u2014\u200aDaft Punk</a> or <a href=\"https://www.youtube.com/watch?v=LDZX4ooRsWs\" rel=\"nofollow\">Anaconda\u200a\u2014\u200aNicki Minaj</a>.\n\n<h3>Valence / Positiveness</h3>\n\n[caption id=\"attachment_10575\" align=\"aligncenter\" width=\"1742\"]<a href=\"https://databricks.com/wp-content/uploads/2017/03/valence-by-radio-distribution.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/valence-by-radio-distribution.png\" alt=\"\" width=\"1742\" height=\"397\" class=\"size-full wp-image-10575\" /></a> Valence / Positiveness By Radio distribution[/caption]\n\nSame as the danceability, both radio broadcasts show the same kind of positive tracks. Some examples: <a href=\"https://open.spotify.com/track/5nNmj1cLH3r4aA4XDJ2bgY\" rel=\"nofollow\">September\u200a\u2014\u200aEarth Wind &amp; Fire</a>, \n<a href=\"https://open.spotify.com/track/4BLu47sbjr3aJZwxZujgXT\" rel=\"nofollow\">Ska-Boo-Da-Ba\u200a\u2014\u200aThe Skatalites</a>, or <a href=\"https://open.spotify.com/track/5WQ1hIc5d2EVbRQ8qsj8Uh\" rel=\"nofollow\">Hey Ya!\u200a\u2014\u200aOutKast</a>.\n\n[caption id=\"attachment_10556\" align=\"aligncenter\" width=\"667\"]<a href=\"https://databricks.com/wp-content/uploads/2017/03/station-average-valence.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/station-average-valence.png\" alt=\"\" width=\"667\" height=\"257\" class=\"size-full wp-image-10556\" /></a> Valence / Positiveness average by radio[/caption]\n\nTwo others interesting data points, which are not Spotify (<a href=\"http://the.echonest.com/\">Echo Nest</a>) specific, are the BPM (beats per minute) and the songs duration.\n\n<h3>Tempo / Beats per minute</h3>\n\n[caption id=\"attachment_10554\" align=\"aligncenter\" width=\"1643\"]<a href=\"https://databricks.com/wp-content/uploads/2017/03/station-average-tempo-distributed.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/station-average-tempo-distributed.png\" alt=\"\" width=\"1643\" height=\"318\" class=\"size-full wp-image-10554\" /></a> Tempo distribution[/caption]\n\n<h3>Duration</h3>\n\n[caption id=\"attachment_10546\" align=\"aligncenter\" width=\"1504\"]<a href=\"https://databricks.com/wp-content/uploads/2017/03/station-average-duration-distributed.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/station-average-duration-distributed.png\" alt=\"\" width=\"1504\" height=\"431\" class=\"size-full wp-image-10546\" /></a> Duration distribution[/caption]\n\nNova seems to be a bit different from the other radios by playing shorter or longer tracks. Virgin, NRJ and Skyrock are really into 3-minute tracks.\n\nWhen I first saw this graph, I couldn\u2019t help myself to think about this <a href=\"https://open.spotify.com/track/1DhpyURGQ8gAQCYo8dOLQo\" rel=\"nofollow\">Hocus Pocus\u2019 song called \u201cVoyage Immobile\u201d</a> (motionless journey) and this sentence about our un-diversified musical environment:\n\n<em>\u201cJe ne voyais que blocs longs de 3 minutes taill\u00e9 dans le roc et dans le m\u00eame but\u201d</em>\n\n<em>\u201cI could only see 3-minute blocks from the same base with the same goal\u201d</em>\n\n[caption id=\"attachment_10547\" align=\"aligncenter\" width=\"677\"]<a href=\"https://databricks.com/wp-content/uploads/2017/03/station-average-duration.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/station-average-duration.png\" alt=\"\" width=\"677\" height=\"243\" class=\"size-full wp-image-10547\" /></a> The duration average in minutes by radio[/caption]\n\n<h2>Music genres</h2>\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/03/station-music-genre-distribution.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/station-music-genre-distribution.png\" alt=\"Music genre distribution\" width=\"1107\" height=\"784\" class=\"aligncenter size-full wp-image-10562\" /></a>\n\nSpotify got some pretty weird music genres. Have you noticed \u201cpost-teen pop\u201d, \u201cpop christmas\u201d, <em>pop songs you listen during christmas I guess? haha</em>\n\nWe can clearly see that <strong>NRJ and Virgin</strong>, <em>which are very alike</em>, are more about <strong>pop/dance/electro</strong> music; their top 3 genres are: pop, dance pop and tropical house. <strong>Nova</strong> is about <strong>soul, funk and indie music</strong>, and <strong>Skyrock</strong> is more about <strong>rap, dance and pop</strong>.\n\n[caption id=\"attachment_10559\" align=\"aligncenter\" width=\"664\"]<a href=\"https://databricks.com/wp-content/uploads/2017/03/station-genre-variety.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/station-genre-variety.png\" alt=\"\" width=\"664\" height=\"242\" class=\"size-full wp-image-10559\" /></a> Number of different music genres by radio[/caption]\n\n<h3>Hip hop genres</h3>\n\n[caption id=\"attachment_10542\" align=\"aligncenter\" width=\"480\"]<img src=\"https://databricks.com/wp-content/uploads/2017/03/skyrock-logo.jpg\" alt=\"Skyrock Logo\" width=\"480\" height=\"269\" class=\"size-full wp-image-10542\" /> First on rap[/caption]\n\nSkyrock is famous for its motto \u201c1st on Rap\u201d. Let\u2019s compare Hip hop/Rap genres (genres with \u201crap\u201d, \u201chip\u201d or \u201chop\u201d inside the name) with the others radios.\n\n[caption id=\"attachment_10564\" align=\"aligncenter\" width=\"821\"]<a href=\"https://databricks.com/wp-content/uploads/2017/03/station-number-of-hiphop-songs.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/station-number-of-hiphop-songs.png\" alt=\"\" width=\"821\" height=\"140\" class=\"size-full wp-image-10564\" /></a> Number of hip hop, rap songs by radio[/caption]\n\nOK, that\u2019s a close match between Skyrock and Nova. Let\u2019s compare the internal hip hop genres now.\n\n<em>I don\u2019t really care about genres, but there is a lot of confusion between Hip hop, which is a culture, and rap, which is the actual fact of rapping; if you want to learn more, check this <a href=\"https://en.wikipedia.org/wiki/Hip_hop#Culture\" rel=\"nofollow\">Wikipedia Chapter</a>, I also recommend the excellent <a href=\"https://www.netflix.com/title/80141782\" rel=\"nofollow\">Netflix\u2019s documentary \u201cEvolution of Hip Hop\u201d</a></em>\n\n[caption id=\"attachment_10560\" align=\"aligncenter\" width=\"1671\"]<a href=\"https://databricks.com/wp-content/uploads/2017/03/station-hip-hop-genres.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/station-hip-hop-genres.png\" alt=\"\" width=\"1671\" height=\"487\" class=\"size-full wp-image-10560\" /></a> Genres with Hip or Hop or Rap by Radios[/caption]\n\nNova, in orange, is more about indie/alternative/undergroup hip hop music, and Skyrock, in blue, is really more into French rap/trap/hiphop and also popular rap. So let\u2019s fix Skyrock\u2019s motto by \u201c1st on <strong>French</strong> rap\u201d haha.\n\n<h2>Music classifier for Radios\u2019 selection idea</h2>\n\n<a href=\"https://medium.com/@polomarcus/music-recommendation-service-with-the-spotify-api-spark-mllib-and-databricks-7cde9b16d35d\" rel=\"nofollow\">In my last article</a>, I explained how to create your own music recommendation system thanks to these audio features.\n\nA <a href=\"https://en.coursera.org/learn/progfun1\" rel=\"nofollow\">fun project</a> (<em>the link is a tribute to the Scala Guru <a href=\"https://medium.com/@odersky\" rel=\"nofollow\">Martin Odersky</a>, he tends to say too many times that his Scala exercises are fun whereas they are brain melt haha</em>) would be to create an algorithm that will help music selectors to find radios style\u2019s songs.\n\n<h3>Spotify recommendation system</h3>\n\nSpotify\u2019s system is not only based on the audio features we saw earlier. It also analyzes what others similar users listen to. <a href=\"http://www.slideshare.net/sinisalyh/scala-data-pipelines-spotify/5-Recommendation_systems\">This slide contains a nice schema that explains their whole system.</a>\n\n<h2>What\u2019s next?</h2>\n\nThanks to this project, I have built solid foundations to query the Spotify API in Scala; process it thanks to Spark SQL, and visualize it thanks to <a href=\"http://www.databricks.com\">Databricks</a>. I think more projects are about to come, plus Spotify has just released, <em>March 2017</em>, this new endpoint <a href=\"https://developer.spotify.com/news-stories/2017/03/01/new-endpoint-recently-played-tracks/\">\u201cRecently Played Tracks\u201d</a> and ideas are coming.\n\n<h2>Databricks pros and cons</h2>\n\n<h3>Pros</h3>\n\n<ul>\n<li>Free <a href=\"http://databricks.com/try\" target=\"_blank\">community edition</a> with 6Go RAM server</li>\n<li>Awesome and easy-to-use Data Viz</li>\n</ul>\n\n<h3>Cons (or more, what can be better)</h3>\n\n<ul>\n<li>Can only visualize a maximum of 10 elements when using a GROUP BY; the others elements go to one category called \u201cOthers\u201d</li>\n<li>Not possible to choose the color of an entity, so a Radio can be blue on a graph and red on another; it can be sometimes confusing</li>\n<li>Cannot export graph as iframe, so we have to export pictures from the interactive graphs</li>\n<li>Modify SQL on the Data Viz interface</li>\n</ul>\n\n<h2>Thanks</h2>\n\n<strong>Databricks</strong>, for their awesome platform.\n\n<strong>Spotify</strong>, for their easy-to-use API and their <strong>human-readable</strong> documentation\n\n<a href=\"http://www.novaplanet.com/\" rel=\"nofollow\">Radio Nova</a> for being a top music selecta, <em>I would not listen to the same music that I listen to today without you.</em>\n\n<a href=\"https://twitter.com/m_hlimi\" rel=\"nofollow\">Marc H\u2019LIMI</a>, Radio Nova\u2019s advisor, for our exchanges\n\nPierre Trussart, engineer and DJ, <a href=\"https://twitter.com/b_thuillier\" rel=\"nofollow\">Benjamin Thuillier</a>, scala rockstar, Nicolas Duforet, data science master, Justine Mouron, engineer.\n\nMy friends for hearing me talking about this project too often.\n\n<h2>Bonus\u200a\u2014\u200aSpotify Playlists</h2>\n\nTo thank for reading, I created 4 playlists of the most ~200 songs broadcasted sorted by the number of broadcast for:\n\n<ul>\n<li><a href=\"https://open.spotify.com/user/cpolos/playlist/64DWZ46dFb50FaWs9eALIu\" rel=\"nofollow\"><strong>Nova</strong> with Calipso Rose, Brisa Roch\u00e9, Kaytranada, The Roots, M.I.A, The Virgins\u2026</a></li>\n<li><a href=\"https://open.spotify.com/user/cpolos/playlist/0x2SJgvfcBzxfiMTDkTuC4\" rel=\"nofollow\"><strong>Skyrock</strong> with Drake, Alonzo, Major Lazer, Timberlake, Soprano, PNL, Jul\u2026</a></li>\n<li><a href=\"https://open.spotify.com/user/cpolos/playlist/3ySBdFmKboUlMWfqSpLZio\" rel=\"nofollow\"><strong>Virgin</strong> with Imany, Twenty One Pilots, Sia, Kungs, Julian Perretta\u2026</a></li>\n<li><a href=\"https://open.spotify.com/user/cpolos/playlist/7ECHPNZ5fjLggN2rg0poK3\" rel=\"nofollow\"><strong>NRJ</strong> with Enrique Iglesias, Soprano, Coldplay, Kungs, Amir, MHD, Tal\u2026</a></li>\n</ul>"}
{"status": "publish", "description": null, "creator": "jules_damji", "link": "https://databricks.com/blog/2017/03/28/demand-webinar-faq-apache-spark-mllib-2-x-productionize-machine-learning-models.html", "authors": null, "id": 10584, "categories": ["Apache Spark", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2017-03-28", "tz": "UTC", "createdOn": "2017-03-28"}, "title": "On-Demand Webinar and FAQ: Apache Spark MLlib 2.x: How to Productionize your Machine Learning Models", "slug": "demand-webinar-faq-apache-spark-mllib-2-x-productionize-machine-learning-models", "content": "On March 9th, we hosted a live webinar\u2014<a href=\"http://go.databricks.com/apache-spark-mllib-2.x-how-to-productionize-your-machine-learning-models\">Apache Spark MLlib 2.x: How to Productionize your Machine Learning Models</a>\u2014to address the following questions:\n\n<ol>\n<li>How do you deploy machine learning models to a production environment?</li>\n<li>How do you embed what you've learned into customer facing data applications?</li>\n<li>What are the best practices from Databricks on how customers productionize machine learning models?</li>\n</ol>\n\nTo address the above concerns, we did a deep dive with actual customer case studies and showed live tutorials of a few example architectures and code in Python, Scala, Java and SQL.\n\nIf you missed the webinar, you can view it on-demand <a href=\"http://go.databricks.com/apache-spark-mllib-migrating-ml-workloads-to-dataframes-webinar\">here</a>, and the <a href=\"https://www.slideshare.net/julesdamji/apache-spark-mllib-2x-how-to-productionize-your-machine-learning-models\">slides</a> and <a href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/1526931011080774/1904316851197504/6320440561800420/latest.html\">notebook</a> are accessible as attachments to the webinar.\n\nToward the end, we did a Q&amp;A, and below are all the questions with links to forums with their answers. (Follow the links below to view the answers.)\n\n<ul>\n<li><a href=\"https://forums.databricks.com/questions/11153/i-thought-that-machine-learning-ml-is-an-upgrade-f.html#answer-11154\">I thought that Machine Learning (ML) is an upgrade from MLlib. Is MLlib 2.x more update to date than ML?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11156/if-i-have-a-mainframe-deployment-environment-where.html#answer-11157\">PipelineModel instances all have Dataset objects as input and output, and creating a Dataset requires having a SparkSession active. (Right?) If I have a mainframe deployment environment where I just want to give it a single record and get a single record back, what are my options?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11158/most-of-the-models-in-mllib-support-pmml-export-wh.html#answer-11159\">Most of the models in MLlib support PMML export. What was the motivation for developing a proprietary real-time scoring model export?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11160/i-assume-this-is-a-proprietary-format-for-exportin.html#answer-11161\">I assume this is a proprietary format for exporting the model. Why not use an open standard like pmml?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11163/are-there-any-mllib-standard-implementations-of-cl.html\">Are there any MLlib standard implementations of clustering algorithms other than k-means?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11165/it-seems-like-one-might-want-to-use-similar-devops.html#answer-11166\">It seems like one might want to use similar DevOPs CD/CI techniques and apply to ML and model development. How do you see the flow and what products would help (e.g. like a Jenkins product, a build tool...?) to use with DevOps CD/CI scenario?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11167/what-common-apis-have-you-seen-that-are-used-for-s.html#answer-11168\">What common APIs have you seen that are used for scoring an Apache Spark ALS model in real-time?</a></li>\n<li>1) <a href=\"https://forums.databricks.com/questions/11170/why-does-the-spark-model-scoring-eg-decision-tree.html#answer-11171\">Why does the Spark model scoring (e.g decision tree ) make it hard or impossible to get a probability and easy to get a prediction (which is not very useful). 2) How can you export a model in a readable form (e.g. PMML) or generate code?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11172/is-this-databricks-library-compatible-with-the-dat.html#answer-11173\">Is this databricks library compatible with the dataset-based SparkML (as opposed to MLlib)?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11175/can-you-give-some-prediction-when-randomforest-wil.html#answer-11176\">Can you give some prediction when RandomForest will become available in dbml-local?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11177/seems-like-we-should-be-able-to-make-an-aws-lambda.html#answer-11178\">Seems like we should be able to make an AWS Lambda microservice that picks up the trained model from S3, and uses the dbml-local library to make the predictions?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11179/can-you-give-an-example-use-case-where-a-customer.html#answer-11180\">Can you give an example use case where a customer needed to train a model on Apache Spark, but deploy on an external system? How common is that?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11181/weve-been-waiting-for-dbml-local-for-a-long-time-g.html#answer-11182\">We've been waiting for dbml-local for a long time! Great addition! When do you expect all classifiers in spark to be available in dbml-local?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11183/are-all-the-spark-mllib-models-available-to-use-wi.html#answer-11184\">Are all the spark mllib models available to use with dbml-local?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11186/is-there-support-to-export-the-model-as-a-pojo-obj.html#answer-11187\">Is there support to export the model as a pojo object?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11189/is-there-any-plan-to-implement-k-mediods-in-apache.html#answer-11190\">Is there any plan to implement k-mediods in Apache Spark</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11192/are-you-planning-to-provide-dbml-outside-of-databr.html#answer-11193\">Are you planning to provide dbml outside of databricks? 2. how does dbml relate to tensorframes?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11195/can-you-comment-on-dataframe-model-and-mllib-for-p.html#answer-11196\">Can you comment on DataFrame model and MLlib for production?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11198/do-you-support-other-data-formats-such-as-netcdf.html#answer-11199\">Do you support other data formats such as netCDF?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11200/will-scoring-consider-ml-pipeline-activities-like.html#answer-11201\">Will scoring consider ML pipeline activities like feature extraction?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11203/is-there-any-plan-to-publish-rest-apis-in-apache-s.html#answer-11204\">Is there any plan to publish rest APIs in Apache Spark itself to submit spark jobs?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11205/will-the-exportmodel-functionality-available-to-ot.html#answer-11206\">Will the exportModel functionality available to other language like python or R?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11207/in-the-decision-tree-visualization-can-the-real-fe.html#answer-11208\">In the decision tree visualization, can the real feature names (instead of feature 1, 2, 3, etc.) be displayed in the visualization?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11209/as-of-now-there-is-support-only-for-logistics-regr.html#answer-11210\">As of now there is support only for logistics regression model to be used outside of Apache Spark?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11211/what-were-the-pros-and-cons-of-the-3-different-sch.html#answer-11212\">What were the pros and cons of the 3 different schemes that you presented to productionalize ML models? Why specifically demonstrate the third option?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11213/do-you-plan-to-support-other-ml-libraries-in-addit.html#answer-11214\">Do you plan to support other ML libraries in addition to MLlib?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11216/dont-you-think-pmml-is-the-standard-for-exchange-f.html#answer-11217\">Don't you think PMML is the standard for exchange format for predictive models?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11218/is-dbml-library-available-for-community.html#answer-11219\">Is dbml library available for community?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11220/could-you-please-clarify-what-this-model-score-opt.html#answer-11221\">Could you please clarify what this model score option (private beta?) is? Is that available to all paying customers? If we do not use that, I'd like to know what we have to do achieve the same thing with Databricks, Apache \u00a0Spark and MLlib.</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11222/for-raw-input-how-features-are-being-computed-that.html#answer-11223\">For raw input, how features are being computed that are being passed to model which you showed in eclipse?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11224/how-do-you-compare-the-quality-and-efficiency-betw.html#answer-11225\">How do you compare the quality and efficiency between spark ML 2.1 and scikit-learn?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11226/where-can-you-obtain-dbml-local-jar-only-available.html#answer-11227\">Where can you obtain dbml-local jar? Only available to Databricks customers?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11229/today-some-productionized-machine-learning-models.html#answer-11230\">Today, some productionized machine learning models are updated each days. Do you have you a solution to obtain the optimized parameters model ? (RandomSearchCrossValidation, but it takes a certain time on a large and distributed configuration..)</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11233/what-if-we-build-a-modeling-technology-of-our-own.html#answer-11234\">What if we build a modeling technology of our own ? (creating a modeling class, based on Scala or Python libraries of our own), how would we ensure this could be deployed using the same approach you've shown?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11238/is-there-any-focus-on-increasing-the-performance-o.html#answer-11239\">I have found spark.ml gradient boosted trees to be slower than other packages, such as h2o sparkling water. Is there any focus on increasing the performance of the gradient boosted trees or better incorporating another package such as h2o sparkling water or xgboost4j?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11240/can-you-talk-about-how-you-would-replicate-the-spa.html#answer-11241\">Can you talk about how you would replicate the Spark training pipeline (string indexing, vector assembling, etc) in this application?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11242/how-would-an-ensemble-of-models-run-using-this-new.html#answer-11243\">How would an ensemble of models run using this new scoring approach? By creating and saving an ensemble pipeline?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11244/what-is-the-best-way-to-deploy-a-predictive-model.html#answer-11245\">What is the best way to deploy a predictive model or recommendation engine if the scoring environment is on an IOS app?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11246/dbml-local-looks-very-much-like-mleap-open-source.html#answer-11247\">dbml-local looks very much like mleap open-source project. Isn't it better for DB to contribute to that?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11248/can-spark-mllib-or-dbml-local-somehow-read-scikit.html#answer-11249\">Can Spark MLlib (or dbml-local) somehow read scikit-learn's model file?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11250/from-performance-perspective-have-you-done-any-per.html#answer-11251\">From performance perspective, have you done any performance comparison between spark.ml and sklearn (same algorithm and parameter)? And is there a list of algorithms that will run really well on Apache Spark?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11252/when-using-or-doing-the-local-model-option-calcula.html#answer-11253\">When using or doing the 'local model' option, calculating 100 features has overhead- is precomputing always necessary?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11254/where-can-i-find-more-information-on-dbml-local-ca.html#answer-11255\">Where can I find more information on dbml-local? Can't find it on your GitHub and not getting many results when searching on google.</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11274/can-you-talk-about-the-option-to-save-as-pmml-1.html#answer-11275\">Can you talk about the option to save as PMML?</a></li>\n</ul>\n\nIf you'd like free access to Databricks, you can access the <a href=\"https://databricks.com/try\">free trial here</a>."}
{"status": "publish", "description": null, "creator": "jules_damji", "link": "https://databricks.com/blog/2017/03/30/tenth-spark-summit-with-a-terrific-agenda-for-all.html", "authors": null, "id": 10596, "categories": ["Announcements", "Company Blog", "Events"], "dates": {"publishedOn": "2017-03-30", "tz": "UTC", "createdOn": "2017-03-30"}, "title": "The Tenth Spark Summit with a Terrific Agenda for All", "slug": "tenth-spark-summit-with-a-terrific-agenda-for-all", "content": "<a href=\"https://spark-summit.org/2017/\"><img class=\"alignnone size-medium\" src=\"https://databricks.com/wp-content/uploads/2017/03/SS2017-BLOG-IMAGE3.jpg\" alt=\"Spark Summit 2017\" width=\"1480\" height=\"620\" /></a>\n<br>\nThe number 10 is often used as a measuring yardstick to denote achievement, attainment or accomplishment: the 10th anniversary; a perfect score of 10; top 10 playlist; or 10 revolutionary technologies of the last 10 decades.\n\nFor Databricks, and the Apache Spark community, this 10th Spark Summit is our hallmark and height of achievement. Our first <a href=\"https://spark-summit.org/2013/\" target=\"_blank\">Spark Summit 2013</a> had only two tracks, spread over two days, with the agenda focused only toward developers and data scientists\u2014and rightly so. For we believed then (and still do now) that <a href=\"https://thenewkingmakers.com/\" target=\"_blank\">developers are the new kingmakers</a>.\n\nYet over the years, the adoption of Apache Spark has expanded, deepened and flourished, in scope, breadth, and depth in areas of <a href=\"https://spark-summit.org/east-2017/speakers/mike-gualtieri/\" target=\"_blank\">artificial intelligence</a> and other <a href=\"https://databricks.com/blog/2016/09/27/spark-survey-2016-released.html\" target=\"_blank\">vertical industries</a>. It has become part and parcel of <a href=\"https://www.brighttalk.com/webcast/12641/174391/webinar-with-forrester-apache-spark-are-you-ready\" target=\"_blank\">enterprises\u2019 deployment option</a> where processing distributed data for advanced analytics at blazing speed and at terabyte scale is imperative.\n\nTo describe such growth and adoption is to account not only for the expansion of Spark Summit\u2019s agenda, tracks, larger venue, and sessions but also for the growing numbers of enterprises and attendees participating and sharing their usage and knowledge of Spark at each subsequent Spark Summit since 2013.\n\nUnder the judicious guidance of <a href=\"https://www.linkedin.com/in/reynoldxin\" target=\"_blank\">Reynold Xin</a> and <a href=\"https://svds.com/spark-summit-ignition-enterprise/?utm_content=bufferd3010&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer\" target=\"_blank\">Edd Wilder-James</a> as co-chairs of the Spark Summit Program, we\u2019ve crafted a comprehensive program, marking the summit with an agenda curated and calibrated to resonate with all attendees. Held at a bigger venue, <a href=\"http://www.moscone.com/\" target=\"_blank\">The Moscone Center</a>, the 3-day program includes:\n\n<ul>\n    <li>over 160 sessions across<a href=\"https://spark-summit.org/2017/schedule/\" target=\"_blank\"> nine distinct tracks</a>, with new tracks for streaming and deep-dives, for you to choose from;</li>\n    <li> two days of <a href=\"https://spark-summit.org/2017/apache-spark-training/\" target=\"_blank\">Apache Spark training</a>, for beginners and advanced Spark users;</li>\n    <li>first-time <a href=\"http://go.spark-summit.org/2017-hackathon?\" target=\"_blank\">Apache Spark Hackathon,</a> for small teams to work on challenging problems;</li>\n    <li>two <a href=\"https://www.meetup.com/spark-users/\" target=\"_blank\">Bay Area Apache Spark Meetups</a>, for tech-talks and networking; and</li>\n    <li> an attendee party, for all to celebrate at the end. </li>\n</ul>\n\nSo please check our <a href=\"https://spark-summit.org/2017/schedule/\" target=\"_blank\">schedule</a> today, and <a href=\"https://prevalentdesignevents.com/sparksummit/ss17/\" target=\"_blank\">register for the early bird discount rate</a> before <strong>April 7, 2017</strong>, to save over $500.\n\nWe hope to see you at the oasis of Spark knowledge in San Francisco this summer to celebrate our achievement with the rest of the Spark community: This tenth summit is our mark of pride; make it yours too!\n\n<a href=\"https://spark-summit.org/2017/\"><img class=\"alignnone size-medium\" src=\"https://databricks.com/wp-content/uploads/2017/03/SS2017-BLOG-IMAGE2.png\" alt=\"Spark Summit 2017\" width=\"1480\" height=\"220\" /></a>"}
{"status": "publish", "description": null, "creator": "jakebellacera", "link": "https://databricks.com/blog/2017/04/01/next-generation-physical-planning-in-apache-spark.html", "authors": null, "id": 10644, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2017-04-01", "tz": "UTC", "createdOn": "2017-04-01"}, "title": "Next Generation Physical Planning in Apache Spark", "slug": "next-generation-physical-planning-in-apache-spark", "content": "<blockquote style=\"margin-bottom: 2em;\">Never underestimate the bandwidth of a station wagon full of tapes hurtling down the highway.\n<cite>\u2014 Andrew Tanenbaum, 1981</cite></blockquote>\n\n<strong><img style=\"display: inline-block; margin-top: -1em; margin-right: -.6em;\" src=\"https://databricks.com/wp-content/uploads/2017/03/really-fancy-i.png\" alt=\"I\" width=\"49\" height=\"40\" />magine</strong> a cold, windy day at Lake Tahoe in California. When a group of <a href=\"https://databricks.com/company/team\">Bricksters</a> who were cross-country skiing stopped for lunch, one found a 4TB hard drive in their backpack. One brickster remarked that this data had traveled by car over 200 miles in less than 8 hours \u2014 accounting for a speed of over 1 Gb/s. This led to an idea: What if Apache Spark leveraged physical storage media\u00a0and vehicles to move data?\n\n[caption id=\"attachment_10660\" align=\"alignright\" width=\"337\"]<img class=\"size-full wp-image-10660\" src=\"https://databricks.com/wp-content/uploads/2017/03/db-skiing-trip.jpg\" alt=\"Photo from the Databricks Tahoe Ski Trip\" width=\"337\" height=\"223\" /> High throughput physical data channel leading from a data lake (South Lake Tahoe).[/caption]\n\n<h2>Transcontinental Data Transfer Shortcomings</h2>\n\nIn this era of high-volume, inter-continental data processing, with globally distributed, exponentially growing data volumes, networks are too slow. A single link between two nodes in a datacenter may be as slow as 10Gb/s -- and cross-datacenter links are often 100x slower! So how do we better solve the everyday problem of transferring billions, trillions, or quadrillions of bytes around\u00a0the world?\n\nWhile for analytical queries it is possible to <a href=\"https://people.eecs.berkeley.edu/~qifan/papers/gda.pdf\">optimize global execution via proper task and data placement</a>, sometimes data movement is unavoidable, e.g. due to the nature of an <a href=\"https://en.wikipedia.org/wiki/Extract,_transform,_load\">ETL job</a>, compute constraints, or local regulations. We found a number of such use cases when we talked with our customers, including:\n\n<ul>\n    <li>Recurring transfers due to\n<ul>\n    <li>Desire to retain data in particular region (e.g. safe harbor laws)</li>\n    <li>Excess purchased compute capacity available in certain regions</li>\n    <li>Streaming ETL of cross-continental data</li>\n</ul>\n</li>\n    <li>One-time transfer tasks, such as\n<ul>\n    <li>Moving\u00a0data to implement\u00a0with data architecture changes</li>\n    <li>Disaster recovery (adding or recovering data replicas)</li>\n</ul>\n</li>\n</ul>\n\nFor example, consider a Spark job in which thousands of terabytes of data distributed in the Mumbai, Singapore, and Seoul regions need to be moved to Sydney, where there is more purchased compute capacity available. We'd also like to repartition the data into an appropriate number of Parquet-encoded files to optimize performance.\n\nNormally, such a job would be expressed in Spark as follows:\n\n<pre><code class=\"scala\">val df = spark.read.parquet(\n\"s3a://databricks-prod-metrics-mumbai/\",\n\"s3a://databricks-prod-metrics-singapore/\",\n\"s3a://databricks-prod-metrics-seoul/\")\n\ndf.repartition(250000).write.parquet(\"s3a://databricks-prod-metrics-sydney\")\n</code></pre>\n\nHowever, if a user were to run this job, they would quickly see\u00a0two\u00a0problems.\n\n<ol>\n<li>Cross-region data transfer across the Internet is incredibly slow. Compared to intra-datacenter networks, Internet egress is orders of magnitude slower.</li>\n<li>Cross-region data transfer is incredibly expensive. Large data transfers can sometimes cost into the millions of dollars.</li>\n</ol>\n\nMost users, faced with these insurmountable time and cost obstacles, would have to turn to hand-optimizing their data transfers. More sophisticated users may seek to optimize data transfers using private links. Others would consult with maritime shipping agencies to figure out how to ship their data -- in secure physical containers.\n\n[row][col sm=6]\n\n[caption id=\"attachment_10659\" align=\"aligncenter\" width=\"532\"]<img class=\"size-full wp-image-10659\" src=\"https://databricks.com/wp-content/uploads/2017/03/capp-undersea-cables.jpg\" alt=\"CAPP undersea cable plan map\" width=\"532\" height=\"522\" /> A complicated set of undersea cables allows for cross-continental Internet traffic.[/caption]\n\n[/col][col sm=6]\n\n[caption id=\"attachment_10657\" align=\"aligncenter\" width=\"534\"]<img class=\"size-full wp-image-10657\" src=\"https://databricks.com/wp-content/uploads/2017/03/capp-maritime-routes.jpg\" alt=\"CAPP maritime routes plan\" width=\"534\" height=\"522\" /> Maritime routes allow for much higher throughput data transfer, at the cost of latency and logistics overhead.[/caption]\n\n[/col][/row]\n\nAt Databricks, our vision is to make Big Data simple. So we find these sort of workarounds unacceptable. Inspired by their experience with the efficiency of cross-country skiing, our\u00a0group of engineers turned their efforts towards addressing this issue.\n\n<h2>Introducing Catalyst APP (Actual Physical Planning)</h2>\n\nEnter <strong>Catalyst actual physical planning</strong>. Combining the power of the <a href=\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\">Spark Catalyst optimizer</a> with <a href=\"https://aws.amazon.com/snowmobile/\">Amazon Snowmobile</a>, Spark identifies queries running with compute in one region and data in another region, and adaptively decides to migrate the data to the local datacenter before running the query. Prior to this approach, all data access would necessarily occur cross-region, going over public Internet links and capped in a best-case scenario at around 100 Gb/s.\n\n[caption id=\"attachment_10663\" align=\"aligncenter\" width=\"1024\"]<img class=\"size-full wp-image-10663\" src=\"https://databricks.com/wp-content/uploads/2017/03/spark-catalyst-optimization-pipeline.png\" alt=\"\" width=\"1024\" height=\"235\" /> Overview of the Spark Catalyst optimization pipeline.[/caption]\n\nCatalyst APP uses\u00a0the <a href=\"https://aws.amazon.com/snowball/\">Amazon Snowball</a> and <a href=\"https://aws.amazon.com/snowmobile/\">Snowmobile</a> APIs to schedule one or more trucks (depending on the amount of data and desired parallelism) to one datacenter and ship it to the target datacenter. Once the transfer is complete, the query begins on the local data.\n\nUsing this technique, we\u2019ve found we can achieve 60x the performance while being 30x cheaper than the alternatives.\n\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Bandwidth</th>\n<th>Cost per exabyte</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Over-the-Internet</td>\n<td>~100 Gb/s</td>\n<td>$80mm ($0.08/GB)</td>\n</tr>\n<tr>\n<td>Mail (USPS) (250k 4TB hard drives)</td>\n<td>~53 Gb/s (1 person boxing and unboxing drives constantly for 4 years)</td>\n<td>$2.25mm ($1.25mm shipping, $1mm min. salary)</td>\n</tr>\n<tr>\n<td>Lay your own undersea cable</td>\n<td>~60,000 Gb/s (but takes a couple years to build)</td>\n<td>$300mm (fixed cost)</td>\n</tr>\n<tr>\n<td>Snowmobile (10 trucks, 2 week travel time)</td>\n<td>~6,614 Gb/s</td>\n<td>$2.5mm ($0.0025/GB)</td>\n</tr>\n</tbody>\n<tfoot>\n<tr>\n<td>Improvement</td>\n<td>66x faster than Internet</td>\n<td>32x cheaper than Internet</td>\n</tr>\n</tfoot>\n</table>\n\nLet us describe how Catalyst APP works in detail by considering the previous example of an ETL job. This job requires executing\u00a0three steps:\n\n<ol>\n<li>First, data needs to be read from the three source regions.</li>\n<li>Second, the data needs to be <a href=\"https://databricks.com/blog/2014/10/10/spark-petabyte-sort.html\">efficiently partitioned</a> into 250,000 slices.</li>\n<li>Finally, the data needs to be written into the destination region (Sydney).</li>\n</ol>\n\n[caption id=\"attachment_10658\" align=\"aligncenter\" width=\"623\"]<img class=\"size-full wp-image-10658\" style=\"border: 1px solid #000;\" src=\"https://databricks.com/wp-content/uploads/2017/03/capp-spark-transferring-data.png\" alt=\"\" width=\"623\" height=\"464\" /> Spark cluster in Oregon transferring data from multiple Asia regions to Sydney. Projection does not indicate optimal transfer route, which depends on <a href=\"https://drive.google.com/file/d/0B3Um1hpy8q7gVjhVT3dGUWFxRm8/view\">how the path is optimized for partition and piracy tolerance</a>.[/caption]\n\nNormally, Catalyst will only consider one physical strategy for each of these steps, since it has no alternative but to access the data over the network from the Spark executors. This unoptimized query plan is simple:\n\n<pre><code>== Physical Plan ==\nExchange RoundRobinPartitioning(250000)\n+- *FileScan parquet [id#964L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3a:/databricks-prod-metrics-mumbai/, s3a:/databricks-prod-metrics-singapore/, s3a:/databricks-prod-metrics-seoul/], PartitionFilters: [], PushedFilters: [], ReadSchema: struct\n</code></pre>\n\nWhen we ran the job using\u00a0this na\u00efve plan, it never finished execution because\u00a0the WAN transfer was too slow, despite using hundreds of executors. We also saw our AWS bill\u00a0skyrocket. Eventually, we gave up and cancelled the job after a few months:\n\n[caption id=\"attachment_10661\" align=\"aligncenter\" width=\"290\"]<img class=\"size-full wp-image-10661\" src=\"https://databricks.com/wp-content/uploads/2017/03/job-using-cross-region-internet-access.png\" alt=\"\" width=\"290\" height=\"288\" /> Plan for job using direct cross-region Internet access. The job did not complete.[/caption]\n\nHowever, with Catalyst <em><strong>actual physical planning</strong></em>, Catalyst can consider alternate strategies for\u00a0moving the data. Based on data statistics and real-time\u00a0pricing information, it determines the optimal physical strategy for executing transfers. In the optimized job, from our cluster in the Oregon (us-west-2) region, Spark ingests the data from Mumbai, Singapore, and Seoul via Snowmobile, performs a local <a href=\"https://databricks.com/blog/2014/10/10/spark-petabyte-sort.html\">petabyte-scale repartitioning</a>, and then writes it out to Sydney again via Snowmobile.\n\nThis optimized plan completed in only a few weeks -- more than fast enough given the scale of the transfer. In fact, if not for some straggler shipments when crossing the Pacific (more on that later) it would have completed significantly faster:\n\n[caption id=\"attachment_10662\" align=\"aligncenter\" width=\"834\"]<img class=\"size-full wp-image-10662\" src=\"https://databricks.com/wp-content/uploads/2017/03/petabyte-job-using-capp-snowmobile.png\" alt=\"\" width=\"834\" height=\"617\" /> Plan for a 61 Petabyte job using Catalyst APP + Snowmobile. Completed in &lt;5 weeks.[/caption]\n\n<h2>Future Optimizations</h2>\n\nWhile Catalyst <em>actual physical planning</em> is already a\u00a0breakthrough, we also plan on implementing several\u00a0other physical planning techniques in Spark:\n\n<b>Physical Shuffle:</b>\u00a0The best <a href=\"http://sortbenchmark.org/\">distributed sorting implementations</a>, even using Apache Spark, take hundreds of dollars in compute cost to sort 100 TB of data. In contrast, using physical data containers, an experienced intern at Databricks can sort over 10 PB per hour. We are excited to offer this physical operator to our customers soon. If you are <a href=\"https://databricks.com/company/careers\">interested in an internship</a>, applications for that are also open.\n\n[caption id=\"attachment_10687\" align=\"alignnone\" width=\"434\"]<a href=\"https://databricks.com/wp-content/uploads/2017/03/memory-cards-2013-pile-3.jpg\"><img class=\" wp-image-10687\" src=\"https://databricks.com/wp-content/uploads/2017/03/memory-cards-2013-pile-3.jpg\" alt=\"\" width=\"434\" height=\"323\" /></a> 3 TB of data partially sorted using the Catalyst Actual Physical Shuffle operator.[/caption]\n\n<b>Actual Broadcast Join:</b>\u00a0Many Spark applications, ranging from SQL queries to machine learning algorithms, heavily use <a href=\"https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala\">broadcast joins</a>. On traditional intercontinental networks, broadcast joins soon become impractical for large tables due to congestion. However, the physical electromagnetic spectrum is naturally a broadcast medium. Thanks to the falling cost of <a href=\"http://www.theverge.com/2017/3/30/15117096/spacex-launch-reusable-rocket-success-falcon-9-landing\">satellite technology</a>, we're excited to (literally) launch a Physical Broadcast operator that can transmit terabyte-sized tables to globally distributed datacenters.\n\n<strong>Serverless Transfer:</strong> If the query plan includes no intermediate transformations, it is possible to dispatch Snowmobile jobs that write directly to the output\u00a0destination. This cuts cost and latency\u00a0by removing\u00a0the need for a cluster. In fact, thanks to <a href=\"https://databricks.com/blog/2016/12/21/deep-learning-on-databricks.html\">deep learning on Apache Spark</a>, future transfers could be <a href=\"https://databricks.com/blog/2016/12/21/deep-learning-on-databricks.html\">both serverless and driverless</a>.\n\n<h2>CAPP (Catalyst Actual Physical Planning) Theorem</h2>\n\nIn the course of this work, we noticed that in addition to the expected weather delays, piracy is a real problem for high throughput cross-continental data transfers. This is especially true when using low-cost shipping, creating a tradeoff between Piracy and Cost. We formalized this tradeoff in a new paper on an extension to\u00a0the <a href=\"https://en.wikipedia.org/wiki/CAP_theorem\">CAP Theorem</a> called <a href=\"https://drive.google.com/file/d/0B3Um1hpy8q7gVjhVT3dGUWFxRm8/view\">CAPP</a>.\u00a0There, we discuss how we ultimately must choose two of the following: <a href=\"https://drive.google.com/file/d/0B3Um1hpy8q7gVjhVT3dGUWFxRm8/view\">Consistency, Availability, Partition-tolerance, and Piracy-proofness</a>.\n\nThankfully Databricks\u00a0can automatically encrypt data in transit, which means that Catalyst APP is safe even for organizations with the <a href=\"https://databricks.com/company/newsroom/press-releases/databricks-announces-hipaa-compliance-apache-spark-based-platform-achieves-aws-public-sector-partner-status\">most stringent data security requirements</a>.\n\n<h2>Conclusion</h2>\n\nCatalyst <em>actual physical planning</em> enables a new class of exabyte-scale, cross-continent ETL workloads on Databricks, moving us one step closer to\u00a0our vision of making Big Data simple. Watch out for our entry in this year\u2019s <a href=\"http://sortbenchmark.org/\">international sorting competition</a>. Catalyst <em>actual physical planning</em> is now available\u00a0in private preview for Databricks users. Please <a href=\"https://go.databricks.com/contact-databricks\">contact us</a> if you are interested in early access.\n\nAlso, Happy April 1st!"}
{"status": "publish", "description": null, "creator": "jakebellacera", "link": "https://databricks.com/blog/2017/04/03/take-reports-concept-production-pyspark-databricks.html", "authors": null, "id": 10674, "categories": ["Company Blog", "Customers"], "dates": {"publishedOn": "2017-04-03", "tz": "UTC", "createdOn": "2017-04-03"}, "title": "Take Reports From Concept to Production with PySpark and Databricks", "slug": "take-reports-concept-production-pyspark-databricks", "content": "<h2>Introduction: What is MediaMath?</h2>\n\nMediaMath is a demand-side media buying and data management platform. This means that brands and ad agencies can use our software to programmatically buy advertisements as well as manage and use the data that they have collected from their users. We serve over a billion ads each day, and track over 4 billion events that occur on the sites of our customers on a busy day. This wealth of data makes it easy to imagine novel reports in response to nearly any situation. Turning these ideas into scalable products consumable by our clients is challenging however.\n\n[caption id=\"attachment_10676\" align=\"aligncenter\" width=\"706\"]<a href=\"https://databricks.com/wp-content/uploads/2017/03/mediamath-db-chart.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/mediamath-db-chart.png\" alt=\"\" width=\"706\" height=\"733\" class=\"size-full wp-image-10676\" /></a> The typical lifecycle of a new report. Popular reports are first built into a custom web-app for clients. Extremely popular ones are built into the UI of our core product.[/caption]\n\n<h2>Reporting at MediaMath</h2>\n\nTypically the life cycle of a new report we dream up is:\n\nProof of concept is easy. All it takes is a novel client request or a bright idea combined with a scrappy analyst, and you\u2019ve got a great idea for a new product. Building a proof of concept into a custom web app is harder, but should be achievable by a savvy team with a few days to dedicate to the project. Including the report in the core product is often prohibitively hard, as it requires coordination between potentially many teams with potentially competing priorities. This blog will focus on the second stage of this process: Turning a concept report into a scalable web app for client consumption, a process that Databricks has significantly streamlined for us.\n\n<h2>The Audience Index Report: What is it?</h2>\n\nThe Audience Index Report (<strong>AIR</strong>) was developed to help current MediaMath clients understand the demographic makeup of users visiting their sites. Using segment data containing (anonymized of course) demographic and behavioral data of our users, the AIR provides a measure of the observed number of site-visitors from a segment compared to the expected number of site-visitors from that segment. The measure of this difference is referred to as the Index for segment \ud835\udc60 and site (also referred to as \u2018pixel\u2019) \ud835\udc5d. Please refer to the appendix for a more detailed description of the Index. For now, you should know that in order to compute the index for a site-segment you need to know four quantities:\n\n<ol>\n<li>|\ud835\udc60 \u2229 \ud835\udc5d| : the number of users in segment \ud835\udc60 who also fired pixel \ud835\udc5d (a pixel is used to track site visits)</li>\n<li>|\ud835\udc3a<sub>\ud835\udc60</sub> \u2229 \ud835\udc5d| : the number of users in segment group \ud835\udc3a who also fired pixel \ud835\udc5d (\ud835\udc3a is the collection of segments to which \ud835\udc60 belongs)</li>\n<li>|\ud835\udc60| : the number of users in segment \ud835\udc60</li>\n<li>|\ud835\udc3a<sub>\ud835\udc60</sub>| : the number of users in the segment group \ud835\udc3a</li>\n</ol>\n\nKnowing the index of a site-segment is useful because it allows MediaMath clients to quantify the demographic and behavioral features of the users browsing their web content. It also is beneficial to our partners, because our clients can take insights gleaned from the report and then target an appropriate audience population (also known as a segment) by buying data from our partners.\n\n<h2>Producing the AIR - Why Databricks?</h2>\n\nAt a very high level, producing the AIR requires the following:\n\n<ol>\n<li>Process segment data to make it useable</li>\n<li>Join processed segment data to first party data and aggregate</li>\n<li>Write results to a relational database to serve our web app</li>\n</ol>\n\nI chose to implement this workflow with Apache Spark in the end, despite how primarily ETL heavy it was. I chose Spark for a couple of reasons, but it was primarily because much of the processing required was awkward to express with SQL. Spark\u2019s RDD APIs for Python provided the low-level customization I needed for the core ETL work. Additionally RDDs are readily transformed into DataFrames, so once I was done with the messy transformations I could slap a schema on my RDDs and use the very convenient DataFrame APIs to further aggregate them or write the results to S3 or a database.\n\nThe Databricks platform was also convenient because it brought all of the overhead required to run this workflow into one place. The Databricks UI is focused on notebooks, which suits this workflow well. I was able to create a couple of classes that handled all of the ETL, logic, logging and extra monitoring that I needed. I imported these classes into a handler notebook and used the Databricks job scheduler to configure the details of the cluster that this workflow runs on and to run the job itself. Now I\u2019ve got my entire workflow running from just one Python notebook in Databricks! This convenience sped up the development process tremendously compared to previous projects and was just a lot more fun.\n\n[caption id=\"attachment_10677\" align=\"aligncenter\" width=\"642\"]<img src=\"https://databricks.com/wp-content/uploads/2017/03/mediamath-db-workflow.png\" alt=\"\" width=\"642\" height=\"152\" class=\"size-full wp-image-10677\" /> Databricks greatly reduces the overhead required for building an effective, back end workflow for our reports. Now many data engineering tasks are trivial and more energy can be focused on producing content, rather than maintaining overhead.[/caption]\n\n<h3>Processing segment data</h3>\n\nLet\u2019s dig into how this report is generated. A user is defined to be \u2018in segment\u2019 if they have been added to the segment at least once in the last 30 days. Given this definition, the most useful format for the segment data is a key/value system. I will refer to this dataset as UDB (User DataBase). I chose to store the records as sequence files in S3 with the following structure:\n\n<table>\n<thead>\n<tr>\n<th>Key</th>\n<th>Value</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>UserID</td>\n<td>Nested Dictionary with segmentID and max and min timestamps corresponding to the time when the user was added to the segment.</td>\n</tr>\n</tbody>\n</table>\n\nHere is an example of one record from UDB:\n\n<pre><code>(u'49ce54b1-c829-4d00-b490-a9443a2829b5',\n  {11: {u'max': 1488499293, u'min': 1486658209},\n  101: {u'max': 1488499293, u'min': 1486658209},\n  123: {u'max': 1488499293, u'min': 1486230978}})\n</code></pre>\n\nAn added bonus here is that the first party data can be stored in exactly this same way, only in place of the segmentID we use pixelID (an identifier for the site). We produce this dataset by using the new day\u2019s data to update the current state of UDB each day. Here\u2019s what this step looks like:\n\n<pre><code class=\"python\"># grab new data from a hive table\nnew_data = sqlContext.sql(self.query.format(self.current_date))\n  .rdd.map(lambda x: (x[0],x))\n  .combineByKey(createCombiner,mergeValue,mergeCombiners)\n  .partitionBy(self.partitions)\n\n# use the new data to update the current state\nself.data = self.data.fullOuterJoin(new_data)\n  .mapValues(combine_join_results)\n\n# write out current state to S3 as sequence files\nself.data\n  .saveAsSequenceFile(\n    self.path.format(self.current_date),\n    compressionCodecClass=\"org.apache.hadoop.io.compress.DefaultCodec\"\n  )\n</code></pre>\n\nWe are well prepared now for Step 2: Joining and aggregating the data.\n\n<h3>Joining data and aggregating</h3>\n\nSince our intent is to understand the demographic information for each site, all we have to do is join the Site data to UDB. Site data and UDB are both stored as pairRDDs and are readily joined to produce a record like this:\n\n<pre><code>(u'49ce54b1-c829-4d00-b490-a9443a2829b5',   #key                 \n  ({11: {u'max': 1488499293, u'min': 1486658209},   #segments            \n    101: {u'max': 1488499293, u'min': 1486658209},             \n    123: {u'max': 1488499293, u'min': 1486230978}},            \n   {457455: {u'max': 1489356106, u'min': 1489355833}, #sites      \n    1016015: {u'max': 1489357958, u'min': 1489327086},     \n    1017238: {u'max': 1489355286, u'min': 1486658207}}))\n</code></pre>\n\nAfter the join it\u2019s just a matter of counting up all of the siteID-segmentID and siteID-segmentGroup combinations we saw. This sounds easy, but it is the ugly part. Since one active user may have visited many sites and be in many segments, exploding the nested records actually causes quite a bit of extra data (up to |\ud835\udc5d| \u22c5 |\ud835\udc60| records for each user) so care must be taken to maintain an appropriate level of parallelism here. Using our example above, our result dataset would look like this:\n\n<pre><code>(11, 457455),\n(11, 1016015),\n(101, 457455),\n(101, 1016015),\n(123,457455),\n(123,1016015),\n(123,1017238)\n</code></pre>\n\nNotice how there are only seven lines here rather than nine. This is because we enforce the condition that a user must be in a segment before the first time they visit a site in order to be included in this report. Two records are scrubbed out here for that reason. Now I can convert this dataset into a DataFrame and aggregate it appropriately (<code>count()</code> grouping by site and segment). Since the result is itself a DataFrame, we are well set up for step 3 - writing to the relational database. This is the workflow for |\ud835\udc5d \u2229 \ud835\udc60|. The workflow for |\ud835\udc5d \u2229 \ud835\udc3a<sup>\ud835\udc60</sup>| is similar, and I\u2019ll omit it.\n\n<h3>Writing to the relational database</h3>\n\nWe use an AWS hosted PostgreSQL RDS to serve data to our web-app. Spark\u2019s JDBC connector makes it trivial to write the contents of a DataFrame to a relational database such as this. Using PySpark and Postgres, you can run something like this:\n\n<pre><code class=\"python\">jdbcUrl='jdbc:postgresql://MY_HOST:PORT/MY_DATABASE?user=xx&amp;password=xx'\nyourDataframe.write.jdbc(jdbcUrl,YourTableName)\n</code></pre>\n\nYou can even bake table management into your class methods to streamline the process of updating a live table. For example if you can\u2019t use the <code>mode=overwrite</code> option of the <code>write.jdbc()</code> method (since the target table may be a production table you don\u2019t want to be down while you overwrite it), you can define a function like this:\n\n<pre><code class=\"python\">def write_and_replace_table(TABLENAME):\n  stage_table=TABLENAME + '_stage'\n  temp_table=TABLENAME + '_tmp'\n  #write the contents of the DF to RDS\n  yourDataframe\n    .write\n    .jdbc(jdbcUrl,stage_table,mode='overwrite')\n  #manage the table swap\n  rename_table(TABLENAME,temp_table)\n  rename_table(stage_table,TABLENAME)\n  rename_table(temp_table,stage_table)\n</code></pre>\n\n<h2>Conclusion</h2>\n\nNow we\u2019ve got everything in an Amazon RDS where careful database design allows our app to serve data quickly. We\u2019ve managed to take hundreds of terabytes of data (this report aggregates the last 30 days) and condense it into a consumable report widely used by clients at MediaMath. Databricks provides a convenient platform to abstract away the painful parts of the orchestration and monitoring of this process. With Databricks we were able to focus on the features of the report rather than overhead, and have fun while we were doing it.\n\n[caption id=\"attachment_10675\" align=\"aligncenter\" width=\"825\"]<a href=\"https://databricks.com/wp-content/uploads/2017/03/mediamath-app-sample.jpg\"><img src=\"https://databricks.com/wp-content/uploads/2017/03/mediamath-app-sample.jpg\" alt=\"\" width=\"825\" height=\"606\" class=\"size-full wp-image-10675\" /></a> An example of the finished report. Segments that are over-indexing are shown in blue, while those that are under-indexing are shown in red. The advertiser and pixel names have been removed. Props to Sr Data Visualization Engineer Nicola Ricci for creating such an attractive visualization.[/caption]"}
{"status": "publish", "description": null, "creator": "jakebellacera", "link": "https://databricks.com/blog/2017/04/04/real-time-end-to-end-integration-with-apache-kafka-in-apache-sparks-structured-streaming.html", "authors": null, "id": 10693, "categories": ["Apache Spark", "Engineering Blog", "Streaming"], "dates": {"publishedOn": "2017-04-04", "tz": "UTC", "createdOn": "2017-04-04"}, "title": "Real-Time End-to-End Integration with Apache Kafka in Apache Spark\u2019s Structured Streaming", "slug": "real-time-end-to-end-integration-with-apache-kafka-in-apache-sparks-structured-streaming", "content": "[dbce_cta href=\"https://docs.databricks.com/_static/notebooks/structured-streaming-etl-kafka.html\"]View the Notebook in Databricks Community Edition[/dbce_cta]\n\nStructured Streaming APIs enable building end-to-end streaming applications called <a href=\"https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html\">continuous applications</a> in a consistent, fault-tolerant manner that can handle all of the complexities of writing such applications. It does so without having to reason about the nitty-gritty details of streaming itself and by allowing the usage of familiar concepts within Spark SQL such as DataFrames and Datasets. All of this has led to a high interest in use cases wanting to tap into it. From <a href=\"https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html\">introduction</a>, to <a href=\"https://databricks.com/blog/2017/01/19/real-time-streaming-etl-structured-streaming-apache-spark-2-1.html\">ETL</a>, to <a href=\"https://databricks.com/blog/2017/02/23/working-complex-data-formats-structured-streaming-apache-spark-2-1.html\">complex data formats</a>, there has been a wide coverage of this topic. Structured Streaming is also integrated with third party components such as Kafka, HDFS, S3, RDBMS, etc.\n\nIn this blog, I'll cover an end-to-end integration with Kafka, consuming messages from it, doing simple to complex windowing ETL, and pushing the desired output to various sinks such as memory, console, file, databases, and back to Kafka itself. In the case of writing to files, I'll\u00a0cover writing new data under existing partitioned tables as well.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-1-overview.png\" alt=\"An overview of what our end-to-end integration will look like.\" width=\"702\" height=\"311\" class=\"aligncenter size-full wp-image-10695\" />\n\n<h2>Connecting to a Kafka Topic</h2>\n\nLet's assume you have a <a href=\"https://kafka.apache.org/quickstart\">Kafka cluster</a> that you can connect to and you are looking to use Spark's Structured Streaming to ingest and process messages from a topic. The Databricks platform already includes an Apache Kafka 0.10 connector for Structured Streaming, so it is easy to set up a stream to read messages:\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-2-kafka-stream.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-2-kafka-stream.png\" alt=\"Example showing how to set up an Apache Kafka connector for Structured Streaming in Databricks\" width=\"949\" height=\"200\" class=\"aligncenter size-full wp-image-10696\" /></a>\n\nThere are a number of options that can be specified while reading streams. The details of those options can be found <a href=\"https://spark.apache.org/docs/2.1.0/structured-streaming-kafka-integration.html\">here</a>.\n\nLet's quickly look at the schema for <code>streamingInputDF</code> DataFrame that we set up above.\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-3-kafka-stream-dataframe-schema.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-3-kafka-stream-dataframe-schema.png\" alt=\"An example of the streamingInputDF DataFrame schema\" width=\"801\" height=\"246\" class=\"aligncenter size-full wp-image-10697\" /></a>\n\nIt includes key, value, topic, partition, offset, timestamp and timestampType fields. We can pick and choose the ones as needed for our processing. The \u2018value\u2019 field is the actual data, and timestamp is message arrival timestamp. In windowing cases, we should not confuse this timestamp with what might be included in the messages itself which is more relevant most of the time.\n\n<h2>Streaming ETL</h2>\n\nNow that the stream is set up, we can start doing the required ETL on it to extract meaningful insights. Notice that <code>streamingInputDF</code> is a DataFrame. Since DataFrames are essentially an untyped Dataset of rows, we can perform similar operations to them.\n\nLet\u2019s say that the generic ISP hit JSON data is being pushed to the Kafka <code>&lt;topic&gt;</code> above. An example value would look like this:\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-4-example-value.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-4-example-value.png\" alt=\"An example JSON value from the Kafka stream\" width=\"736\" height=\"316\" class=\"aligncenter size-full wp-image-10698\" /></a>\n\nIt is now possible to do interesting analysis quickly, such as how many users are coming in from a zipcode, what ISP do users come in from, etc. We can then create dashboards that can be shared to the rest of our organization. Let\u2019s dive in:\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-5-realtime-analysis.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-5-realtime-analysis.png\" alt=\"Display query example\" width=\"759\" height=\"470\" class=\"aligncenter size-full wp-image-10699\" /></a>\n\nNotice in the command above, we are able to parse the zipcode out of incoming JSON messages, group them and do a count, all in real-time as we are reading data from the Kafka topic. Once we have the count, we can display it, which fires the streaming job in the background and continuously updates the counts as new messages arrive. This auto-updating chart can now be shared as an <a href=\"https://databricks.com/blog/2016/02/17/introducing-databricks-dashboards.html\">access-controlled dashboard in Databricks</a> with the rest of our organization.\n\n<h2>Windowing</h2>\n\nNow that we have parse, select, groupBy and count queries continuously executing, what if we want to find out traffic per zip code for a 10 minute window interval, with sliding duration of 5 minutes starting 2 minutes past the hour?\n\nIn this case, the incoming JSON contains timestamp in \u2018hittime,\u2019 so let\u2019s use that to query the traffic per each 10 minute window.\n\n<blockquote>\n  Note that in Structured Streaming, <em>windowing</em> is considered a <em>groupBy</em> operation. The pie charts below represents each 10 minute window.\n</blockquote>\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-6-windowing.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-6-windowing.png\" alt=\"Structured Streaming windowing function examples\" width=\"841\" height=\"693\" class=\"aligncenter size-full wp-image-10700\" /></a>\n\n<h2>Output Options</h2>\n\nSo far, we have seen the end results being displayed automatically. If we want more control in terms output options, there are a variety of output modes available. For instance, if we need to debug, you may wish to select the console output. If we need to be able to query the dataset interactively as data is being consumed, the memory output would be an ideal choice. Similarly, the output can be written to files, external databases, or even streamed back to Kafka.\n\nLet\u2019s go over these options in detail.\n\n<h3>Memory</h3>\n\nIn this scenario, data is stored as an in-memory table. From here, users are able to query the dataset using SQL. The name of the table is specified from the <code>queryName</code> option. Note we continue to use <code>streamingSelectDF</code> from the above windowing example.\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-7-store-in-memory.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-7-store-in-memory.png\" alt=\"An example of a user querying an in-memory Structured Streaming table via SQL\" width=\"713\" height=\"584\" class=\"aligncenter size-full wp-image-10701\" /></a>\n\nFrom here, you are now able to do more interesting analysis just as you would on a regular table while the data is automatically being updated.\n\n<h3>Console</h3>\n\nIn this scenario, output is printed to console/stdout log.\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-8-output-to-console.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-8-output-to-console.png\" alt=\"An example of a user printing Kafka output to a console log\" width=\"577\" height=\"660\" class=\"aligncenter size-full wp-image-10702\" /></a>\n\n<h3>File</h3>\n\nThis scenario is ideal for long-term persistence of output. Unlike memory and console sinks, files and directories are fault-tolerant. As such, this option requires a checkpoint directory, where state is maintained for fault-tolerance.\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-9-write-to-parquet-file.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-9-write-to-parquet-file.png\" alt=\"An example of a Kafka stream being outputted to a parquet file\" width=\"725\" height=\"197\" class=\"aligncenter size-full wp-image-10703\" /></a>\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-10-read-file.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-10-read-file.png\" alt=\"Example of a user mounting a parquet file\" width=\"872\" height=\"292\" class=\"aligncenter size-full wp-image-10704\" /></a>\n\nOnce the data is saved, it can be queried as one would do in Spark with any other dataset.\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-11-stream-from-parquet-file.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-11-stream-from-parquet-file.png\" alt=\"Example of a user filtering data from a parquet file\" width=\"857\" height=\"150\" class=\"aligncenter size-full wp-image-10705\" /></a>\n\nThe other advantage of file output sinks is that you can dynamically partition incoming messages by any variation of columns. In this particular example, we can partition by \u2018zipcode\u2019 and \u2018day\u2019. This can help make queries faster as chunks of data could be skipped just by referencing individual partitions.\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-12-dynamic-partition-dataframe.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-12-dynamic-partition-dataframe.png\" alt=\"Partitioning a file sink\" width=\"969\" height=\"186\" class=\"aligncenter size-full wp-image-10706\" /></a>\n\nWe could then partition the incoming data by \u2018zip\u2019 by \u2018day\u2019.\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-13-dynamic-partition-by-column.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-13-dynamic-partition-by-column.png\" alt=\"Partitioning a stream of data\" width=\"680\" height=\"214\" class=\"aligncenter size-full wp-image-10707\" /></a>\n\nLet\u2019s look at the output directory.\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-14-output-directory.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-14-output-directory.png\" alt=\"\" width=\"800\" height=\"304\" class=\"aligncenter size-full wp-image-10708\" /></a>\n\nNow, the partitioned data can be used directly in datasets and DataFrames, and if a table is created pointing to the directory where files are written to, Spark SQL can be used to query the data.\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-15-sql-table-from-dataframe.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-15-sql-table-from-dataframe.png\" alt=\"Observing the output directory on the filesystem\" width=\"862\" height=\"145\" class=\"aligncenter size-full wp-image-10709\" /></a>\n\nThe one caveat with this approach is that a partition will have to be added to the table for datasets under it to be accessible.\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-16-add-partition-to-table.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-16-add-partition-to-table.png\" alt=\"Adding PARTITIONED BY to the SQL query\" width=\"860\" height=\"84\" class=\"aligncenter size-full wp-image-10710\" /></a>\n\nThe partition reference can be pre-populated beforehand so that as files are created in them; they become instantly available.\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-17-reading-sql-table.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-17-reading-sql-table.png\" alt=\"\" width=\"850\" height=\"238\" class=\"aligncenter size-full wp-image-10711\" /></a>\n\nYou can now perform analysis on the table that is getting automatically updated while persisting data in the correct partition.\n\n<h3>Databases</h3>\n\nOften times we want to be able to write output of streams to external databases such as MySQL. At the time of writing, the Structured Streaming API does not support external databases as sinks; however, when it does, the API option will be as simple as <code>.format(\"jdbc\").start(\"jdbc:mysql/..\")</code>. In the meantime, we can use the foreach sink to accomplish this. Let\u2019s create a custom JDBC Sink that extends <em>ForeachWriter</em> and implements its methods.\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-18-custom-jdbc-sink.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-18-custom-jdbc-sink.png\" alt=\"Code for our custom JDBCSink class which extends ForeachWriter\" width=\"716\" height=\"477\" class=\"aligncenter size-full wp-image-10712\" /></a>\n\nWe can now use the <em>JDBCSink</em>:\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-19-using-custom-jdbc-sink.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-19-using-custom-jdbc-sink.png\" alt=\"Example of our custom JDBCSink in use\" width=\"720\" height=\"329\" class=\"aligncenter size-full wp-image-10713\" /></a>\n\nAs batches are complete, counts by zip could be INSERTed/UPSERTed into MySQL as needed.\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-20-mysql-select-example.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-20-mysql-select-example.png\" alt=\"Viewing rows from the zip_test table in MySQL\" width=\"523\" height=\"130\" class=\"aligncenter size-full wp-image-10714\" /></a>\n\n<h3>Kafka</h3>\n\nSimilar to writing to databases, the current Structured Streaming API does not support the \u201ckafka\u201d format, but this will be available in the next version. In the meantime, we can create a custom class named <em>KafkaSink` which extends _ForeachWriter</em>. Let\u2019s see how that looks:\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-21-kafka-sink.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-21-kafka-sink.png\" alt=\"Code for our custom KafkaSink class which extends ForeachWriter\" width=\"973\" height=\"629\" class=\"aligncenter size-full wp-image-10715\" /></a>\n\nNow we can use the writer:\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-22-kafka-sink-writer.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-22-kafka-sink-writer.png\" alt=\"Example showing the KafkaSink in action\" width=\"694\" height=\"578\" class=\"aligncenter size-full wp-image-10716\" /></a>\n\nYou can now see that we are pumping messages back to Kafka topic <code>&lt;topic2&gt;</code>. In this case we are pushing updated <code>zipcode:count</code> at the end of each batch. The other thing to note is that streaming Dashboard provides insights into incoming messages versus processing rate, batch duration and raw data that is used to generate it. This comes in very handy when debugging issues and monitoring system.\n\nOn the Kafka consumer side, we can see:\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-23-kafka-sink-example.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/04/structured-streaming-kafka-blog-image-23-kafka-sink-example.png\" alt=\"Example output from Kafka console consumer\" width=\"639\" height=\"129\" class=\"aligncenter size-full wp-image-10717\" /></a>\n\nIn this case, we are running in \u201cupdate\u201d output mode. As messages are being consumed, zipcodes that are getting updated during that batch are being pushed back to Kafka. Zipcodes that do not get updated are not being sent. You can also run in \u201ccomplete\u201d mode, as we did in the database sink above, in which all of the zipcodes with latest count will be sent, even if some of the zipcode counts did not change since the last batch.\n\n<h2>Conclusion</h2>\n\nAt a high level, I covered Structured Streaming integration with Kafka. Also, I showed how you could use various sinks and sources using the APIs. One thing to note is that what we have gone through here is equally relevant to other streams: sockets, directory, etc. For instance, if you wish to consume a socket source and push processed messages to MySQL, the sample here should be able to do just that simply by changing the stream. Also, examples showing <em>ForeachWriter</em> could be used for fanning out writes to multiple downstream systems. I plan to cover deeper insights into fanning out as well as sinks covered here in more detail in subsequent posts.\n\nThe example code we used in this blog is available as a <a href=\"https://docs.databricks.com/_static/notebooks/structured-streaming-etl-kafka.html\">Databricks Notebook</a>. You can start experimenting with Structured Streaming today by signing up for a free <a href=\"https://databricks.com/try\">Databricks Community Edition</a> account. If you have questions, or would like to get started with Databricks, please <a href=\"http://go.databricks.com/contact-databricks\">contact us</a>.\n\nFinally, I encourage you to read our series of blogs on Structured Streaming:\n\n<ul>\n<li><a href=\"https://databricks.com/blog/2017/01/19/real-time-streaming-etl-structured-streaming-apache-spark-2-1.html\">Real-time Streaming ETL with Structured Streaming in Apache Spark 2.1</a></li>\n<li><a href=\"https://databricks.com/blog/2017/02/23/working-complex-data-formats-structured-streaming-apache-spark-2-1.html\">Working with Complex Data Formats with Structured Streaming in Apache Spark 2.1</a></li>\n</ul>"}
{"status": "publish", "description": null, "creator": "bill", "link": "https://databricks.com/blog/2017/04/17/query-watchdog-handling-disruptive-queries-spark-sql.html", "authors": null, "id": 10752, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2017-04-17", "tz": "UTC", "createdOn": "2017-04-17"}, "title": "Query Watchdog: Handling Disruptive Queries in Spark SQL", "slug": "query-watchdog-handling-disruptive-queries-spark-sql", "content": "At Databricks, our users range from SQL Analysts who explore data through JDBC connections and SQL Notebooks to Data Engineers orchestrating large scale ETL jobs. While this is great for data democratization, one challenge associated with exploratory data analysis is handling rogue queries that appear as if they will finish, but never actually will. These queries can be extremely slow, saturate cluster resources, and deprive others to share the same cluster.\n\n<h2>A Motivating Example: Skewed Joins</h2>\n\nTo motivate this circumstance, let\u2019s walk through a particular workflow. A SQL analyst in my organization is just getting ramped up on Databricks and the data we have stored there. She\u2019s performing some ad hoc queries to get familiar with the data layout and contents but isn\u2019t quite aware of some of the implications of joining certain tables together. For instance, these tables may have extremely skewed data.\n\nSince this customer is on Databricks, they use a Shared Autoscaling cluster that makes it easy for multiple users to use a single cluster at the same time. For this example, let\u2019s imagine that we have two tables that each have a million rows.\n\n<pre><code class=\"scala\">%scala\nimport org.apache.spark.sql.functions._\nspark.conf.set(\"spark.sql.shuffle.partitions\", 10)\n\nspark.range(1000000)\n  .withColumn(\"join_key\", lit(\" \"))\n  .createOrReplaceTempView(\"table_x\")\nspark.range(1000000)\n  .withColumn(\"join_key\", lit(\" \"))\n  .createOrReplaceTempView(\"table_y\")\n</code></pre>\n\nThese table sizes are easy for Apache Spark to manage. However, you will notice that each includes a <code>join_key</code> column with an empty string for every row. This may happen if your data is not perfectly clean or if there is significant data skew where some keys are vastly more prevalent than others in a table.\n\nOur analyst, getting started and trying to understand the data, is joining these two tables on their keys. She doesn\u2019t quite realize that this will produce <em>one trillion rows</em> and all of those rows will be produced on a single executor (the executor that gets the \u201d \u201d value).\n\n<pre><code class=\"sql\">%sql\n\nSELECT\n  id, count()\nFROM\n  (SELECT\n    x.id\n  FROM\n    table_x x\n  JOIN\n    table_y y\n  on x.join_key = y.join_key)\nGROUP BY id\n-- you'll want to cancel this query.\n</code></pre>\n\nThis query will appear to be running but without knowing about the data, the user will see that there\u2019s \u201conly\u201d a single task left over the course of executing the job. Unfortunately, this query will never finish as well leaving her frustrated and confused about why it did not work.\n\n<h2>Enter the Query Watchdog</h2>\n\nThe previous query would cause problems on many different systems, regardless of whether you\u2019re using Databricks or another data warehousing tool. Luckily, as an user of Databricks, this customer has a feature available that can help solve this problem called the <em>Query Watchdog</em>.\n\n<blockquote>\n  <strong>Note:</strong> Query Watchdog is available on clusters created with version 2.1-db3 and greater.\n</blockquote>\n\nA Query Watchdog is a simple process that checks whether or not a given query is creating too many output rows for the number of input rows at a task level. We can set a property to control this and in this example we will use a ratio of 1000 (which is the default).\n\n<pre><code class=\"scala\">%scala\n\nval maximumOutputRowRatio = 1000L\nspark.conf.set(\"spark.databricks.queryWatchdog.enabled\", true)\nspark.conf.set(\"spark.databricks.queryWatchdog.outputRatioThreshold\", maximumOutputRowRatio)\n</code></pre>\n\n<pre><code class=\"sql\">%sql\n\nSET spark.databricks.queryWatchdog.enabled=true\nSET spark.databricks.queryWatchdog.outputRatioThreshold=100\n</code></pre>\n\nThe two properties we set above will (1) turn on the Query Watchdog and (2) declare that any given task should never produce more than 1000x the number of input rows. The output ratio is completely customizable. We recommend starting lower and seeing what threshold works well for you and your team. The range of 1,000 to 10,000 is likely a good starting point.\n\nNow when the user goes to run the following query, it will fail after several minutes. Not only do Query Watchdog settings prevent users hogging cluster resources for jobs that will never complete, it will also save users time by fast-failing a query that would have never completed. For example, the following query will fail after several minutes because it exceeded the ratio - here\u2019s what I\u2019ll see as a user.\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/04/databricks-query-watchdog-example.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/04/databricks-query-watchdog-example.png\" alt=\"An example of Query Watchdog in action.\" width=\"894\" height=\"446\" class=\"aligncenter size-full wp-image-10753\" /></a>\n\nAs users or administrators we can change these output ratio at runtime using the aforementioned configurations to control the queries that our clusters will tolerate. This relieves me, as an administrator, from having to constantly monitor usage on shared clusters by my different users and providing some \u201cbumpers\u201d for rogue queries.\n\n<h2>Tuning and Migration Guide</h2>\n\nThese previous two properties are often enough for most use cases, but in case we\u2019d like to control things further, we can set another two properties. These parameters specify the minimum time a given task in a query must run before cancelling it. We can set this to a higher value if we\u2019d like to give it a chance to still produce a large amount of rows (per task). The second parameter allows me to set a minimum number of output rows for a task in that query. For instance, we can set this to ten million if we want to stop a query only after a task in that query has produced ten million rows. Anything less and the query would still succeed (even if the ratio was exceeded).\n\n<pre><code class=\"scala\">spark.conf.set(\"spark.databricks.queryWatchdog.minTimeSecs\", 10L)\nspark.conf.set(\"spark.databricks.queryWatchdog.minOutputRows\", 100000L) \n</code></pre>\n\n<h3>When is the Query Watchdog a good choice?</h3>\n\nThis is a great choice for a cluster being used for interactive queries where SQL analysts and data scientists are sharing a given cluster since it avoids wasting users\u2019 time and gives users the chance to correct or rewrite queries. If you set this value in a notebook, it is important to know that it will not persist across restarts. We recommend setting as a cluster configuration, at cluster launch time, if you would like to set this property for all users of a given cluster.\n\n<h3>When is it a bad choice?</h3>\n\nThis configuration is less helpful for production ETL workloads because users cannot fix their queries on the fly in a production job. In general, we do not advise eagerly cancelling queries used in an ETL scenario because there typically isn\u2019t a human in the loop to correct the error. We recommend avoiding the use of this tool outside ad hoc analytics clusters.\n\n<a href=\"https://docs.databricks.com/spark/latest/spark-sql/query-watchdog.html\">Read more about the Query Watchdog feature in our documentation</a>\n\nAt Databricks, we\u2019re always trying make user experience easier. Try it for yourself, and <a href=\"https://databricks.com/try\">signup\u00a0today</a>."}
{"status": "publish", "description": null, "creator": "michael", "link": "https://databricks.com/blog/2017/04/26/processing-data-in-apache-kafka-with-structured-streaming-in-apache-spark-2-2.html", "authors": null, "id": 10771, "categories": ["Apache Spark", "Engineering Blog", "Streaming"], "dates": {"publishedOn": "2017-04-26", "tz": "UTC", "createdOn": "2017-04-26"}, "title": "Processing Data in Apache Kafka with Structured Streaming in Apache Spark 2.2", "slug": "processing-data-in-apache-kafka-with-structured-streaming-in-apache-spark-2-2", "content": "<em>This is the third post in a <a href=\"https://databricks.com/blog/2017/01/19/real-time-streaming-etl-structured-streaming-apache-spark-2-1.html\">multi-part series</a> about how you can perform complex streaming analytics using Apache Spark.</em>\n\n<hr />\n\nIn this blog, we will show how Spark SQL's APIs can be leveraged to consume and transform complex data streams from <a href=\"https://kafka.apache.org/\">Apache Kafka</a>. Using these simple APIs, you can express complex transformations like exactly-once event-time aggregation and output the results to a variety of systems. Together, you can use Apache Spark and Apache Kafka to:\n\n<ul>\n<li>Transform and augment real-time data read from Apache Kafka using the same APIs as  working with batch data.</li>\n<li>Integrate data read from Kafka with information stored in other systems including S3, HDFS, or MySQL.</li>\n<li>Automatically benefit from incremental execution provided by the Catalyst optimizer and subsequent efficient code generation by Tungsten.</li>\n</ul>\n\nWe start with a review of Kafka terminology and then present examples of Structured Streaming queries that read data from and write data to Apache Kafka. And finally, we\u2019ll explore an end-to-end real-world use case.\n\n<h2>Apache Kafka</h2>\n\nKafka is a distributed pub-sub messaging system that is popular for ingesting real-time data streams and making them available to downstream consumers in a parallel and fault-tolerant manner. This renders Kafka suitable for building real-time streaming data pipelines that reliably move data between heterogeneous processing systems. Before we dive into the details of Structured Streaming's Kafka support, let\u2019s recap some basic concepts and terms.\n\nData in Kafka is organized into <em>topics</em> that are split into <em>partitions</em> for parallelism. Each partition is an ordered, immutable sequence of <em>records</em>, and can be thought of as a structured commit log. Producers append records to the tail of these logs and <em>consumers</em> read the logs at their own pace. Multiple consumers can <em>subscribe</em> to a topic and receive incoming records as they arrive. As new records arrive to a partition in a Kafka topic, they are assigned a sequential id number called the <em>offset</em>. A Kafka cluster retains all published records\u2014whether or not they have been consumed\u2014for a configurable retention period, after which they are marked for deletion.\n\n<h3>Specifying What Data to Read from Kafka</h3>\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/04/kafka-topic.png\" alt=\"Breakdown of a Kafka topic\" width=\"700\" height=\"554\" class=\"aligncenter size-full wp-image-10773\" />\n\nA Kafka topic can be viewed as an infinite stream where data is retained for a configurable amount of time. The infinite nature of this stream means that when starting a new query, we have to first decide what data to read and where in time we are going to begin.  At a high level, there are three choices:\n\n<ul>\n<li><em>earliest</em> \u2014 start reading at the beginning of the stream. This excludes data that has already been deleted from Kafka because it was older than the retention period (\u201caged out\u201d data).</li>\n<li><em>latest</em> \u2014 start now, processing only new data that arrives after the query has started.</li>\n<li><em>per-partition assignment</em> \u2014 specify the precise offset to start from for every partition, allowing fine-grained control over exactly where processing should start. For example, if we want to pick up exactly where some other system or query left off, then this option can be leveraged.</li>\n</ul>\n\nAs you will see below, the <em>startingOffsets</em> option accepts one of the three options above, and is only used when starting a query from a fresh checkpoint. If you <a href=\"http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#recovering-from-failures-with-checkpointing\">restart a query from an existing checkpoint</a>, then it will always resume exactly where it left off, except when the data at that offset has been aged out. If any unprocessed data was aged out, the query behavior will depend on what is set by the <em>failOnDataLoss</em> option, which is described in the <a href=\"https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html\">Kafka Integration Guide</a>.\n\nExisting users of the KafkaConsumer will notice that Structured Streaming provides a more granular version of the configuration option, <code>auto.offset.reset</code>.  Instead of one option, we split these concerns into two different parameters, one that says what to do when the stream is first starting (<em>startingOffsets</em>), and another that handles what to do if the query is not able to pick up from where it left off, because the desired data has already been aged out (<em>failOnDataLoss</em>).\n\n<h2>Apache Kafka support in Structured Streaming</h2>\n\nStructured Streaming provides a unified batch and streaming API that enables us to view data published to Kafka as a DataFrame. When processing unbounded data in a streaming fashion, we use the same API and get the same data consistency guarantees as in batch processing. The system ensures end-to-end exactly-once fault-tolerance guarantees, so that a user does not have to reason about low-level aspects of streaming.\n\nLet's examine and explore examples of reading from and writing to Kafka, followed by an end-to-end application.\n\n<h3>Reading Records from Kafka topics</h3>\n\nThe first step is to specify the location of our Kafka cluster and which topic we are interested in reading from. Spark allows you to read an individual topic, a specific set of topics, a regex pattern of topics, or even a specific set of partitions belonging to a set of topics. We will only look at an example of reading from an individual topic, the other possibilities are covered in the <a href=\"https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html\">Kafka Integration Guide</a>.\n\n<pre><code class=\"python\"># Construct a streaming DataFrame that reads from topic1\ndf = spark \\\n  .readStream \\\n  .format(\"kafka\") \\\n  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n  .option(\"subscribe\", \"topic1\") \\\n  .option(\"startingOffsets\", \"earliest\") \\\n  .load()\n</code></pre>\n\nThe DataFrame above is a streaming DataFrame subscribed to \u201ctopic1\u201d. The configuration is set by providing options to the <em>DataStreamReader</em>, and the minimal required parameters are the location of the <em>kafka.bootstrap.servers</em> (i.e. <code>host:port</code>) and the topic that we want to <em>subscribe</em> to. Here, we have also specified <em>startingOffsets</em> to be \u201cearliest\u201d, which will read all data available in the topic at the start of the query. If the <em>startingOffsets</em> option is not specified, the default value of \u201clatest\u201d is used and only data that arrives after the query starts will be processed.\n\n<code>df.printSchema()</code> reveals the schema of our DataFrame.\n\n<pre><code>root\n |-- key: binary (nullable = true)\n |-- value: binary (nullable = true)\n |-- topic: string (nullable = true)\n |-- partition: integer (nullable = true)\n |-- offset: long (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n |-- timestampType: integer (nullable = true)\n</code></pre>\n\nThe returned DataFrame contains all the familiar fields of a Kafka record and its associated metadata.  We can now use all of the familiar DataFrame or Dataset operations to transform the result. Typically, however, we'll start by parsing the binary values present in the key and value columns. How to interpret these blobs is application specific. Fortunately, Spark SQL contains many built-in transformations for common types of serialization as we'll show below.\n\n<h4>Data Stored as a UTF8 String</h4>\n\nIf the bytes of the Kafka records represent UTF8 strings, we can simply use a cast to convert the binary data into the correct type.\n\n<pre><code class=\"python\">df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n</code></pre>\n\n<h4>Data Stored as JSON</h4>\n\nJSON is another common format for data that is written to Kafka. In this case, we can use the built-in <code>from_json</code> function along with the expected schema to convert a binary value into a Spark SQL struct.\n\n<pre><code class=\"python\"># value schema: { \"a\": 1, \"b\": \"string\" }\nschema = StructType().add(\"a\", IntegerType()).add(\"b\", StringType())\ndf.select( \\\n  col(\"key\").cast(\"string\"),\n  from_json(col(\"value\").cast(\"string\"), schema))\n</code></pre>\n\n<h4>User Defined Serializers and Deserializers</h4>\n\nIn some cases, you may already have code that implements the <a href=\"https://kafka.apache.org/0100/javadoc/org/apache/kafka/common/serialization/Deserializer.html\">Kafka Deserializer interface</a>. You can take advantage of this code by wrapping it as a user defined function (UDF) using the Scala code shown below.\n\n<pre><code class=\"scala\">object MyDeserializerWrapper {\n  val deser = new MyDeserializer\n}\nspark.udf.register(\"deserialize\", (topic: String, bytes: Array[Byte]) =&gt; \n  MyDeserializerWrapper.deser.deserialize(topic, bytes)\n)\n\ndf.selectExpr(\"\"\"deserialize(\"topic1\", value) AS message\"\"\")\n</code></pre>\n\nNote that the DataFrame code above is analogous to specifying <code>value.deserializer</code> when using the standard Kafka consumer.\n\n<h3>Using Spark as a Kafka Producer</h3>\n\nWriting data from any Spark supported data source into Kafka is as simple as calling <code>writeStream</code> on any DataFrame that contains a column named \"value\", and optionally a column named \"key\". If a key column is not specified, then a null valued key column will be automatically added. A null valued key column may, in some cases, <a href=\"https://cwiki.apache.org/confluence/display/KAFKA/FAQ#FAQ-Whyisdatanotevenlydistributedamongpartitionswhenapartitioningkeyisnotspecified?\">lead to uneven data partitioning in Kafka</a>, and should be used with care.\n\nThe destination topic for the records of the DataFrame can either be specified statically as an option to the <em>DataStreamWriter</em> or on a per-record basis as a column named \"topic\" in the DataFrame.\n\n<pre><code class=\"python\"># Write key-value data from a DataFrame to a Kafka topic specified in an option\nquery = df \\\n  .selectExpr(\"CAST(userId AS STRING) AS key\", \"to_json(struct(*)) AS value\") \\\n  .writeStream \\\n  .format(\"kafka\") \\\n  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n  .option(\"topic\", \"topic1\") \\\n  .option(\"checkpointLocation\", \"/path/to/HDFS/dir\") \\\n  .start()\n</code></pre>\n\nThe above query takes a DataFrame containing user information and writes it to Kafka. The userId is serialized as a string and used as the key. We take all the columns of the DataFrame and serialize them as a JSON string, putting the results in the value of the record.\n\nThe two required options for writing to Kafka are the <em>kafka.bootstrap.servers</em> and the <em>checkpointLocation</em>. As in the above example, an additional topic option can be used to set a single topic to write to, and this option will override the \u201ctopic\u201d column if it exists in the DataFrame.\n\n<h2>End-to-End Example with Nest Devices</h2>\n\nIn this section, we will explore an end-to-end pipeline involving Kafka along with other data sources and sinks. We will work with a data set involving a collection of <a href=\"https://nest.com/\">Nest</a> device logs, with a JSON format <a href=\"https://developers.nest.com/documentation/api-reference\">described here</a>. We\u2019ll specifically examine data from Nest\u2019s cameras, which look like the following JSON:\n\n<pre><code class=\"json\">\"devices\": {\n  \"cameras\": {\n    \"device_id\": \"awJo6rH...\",\n    \"last_event\": {\n      \"has_sound\": true,\n      \"has_motion\": true,\n      \"has_person\": true,\n      \"start_time\": \"2016-12-29T00:00:00.000Z\",\n      \"end_time\": \"2016-12-29T18:42:00.000Z\"\n    }\n  }\n}\n</code></pre>\n\nWe'll also be joining with a static dataset (called \u201cdevice_locations\u201d) that contains a mapping from <code>device_id</code> to the <code>zip_code</code> where the device was registered.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/04/nest-kafka.png\" alt=\"Illustration of how our workflow will be structured\" width=\"700\" height=\"419\" class=\"aligncenter size-full wp-image-10774\" />\n\nAt a high-level, the desired workflow looks like the graph above. Given a stream of updates from Nest cameras, we want to use Spark to perform several different tasks:\n\n<ul>\n<li>Create an efficient, queryable historical archive of all events using a columnar format like Parquet.</li>\n<li>Perform low-latency event-time aggregation and push the results back to Kafka for other consumers.</li>\n<li>Perform batch reporting on the data stored in a compacted topic in Kafka.</li>\n</ul>\n\nWhile these may sound like wildly different use-cases, you can perform all of them using DataFrames and Structured Streaming in a single end-to-end Spark application! In the following sections, we'll walk through individual steps, starting from ingest to processing to storing aggregated results.\n\n<h3>Read Nest Device Logs From Kafka</h3>\n\nOur first step is to read the raw Nest data stream from Kafka and project out the camera data that we are interested in. We first parse the Nest JSON from the Kafka records, by calling the <code>from_json</code> function and supplying the expected JSON schema and timestamp format. Then, we apply various transformations to the data and project the columns related to camera data in order to simplify working with the data in the sections to follow.\n\n<h4>Expected Schema for JSON data</h4>\n\n<pre><code class=\"python\">schema = StructType() \\\n  .add(\"metadata\", StructType() \\\n    .add(\"access_token\", StringType()) \\\n    .add(\"client_version\", IntegerType())) \\\n  .add(\"devices\", StructType() \\\n    .add(\"thermostats\", MapType(StringType(), StructType().add(...))) \\\n    .add(\"smoke_co_alarms\", MapType(StringType(), StructType().add(...))) \\\n    .add(\"cameras\", MapType(StringType(), StructType().add(...))) \\\n    .add(\"companyName\", StructType().add(...))) \\\n  .add(\"structures\", MapType(StringType(), StructType().add(...)))\n\nnestTimestampFormat = \"yyyy-MM-dd'T'HH:mm:ss.sss'Z'\"\n</code></pre>\n\n<h4>Parse the Raw JSON</h4>\n\n<pre><code class=\"python\">jsonOptions = { \"timestampFormat\": nestTimestampFormat }\nparsed = spark \\\n  .readStream \\\n  .format(\"kafka\") \\\n  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n  .option(\"subscribe\", \"nest-logs\") \\\n  .load() \\\n  .select(from_json(col(\"value\").cast(\"string\"), schema, jsonOptions).alias(\"parsed_value\"))\n</code></pre>\n\n<h4>Project Relevant Columns</h4>\n\n<pre><code class=\"python\">camera = parsed \\\n  .select(explode(\"parsed_value.devices.cameras\")) \\\n  .select(\"value.*\")\n\nsightings = camera \\\n  .select(\"device_id\", \"last_event.has_person\", \"last_event.start_time\") \\\n  .where(col(\"has_person\") == True)\n</code></pre>\n\nTo create the <code>camera</code> DataFrame, we first unnest the \u201ccameras\u201d json field to make it top level. Since \u201ccameras\u201d is a MapType, each resulting row contains a map of key-value pairs. So, we use the <code>explode</code> function to create a new row for each key-value pair, flattening the data. Lastly, we use star () to unnest the \u201cvalue\u201d column. The following is the result of calling <code>camera.printSchema()</code>\n\n<pre><code>root\n |-- device_id: string (nullable = true)\n |-- software_version: string (nullable = true)\n |-- structure_id: string (nullable = true)\n |-- where_id: string (nullable = true)\n |-- where_name: string (nullable = true)\n |-- name: string (nullable = true)\n |-- name_long: string (nullable = true)\n |-- is_online: boolean (nullable = true)\n |-- is_streaming: boolean (nullable = true)\n |-- is_audio_input_enable: boolean (nullable = true)\n |-- last_is_online_change: timestamp (nullable = true)\n |-- is_video_history_enabled: boolean (nullable = true)\n |-- web_url: string (nullable = true)\n |-- app_url: string (nullable = true)\n |-- is_public_share_enabled: boolean (nullable = true)\n |-- activity_zones: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- name: string (nullable = true)\n |    |    |-- id: string (nullable = true)\n |-- public_share_url: string (nullable = true)\n |-- snapshot_url: string (nullable = true)\n |-- last_event: struct (nullable = true)\n |    |-- has_sound: boolean (nullable = true)\n |    |-- has_motion: boolean (nullable = true)\n |    |-- has_person: boolean (nullable = true)\n |    |-- start_time: timestamp (nullable = true)\n |    |-- end_time: timestamp (nullable = true)\n |    |-- urls_expire_time: timestamp (nullable = true)\n |    |-- web_url: string (nullable = true)\n |    |-- app_url: string (nullable = true)\n |    |-- image_url: string (nullable = true)\n |    |-- animated_image_url: string (nullable = true)\n |    |-- activity_zone_ids: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n</code></pre>\n\n<h3>Aggregate and Write Back to Kafka</h3>\n\nWe will now process the <code>sightings</code> DataFrame by augmenting each sighting with its location. Recall that we have some location data that lets us look up the zip code of a device by its device id. We first create a DataFrame representing this location data, and then join it with the <code>sightings</code> DataFrame, matching on device id. What we are doing here is joining the <em>streaming</em> DataFrame <code>sightings</code> with a <em>static</em> DataFrame of locations!\n\n<h4>Add Location Data</h4>\n\n<pre><code class=\"python\">locationDF = spark.table(\"device_locations\").select(\"device_id\", \"zip_code\")\nsightingLoc = sightings.join(locationDF, \"device_id\")\n</code></pre>\n\n<h4>Aggregate Statistics and Write Out to Kafka</h4>\n\nNow, let\u2019s generate a streaming aggregate that counts the number of camera person sightings in each zip code for each hour, and write it out to a compacted Kafka topic<sup id=\"fnref-10771-1\"><a href=\"#fn-10771-1\" class=\"jetpack-footnote\">1</a></sup> called \u201cnest-camera-stats\u201d.\n\n<pre><code class=\"python\">sightingLoc \\\n  .groupBy(\"zip_code\", window(\"start_time\", \"1 hour\")) \\\n  .count() \\\n  .select( \\\n    to_json(struct(\"zip_code\", \"window\")).alias(\"key\"),\n    col(\"count\").cast(\"string\").alias(\"value\")) \\\n  .writeStream \\\n  .format(\"kafka\") \\\n  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n  .option(\"topic\", \"nest-camera-stats\") \\\n  .option(\"checkpointLocation\", \"/path/to/HDFS/dir\") \\\n  .outputMode(\"complete\") \\\n  .start()\n</code></pre>\n\nThe above query will process any sighting as it occurs and write out the updated count of the sighting to Kafka, keyed on the zip code and hour window of the sighting. Over time, many updates to the same key will result in many records with that key, and Kafka topic compaction will delete older updates as new values arrive for the key. This way, compaction tries to ensure that eventually, only the latest value is kept for any given key.\n\n<h3>Archive Results in Persistent Storage</h3>\n\nIn addition to writing out aggregation results to Kafka, we may want to save the raw camera records in persistent storage for later use. The following example writes out the <code>camera</code> DataFrame to S3 in Parquet format. We have chosen Parquet for compression and columnar storage, though many different formats such as ORC, Avro, CSV, etc. are supported to tailor to varied use-cases.\n\n<pre><code class=\"python\">camera.writeStream \\\n  .format(\"parquet\") \\\n  .option(\"startingOffsets\", \"earliest\") \\\n  .option(\"path\", \"s3://nest-logs\") \\\n  .option(\"checkpointLocation\", \"/path/to/HDFS/dir\") \\\n  .start()\n</code></pre>\n\nNote that we can simply reuse the same <code>camera</code> DataFrame to start multiple streaming queries. For instance, we can query the DataFrame to get a list of cameras that are offline, and send a notification to the network operations center for further investigation.\n\n<h3>Batch Query for Reporting</h3>\n\nOur next example is going to run a batch query over the Kafka \u201cnest-camera-stats\u201d compacted topic and generate a report showing zip codes with a significant number of sightings.\n\nWriting batch queries is similar to streaming queries with the exception that we use the <code>read</code> method instead of the <code>readStream</code> method and <code>write</code> instead of <code>writeStream</code>.\n\n<h4>Batch Read and Format the Data</h4>\n\n<pre><code class=\"python\">report = spark \\\n  .read \\\n  .format(\"kafka\") \\\n  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n  .option(\"subscribe\", \"nest-camera-stats\") \\\n  .load() \\\n  .select( \\\n    json_tuple(col(\"key\").cast(\"string\"), \"zip_code\", \"window\").alias(\"zip_code\", \"window\"),\n    col(\"value\").cast(\"string\").cast(\"integer\").alias(\"count\")) \\\n  .where(\"count &gt; 1000\") \\\n  .select(\"zip_code\", \"window\") \\\n  .distinct()\n</code></pre>\n\nThis report DataFrame can be used for reporting or to create a real-time dashboard showing events with extreme sightings.\n\n<h2>Conclusion</h2>\n\nIn this blog post, we showed examples of consuming and transforming real-time data streams from Kafka. We implemented an end-to-end example of a <a href=\"https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html\">continuous application</a>, demonstrating the conciseness and ease of programming with Structured Streaming APIs, while leveraging the powerful exactly-once semantics these APIs provide.\n\nIn the future blog posts in this series, we\u2019ll cover more on:\n\n<ul>\n<li>Monitoring your streaming applications</li>\n<li>Computing event-time aggregations with Structured Streaming</li>\n</ul>\n\nIf you want to learn more about the Structured Streaming, here are a few useful links:\n\n<ul>\n<li>Previous blogs posts explaining the motivation and concepts of Structured Streaming:\n\n<ul>\n<li><a href=\"https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html\">Continuous Applications: Evolving Streaming in Apache Spark 2.0</a></li>\n<li><a href=\"https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html\">Structured Streaming In Apache Spark</a></li>\n<li><a href=\"https://databricks.com/blog/2017/01/19/real-time-streaming-etl-structured-streaming-apache-spark-2-1.html\">Real-time Streaming ETL with Structured Streaming in Apache Spark 2.1</a></li>\n<li><a href=\"https://databricks.com/blog/2017/02/23/working-complex-data-formats-structured-streaming-apache-spark-2-1.html\">Working with Complex with Structured Streaming in Apache Spark 2.1</a></li>\n</ul></li>\n<li><a href=\"http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\">Structured Streaming Programming Guide</a></li>\n<li><a href=\"https://spark-summit.org/east-2017/events/making-structured-streaming-ready-for-production-updates-and-future-directions/\">Talk at Spark Summit 2017 East - Making Structured Streaming Ready for Production and Future Directions</a></li>\n</ul>\n\nTo try Structured Streaming in Apache Spark 2.1, <a href=\"https://databricks.com/try-databricks\">try Databricks today</a>.\n\n<hr />\n\n<h2>Additional Configuration</h2>\n\n<dl>\n<dt><a href=\"https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html\">Kafka Integration Guide</a></dt>\n<dd>Contains further examples and Spark specific configuration options for processing data in Kafka.</dd>\n\n<dt>Kafka <a href=\"http://kafka.apache.org/documentation.html#newconsumerconfigs\">Consumer</a> and <a href=\"http://kafka.apache.org/documentation/#producerconfigs\">Producer</a> Configuration Docs</dt>\n<dd>\nKafka\u2019s own configurations can be set via <code>DataStreamReader.option</code> and <code>DataStreamWriter.option</code> with the <code>kafka.</code> prefix, e.g:\n\n<pre><code class=\"python\">stream.option(\"kafka.bootstrap.servers\", \"host:port\")\n</code></pre>\n\nFor possible <code>kafka</code> parameters, see the <a href=\"http://kafka.apache.org/documentation.html#newconsumerconfigs\">Kafka consumer config</a> docs for parameters related to reading data, and the <a href=\"http://kafka.apache.org/documentation/#producerconfigs\">Kafka producer config</a> docs for parameters related to writing data.\n\nSee the <a href=\"https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html\">Kafka Integration Guide</a> for the list of options managed by Spark, which are consequently not configurable.\n</dd>\n</dl>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n\n<li id=\"fn-10771-1\">\nA compacted Kafka topic is a topic where retention is enforced by compaction to ensure that the log is guaranteed to have at least the last state for each key. See <a href=\"https://kafka.apache.org/documentation/#compaction\">Kafka Log Compaction</a> for more information.&#160;<a href=\"#fnref-10771-1\">&#8617;</a>\n</li>\n\n</ol>\n</div>"}
{"status": "publish", "description": null, "creator": "tdas", "link": "https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html", "authors": null, "id": 10799, "categories": ["Apache Spark", "Engineering Blog", "Streaming"], "dates": {"publishedOn": "2017-05-08", "tz": "UTC", "createdOn": "2017-05-08"}, "title": "Event-time Aggregation and Watermarking in Apache Spark\u2019s Structured Streaming", "slug": "event-time-aggregation-watermarking-apache-sparks-structured-streaming", "content": "<em>This is the fourth post in a <a href=\"https://databricks.com/blog/2017/01/19/real-time-streaming-etl-structured-streaming-apache-spark-2-1.html\">multi-part series</a> about how you can perform complex streaming analytics using Apache Spark.</em>\n\n<hr />\n\n<a href=\"https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html\">Continuous applications</a> often require near real-time decisions on real-time aggregated statistics\u2014such as health of and readings from IoT devices or detecting anomalous behavior. In this blog, we will explore how easily streaming aggregations can be expressed in Structured Streaming, and how naturally late, and out-of-order data is handled.\n\n<h2>Streaming Aggregations</h2>\n\nStructured Streaming allows users to express the same streaming query as a batch query, and the Spark SQL engine incrementalizes the query and executes on streaming data. For example, suppose you have a streaming DataFrame having events with signal strength from IoT devices, and you want to calculate the running average signal strength for each device, then you would write the following Python code:\n\n<pre><code class=\"python\"># DataFrame w/ schema [eventTime: timestamp, deviceId: string, signal: bigint]\neventsDF = ... \n\navgSignalDF = eventsDF.groupBy(\"deviceId\").avg(\"signal\")\n</code></pre>\n\nThis code is no different if eventsDF was a DataFrame on static data. However, in this case, the average will be continuously updated as new events arrive. You choose different <em><a href=\"http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#starting-streaming-queries\">output modes</a></em> for writing the updated averages to external systems like file systems and databases. Furthermore, you can also implement custom aggregations using Spark\u2019s <a href=\"https://docs.databricks.com/spark/latest/spark-sql/udaf-scala.html\">user-defined aggregation function (UDAFs)</a>.\n\n<h2>Aggregations on Windows over Event-Time</h2>\n\nIn many cases, rather than running aggregations over the whole stream, you want aggregations over data bucketed by time windows (say, every 5 minutes or every hour). In our earlier example, it\u2019s insightful to see what is the average signal strength in last 5 minutes in case if the devices have started to behave anomalously. Also, this 5 minute window should be based on the timestamp embedded in the data (aka. event-time) and not on the time it is being processed (aka. processing-time).\n\nEarlier Spark Streaming DStream APIs made it hard to express such event-time windows as the API was designed solely for processing-time windows (that is, windows on the time the data arrived in Spark). In Structured Streaming, expressing such windows on event-time is simply performing a special grouping using the <code>window()</code> function. For example, counts over 5 minute tumbling (non-overlapping) windows on the eventTime column in the event is as following.\n\n<pre><code class=\"python\">from pyspark.sql.functions import *\n\nwindowedAvgSignalDF = \\\n  eventsDF \\\n    .groupBy(window(\"eventTime\", \"5 minute\")) \\\n    .count()\n</code></pre>\n\nIn the above query, every record is going to be assigned to a 5 minute tumbling window as illustrated below.<img src=\"https://databricks.com/wp-content/uploads/2017/05/mapping-of-event-time-to-5-min-tumbling-windows.png\" alt=\"Mapping of event-time to 5 min tumbling windows\" width=\"764\" height=\"405\" class=\"aligncenter size-full wp-image-10803\" />\n\nEach window is a group for which running counts are calculated. You can also define overlapping windows by specifying both the window length and the sliding interval. For example:\n\n<pre><code class=\"python\">from pyspark.sql.functions import *\n\nwindowedAvgSignalDF = \\\n  eventsDF \\\n    .groupBy(window(\"eventTime\", \"10 minutes\", \"5 minutes\")) \\\n    .count()\n</code></pre>\n\nIn the above query, every record will be assigned to multiple overlapping windows as illustrated below.<img src=\"https://databricks.com/wp-content/uploads/2017/05/mapping-of-event-time-to-overlapping-windows-of-length-10-mins-and-sliding-interval-5-mins.png\" alt=\"Mapping of event-time to overlapping windows of length 10 mins and sliding interval 5 mins\" width=\"765\" height=\"415\" class=\"aligncenter size-full wp-image-10804\" />\n\nThis grouping strategy automatically handles late and out-of-order data \u2014 the late event would just update older window groups instead of the latest ones. Here is an end-to-end illustration of a query that is grouped by both the <code>deviceId</code> and the overlapping windows. The illustration below shows how the final result of a query changes after new data is processed with 5 minute triggers when you are grouping by both <code>deviceId</code> and sliding windows (for  brevity, the \u201csignal\u201d field is omitted).\n\n<pre><code class=\"python\">windowedCountsDF = \\\n  eventsDF \\\n    .groupBy(\n      \"deviceId\",\n      window(\"eventTime\", \"10 minutes\", \"5 minutes\")) \\\n    .count()\n</code></pre>\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/05/late-data-handling-in-windowed-grouped-aggregation.png\" alt=\"Late data handling in Windowed Grouped Aggregation\" width=\"777\" height=\"417\" class=\"aligncenter size-full wp-image-10802\" />\n\nNote how the late, out-of-order record [12:04, dev2] updated an old window\u2019s count.\n\n<h2>Stateful Incremental Execution</h2>\n\nWhile executing any streaming aggregation query, the Spark SQL engine internally maintains the intermediate aggregations as fault-tolerant state. This state is structured as key-value pairs, where the key is the group, and the value is the intermediate aggregation. These pairs are stored in an in-memory, versioned, key-value \u201cstate store\u201d in the Spark executors that  is checkpointed using write ahead logs in an HDFS-compatible file system (in the configured <a href=\"http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\">checkpoint location</a>). At every trigger, the state is read and updated in the state store, and all updates are saved to the write ahead log. In case of any failure, the correct version of the state is restored from checkpoint information, and the query proceeds from the point it failed. Together with replayable sources, and idempotent sinks, Structured Streaming ensures exactly-once guarantees for stateful stream processing.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/05/fault-tolerant-exactly-once-stateful-stream-processing-in-structured-streaming.png\" alt=\"Fault tolerant, exactly-once stateful stream processing in Structured Streaming\" width=\"710\" height=\"415\" class=\"aligncenter size-full wp-image-10801\" />\n\nThis fault-tolerant state management naturally incurs some processing overheads. To keep these overheads bounded within acceptable limits, the size of the state data should not grow indefinitely. However, with sliding windows, the number of windows/groups will grow indefinitely, and so can the size of state (proportional to the number of groups). To bound the state size, we have to be able to drop old aggregates that are not going to be updated any more, for example  seven  day old averages. We achieve this using <em>watermarking</em>.\n\n<h2>Watermarking to Limit State while Handling Late Data</h2>\n\nAs mentioned before, the arrival of late data can result in updates to older windows. This complicates the process of defining which old aggregates are not going to be updated and therefore can be dropped from the state store to limit the state size. In Apache Spark 2.1, we have introduced <em><strong>watermarking</strong></em> that enables automatic dropping of old state data.\n\nWatermark is a moving threshold in event-time that trails behind the maximum event-time seen by the query in the processed data. The trailing gap defines how long we will wait for late data to arrive. By knowing the point at which no more data will arrive for a given group, we can limit the total amount of state that we need to maintain for a query. For example, suppose the configured maximum lateness is 10 minutes. That means the events that are up to 10 minutes late will be allowed to aggregate. And if the maximum observed event time is 12:33, then all the future events with event-time older than 12:23 will be considered as \u201ctoo late\u201d and dropped. Additionally, all the state for windows older than 12:23 will be cleared. You can set this parameter based on the requirements of your application \u2014 larger values of this parameter allows data to arrive later but at the cost of increased state size, that is, memory usage and vice versa.\n\nHere is the earlier example but with watermarking.\n\n<pre><code class=\"python\">windowedCountsDF = \\\n  eventsDF \\\n    .withWatermark(\"eventTime\", \"10 minutes\") \\\n    .groupBy(\n      \"deviceId\",\n      window(\"eventTime\", \"10 minutes\", \"5 minutes\")) \\\n    .count()\n</code></pre>\n\nWhen this query is executed, Spark SQL will automatically keep track of the maximum observed value of the eventTime column, update the watermark and clear old state. This is illustrated below.\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/05/watermarking-in-windowed-grouped-aggregation.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/05/watermarking-in-windowed-grouped-aggregation.png\" alt=\"Watermarking in windowed grouped aggregation\" width=\"825\" height=\"581\" class=\"aligncenter size-full wp-image-10805\" /></a>\n\nNote the two events that arrive between the processing-times 12:20 and 12:25. The watermark is used to differentiate between the late and the \u201ctoo-late\u201d events and treat them accordingly.\n\n<h2>Conclusion</h2>\n\nIn short, I covered Structured Streaming\u2019s windowing strategy to handle key streaming aggregations: windows over event-time and late and out-of-order data. Using this windowing strategy allows Structured Streaming engine to implement watermarking, in which late data can be discarded. As a result of this design, we can manage the size of the state-store.\n\nIn the upcoming version of Apache Spark 2.2, we have added more advanced stateful stream processing operations to streaming DataFrames/Datasets. Stay tuned to this blog series for  more information. If you want to learn more about Structured Streaming, read our previous posts in the series.\n\n<ul>\n<li><a href=\"https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html\">Structured Streaming In Apache Spark</a></li>\n<li><a href=\"https://databricks.com/blog/2017/01/19/real-time-streaming-etl-structured-streaming-apache-spark-2-1.html\">Real-time Streaming ETL with Structured Streaming in Apache Spark 2.1</a></li>\n<li><a href=\"https://databricks.com/blog/2017/01/19/real-time-streaming-etl-structured-streaming-apache-spark-2-1.html\">Working with Complex Data Formats with Structured Streaming in Apache Spark 2.1</a></li>\n<li><a href=\"https://databricks.com/blog/2017/04/26/processing-data-in-apache-kafka-with-structured-streaming-in-apache-spark-2-2.html\">Processing Data in Apache Kafka with Structured Streaming in Apache Spark 2.2</a></li>\n</ul>\n\nTo try Structured Streaming in Apache Spark 2.0, <a href=\"https://databricks.com/try\">try Databricks today</a>."}
{"status": "publish", "description": null, "creator": "joseph", "link": "https://databricks.com/blog/2017/05/09/detecting-abuse-scale-locality-sensitive-hashing-uber-engineering.html", "authors": null, "id": 10812, "categories": ["Ecosystem", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2017-05-09", "tz": "UTC", "createdOn": "2017-05-09"}, "title": "Detecting Abuse at Scale: Locality Sensitive Hashing at Uber Engineering", "slug": "detecting-abuse-scale-locality-sensitive-hashing-uber-engineering", "content": "<em>This is a <a href=\"https://eng.uber.com/lsh/\">cross blog post</a> effort between Databricks and Uber Engineering. Yun Ni is a software engineer on Uber\u2019s Machine Learning Platform team, Kelvin Chu is technical lead engineer on Uber\u2019s Complex Data Processing/Speak team, and Joseph Bradley is a software engineer on Databricks\u2019 Machine Learning team.</em>\n\n<hr />\n\nWith 5 million Uber trips taken daily by users worldwide, it is important for Uber engineers to ensure that data is accurate. If used correctly, metadata and aggregate data can quickly detect platform abuse, from spam to fake accounts and payment fraud. Amplifying the right data signals makes detection more precise and thus, more reliable.\n\nTo address this challenge in our systems and others, Uber Engineering and Databricks worked together to contribute <a href=\"http://www.mit.edu/~andoni/LSH/\">Locality Sensitive Hashing (LSH)</a> to <a href=\"https://databricks.com/blog/2016/12/29/introducing-apache-spark-2-1.html\">Apache Spark 2.1</a>. LSH is a randomized algorithm and hashing technique commonly used in large-scale machine learning tasks including clustering and approximate <a href=\"https://en.wikipedia.org/wiki/Nearest_neighbor_search\">nearest neighbor search</a>.\n\nIn this article, we will demonstrate how this powerful tool is used by Uber to detect fraudulent trips at scale.\n\n<h2>Why LSH?</h2>\n\nBefore Uber Engineering implemented LSH, we used the N^2 approach to sift through trips; while accurate, the N^2 approach was ultimately too time-consuming, volume-intensive, and hardware-reliant for Uber\u2019s size and scale.\n\nThe general idea of LSH is to use a family of functions (known as LSH families) to hash data points into buckets so that data points near each other are located in the same buckets with high probability, while data points  far from each other are likely in different buckets. This makes it easier to identify trips with various degrees of overlap.\n\nFor reference, LSH is a multi-use technology with myriad applications, including:\n\n<ul>\n<li><strong>Near-duplicate detection:</strong> LSH is commonly used to deduplicate large quantities of documents, webpages, and other files.</li>\n<li><strong>Genome-wide association study:</strong> Biologists often use LSH to identify similar gene expressions in genome databases.</li>\n<li><strong>Large-scale image search:</strong> Google used LSH along with PageRank to build their image search technology <a href=\"https://research.google.com/pubs/pub34634.html\">VisualRank</a>.</li>\n<li><strong>Audio/video fingerprinting:</strong> In multimedia technologies, LSH is widely used as a fingerprinting technique A/V data.</li>\n</ul>\n\n<h2>LSH at Uber</h2>\n\nThe primary LSH use case at Uber is detecting similar trips based on their spatial properties, a method of identifying fraudulent drivers. Uber engineers presented on this use case <a href=\"https://spark-summit.org/2016/events/locality-sensitive-hashing-by-spark/\">during Spark Summit 2016</a>, where they discussed our team\u2019s motivations behind using LSH on the Spark framework to broadcast join all trips and sift through fraudulent ones. Our motivations for using LSH on Spark are threefold:\n\n<ol>\n<li>Spark is integral to Uber\u2019s operations, and many internal teams currently use Spark for various types of complex data processing including machine learning, spatial data processing, time series computation, analytics and prediction, and ad hoc data science exploration. In fact, Uber uses almost all Spark components such as <a href=\"http://spark.apache.org/mllib/\">MLlib</a>, <a href=\"http://spark.apache.org/sql/\">Spark SQL</a>, <a href=\"http://spark.apache.org/streaming/\">Spark Streaming</a>, and direct <a href=\"https://spark.apache.org/docs/1.6.0/api/java/org/apache/spark/rdd/HadoopRDD.html\">RDD</a> processing on both <a href=\"https://www.oreilly.com/ideas/a-tale-of-two-clusters-mesos-and-yarn\">YARN and Mesos</a>; since our infrastructure and <a href=\"https://spark-summit.org/2016/events/spark-uber-development-kit/\">tools</a> are built around Spark, and our engineers can create and manage Spark applications easily. </li>\n<li>Spark makes it efficient to do data cleaning and feature engineering before any actual machine learning is conducted, making the number-crunching much faster. Uber\u2019s high volume of collected data makes solving this problem by basic approaches unscalable and very slow. </li>\n<li>We do not need an exact solution for this equation, so there is no need to purchase and maintain additional hardware. In this case, approximations provide us with enough information to make judgment calls on potentially fraudulent activity and, in this case, are good enough to solve our problems. LSH allows us to trade some precision to save a lot of hardware resources. </li>\n</ol>\n\nFor these reasons, solving the problem by deploying LSH on Spark was the right choice for our business goals: scale, scale, and scale again.\n\nAt a high level, our approach to using LSH has three steps. First, we create a feature vector for each trip by breaking it down into area segments of equal size. Then, we hash the vectors by <a href=\"https://en.wikipedia.org/wiki/MinHash\">MinHash</a> for <a href=\"https://en.wikipedia.org/wiki/Jaccard_index\">Jaccard distance</a> function. Lastly, we either do Similarity Join in batch or <a href=\"https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\">k-Nearest Neighbor</a> search in real-time. <strong>Compared to the basic brute-force approach of detecting fraud, our datasets enabled Spark jobs to finish faster by a full order of magnitude</strong> (from about 55 hours with the N^2 method to 4 hours using LSH).\n\n<h2>API Tutorial</h2>\n\nTo best demonstrate how LSH works, we will walk through an example of using MinHashLSH on the <a href=\"https://aws.amazon.com/cn/datasets/wikipedia-extraction-wex/\">Wikipedia Extraction (WEX) dataset</a> to find similar articles.\n\nEach LSH family is linked to its metric space. In Spark 2.1, there are two LSH Estimators:\n\n<ul>\n<li>BucketedRandomProjectionLSH for <a href=\"https://en.wikipedia.org/wiki/Euclidean_distance\">Euclidean Distance</a></li>\n<li>MinHashLSH for Jaccard Distance</li>\n</ul>\n\nIn this scenario, we will use MinHashLSH since we will work with real-valued feature vectors of word counts.\n\n<h3>Load Raw Data</h3>\n\nAfter setting up our Spark cluster and mounting <a href=\"https://aws.amazon.com/datasets/wikipedia-extraction-wex/\">WEX dataset</a>, we upload a sample of WEX data to <a href=\"http://hadoop.apache.org/docs/r1.0.4/hdfs_design.html#Introduction\">HDFS</a> based on our cluster size. In the Spark shell, we load the sample data in HDFS\n\n<pre><code class=\"scala\">// Read RDD from HDFS\nimport org.apache.spark.ml.feature._\nimport org.apache.spark.ml.linalg._\nimport org.apache.spark.sql.types._\nval df = spark.read.option(\"delimiter\",\"\\t\").csv(\"/user/hadoop/testdata.tsv\")\nval dfUsed = df.select(col(\"_c1\").as(\"title\"), col(\"_c4\").as(\"content\")).filter(col(\"content\") !== null)\ndfUsed.show()\n</code></pre>\n\n[caption id=\"attachment_10813\" align=\"aligncenter\" width=\"395\"]<img src=\"https://databricks.com/wp-content/uploads/2017/05/uber-lsh-fig-1-wikipedia-extraction-dataset.png\" alt=\"Console output showing the first 20 rows of the Wikipedia Extraction (WEX) dataset\" width=\"395\" height=\"428\" class=\"size-full wp-image-10813\" /> <strong>Figure 1:</strong> Wikipedia articles are represented as titles and content.[/caption]\n\nFigure 1 shows the results of our previous code, displaying articles by title and subject matter. We will use the content as our hashing keys and approximately find similar Wikipedia articles in the following experiments.\n\n<h3>Prepare Feature Vectors</h3>\n\nMinHash is a very common LSH technique for quickly estimating how similar two sets are to each other. In MinHashLSH implemented in Spark, we represent each set as a binary sparse vector. In this step, we will convert the contents of Wikipedia articles into vectors.\n\nUsing the following code for feature engineering, we split the article content into words (Tokenizer), create feature vectors of word counts (CountVectorizer), and remove empty articles:\n\n<pre><code class=\"scala\">// Tokenize the wiki content\nval tokenizer = new Tokenizer().setInputCol(\"content\").setOutputCol(\"words\")\nval wordsDf = tokenizer.transform(dfUsed)\n\n// Word count to vector for each wiki content\nval vocabSize = 1000000\nval cvModel: CountVectorizerModel = new CountVectorizer().setInputCol(\"words\").setOutputCol(\"features\").setVocabSize(vocabSize).setMinDF(10).fit(wordsDf)\nval isNoneZeroVector = udf({v: Vector =&gt; v.numNonzeros &gt; 0}, DataTypes.BooleanType)\nval vectorizedDf = cvModel.transform(wordsDf).filter(isNoneZeroVector(col(\"features\"))).select(col(\"title\"), col(\"features\"))\nvectorizedDf.show()\n</code></pre>\n\n[caption id=\"attachment_10814\" align=\"aligncenter\" width=\"390\"]<img src=\"https://databricks.com/wp-content/uploads/2017/05/uber-lsh-fig-2-wikipedia-articles-converted-to-binary-sparse-vectors.png\" alt=\"Console output showing the first 20 rows of the Wikipedia Extraction (WEX) dataset converted to binary sparse vectors\" width=\"390\" height=\"426\" class=\"size-full wp-image-10814\" /> <strong>Figure 2:</strong> After feature engineering our code, the contents of Wikipedia articles are converted to binary sparse vectors.[/caption]\n\n<h3>Fit and Query an LSH Model</h3>\n\nIn order to use MinHashLSH, we first fit a MinHashLSH model on our featurized data with the below command:\n\n<pre><code class=\"scala\">val mh = new MinHashLSH().setNumHashTables(3).setInputCol(\"features\").setOutputCol(\"hashValues\")\nval model = mh.fit(vectorizedDf)\n</code></pre>\n\nWe can make several types of queries with our LSH model, but for the purposes of this tutorial, we first run a feature transformation on the dataset:\n\n<pre><code class=\"scala\">model.transform(vectorizedDf).show()\n</code></pre>\n\nThis command provides us with the hash values, which can be useful for manual joins and for feature generation.\n\n[caption id=\"attachment_10815\" align=\"aligncenter\" width=\"578\"]<img src=\"https://databricks.com/wp-content/uploads/2017/05/uber-lsh-fig-3-hash-with-array-of-vectors.png\" alt=\"Console output showing the Wikipedia Extraction (WEX) dataset alongside the new column of vectors created by MinHashLSH\" width=\"578\" height=\"398\" class=\"size-full wp-image-10815\" /> <strong>Figure 3:</strong> MinHashLSH will add a new column to store hashes. Each hash is represented as an array of vectors.[/caption]\n\nNext, we run an approximate nearest neighbor search to find the data point closest to our target.  For the sake of demonstration, we search for articles with content approximately matching the phrase <em>united states</em>.\n\n<pre><code class=\"scala\">val key = Vectors.sparse(vocabSize, Seq((cvModel.vocabulary.indexOf(\"united\"), 1.0), (cvModel.vocabulary.indexOf(\"states\"), 1.0)))\nval k = 40\nmodel.approxNearestNeighbors(vectorizedDf, key, k).show()\n</code></pre>\n\n[caption id=\"attachment_10816\" align=\"aligncenter\" width=\"649\"]<img src=\"https://databricks.com/wp-content/uploads/2017/05/uber-lsh-fig-4-nearest-neighbor-search.png\" alt=\"Console output showing the results of a nearest neighbor search on the Wikipedia Extraction (WEX) dataset\" width=\"649\" height=\"401\" class=\"size-full wp-image-10816\" /> <strong>Figure 4:</strong> An approximate nearest neighbor search finds Wikipedia articles related to the \u201cunited states.\u201d[/caption]\n\nFinally, we run an approximate similarity join to find similar pairs of articles within the same dataset:\n\n<pre><code class=\"scala\">// Self Join\nval threshold = 0.8\nmodel.approxSimilarityJoin(vectorizedDf, vectorizedDf, threshold).filter(\"distCol != 0\").show()\n</code></pre>\n\nWhile we use a self join, below, we could also join different datasets to get the same results.[caption id=\"attachment_10817\" align=\"aligncenter\" width=\"559\"]<img src=\"https://databricks.com/wp-content/uploads/2017/05/uber-lsh-fig-5-similarity-join-lists.png\" alt=\"Console output showing an approximate similarity join list of Wikipedia articles\" width=\"559\" height=\"404\" class=\"size-full wp-image-10817\" /> <strong>Figure 5:</strong> An approximate similarity join lists similar Wikipedia articles, setting the number of hash tables.[/caption]\n\nFigure 5 demonstrates how to set the number of <a href=\"https://en.wikipedia.org/wiki/Hash_table\">hash tables</a>. For an approximate similarity join and the approximate nearest neighbor command, the number of hash tables can be used to trade off between running time and false positive rate (<a href=\"https://en.wikipedia.org/wiki/Locality-sensitive_hashing\">OR-amplification</a>). Increasing the number of hash tables will increase the accuracy (a positive), but also the program\u2019s communication cost and running time. By default, the number of hash tables is set to one.\n\nTo gain additional practice using LSH in Spark 2.1, you can also run smaller examples in the Spark distribution for <a href=\"https://github.com/apache/spark/blob/8a51cfdcad5f8397558ed2e245eb03650f37ce66/examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala\">BucketRandomProjectionLSH</a> and <a href=\"https://github.com/apache/spark/blob/8a51cfdcad5f8397558ed2e245eb03650f37ce66/examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\">MinHashLSH</a>.\n\n<h2>Performance Tests</h2>\n\nIn order to gauge performance, we benchmarked our implementations of MinHashLSH on the WEX dataset. Using an AWS cloud, we used 16 executors (m3.xlarge instances) to perform an approximate nearest neighbor search and approximate similarity join on a sample of WEX datasets.\n\n[caption id=\"attachment_10818\" align=\"aligncenter\" width=\"1224\"]<img src=\"https://databricks.com/wp-content/uploads/2017/05/uber-lsh-fig-6-performance-test-results.jpg\" alt=\"Charts displaying performance comparison\" width=\"1224\" height=\"371\" class=\"size-full wp-image-10818\" /> <strong>Figure 6:</strong> With numHashTables=5, approximate nearest neighbor ran 2x faster than full scan (as shown on right). With numHashTables=3, approximate similarity join ran 3x-5x faster than full join and filter (as shown on left).[/caption]\n\nIn the tables above, we can see that approximate nearest neighbor ran 2x faster than full scan with the number of hash tables set to five, while approximate similarity join ran 3x-5x faster depending on the number of output rows and hash tables.\n\nOur experiment also shows that despite their short run time, the algorithms achieved high accuracy compared to the results of brute-force methods as ground truth. Meanwhile, approximate nearest neighbor search achieved 85% accuracy for the 40 returned rows, while our approximate similarity join successfully found 93% of the nearby row pairs. This speed-accuracy trade-off has made LSH a powerful tool in detecting fraudulent trips from terabytes of data every day.\n\n<h2>Next Steps</h2>\n\nWhile our LSH model has helped Uber identify fraudulent driver activity, our work is far from complete. During our initial implementation of LSH, we planned a number of features to deploy in future releases. The high priority features include:\n\n<ol>\n<li><a href=\"https://issues.apache.org/jira/browse/SPARK-18450\">SPARK-18450</a>: Besides specifying the number of hash tables needed to complete the search, this new feature  users to define the number of hash functions in each hash table.  This change will also provide full support for AND/OR-compound amplification.</li>\n<li><a href=\"https://issues.apache.org/jira/browse/SPARK-18082\">SPARK-18082</a> &amp; <a href=\"https://issues.apache.org/jira/browse/SPARK-18083\">SPARK-18083</a>: There are other LSH families we want to implement. These two updates will enable  bit sampling for the Hamming distance between two data points and signs of random projection for cosine distance that are commonly used in machine learning tasks.</li>\n<li><a href=\"https://issues.apache.org/jira/browse/SPARK-18454\">SPARK-18454</a>: A third feature will improve the API of the approximate nearest neighbor search. This new search, a multi-probe similarity search can improve the search quality without the requirement for a large number of hash tables. </li>\n</ol>\n\nWe welcome your feedback as we continue to develop and scale our project to incorporate the above features\u2014and many others."}
{"status": "publish", "description": null, "creator": "Wayne Chan", "link": "https://databricks.com/blog/2017/05/19/persistent-clusters-simplifying-cluster-management-analytics.html", "authors": null, "id": 10831, "categories": ["Announcements", "Company Blog", "Product"], "dates": {"publishedOn": "2017-05-19", "tz": "UTC", "createdOn": "2017-05-19"}, "title": "Persistent Clusters: Simplifying Cluster Management for Analytics", "slug": "persistent-clusters-simplifying-cluster-management-analytics", "content": "Today we are excited to announce persistent clusters for analytics in Databricks. With persistent clusters, users no longer need to go through the laborious process of providing all the cluster and Apache Spark configuration every time they need a cluster. Admins can provide all the configurations to setup multiple clusters once and then allow users of their organization to easily start and terminate those clusters based on usage needs.\n\nPersistent clusters will empower the admins to set up clusters and enforce certain limits while giving the regular users control over creating and terminating the clusters when they want them. When setup with the right permissions, users just need to click on a \u2018Start\u2019 button when they need the cluster without having to worry about the complexities of creating and terminating clusters.\n\nThis will tremendously simplify how different teams work with clusters on Databricks and will also allow them to better optimize and control infrastructure costs.\n\n<h2>Cluster Management Complexities</h2>\n\nClusters are pivotal for working with data. In Databricks, different users can set up clusters with different configurations based on their use cases, workload needs, resource requirements and the volume of the data they are processing. Databricks empowers the users to set up a cluster in a myriad of ways to meet their needs. Some examples:\n\n<ul>\n<li>Teams can provide specific cloud instance types and availability zones if they have purchased reserved instances from the cloud provider.</li>\n<li>Teams can provide a hybrid composition of on-demand and spot instances to maintain a balance between costs and reliability.</li>\n<li>Data engineers and scientists can fine-tune their jobs with custom Spark parameters.</li>\n<li>Admins can provide access control permissions and IAM roles to restrict access to the cluster and data.</li>\n</ul>\n\nOn the flip side, teams also run into problems as they use clusters on a more day-to-day basis:\n\n<ul>\n<li><strong>Auto-termination of clusters:</strong> Since organizations get billed based on their cluster usage, teams want the ability to automatically terminate the clusters that are not being in use to avoid incurring unnecessary compute costs.</li>\n<li><strong>Recreation of clusters is cumbersome:</strong> As mentioned above, users want to set up clusters with different custom configurations to meet their needs. In such cases, it is really painful to provide all these parameters manually every time a cluster is required again.</li>\n<li><strong>No enforcement of cluster parameters:</strong> Admins don\u2019t have control over how users create and setup their clusters. Very often, admins want to enforce certain configurations like:\n\n<ul>\n<li>Tags to charge back costs.</li>\n<li>IAM roles to control access to data.</li>\n<li>Spot instance to save costs.</li>\n</ul></li>\n<li><strong>Reattaching libraries, notebooks, jobs every time:</strong> It is common to set up a cluster with multiple libraries installed and many notebooks and jobs attached to the cluster. Terminating such a cluster means all that information is lost and will need to be reattached again. This becomes a big nightmare as more users start using the platform.</li>\n</ul>\n\n<h2>Simplifying Cluster Management with Persistent Clusters</h2>\n\n\u201cPersistent clusters\u201d is a series of features to help administrators and teams resolve the problem around easily terminating and recreating clusters to address the aforementioned issues. Let\u2019s walk through these features with some examples and use cases.\n\nImagine two teams in an organization using Databricks - a data engineering team running production batch pipelines and a data science team doing ad-hoc analytics on historical data. The two teams want two different types of clusters - one for development and one for production.\n\n<h3>Creating and Cloning Clusters</h3>\n\nThe Databricks admin can create four different persistent clusters for these purposes. Based on the team\u2019s usage needs, the admin can set up the cluster with different configurations for instance types, <a href=\"https://docs.databricks.com/user-guide/clusters/sizing.html\">auto-scaling limits</a>, <a href=\"https://docs.databricks.com/user-guide/clusters/composition.html\">spot and on-demand composition</a>, <a href=\"https://docs.databricks.com/user-guide/clusters/log-delivery.html\">logging</a> and <a href=\"https://docs.databricks.com/user-guide/clusters/ssh.html\">SSH parameters</a>, etc.\n\nThe admin can also clone an existing cluster if the new cluster needs to have a similar configuration as one of the existing ones. The cluster creation form will be pre-filled with configurations of the cluster being cloned.\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/05/Clone-clusters.png\"><img class=\"aligncenter size-full wp-image-10832\" src=\"https://databricks.com/wp-content/uploads/2017/05/Clone-clusters.png\" alt=\"\" width=\"1117\" height=\"317\" /></a>\n\n<h3>Enabling Auto-Termination to Save Costs</h3>\n\nOne of Databricks\u2019 most requested features has been auto-termination. Now during cluster creation, the cluster creator can enable auto-termination to save costs. For every cluster, the admin can specify the amount of idle time after which the cluster needs to be terminated. Databricks uses a low-level Spark-native task tracking algorithm to determine the cluster\u2019s idle period and automatically terminates them after the specified idle time to save costs both with Databricks and the cloud provider.\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/05/auto-term.png\"><img class=\"alignnone size-full wp-image-10835\" src=\"https://databricks.com/wp-content/uploads/2017/05/auto-term.png\" alt=\"\" width=\"766\" height=\"106\" /></a>\n\n<h3>Setting up the right cluster permissions</h3>\n\nOnce the clusters are created, the admin can now give the right permissions for the users of their platform. The below table captures the different permissions and the actions allowed with them:\n\n<table class=\"table\">\n<thead>\n<tr>\n<th></th>\n<th style=\"text-align: center;\" colspan=\"3\">Permission Levels</th>\n</tr>\n<tr>\n<th>Actions</th>\n<th style=\"text-align: center;\">Can Attach To</th>\n<th style=\"text-align: center;\">Can Restart</th>\n<th style=\"text-align: center;\">Can Manage</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>View Spark UI &amp; logs</td>\n<td class=\"h-color-green\" style=\"text-align: center; vertical-align: middle;\">[db_icon name=\"checkmark\"]<span class=\"sr-only\">Yes</span></td>\n<td class=\"h-color-green\" style=\"text-align: center; vertical-align: middle;\">[db_icon name=\"checkmark\"]<span class=\"sr-only\">Yes</span></td>\n<td class=\"h-color-green\" style=\"text-align: center; vertical-align: middle;\">[db_icon name=\"checkmark\"]<span class=\"sr-only\">Yes</span></td>\n</tr>\n<tr>\n<td>Attach notebooks and libraries</td>\n<td class=\"h-color-green\" style=\"text-align: center; vertical-align: middle;\">[db_icon name=\"checkmark\"]<span class=\"sr-only\">Yes</span></td>\n<td class=\"h-color-green\" style=\"text-align: center; vertical-align: middle;\">[db_icon name=\"checkmark\"]<span class=\"sr-only\">Yes</span></td>\n<td class=\"h-color-green\" style=\"text-align: center; vertical-align: middle;\">[db_icon name=\"checkmark\"]<span class=\"sr-only\">Yes</span></td>\n</tr>\n<tr>\n<td>Start cluster</td>\n<td></td>\n<td class=\"h-color-green\" style=\"text-align: center; vertical-align: middle;\">[db_icon name=\"checkmark\"]<span class=\"sr-only\">Yes</span></td>\n<td class=\"h-color-green\" style=\"text-align: center; vertical-align: middle;\">[db_icon name=\"checkmark\"]<span class=\"sr-only\">Yes</span></td>\n</tr>\n<tr>\n<td>Terminate cluster</td>\n<td></td>\n<td class=\"h-color-green\" style=\"text-align: center; vertical-align: middle;\">[db_icon name=\"checkmark\"]<span class=\"sr-only\">Yes</span></td>\n<td class=\"h-color-green\" style=\"text-align: center; vertical-align: middle;\">[db_icon name=\"checkmark\"]<span class=\"sr-only\">Yes</span></td>\n</tr>\n<tr>\n<td>Restart cluster</td>\n<td></td>\n<td class=\"h-color-green\" style=\"text-align: center; vertical-align: middle;\">[db_icon name=\"checkmark\"]<span class=\"sr-only\">Yes</span></td>\n<td class=\"h-color-green\" style=\"text-align: center; vertical-align: middle;\">[db_icon name=\"checkmark\"]<span class=\"sr-only\">Yes</span></td>\n</tr>\n<tr>\n<td>Resize cluster</td>\n<td></td>\n<td></td>\n<td class=\"h-color-green\" style=\"text-align: center; vertical-align: middle;\">[db_icon name=\"checkmark\"]<span class=\"sr-only\">Yes</span></td>\n</tr>\n<tr>\n<td>Give other users access to the cluster</td>\n<td></td>\n<td></td>\n<td class=\"h-color-green\" style=\"text-align: center; vertical-align: middle;\">[db_icon name=\"checkmark\"]<span class=\"sr-only\">Yes</span></td>\n</tr>\n</tbody>\n</table>\n\nDatabricks recommends the following guidelines for admins to setup the permissions for clusters:\n\n<ul>\n<li><strong>Can Attach To:</strong> Use this permission if the users should not have a direct influence on the cost. They can attach notebooks and libraries and run commands but they cannot start or terminate the clusters.</li>\n<li><strong>Can Restart:</strong> Use this permission if you would like your users to freely start and terminate the cluster as well. They have an impact on the cost, but they cannot modify the max limits and other cluster configurations that are set.</li>\n<li><strong>Can Manage:</strong> Use this permission if you want to give a user full control of the cluster. They can adjust the max limits and also give other users permissions to access the cluster.</li>\n</ul>\n\n<h3>Enforcing Cluster Configuration</h3>\n\nAdmins can also enforce certain configurations for clusters so that users cannot change them. Databricks recommends the following workflow for organizations that need to enforce configurations:\n\n<ol>\n<li>Disable \u2018Allow cluster creation\u2019 option in the admin console for all the non-admin users.\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/05/enforcing-clusters.png\"><img class=\"alignnone size-large wp-image-10863\" src=\"https://databricks.com/wp-content/uploads/2017/05/enforcing-clusters-1024x275.png\" alt=\"\" width=\"1024\" height=\"275\" /></a></p></li>\n<li><p>After creating all of the cluster configurations that you would like your users to use, you can give all the users who need to have access to that given cluster \u2018Can Restart\u2019 permission. This will allow a given user, with that permission, to freely start and terminate\u00a0the cluster without having to set up all the configurations manually.\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/05/permissions.png\"><img class=\"alignnone size-medium wp-image-10836\" src=\"https://databricks.com/wp-content/uploads/2017/05/permissions-300x162.png\" alt=\"\" width=\"300\" height=\"162\" /></a></p></li>\n</ol>\n\n<h3>Starting Clusters</h3>\n\n<p>As mentioned above, once a user has \u2018Can Restart\u2019 permission, they simply need to click the \u2018Start\u2019 button in the cluster page to start that cluster again. The cluster will be recreated with all the existing configuration and all libraries and notebooks will reattach automatically. This functionality is also available through the REST API for users <a href=\"https://docs.databricks.com/api/latest/clusters.html#start\">provisioning the clusters through REST APIs</a>.\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/05/start-clusters.png\"><img class=\"alignnone size-large wp-image-10837\" src=\"https://databricks.com/wp-content/uploads/2017/05/start-clusters.png\" alt=\"\" width=\"787\" height=\"192\" /></a>\n\n<h3>Terminating Clusters</h3>\n\nPreviously in Databricks, users or administrators would have to manually terminate clusters when they were done with them. Now, if auto-termination was specified at cluster creation, then users never need to remember to turn off a cluster when they are done - Databricks will handle that for them. If manual termination is needed, then users who have \u2018Can Restart\u2019 permission to a cluster can terminate the cluster any time.\n\n<h2>What\u2019s next</h2>\n\nWant to learn more about other cluster management features? Check out the <a href=\"https://docs.databricks.com/user-guide/clusters/index.html\">user guide</a> for all the existing functionalities and best practices.\n\nTry out the new cluster management functionalities by <a href=\"https://accounts.cloud.databricks.com/registration.html#signup\">signing up for a free trial of Databricks</a>."}
{"status": "publish", "description": null, "creator": "bill", "link": "https://databricks.com/blog/2017/05/18/taking-apache-sparks-structured-structured-streaming-to-production.html", "authors": null, "id": 10848, "categories": ["Apache Spark", "Engineering Blog", "Streaming"], "dates": {"publishedOn": "2017-05-18", "tz": "UTC", "createdOn": "2017-05-18"}, "title": "Taking Apache Spark\u2019s Structured Streaming to Production", "slug": "taking-apache-sparks-structured-structured-streaming-to-production", "content": "<em>This is the fifth post in a <a href=\"https://databricks.com/blog/2017/01/19/real-time-streaming-etl-structured-streaming-apache-spark-2-1.html\">multi-part series</a> about how you can perform complex streaming analytics using Apache Spark.</em>\n\n<hr />\n\nAt Databricks, we\u2019ve migrated our production pipelines to Structured Streaming over the past several months and wanted to share our out-of-the-box deployment model to allow our customers to rapidly build production pipelines in Databricks.\n\nA production application requires monitoring, alerting, and an automatic (cloud native) approach to failure recovery. This post will not just walk you through the APIs available for tackling these challenges but will also show you how Databricks makes running Structured Streaming in production simple.\n\n<h2>Metrics and Monitoring</h2>\n\n<a href=\"https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html\">Structured Streaming in Apache Spark</a> provides a simple programmatic API to get information about a stream that is currently executing. There are two key commands that you can run on a currently active stream in order to get relevant information about the query execution in progress: a command to get the current <em>status</em> of the query and a command to get <em>recentProgress</em> of the query.\n\n<h3>Status</h3>\n\nThe first question you might ask is, \"what processing is my stream performing right now?\"  The status maintains information about the current state of the stream, and is accessible through the object that was returned when you started the query. For example, you might have a simple counts stream that provides counts of IOT devices defined by the following query.\n\n<pre><code class=\"scala\">query = streamingCountsDF \\\n    .writeStream \\\n    .format(\"memory\") \\\n    .queryName(\"counts\") \\\n    .outputMode(\"complete\") \\\n    .start()\n</code></pre>\n\nRunning <code>query.status</code> will return the current status of the stream. This gives us details about what is happening at that point in time in the stream.\n\n<pre><code class=\"json\">{\n  \"message\" : \"Getting offsets from FileStreamSource[dbfs:/databricks-datasets/structured-streaming/events]\",\n  \"isDataAvailable\" : true,\n  \"isTriggerActive\" : true\n}\n</code></pre>\n\nDatabricks notebooks give you a simple way to see that status of any streaming query. Simply hover over the <img src=\"https://databricks.com/wp-content/uploads/2017/05/streaming-icon.png\" alt=\"green streaming\" width=\"23\" height=\"24\" style=\"display: inline;\" /> icon available in a streaming query. You\u2019ll get the same information, making it much more convenient to quickly understand the state of your stream.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/05/databricks-stream-state-example.png\" alt=\"Screenshot of a user observing a stream&#039;s state in Databricks\" width=\"410\" height=\"327\" class=\"aligncenter size-full wp-image-10851\" />\n\n<h3>Recent Progress</h3>\n\nWhile the query status is certainly important, equally important is an ability to view query\u2019s historical progress. Progress metadata will allow us to answer questions like \"At what rate am I processing tuples?\" or \"How fast are tuples arriving from the source?\"\n\nBy running <code>stream.recentProgress</code> you\u2019ll get access to some more time-based information like the processing rate and batch durations. However, a picture is worth a thousand JSON blobs, so at Databricks, we created visualizations in order to facilitate rapid analysis of the recent progress of the stream.\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/05/databricks-stream-progress-dashboard-screenshot.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/05/databricks-stream-progress-dashboard-screenshot.png\" alt=\"Screenshot of a user observing their stream&#039;s recent progress via dashboards\" width=\"592\" height=\"299\" class=\"aligncenter size-full wp-image-10852\" /></a>\n\nLet\u2019s explore why we chose to display these metrics and why they\u2019re important for you to understand.\n\n<h4>Input Rate and Processing Rate</h4>\n\nThe input rate specifies how much data is flowing into Structured Streaming from a system like Kafka or Kinesis. The processing rate is how quickly we were able to analyze that data. In the ideal case, these should vary consistently together; however, they will vary according to how much input data exists when processing starts. If the input rate far outpaces the processing rate, our streams will fall behind, and we will have to scale the cluster up to a larger size to handle the greater load.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/05/input-vs-processing-rate-dashboard.png\" alt=\"Screenshot of the Input vs Data Processing Rate dashboard\" width=\"374\" height=\"260\" class=\"aligncenter size-full wp-image-10853\" />\n\n<h4>Batch Duration</h4>\n\nNearly all streaming systems utilize batching to operate at any reasonable throughput (some have an option of high latency in exchange for lower throughput). Structured Streaming achieves both. As it operates on the data, you will likely see this oscillate as Structured Streaming processes varying numbers of events over time. On this single core cluster on Community Edition, we can see that our batch duration is oscillating consistently around three seconds. Larger clusters will naturally have much faster processing rates as well as much shorter batch durations.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/05/batch-duration-dashboard.png\" alt=\"Screenshot of the Batch Duration dashboard\" width=\"365\" height=\"263\" class=\"aligncenter size-full wp-image-10854\" />\n\n<h2>Production Alerting on Streaming Jobs</h2>\n\nMetrics and Monitoring are all well and good, but in order to react quickly to any issues that arise without having to babysit your streaming jobs all day, you\u2019re going to need a robust alerting story. Databricks makes alerting easy by allowing you to run your Streaming jobs as production pipelines.\n\nFor instance, let\u2019s define a Databricks jobs with the following specifications:\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/05/job-alerting-example.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/05/job-alerting-example.png\" alt=\"Screenshot showing a Spark job in Databricks with a custom alert set\" width=\"892\" height=\"283\" class=\"aligncenter size-full wp-image-10855\" /></a>\n\nNotice how we <a href=\"https://www.pagerduty.com/docs/guides/email-integration-guide/\">set an email address to trigger an alert in PagerDuty</a>. This will trigger a product alert (or to the level that you specify) when the job fails.\n\n<h2>Automated Failure Recovery</h2>\n\nWhile alerting is convenient, having to force a human to respond to an outage is inconvenient at best and impossible at worst. In order to truly productionize Structured Streaming, you\u2019re going to want to be able to recover automatically to failures as quickly as you can, while ensuring data consistency and no data loss. Databricks makes this seamless: simply set the number of retries before a <em>unrecoverable failure</em> and Databricks will try to recover the streaming job automatically for you. On each failure, you can trigger a notification as a production outage.\n\nYou get the best of both worlds. The system will attempt to self-heal while keeping employees and developers informed of the status.\n\n<h2>Updating Your Application</h2>\n\nThere are two circumstances that you need to reason about when you are updating your streaming application. For the most part, if you\u2019re not changing significant business logic (like the output schema) you can simply restart the streaming job using the same checkpoint directory. The new updated streaming application will pick up where it left off and continue functioning.\n\nHowever, if you\u2019re changing stateful operations (like aggregations or the output schema), the update is a bit more involved. You\u2019ll have to start an entirely new stream with a new checkpoint directory. Luckily, it\u2019s easy to start up another stream in Databricks in order to run both in parallel while you transition to the new stream.\n\n<h2>Advanced Alerting and Monitoring</h2>\n\nThere are several other advanced monitoring techniques that Databricks supports as well. For example, you can output notifications using a system like <a href=\"https://www.datadoghq.com/\">Datadog</a>, <a href=\"https://kafka.apache.org/\">Apache Kafka</a>, or <a href=\"https://github.com/dropwizard/metrics\">Coda Hale Metrics</a>. These advanced techniques can be used to implement external monitoring and alerting systems.\n\nBelow is an example of how you can create a <a href=\"http://spark.apache.org/docs/2.1.0/api/java/org/apache/spark/sql/streaming/StreamingQueryListener.QueryTerminatedEvent.html\">StreamingQueryListener</a> that will forward all query progress information to Kafka.\n\n<pre><code class=\"scala\">class KafkaMetrics(servers: String) extends StreamingQueryListener {\n  val kafkaProperties = new Properties()\n  kafkaProperties.put(\"bootstrap.servers\", servers)\n  kafkaProperties.put(\"key.serializer\", \"kafkashaded.org.apache.kafka.common.serialization.StringSerializer\")\n  kafkaProperties.put(\"value.serializer\", \"kafkashaded.org.apache.kafka.common.serialization.StringSerializer\")\n\n  val producer = new KafkaProducer[String, String](kafkaProperties)\n\n  def onQueryProgress(event: org.apache.spark.sql.streaming.StreamingQueryListener.QueryProgressEvent): Unit = {\n    producer.send(new ProducerRecord(\"streaming-metrics\", event.progress.json))\n  }\n  def onQueryStarted(event: org.apache.spark.sql.streaming.StreamingQueryListener.QueryStartedEvent): Unit = {}\n  def onQueryTerminated(event: org.apache.spark.sql.streaming.StreamingQueryListener.QueryTerminatedEvent): Unit = {}\n}\n</code></pre>\n\n<h2>Conclusion</h2>\n\nIn this post, we showed how simple it is to take Structured Streaming from prototype to production using Databricks. To read more about other aspects of Structured Streaming, read our series of blogs:\n\n<ul>\n<li><a href=\"https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html\">Structured Streaming In Apache Spark</a></li>\n<li><a href=\"https://databricks.com/blog/2017/01/19/real-time-streaming-etl-structured-streaming-apache-spark-2-1.html\">Real-time Streaming ETL with Structured Streaming in Apache Spark 2.1</a></li>\n<li><a href=\"https://databricks.com/blog/2017/02/23/working-complex-data-formats-structured-streaming-apache-spark-2-1.html\">Working with Complex Data Formats with Structured Streaming in Apache Spark 2.1</a></li>\n<li><a href=\"https://databricks.com/blog/2017/04/26/processing-data-in-apache-kafka-with-structured-streaming-in-apache-spark-2-2.html\">Processing Data in Apache Kafka with Structured Streaming in Apache Spark 2.2</a></li>\n<li><a href=\"https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html\">Event-time Aggregation and Watermarking in Apache Spark\u2019s Structured Streaming</a></li>\n</ul>\n\nYou can learn more about using streaming from the <a href=\"https://docs.databricks.com/spark/latest/structured-streaming/index.html\">Databricks Documentation</a> or sign up to <a href=\"https://databricks.com/try-databricks\">start a free trial today</a>."}
{"status": "publish", "description": null, "creator": "jules_damji", "link": "https://databricks.com/blog/2017/05/23/demand-webinar-faq-deep-learning-apache-spark-workflows-best-practices.html", "authors": null, "id": 10888, "categories": ["Apache Spark", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2017-05-23", "tz": "UTC", "createdOn": "2017-05-23"}, "title": "On-Demand Webinar and FAQ: Deep Learning and Apache Spark: Workflows and Best Practices", "slug": "demand-webinar-faq-deep-learning-apache-spark-workflows-best-practices", "content": "On May 4th, we hosted a live webinar \u2014 <a href=\"http://go.databricks.com/deep-learning-on-apache-spark\">Deep Learning and Apache Spark: Workflows and Best Practices</a>. Rather than comparing deep learning systems or specific optimizations, this webinar focused on issues that are common to deep learning frameworks when running on an Apache Spark cluster, including:\n\n<ol>\n<li>how to optimize cluster setup; </li>\n<li>how to ideally configure the Spark cluster;</li>\n<li>how to ingest data; and</li>\n<li>how to monitor long-running jobs?</li>\n</ol>\n\n<h2>Recording and Slides</h2>\n\nIf you missed the webinar, you can view it <a href=\"http://go.databricks.com/deep-learning-on-apache-spark\">on-demand here</a> and the <a href=\"https://www.slideshare.net/databricks/deep-learning-on-apache-spark-workflows-and-best-practices-75721389\">slides are accessible</a> as attachments to the webinar.\n\n<h2>Q&amp;A</h2>\n\nToward the end, we held a Q&amp;A, and below are all the questions with links to the forum with their answers. <em>(Follow the link to view the answers.)</em>\n\n<ul>\n<li><a href=\"https://forums.databricks.com/questions/11576/how-can-i-become-an-expert-in-apache-spark.html#answer-11577\">How can I become an expert in Apache Spark?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11578/what-is-the-best-way-to-integrate-spark-into-web-a.html#answer-11579\">What is the best way to integrate Spark into Web application? Will it make any difference if we use Spark concepts in a web application</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11580/are-there-plans-to-integrate-the-firmament-cluster.html#answer-11581\">Are there plans to integrate the Firmament cluster scheduler (http://firmament.io/) into Databricks in the future</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11582/is-there-a-docker-image-spark-with-gpu-available-t.html#answer-11583\">Is there a docker image Spark with GPU available to try on?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11584/which-spark-version-is-required-to-work-with-deep.html#answer-11585\">Which Spark version is required to work with Deep Learning frameworks</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11586/is-tensorframe-available-for-open-source-version-o.html\">Is TensorFrame available for open source version on Spark or do we need to use databricks product?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11588/pyspark-seems-the-good-approach-for-dl-and-spark-a.html#answer-11589\">PySpark seems the good approach for DL and Spark at this moment. But for monitoring, Scala sounds like a better choice. What is your thought about which should be the language that should be used more in DL in the future?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11590/where-can-i-see-a-notebook-example-of-tensorframes.html#answer-11591\">Where can I see a notebook example of tensorFrames</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11593/are-you-offering-any-discount-code-for-the-spark-s.html#answer-11594\">Are you offering any discount code for the Spark Summit to the attendees?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11596/do-you-have-any-thoughts-on-combining-apache-spark.html#answer-11597\">Do you have any thoughts on combining Spark with Tensorflow (e.g. frameworks) vs integrated Tensorflow Distributed (via clusterspec)?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11598/are-some-limitations-mentioned-in-the-webinar-pecu.html#answer-11599\">Are some limitations mentioned in the webinar peculiar to running Spark on Databricks or also apply to Spark running on outside of Databricks? If yes, is there a way Databricks is coming up with ways to mitigate limitations?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11600/thanks-for-the-great-talk-how-can-we-access-the-sl.html#answer-11601\">Thanks for the great talk. How can we access the slides?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11602/how-is-nvidia-docker-image-different-from-databric.html#answer-11603\">How is nvidia-docker image different from Databricks stack vs PaperScale stack?</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11604/does-it-databricks-support-h2oai-based-deep-learni.html#answer-11605\">Does it [Databricks] support H2o.ai based deep learning i.e. sparkling water and deep water</a></li>\n<li><a href=\"https://forums.databricks.com/questions/11606/can-the-users-specify-the-image-filters-used-in-th.html#answer-11607\">Can the users specify the [image] filters used in the filtering process without using the default filters? If yes, how can we specify the filters</a></li>\n</ul>\n\nIf you\u2019d like to perform deep learning on Databricks, <a href=\"https://databricks.com/try\">start your 14-day free trial today.</a>"}
{"status": "publish", "description": null, "creator": "jakebellacera", "link": "https://databricks.com/blog/2017/05/22/running-streaming-jobs-day-10x-cost-savings.html", "authors": null, "id": 10897, "categories": ["Apache Spark", "Engineering Blog", "Streaming"], "dates": {"publishedOn": "2017-05-22", "tz": "UTC", "createdOn": "2017-05-22"}, "title": "Running Streaming Jobs Once a Day For 10x Cost Savings", "slug": "running-streaming-jobs-day-10x-cost-savings", "content": "<em>This is the sixth post in a <a href=\"https://databricks.com/blog/2017/01/19/real-time-streaming-etl-structured-streaming-apache-spark-2-1.html\">multi-part series</a> about how you can perform complex streaming analytics using Apache Spark.</em>\n\n<hr />\n\nTraditionally, when people think about streaming, terms such as \u201creal-time,\u201d \u201c24/7,\u201d or \u201calways on\u201d come to mind. You may have cases where data only arrives at fixed intervals. That is, data appears every hour or once a day. For these use cases, it is still beneficial to perform incremental processing on this data. However, it would be wasteful to keep a cluster up and running 24/7 just to perform a short amount of processing once a day.\n\nFortunately, by using the new Run Once trigger feature added to Structured Streaming in Spark 2.2, you will get all the benefits of the Catalyst Optimizer incrementalizing your workload, and the cost savings of not having an idle cluster lying around. In this post, we will examine how to employ triggers to accomplish both.\n\n<h2>Triggers in Structured Streaming</h2>\n\nIn Structured Streaming, triggers are used to specify how often a streaming query should produce results. Once a trigger fires, Spark checks to see if there is new data available. If there is new data, then the query is executed incrementally on whatever has arrived since the last trigger. If there is no new data, then the stream sleeps until the next trigger fires.\n\nThe default behavior of Structured Streaming is to run with the lowest latency possible, so triggers fire as soon as the previous trigger finishes. For use cases with lower latency requirements, Structured Streaming supports a ProcessingTime trigger which will fire every user-provided interval, for example every minute.\n\nWhile this is great, it still requires the cluster to remain running 24/7. In contrast, a RunOnce trigger will fire only once and then will stop the query.  As we'll see below, this lets you effectively utilize an external scheduling mechanism such as Databricks Jobs.\n\nTriggers are specified when you start your streams.\n\n[code_tabs]\n\n<pre><code class=\"python\"># Load your Streaming DataFrame\nsdf = spark.readStream.load(path=\"/in/path\", format=\"json\", schema=my_schema)\n# Perform transformations and then write\u2026\nsdf.writeStream.trigger(once=True).start(path=\"/out/path\", format=\"parquet\")\n</code></pre>\n\n<pre><code class=\"scala\">import org.apache.spark.sql.streaming.Trigger\n\n// Load your Streaming DataFrame\nval sdf = spark.readStream.format(\"json\").schema(my_schema).load(\"/in/path\")\n// Perform transformations and then write\u2026\nsdf.writeStream.trigger(Trigger.Once).format(\"parquet\").start(\"/out/path\")\n</code></pre>\n\n[/code_tabs]\n\n<h2>Why Streaming and RunOnce is Better than Batch</h2>\n\nYou may ask, how is this different than simply running a batch job? Let\u2019s go over the benefits of running Structured Streaming over a batch job.\n\n<h3>Bookkeeping</h3>\n\nWhen you\u2019re running a batch job that performs incremental updates, you generally have to deal with figuring out what data is new, what you should process, and what you should not. Structured Streaming already does all this for you. In writing general streaming applications, you should only care about the business logic, and not the low-level bookkeeping.\n\n<h3>Table Level Atomicity</h3>\n\nThe most important feature of a big data processing engine is how it can tolerate faults and failures. The ETL jobs may (in practice, often will) fail. If your job fails, then you need to ensure that the output of your job should be cleaned up, otherwise you will end up with duplicate or garbage data after the next successful run of your job.\n\nWhile using Structured Streaming to write out a file-based table, Structured Streaming commits all files created by the job to a log after each successful trigger. When Spark reads back the table, it uses this log to figure out which files are valid. This ensures that garbage introduced by failures are not consumed by downstream applications.\n\n<h3>Stateful Operations Across Runs</h3>\n\nIf your data pipeline has the possibility of generating duplicate records, but you would like exactly once semantics, how do you achieve that with a batch workload? With Structured Streaming, it\u2019s as easy as setting a watermark and using <code>dropDuplicates()</code>. By configuring the watermark long enough to encompass several runs of your streaming job, you will make sure that you don\u2019t get duplicate data <strong>across</strong> runs.\n\n<h2>Cost Savings</h2>\n\nRunning a 24/7 streaming job is a costly ordeal. You may have use cases where latency of hours is acceptable, or data comes in hourly or daily. To get all the benefits of Structured Streaming described above, you may think you need to keep a cluster up and running all the time. But now, with the \u201cexecute once\u201d trigger, you don\u2019t need to!\n\nAt Databricks, we had a <a href=\"http://go.databricks.com/databricks-data-pipeline\">two stage data pipeline</a>, consisting of one incremental job that would make the latest data available, and one job at the end of the day that processed the whole day\u2019s worth of data, performed de-duplication, and overwrote the output of the incremental job. The second job would use considerably larger resources than the first job (4x), and would run much longer as well (3x). We were able to get rid of the second job in many of our pipelines that amounted to a 10x total cost savings. We were also able to clean up a lot of code in our codebase with the new execute once trigger. Those are cost savings that makes both financial and engineering managers happy!\n\n<h2>Scheduling Runs with Databricks</h2>\n\n<a href=\"https://databricks.com/blog/2015/03/18/databricks-launches-jobs-feature-for-production-workloads.html\">Databricks\u2019 Jobs scheduler</a> allows users to schedule production jobs with a few simple clicks. Jobs scheduler is ideal for scheduling Structured Streaming jobs that run with the execute once trigger.\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/05/databricks-job-scheduler-screenshot.png\"><img src=\"https://databricks.com/wp-content/uploads/2017/05/databricks-job-scheduler-screenshot.png\" alt=\"Screenshot of the Job Scheduler in Databricks\" width=\"1000\" height=\"525\" class=\"aligncenter size-full wp-image-10898\" /></a>\n\nAt Databricks, we use the Jobs scheduler to run all of our production jobs. As engineers, we ensure  that the business logic within our ETL job is well tested. We upload our code to Databricks as a library, and we set up notebooks to set the configurations for the ETL job such as the input file directory. The rest is up to Databricks to manage clusters, schedule and execute the jobs, and Structured Streaming to figure out which files are new, and process incoming data. The end result is an end-to-end \u2014 from data origin to data warehouse, not only within Spark \u2014 exactly once data pipeline. Check out <a href=\"https://docs.databricks.com/spark/latest/structured-streaming/production.html\">our documentation</a> on how to best run Structured Streaming with Jobs.\n\n<h2>Summary</h2>\n\nIn this blog post we introduced the new \u201cexecute once\u201d trigger for Structured Streaming. While the execute once trigger resembles running a batch job, we discussed all the benefits it has over the batch job approach, specifically:\n\n<ul>\n<li>Managing all the bookkeeping of what data to process</li>\n<li>Providing table level atomicity for ETL jobs to a file store</li>\n<li>Ensuring stateful operations across runs of the job, which allow for easy de-duplication</li>\n</ul>\n\nIn addition to all these benefits over batch processing, you also get the cost savings of not having an idle 24/7 cluster up and running for an irregular streaming job. The best of both worlds for batch and streaming processing are now under your fingertips.\n\nTry Structured Streaming today in Databricks by <a href=\"http://databricks.com/try\">signing up for a 14-day free trial </a>.\n\nOther parts of this blog series explain other benefits as well:\n\n<ul>\n<li><a href=\"https://databricks.com/blog/2017/01/19/real-time-streaming-etl-structured-streaming-apache-spark-2-1.html\">Real-time Streaming ETL with Structured Streaming in Apache Spark 2.1</a></li>\n<li><a href=\"https://databricks.com/blog/2017/02/23/working-complex-data-formats-structured-streaming-apache-spark-2-1.html\">Working with Complex Data Formats with Structured Streaming in Apache Spark 2.1</a></li>\n<li><a href=\"https://databricks.com/blog/2017/04/26/processing-data-in-apache-kafka-with-structured-streaming-in-apache-spark-2-2.html\">Processing Data in Apache Kafka with Structured Streaming in Apache Spark 2.2</a></li>\n<li><a href=\"https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html\">Event-time Aggregation and Watermarking in Apache Spark\u2019s Structured Streaming</a></li>\n<li><a href=\"https://databricks.com/blog/2017/05/18/taking-apache-sparks-structured-structured-streaming-to-production.html\">Taking Apache Spark\u2019s Structured Structured Streaming to Production</a></li>\n<li><a href=\"https://databricks.com/blog/2017/05/22/running-streaming-jobs-day-10x-cost-savings.html\">Running Streaming Jobs Once a Day For 10x Cost Savings</a></li>\n</ul>"}
{"status": "publish", "description": null, "creator": "rxin", "link": "https://databricks.com/blog/2017/05/24/databricks-runtime-3-0-beta-delivers-enterprise-grade-apache-spark.html", "authors": null, "id": 10913, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2017-05-24", "tz": "UTC", "createdOn": "2017-05-24"}, "title": "Databricks Runtime 3.0 Beta Delivers Cloud Optimized Apache Spark", "slug": "databricks-runtime-3-0-beta-delivers-enterprise-grade-apache-spark", "content": "A major value Databricks provides is the automatic provisioning, configuration, and tuning of clusters of machines that process data. Running on these machines are the Databricks Runtime artifacts, which include Apache Spark and additional software such as Scala, Python, DBIO, and DBES. For customers these artifacts provide value: they relieve them from the onus of manual scaling; they tighten up security; they boost I/O performance; and they deliver rapid release versions.\n\nIn the past, the Runtime was co-versioned with upstream Apache Spark. Today, we are changing to a new version scheme that decouples Databricks Runtime versions from Spark versions and allows us to convey major feature updates in Databricks Runtime clearly to our customers. We are also making the beta of Databricks Runtime 3.0, which is the next major release and includes the latest release candidate build of Apache Spark 2.2, available to all customers today. (Note that Spark 2.2 has not yet officially been released by Apache.)\n\nCustomers can select this version when creating a new cluster.\n\n<img class=\"aligncenter wp-image-10916 size-medium\" src=\"https://databricks.com/wp-content/uploads/2017/05/version.png\" alt=\"Databricks Runtime 3.0 versons\" width=\"300\" />\n\nIn this blog post, we will explain what Databricks Runtime is, the added value it provides, and preview some of the major updates in the upcoming 3.0 release.\n\n<h2>Databricks Runtime and Versioning</h2>\n\nDatabricks Runtime is the set of software artifacts that run on the clusters of machines managed by Databricks. It includes Spark but also adds a number of components and updates that substantially improve the usability, performance, and security of big data analytics. The primary differentiations are:\n\n<ol>\n<li><strong>Better Performance with DBIO:</strong> The Databricks I/O module, or DBIO, leverages the vertically integrated stack to significantly improve the performance of Spark in the cloud.</li>\n<li><strong>Stronger Security with DBES:</strong> The Databricks Enterprise Security, or DBES, module adds features such as data encryption at rest and in motion, fine-grained data access control, and auditing to satisfy standard compliance (e.g. HIPAA, SOC2) and the most stringent security requirements as one would expect of large enterprises.</li>\n<li><strong>Significantly lower operational complexity:</strong> With features such as auto-scaling of compute resources and local storage, we put Spark on \u201cautopilot\u201d and markedly reduce the operational complexity and management cost.</li>\n<li><strong>Rapid releases and early access to new features:</strong> Compared to upstream open source releases, Databricks' SaaS offering facilitates quicker release cycles, offering our customers the latest features and bug fixes that are not yet available in open source releases.</li>\n</ol>\n\nExisting Databricks customers might recognize that Databricks Runtime was called \u201ccluster image\u201d and was co-versioned with Spark before this release, for example, the Spark 2.1 line appeared in the Databricks platform as \u201c2.1.0-db1\u201d, \u201c2.1.0-db2\u201d, \u201c2.1.0-db3\u201d, and \u201c2.1.1-db4\u201d. While Spark is a major component of the runtime, the old co-versioning scheme has labeling limitations. The new version scheme decouples Databricks Runtime versions from Spark versions and allows us to convey major feature updates in Databricks Runtime clearly to our customers.\n\nIn effect, Databricks Runtime 3.0 beta includes the release candidate version of Spark 2.2, and all its artifacts, which will be updated automatically as we incorporate bug fixes until it is generally available in June. Next, we will discuss the major features and improvements in this Runtime release.\n\n<h2>Performance and DBIO</h2>\n\nDatabricks Runtime 3.0 includes a number of updates in DBIO that improve performance, data integrity, and security:\n\n<ul>\n<li><strong>Higher S3 throughput:</strong> Improves read and write performance of your Spark jobs.</li>\n<li><strong>More efficient decoding:</strong> Boosts CPU efficiency when decoding common formats.</li>\n<li><strong>Data skipping:</strong> Allows users to leverage statistics on data files to prune files more effectively in query processing.</li>\n<li><strong>Transactional writes to S3:</strong> Features transactional (atomic) writes (both appends and new writes) to S3. Speculation can be turned on safely.</li>\n</ul>\n\nAs part of DBIO artifact, Amazon Redshift connector enhancement includes:\n\n<ul>\n<li><strong>Advanced push down into Redshift:</strong> Query fragments that contain limit, samples, and aggregations can now be pushed down into Redshift for execution to reduce data movement from Redshift clusters to Spark.</li>\n<li><strong>Automatic end-to-end encryption with Redshift:</strong> Data at rest and in transport can be encrypted automatically.</li>\n</ul>\n\nShortly, we will publish a blog showing performance improvements as observed in the TPC-DS benchmarks. To give you a teaser, we compared Databricks Runtime 3.0 and Spark running on EMR, and Databricks is faster on every single query, with a total geomean of 5X improvements on the 99 complex TPC-DS queries. Over 10 queries improved more than 10X in runtime.\n\nA customer tested the latest release and found 4x to 60x improvements on her queries over earlier versions of Spark:\n\n<blockquote>\n  \"The performance has been phenomenal! I could almost accuse you of being a random number generator, except the results are correct!\"\n</blockquote>\n\n<h2>Fine-grained Data Access Control</h2>\n\nAs part of a new feature in DBES for fine-grained data access control for SQL and the DataFrame APIs, database administrators and data owners can now define access control policies on databases, tables, views, and functions in the catalog to restrict access.\n\nUsing standard SQL syntax, access control policies can be defined on views on arbitrary granularity, that is, row level, column level, and aggregate level. This is similar to features available in traditional databases such as Oracle or Microsoft SQL Server, but applies to both SQL and the DataFrame APIs across all supported languages. Even better, it is implemented in a way that does not have any performance penalty and does not require the installation of any additional software.\n\nAs an example, the following example grants user <em>rxin</em> permission to access the aggregate salary per department, but not the individual employee\u2019s salary.\n\n<pre><code class=\"SQL\">CREATE TABLE employee (name string, department string, salary double);\n\nCREATE VIEW dept_salary AS SELECT department, sum(salary) total_salary FROM employee GROUP BY department;\n\nGRANT SELECT ON dept_salary TO rxin;\n</code></pre>\n\nIn the coming weeks, we will publish a series of blogs and relevant documentation with more details on fine-grained data access control.\n\n<h2>Structured Streaming</h2>\n\n<a href=\"https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html\">Structured Streaming</a> was introduced one year ago to Spark as a new way to build <a href=\"https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html\">continuous applications</a>. Not only does it simplify building end-to-end streaming applications by exposing a single API to write streaming queries as you would write batch queries, but it also handles streaming complexities by ensuring exactly-once-semantics, doing incremental stateful aggregations and providing data consistency.\n\nThe Databricks Runtime 3.0 includes the following new features from Spark 2.2:\n\n<ul>\n<li>Support for arbitrary complex stateful processing using <em>[flat]MapGroupsWithState</em></li>\n<li>Support for <a href=\"https://databricks.com/blog/2017/04/26/processing-data-in-apache-kafka-with-structured-streaming-in-apache-spark-2-2.html\">reading and writing data in streaming or batch to/from Apache Kafka</a></li>\n</ul>\n\nIn addition to the upstream improvements, Databricks Runtime 3.0 has been optimized specifically for the cloud deployments, including the following enhancements:\n\n<ul>\n<li>Ability to cut costs dramatically, by combining the <a href=\"https://databricks.com/blog/2017/05/22/running-streaming-jobs-day-10x-cost-savings.html\">Once Trigger mode with the Databricks Job Scheduler</a></li>\n<li>Production monitoring with <a href=\"https://databricks.com/blog/2017/05/18/taking-apache-sparks-structured-structured-streaming-to-production.html\">integrated throughput and latency metrics</a></li>\n<li>Support for streaming data from <a href=\"https://docs.databricks.com/spark/latest/structured-streaming/kinesis.html\">Amazon's Kinesis</a></li>\n</ul>\n\nFinally, after processing 100s of billions of records in production streams, Databricks is also now considering structured streaming GA and ready for production for our customers.\n\n<h2>Other Notable Updates</h2>\n\n<strong>Higher order functions in SQL for nested data processing</strong>: Exposes a powerful and expressive way to work with nested data types (arrays, structs). See <a href=\"https://databricks.com/blog/2017/05/24/working-with-nested-data-using-higher-order-functions-in-sql-on-databricks.html\">this blog post</a> for more details.\n\n<strong>Improved multi-tenancy</strong>: When multiple users run workloads concurrently on the same cluster, Databricks Runtime 3.0 ensures that these users can get fair shares of the resources, so users running short, interactive queries are not blocked by users running large ETL jobs.\n\n<strong>Auto scaling local storage</strong>: Databricks Runtime 3.0 can automatically configure local storage and scale them on demand. Users no longer need to estimate and provision EBS volumes.\n\n<strong>Cost-based optimizer from Apache Spark</strong>: The most important update in Spark 2.2 is the introduction of a cost-based optimizer. This feature is now available (off by default) in Databricks Runtime 3.0 beta.\n\n<h2>Conclusion</h2>\n\nDatabricks Runtime 3.0 will include Spark 2.2 and more than 1,000 improvements in DBIO, DBES, and Structured Streaming to make data analytics easier, more secure, and more efficient.\n\nWhile we don\u2019t recommend putting any production workloads on this beta yet, we encourage you to give it a spin. The beta release will be updated automatically daily as we incorporate bug fixes in the upstream open source Apache Spark as well as other components until it is generally available in June.\n\nSign up for a <a href=\"https://databricks.com/try\">Databricks trial today</a> to test the full functionality."}
{"status": "publish", "description": null, "creator": "bill", "link": "https://databricks.com/blog/2017/05/24/working-with-nested-data-using-higher-order-functions-in-sql-on-databricks.html", "authors": null, "id": 10927, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2017-05-24", "tz": "UTC", "createdOn": "2017-05-24"}, "title": "Working with Nested Data Using Higher Order Functions in SQL on Databricks", "slug": "working-with-nested-data-using-higher-order-functions-in-sql-on-databricks", "content": "[dbce_cta href=\"https://docs.databricks.com/_static/notebooks/higher-order-functions.html\"]View this notebook on Databricks[/dbce_cta]\n\nNested data types offer Databricks customers and Apache Spark users powerful ways to manipulate structured data. In particular, they allow you to put complex objects like arrays, maps and structures inside of columns. This can help you model your data in a more natural way. While this feature is certainly useful, it can be a bit cumbersome to manipulate data inside of the complex objects because SQL (and Spark) do not have primitives for working with such data. In addition, it is time-consuming, non-performant, and non-trivial.\n\nFor these reasons, we are excited\u00a0to offer higher order functions in SQL in the <a href=\"https://databricks.com/blog/2017/05/24/databricks-runtime-3-0-beta-delivers-enterprise-grade-apache-spark.html\">Databricks Runtime 3.0 Release</a>, allowing users to efficiently create functions, in SQL, to manipulate array based data. Higher-order functions are a simple extension to SQL to manipulate nested data such as arrays. For example, the <code>TRANSFORM</code> expression below shows how we can add a number\u00a0to every element in an array:\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/05/Screen-Shot-2017-05-24-at-12.46.08-PM.png\"><img class=\"alignnone size-full wp-image-10963\" src=\"https://databricks.com/wp-content/uploads/2017/05/Screen-Shot-2017-05-24-at-12.46.08-PM.png\" alt=\"\" width=\"1250\" height=\"363\" /></a>\n\nIn this post, we'll cover previous approaches to nested data manipulation in SQL, followed\u00a0by the higher-order function syntax we have introduced in Databricks.\n\n<h2>Past Approaches</h2>\n\nBefore we introduce the new syntax for array manipulation, let's first discuss the current approaches to manipulating this sort of data in SQL:\n\n<ul>\n<li>built-in functions\u00a0(limited functionality)</li>\n<li>unpack\u00a0the array into individual rows, apply your function, then repack them into an array (many steps, hence inefficient)</li>\n<li>UDFs (not generic or efficient)</li>\n</ul>\n\nWe'll\u00a0explore each of these independently so that you can understand why array manipulation is difficult. Let\u2019s start off with a table with the schema below (see the included notebook for code that\u2019s easy to run).\n\n<pre><code>root\n|-- key: long (nullable = false)\n|-- values: array (nullable = false)\n| |-- element: integer (containsNull = true)\n|-- nested_values: array (nullable = false)\n| |-- element: array (containsNull = false)\n| | |-- element: integer (containsNull = true)\n</code></pre>\n\n<h3>Built-In Functions</h3>\n\nSpark SQL does have some built-in\u00a0functions for manipulating arrays. For example, you can create an array, get its size, get specific elements, check if the array contains an object, and sort the array. Spark SQL also\u00a0supports generators (<code>explode</code>, <code>pos_explode</code> and <code>inline</code>) that allow you to combine the input row with the array elements, and the <code>collect_list</code> aggregate. This functionality may meet your needs for certain tasks, but it is complex to do anything non-trivial, such as computing a custom\u00a0expression of each array element.\n\n<h3>Unpack and Repack</h3>\n\nThe common approach for non-trivial manipulations is the \"unpack and repack\" method. This is a \"Spark SQL native\" way of solving the problem because you don't have to write any custom code; you simply write SQL\u00a0code. The unpack and repack approach works by applying the following steps:\n\n<ol>\n<li>Use <code>LATERAL VIEW explode</code> to flatten the array, and combine the input row with each element in the array;</li>\n<li>Apply a given transformation, in this example <code>value + 1</code>, to each element in the exploded array; and</li>\n<li>Use <code>collect_list</code> or <code>collect_set</code> to create a new array.</li>\n</ol>\n\nWe can see an example of this in the SQL code below:\n\n<pre><code class=\"sql\">SELECT key,\nvalues,\ncollect_list(value + 1) AS values_plus_one\nFROM nested_data\nLATERAL VIEW explode(values) T AS value\nGROUP BY key,\nvalues\n</code></pre>\n\nWhile this approach certainly works, it has a few problems. First,\u00a0you must be absolutely sure that the key you are used for grouping is unique, otherwise the end result will be incorrect. Second,\u00a0there is no guaranteed ordering of arrays in Spark SQL. Specifying an operation that requires a specific ordering nearly guarantees incorrect results.\u00a0Finally,\u00a0the generated Spark SQL plan will likely be very expensive.\n\n<h3>User-Defined Functions (UDFs)</h3>\n\nLastly, we can write custom UDFs to manipulate array data. Our UDFs must define how we traverse an array and how we process the individual elements. Let's see some basic examples in Python and Scala.\n\n[code_tabs]\n\n<pre><code class=\"python\">from pyspark.sql.types import IntegerType\nfrom pyspark.sql.types import ArrayType\n\ndef add_one_to_els(elements):\nreturn [el + 1 for el in elements]\n\nspark.udf.register(\"plusOneIntPython\", add_one_to_els, ArrayType(IntegerType()))\n</code></pre>\n\n<pre><code class=\"scala\">def addOneToElements(elements: Seq[Int]) = elements.map(element =&gt; element + 1)\n\nspark.udf.register(\"plusOneInt\", addOneToElements(_:Seq[Int]):Seq[Int])\n</code></pre>\n\n[/code_tabs]\n\nOnce registered, we can use those functions to manipulate our data in Spark SQL.\n\n<pre><code class=\"sql\">SELECT key,\nvalues,\nplusOneInt(values) AS values_plus_one,\nplusOneIntPython(values) AS values_plus_one_py\nFROM nested_data\n</code></pre>\n\nThis approach has some advantages over the previous version: for example, it maintains element order, unlike the pack and repack method. However, it has\u00a0two key\u00a0disadvantages. First, you have to write functions in other languages than SQL and register them before running. Second, data\u00a0serialization\u00a0into Scala and Python can be very expensive, slowing down UDFs over Spark's SQL <a href=\"https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html\">optimized built-in processing</a>.\n\n<h2>Our Approach:\u00a0Higher Order Functions</h2>\n\nAs observed from the examples above, the traditional ways to manipulate nested data in SQL are cumbersome. To that end, we have built\u00a0a simple solution in Databricks: higher order functions in SQL.\n\n<em><a href=\"https://docs.databricks.com/_static/notebooks/higher-order-functions.html\">Run the following examples in this notebook.</a></em>\n\nOur solution introduces\u00a0two functional programming constructions to SQL: higher order functions and anonymous (lambda) functions. These work together to allow you to define functions that manipulate arrays in SQL. The <em>higher order function</em>, such as <code>TRANSFORM</code>, takes an array\u00a0and a <em>lambda function</em> from the user to run on it. It then calls this lambda function\u00a0on\u00a0each element\u00a0in the array.\n\n<h3>A Simple Example: <code>TRANSFORM</code></h3>\n\nLet's illustrate the previous concepts with the transformation from our previous example. In this case, the higher order function, <code>TRANSFORM</code>, will iterate over the array, apply the associated lambda function to each element, and create a new array. The lambda function, <code>element + 1</code>, specifies how each element is manipulated.\n\n<pre><code class=\"sql\">SELECT key,\nvalues,\nTRANSFORM(values, value -&gt; value + 1) AS values_plus_one\nFROM nested_data\n</code></pre>\n\nTo be abundantly clear, the transformation <code>TRANSFORM(values, value -&gt; value + 1)</code> has two components:\n\n<ol>\n<li><code>TRANSFORM(values..)</code> is the higher order function. This takes an array and an anonymous function as its input. Internally transform will take care of setting up a new array, applying the anonymous function to each element, and assigning the result to the output array.</li>\n<li>The <code>value -&gt; value + 1</code> is an anonymous function. The function is divided into two components separated by a <code>-&gt;</code> symbol:\na. <strong>The argument list.</strong> In this case, we only have one argument: value. We also support multiple arguments by creating a comma separated list of arguments enclosed by parenthesis, for example: <code>(x, y) -&gt; x + y</code>.\nb. <strong>The body.</strong> This is an expression that can use the arguments and outer variables to calculate the new value. In this case, we add 1 to the value argument.</li>\n</ol>\n\n<h4>Capturing Variables</h4>\n\nWe can also use other variables than the arguments in a lambda function; this is called capture. We can use variables defined on the top level, or variables defined in intermediate lambda functions. For example, the following transform adds the key (top level) variable to each element in the values array:\n\n<pre><code class=\"sql\">SELECT key,\nvalues,\nTRANSFORM(values, value -&gt; value + key) AS values_plus_key\nFROM nested_data\n</code></pre>\n\n<h4>Nested Calls</h4>\n\nSometimes data is deeply nested. If you want to transform such data, you can can use nested lambda functions. The following example transforms an array of integer arrays, and adds the key (top level) column and the size of the intermediate array to each element in the nested array.\n\n<pre><code class=\"sql\">SELECT key,\nnested_values,\nTRANSFORM(nested_values,\nvalues -&gt; TRANSFORM(values,\nvalue -&gt; value + key + SIZE(values))) AS new_nested_values\nFROM nested_data\n</code></pre>\n\n<h3>Supported Functions</h3>\n\nWe have added the following higher order functions to the 3.0 version of the <a href=\"https://databricks.com/blog/2017/05/24/databricks-runtime-3-0-beta-delivers-enterprise-grade-apache-spark.html\">Databricks Runtime</a>.\n\n<h4>transform(array&lt;T&gt;, function&lt;T, U&gt;): array&lt;U&gt;</h4>\n\nThis produces an <code>array&lt;u&gt;</code> by applying a <code>function&lt;T, U&gt;</code> to each element of an input <code>array</code>.</u>\n\nNote that the functional programming equivalent operation is map. This has been named transform in order to prevent confusion with the map expression (that creates a map from a key value expression).\n\nThe following query transforms the values array by adding the key value to each element:\n\n<pre><code class=\"sql\">SELECT key,\nvalues,\nTRANSFORM(values, value -&gt; value + key) transformed_values\nFROM nested_data\n</code></pre>\n\n<h4>exists(array&lt;T&gt;, function&lt;T, V, Boolean&gt;): Boolean</h4>\n\nReturn true if predicate <code>function&lt;T, Boolean&gt;</code> holds for any element in input <code>array</code>.\n\nThe following examples checks if the values array contains an elements for which the modulo 10 is equal to 1:\n\n<pre><code class=\"sql\">SELECT key,\nvalues,\nEXISTS(values, value -&gt; value % 10 == 1) filtered_values\nFROM nested_data\n</code></pre>\n\n<h4>filter(array&lt;T&gt;, function&lt;T, Boolean&gt;): array&lt;T&gt;</h4>\n\nProduce an output <code>array</code> from an input <code>array</code> by only only adding elements for which predicate <code>function&lt;T, Boolean&gt;</code> holds.\n\nThe following examples filters the values array only elements with a value &gt; 50 are allowed:\n\n<pre><code class=\"sql\">SELECT key,\nvalues,\nFILTER(values, value -&gt; value &gt; 50) filtered_values\nFROM nested_data\n</code></pre>\n\n<h4>aggregate(array&lt;T&gt;, B, function&lt;B, T, B&gt;, function&lt;B, R&gt;): R</h4>\n\nReduce the elements of <code>array</code> into a single value <code>R</code> by merging the elements into a buffer <code>B</code> using <code>function&lt;B, T, B&gt;</code> and by applying a finish <code>function&lt;B, R&gt;</code> on the final buffer. The initial value <code>B</code> is determined by a zero expression. The finalize function is optional, if you do not specify the function the finalize function the identity function <code>(id -&gt; id)</code> is used.\n\nThis is the only higher order function that takes two lambda functions.\n\nThe following example sums (aggregates) the values array into a single (sum) value. Both a version with a finalize function (<code>summed_values</code>) and one without a finalize function <code>summed_values_simple</code> is shown:\n\n<pre><code class=\"sql\">SELECT key,\nvalues,\nREDUCE(values, 0, (value, acc) -&gt; value + acc, acc -&gt; acc) summed_values,\nREDUCE(values, 0, (value, acc) -&gt; value + acc) summed_values_simple\nFROM nested_data\n</code></pre>\n\nYou can also compute more complex aggregates. The code below shows the computation of the geometric mean of the array elements.\n\n<pre><code class=\"sql\">SELECT key,\nvalues,\nAGGREGATE(values,\n(1.0 AS product, 0 AS N),\n(buffer, value) -&gt; (value * buffer.product, buffer.N + 1),\nbuffer -&gt; Power(buffer.product, 1.0 / buffer.N)) geomean\nFROM nested_data\n</code></pre>\n\n<h2>Conclusion</h2>\n\nHigher order functions will available in <a href=\"https://databricks.com/blog/2017/05/24/databricks-runtime-3-0-beta-delivers-enterprise-grade-apache-spark.html\">Databricks Runtime 3.0</a>. If you have any nested data, be sure to try them!\n\nThis work adds initial support for using higher order functions with nested array\u00a0data. Additional functions and support for map data\u00a0are on their way. Be sure to check out the <a href=\"https://databricks.com/blog/2017/05/24/databricks-runtime-3-0-beta-delivers-enterprise-grade-apache-spark.html\">Databricks blog</a> and <a href=\"https://docs.databricks.com/_static/notebooks/higher-order-functions.html\">documentation</a>."}
{"status": "publish", "description": null, "creator": "Xiangrui", "link": "https://databricks.com/blog/2017/05/25/using-sparklyr-databricks.html", "authors": null, "id": 10972, "categories": ["Apache Spark", "Ecosystem", "Engineering Blog"], "dates": {"publishedOn": "2017-05-25", "tz": "UTC", "createdOn": "2017-05-25"}, "title": "Using sparklyr in Databricks", "slug": "using-sparklyr-databricks", "content": "[dbce_cta href=\"http://dbricks.co/2rZtTD0\"]Try this notebook on Databricks with all instructions as explained in this post notebook[/dbce_cta]\n\nIn September 2016, RStudio announced <a href=\"http://spark.rstudio.com/\">sparklyr</a>, a new <a href=\"https://blog.rstudio.org/2016/09/27/sparklyr-r-interface-for-apache-spark/\">R interface to Apache Spark</a>. sparklyr\u2019s interface to Spark follows the popular <a href=\"https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html\">dplyr</a> syntax. At Databricks, we provide the best place to run Apache Spark and all applications and packages powered by it, from all the languages that Spark supports. sparklyr\u2019s addition to the Spark ecosystem not only complements <a href=\"http://spark.apache.org/docs/latest/sparkr.html\">SparkR</a> but also extends Spark\u2019s reach to new users and communities.\n\nToday, we are happy to announce that sparklyr can be seamlessly used in Databricks clusters running Apache Spark 2.2 or higher with Scala 2.11. In this blog post, we show how you can install and configure sparklyr in Databricks. We also introduce some of the latest improvements in Databricks R Notebooks.\n\n<h2>Clean R Namespace</h2>\n\nWhen we <a href=\"https://databricks.com/blog/2015/07/13/introducing-r-notebooks-in-databricks.html\">released R notebooks</a> in 2015, we integrated SparkR into the notebook: the SparkR package was imported by default in the namespace, and both Spark and SQL Context objects were initialized and configured. Thousands of users have been running R and Spark code in R notebooks. We learned that some of them use our notebooks as a convenient way for single node R data analysis. For these users, the pre-loaded SparkR functions masked several functions from other popular packages, most notably <a href=\"https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html\">dplyr</a>.\n\nTo improve the experience of users who wish to use R notebooks for single node analysis and the new sparklyr users starting with Spark 2.2, we are not importing SparkR by default any more. Users who are interested in single-node R data science can launch single node clusters with large instances and comfortably run their existing single-node R analysis in a clean R namespace.\n\nFor users who wish to use SparkR, the <em>SparkSession</em> object is still initialized and ready to be used right after they import SparkR.\n\n<h2>sparklyr in Databricks</h2>\n\nWe collaborated with our friends at RStudio to enable sparklyr to seamlessly work in Databricks clusters. Starting with sparklyr version 0.5.5, there is a new connection method in sparklyr: <code>databricks</code>. When calling <code>spark_connect(method = \"databricks\")</code> in a Databricks R Notebook, sparklyr will connect to the spark cluster of that notebook. As this cluster is fully managed, you do not need to specify any other information such as version, SPARK_HOME, etc.\n\n<h2>Installing sparklyr</h2>\n\nYou can install sparklyr easily from CRAN:\n\n<pre><code class=\"r\">install.packages(\"sparklyr\")\n</code></pre>\n\n<h2>Configuring sparklyr connection</h2>\n\nConfiguring the sparklyr connection in Databricks cannot be simpler.\n\n<pre><code class=\"r\">library(sparklyr)\nsc &lt;- spark_connect(method = \"databricks\")\n</code></pre>\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/05/image1-1.png\" alt=\"\" width=\"600\" height=\"199\" class=\"aligncenter size-full wp-image-10980\" />\n\n<h2>Using sparklyr API</h2>\n\nAfter setting up the sparklyr connection, you can use all <a href=\"http://spark.rstudio.com/reference/index.html\">sparklyr APIs</a>. You can import and combine sparklyr with <a href=\"http://spark.rstudio.com/dplyr.html\">dplyr</a> or <a href=\"http://spark.rstudio.com/mllib.html\">MLlib</a>. You can also use sparklyr extensions. Note that if the extension packages include third-party JARs, you may need to install those JARs as <a href=\"https://docs.databricks.com/user-guide/libraries.html\">libraries</a> in your workspace.\n\n<pre><code class=\"r\">library(dplyr)\niris_tbl &lt;- copy_to(sc, iris)\niris_summary &lt;- iris_tbl %&gt;% \n    mutate(Sepal_Width = ROUND(Sepal_Width * 2) / 2) %&gt;% \n    group_by(Species, Sepal_Width) %&gt;% \n    summarize(count = n(),\nSepal_Length = mean(Sepal_Length),\nstdev = sd(Sepal_Length)) %&gt;% collect\n\nlibrary(ggplot2)\nggplot(iris_summary, \n   aes(Sepal_Width, Sepal_Length, color = Species)) + \n    geom_line(size = 1.2) +\n    geom_errorbar(aes(\n ymin = Sepal_Length - stdev,\nymax = Sepal_Length + stdev),\n   width = 0.05) +\n    geom_text(aes(label = count), \nvjust = -0.2, hjust = 1.2, color = \"black\") +\n    theme(legend.position=\"top\")\n</code></pre>\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/05/image2.png\" alt=\"\" width=\"960\" height=\"600\" class=\"aligncenter size-full wp-image-10982\" />\n\n<h2>Using SparkR and sparklyr Together</h2>\n\nWe find SparkR and sparklyr complementary. You can use the packages next to each other in a single notebook or job. To do so you can import SparkR along with sparklyr in Databricks notebooks. The SparkR connection is pre-configured in the notebook, and after importing the package, you can start using SparkR API. Also, remember that some of the functions in SparkR mask a number of functions in dplyr.\n\n<pre><code>library(SparkR)\nThe following objects are masked from \u2018package:dplyr\u2019:\n\n    arrange, between, coalesce, collect, contains, count, cume_dist,\n    dense_rank, desc, distinct, explain, filter, first, group_by,\n    intersect, lag, last, lead, mutate, n, n_distinct, ntile,\n    percent_rank, rename, row_number, sample_frac, select, sql,\n    summarize, union\n</code></pre>\n\nIf you import SparkR after you imported dplyr, you can reference the functions in dplyr by using the fully qualified names, for example, <code>dplyr::arrange()</code>. Similarly, if you import dplyr after SparkR the functions in SparkR are masked by dplyr.\n\nAlternatively, you can selectively detach one of the two packages if  you do not need it.\n\n<pre><code class=\"r\">detach(\"package:dplyr\")\n</code></pre>\n\n<h2>Conclusion</h2>\n\nWe are continuously improving Databricks R Notebooks to keep them as the best place to perform reproducible R data analysis, whether it is on distributed data with Apache Spark or single-node computation using packages from existing rich R ecosystem.\n\nAs we demonstrated with a few easy steps, you can now seamlessly use sparklyr on Databricks. You can try it out in our <a href=\"https://databricks.com/try\">Community Edition</a> with <a href=\"https://databricks.com/blog/2017/05/24/databricks-runtime-3-0-beta-delivers-enterprise-grade-apache-spark.html\">Databricks Runtime Beta 3.0</a> that includes the latest release candidate build of Apache Spark 2.2."}
{"status": "publish", "description": null, "creator": "jules_damji", "link": "https://databricks.com/blog/2017/05/26/bay-area-apache-spark-meetup-summary.html", "authors": null, "id": 10994, "categories": ["Company Blog", "Events"], "dates": {"publishedOn": "2017-05-26", "tz": "UTC", "createdOn": "2017-05-26"}, "title": "Bay Area Apache Spark Meetup Summary", "slug": "bay-area-apache-spark-meetup-summary", "content": "On May 16, we held our monthly <a href=\"https://www.meetup.com/spark-users/events/239434677/\">Bay Area Apache Spark Meetup</a> (BASM) at <a href=\"https://www.salesforceiq.com/\">SalesforceIQ</a> in Palo Alto.\n\nIn all, we had three Apache Spark related talks: two from Salesforce\u2019s Data Engineering and Machine Learning team, and one from Databricks\u2019 Apache Spark Structured Streaming team.\n\nFor those who missed the meetup, below are all the videos and links to the presentation slides. You can peruse slides and view the videos at your leisure. To those who helped and attended, thank you for participating and continued community support.\n\n<h2>Dependency Injection in Apache Spark Applications</h2>\n\n<iframe width=\"600\" height=\"300\" src=\"https://www.youtube.com/embed/EaZKX5M68mU\" frameborder=\"0\" allowfullscreen></iframe>\n\n<a href=\"https://www.slideshare.net/databricks/dependency-injection-in-apache-spark-applications\">View the slides here</a>\n\n<h2>Identifying Pricing Request Emails Using Apache Spark and Machine Learning</h2>\n\n<iframe width=\"600\" height=\"300\" src=\"https://www.youtube.com/embed/LfrMNGiR7FY\" frameborder=\"0\" allowfullscreen></iframe>\n\n<a href=\"https://www.slideshare.net/databricks/identifying-pricing-request-emails-using-apache-spark-and-machine-learning\">View the slides here</a>\n\n<h2>Arbitrary Stateful Aggregations in Structured Streaming in Apache Spark</h2>\n\n<iframe width=\"600\" height=\"300\" src=\"https://www.youtube.com/embed/JAb4FIheP28\" frameborder=\"0\" allowfullscreen></iframe>\n\n<a href=\"https://www.slideshare.net/databricks/arbitrary-stateful-aggregations-using-structured-streaming-in-apache-spark\">View the slides here</a>\n\n<h2>What's Next</h2>\n\nOur next BASM will be held on the eve of the <a href=\"https://databricks.com/blog/2017/03/30/tenth-spark-summit-with-a-terrific-agenda-for-all.html\">10th Spark Summit</a> on June 5. You don\u2019t have to be registered for the Spark Summit to attend the meetup, so <a href=\"https://www.meetup.com/spark-users/events/239467755/\">RSVP Now!</a>"}
{"status": "publish", "description": null, "creator": "Xiangrui", "link": "https://databricks.com/blog/2017/05/30/entropy-based-log-redaction-apache-spark-databricks.html", "authors": null, "id": 11005, "categories": ["Apache Spark", "Engineering Blog", "Platform"], "dates": {"publishedOn": "2017-05-30", "tz": "UTC", "createdOn": "2017-05-30"}, "title": "Entropy-based Log Redaction for Apache Spark on Databricks", "slug": "entropy-based-log-redaction-apache-spark-databricks", "content": "<em>This blog post is part of our series of internal engineering blogs on Databricks platform, infrastructure management, tooling, monitoring, and provisioning.</em>\n\nWe love logs at Databricks. And we want to provide users the best experience to access Apache Spark logs. On Databricks, Spark logs are available via built-in Spark UI for clusters. Databricks also supports <a href=\"https://docs.databricks.com/user-guide/clusters/log-delivery.html\">delivering cluster logs to customers\u2019 chosen destination</a>.\n\nWhile we care about the ease of accessing Spark logs, we pay more attention to data security issues that appear in logging. In this blog post, we discuss a specific data security topic on Spark logs: redacting credentials in logs.\n\n<h2>Credentials in Apache Spark Logs</h2>\n\nSpark can load data from various data sources, where users may need to provide credentials for accessing data. For instance, a Spark job may read a dataset from S3 using AWS access and secret access keys:\n\n<pre><code class=\"python\">spark.read.parquet(\"s3a://AKIAABCDEFGHIJKLMNOP:ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmn@test-bucket/test/path\")\n</code></pre>\n\nAnyone who obtained a copy of the keys would have the same access to the corresponding AWS resources. Unfortunately, the S3 URI that contains the credentials may appear in Spark logs in plain text form:\n\n<pre><code>17/01/01 00:00:00 INFO FileScanRDD: Reading File path: s3a://AKIAABCDEFGHIJKLMNOP:ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmn@test-bucket/test/path, range: ...\n</code></pre>\n\nThis could lead to serious data security issues if logs do not receive the same level of protection as credentials. And it is hard for users to control or watch what content are logged by Spark and third-party packages.\n\nTo enhance data security, Databricks redacts credentials from Spark logs at logging time, before log messages are written to the disk. For example, the above log message would appear in redacted form on Databricks:\n\n<pre><code>17/01/01 00:00:00 INFO FileScanRDD: Reading File path: s3a://REDACTED_AWS_ACCESS_KEY:REDACTED_AWS_SECRET_ACCESS_KEY@test-bucket/test/path, range: ...\n</code></pre>\n\nIn the following sections, we\u2019ll discuss how we attempt to properly identify credentials and redact them at logging time. Although we\u2019ll use AWS keys as examples for this blog post, Databricks also redacts other types of credentials.\n\n<h2>Identifying AWS Credentials</h2>\n\nBefore each message gets logged, Databricks scans the whole string to identify any possible credential and redacts them. Because redaction is done at logging time, we cannot afford an overly sophisticated redaction procedure that may hurt the performance. With complexity constraints in mind, log redaction at Databricks is developed based on regular expressions (regexes).\n\nAWS access keys consist of 20-character upper case alphanumeric strings starting with \u201cAKIA,\u201d which can be captured by the following regex:\n\n<pre><code>(?&lt;![A-Z0-9])AKIA([A-Z0-9]){16}(?![A-Z0-9])\n</code></pre>\n\nFor secret keys, there is no explicit pattern except that they are 40-character base64 strings. AWS provided a regex for access keys in the blog, <a href=\"https://aws.amazon.com/cn/blogs/security/a-safer-way-to-distribute-aws-credentials-to-ec2/\">A Safer Way to Distribute AWS Credentials to EC2</a>, which is a good start:\n\n<pre><code>(?&lt;![A-Za-z0-9/+=])[A-Za-z0-9/+=]{40}(?![A-Za-z0-9/+=])\n</code></pre>\n\nHowever, we found this regex insufficient to capture all secret keys. Some secret keys contains special characters such as the forward slash \u201c/\u201d, which needs to be URL-encoded (e.g., \u201c/\u201d encoded as \u201c%2F\u201d) to appear in a file path. Therefore, to identify secret keys in Spark logs, we also need to take into account the URL encoding on special characters. Moreover, both users and Spark may escape special characters in the URI, so in some cases special characters may be encoded twice. For instance, instead of \u201c%2F\u201d, \u201c/\u201d may be encoded and logged as \u201c%252F\u201d. We improved the regular expression accordingly as follows, which matches the secret keys not only in code but also in logs:\n\n<pre><code>(?&lt;![A-Za-z0-9/+])([A-Za-z0-9/+=]|%2F|%2B|%3D|%252F|%252B|%253D){40}(?![A-Za-z0-9/+=])\n</code></pre>\n\n<h2>False Positives and Entropy Thresholding</h2>\n\nOne issue of using the regular expression above is that it is not specific enough for secret keys, which could cause a number of false positives when applying to all Spark logs. For instance, it could also match hashcodes, paths or even class names that happen to be 40 characters long. To reduce the false positive rate without missing real secret keys, more checks are needed after regular expression matching.\n\nOne difference between secret keys and informative false positives is that the secret keys are randomly generated. One way to differentiate secret keys from those false positives is to measure how random they are. Following this idea, we use entropy to determine how likely a matched string is indeed a secret key that was randomly generated.\n\n<blockquote>\n  In information theory, entropy, or Shannon entropy, is the expected value of the information contained in each message (<a href=\"https://en.wikipedia.org/wiki/Entropy_(information_theory)\">Wiki page</a>). This concept can be applied to strings to measure how random they are. We expect randomly generated strings to have higher entropy than meaningful words with the same length.\n</blockquote>\n\nAfter calculating entropy, we set a threshold to decide whether to redact a given string. The threshold is based on the empirical distribution of AWS secret keys and those of known false positive types. One type of false positives is the SHA-1 hashcode, which is 40 characters long and randomly generated. However, the SHA-1 hashcode only consists of hexadecimal digits. Roughly speaking, with the same length, the more distinct characters a string have, the higher its entropy would be. Therefore, the entropy of SHA-1 hashcodes would usually be smaller than that of AWS secret keys, which are generated from base64 character set. By comparing the empirical distribution of entropy of two datasets consisting of AWS secret keys and SHA-1 hashcodes respectively (see figure below), it turns out that there\u2019s a clear gap between those two distributions and we can choose a threshold accordingly.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/05/image1-2.png\" alt=\"Chart showing comparison between distribution of entropy between AWS secret keys and SHA-1 hashcodes\" width=\"1020\" height=\"964\" class=\"aligncenter size-full wp-image-11016\"/>\n\n<h2>Logging-time Redaction with log4j</h2>\n\nLogs generated by Apache Spark jobs at Databricks are mostly handled by the logging service log4j. Databricks customizes <a href=\"https://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/Appender.html\">log4j appenders</a>, for example, console appender and rolling file appender to redact log strings before they arrive at the disk. Note that the redaction is applied to not only normal messages, but also stack traces when exceptions are thrown.\n\nOne interesting lesson learned is that since logging is so pervasive at Databricks, we need to minimize the dependency of customized appenders. Otherwise there could be a circle dependency issue.\n\n<h2>Using IAM Roles Instead of Keys</h2>\n\nIn this blog post, we shared our experience on redacting credentials at logging time. On Databricks this security feature is turned on automatically<sup id=\"fnref-11005-1\"><a href=\"#fn-11005-1\" class=\"jetpack-footnote\">1</a></sup>. Outside of Databricks, users can implement the methods mentioned to improve data security in Spark logs.\n\nHowever, rather than doing redaction at logging time, the best security practice is to simply not use credentials in the first place. Databricks encourages users to <a href=\"https://docs.databricks.com/user-guide/cloud-configurations/aws/iam-roles.html\">use IAM roles to access AWS resources</a>. Users can launch Databricks clusters with IAM roles that grant the workers the corresponding permissions. With IAM-enabled clusters, users no longer need to embed their AWS keys in notebooks to access data, and hence keys won\u2019t appear in logs.\n\nSimilarly, in Databricks File System (DBFS), users can mount their S3 bucket with IAM roles. Users can create clusters with corresponding IAM roles to access data directly without keys.\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n\n<li id=\"fn-11005-1\">\nsee <a href=\"https://docs.databricks.com/user-guide/security.html\">https://docs.databricks.com/user-guide/security.html</a> for details&#160;<a href=\"#fnref-11005-1\">&#8617;</a>\n</li>\n\n</ol>\n</div>"}
{"status": "publish", "description": null, "creator": "rxin", "link": "https://databricks.com/blog/2017/05/31/top-5-reasons-for-choosing-s3-over-hdfs.html", "authors": null, "id": 11027, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2017-05-31", "tz": "UTC", "createdOn": "2017-05-31"}, "title": "Top 5 Reasons for Choosing S3 over HDFS", "slug": "top-5-reasons-for-choosing-s3-over-hdfs", "content": "<em><a href=\"https://spark-summit.org/2017/\">Spark Summit</a> will be next week, June 5-7, at Moscone in San Francisco. Register today.</em>\n\n<hr />\n\nAt Databricks, our engineers guide thousands of organizations to define their big data and cloud strategies. When migrating big data workloads to the cloud, one of the most commonly asked questions is how to evaluate HDFS versus the storage systems provided by cloud providers, such as <a href=\"https://aws.amazon.com/s3/\">Amazon\u2019s S3</a>, <a href=\"https://azure.microsoft.com/en-us/services/storage/blobs/\">Microsoft\u2019s Azure Blob Storage</a>, and <a href=\"https://cloud.google.com/storage/\">Google\u2019s Cloud Storage</a>. In this blog post, we share our thoughts on why cloud storage is the optimal choice for data storage.\n\nIn this discussion, we use Amazon S3 as an example, but the conclusions generalize to other cloud platforms. We compare S3 and HDFS along the following dimensions:\n\n<ol>\n<li>Cost</li>\n<li>Elasticity</li>\n<li>SLA (availability and durability)</li>\n<li>Performance per dollar</li>\n<li>Transactional writes and data integrity</li>\n</ol>\n\n<h2>Cost</h2>\n\nLet\u2019s consider the total cost of storage, which is a combination of storage cost and human cost (to maintain them).\n\nFirst, let\u2019s estimate the cost of storing 1 terabyte of data per month.\n\nAs of May 2017, S3's standard storage price for the first 1TB of data is $23/month. Note that depending on your usage pattern, S3 listing and file transfer might cost money. On the other hand, cold data using infrequent-access storage would cost only half, at $12.5/month. For the purpose of this discussion, let's use $23/month to approximate the cost. S3 does not come with compute capacity but it does give you the freedom to leverage ephemeral clusters and to select instance types best suited for a workload (e.g., compute intensive), rather than simply for what is the best from a storage perspective.\n\nFor HDFS, the most cost-efficient storage instances on EC2 is the d2 family. To be generous and work out the best case for HDFS, we use the following assumptions that are virtually impossible to achieve in practice:\n\n<ul>\n<li>A crystal ball into the future to perfectly predict the storage requirements three years in advance, so we can use the maximum discount using 3-year reserved instances.</li>\n<li>Workloads are stable with a peak-to-trough ratio of 1.0. This means our storage system does not need to be elastic at all.</li>\n<li>Storage utilization is at 70%, and standard HDFS replication factor set at 3.</li>\n</ul>\n\nWith the above assumptions, using d2.8xl instance types ($5.52/hr with 71% discount, 48TB HDD), it costs <em>5.52 x 0.29 x 24 x 30 / 48 x 3 / 0.7 = $103/month</em> for 1TB of data. (Note that with reserved instances, it is possible to achieve lower price on the d2 family.)\n\nSo in terms of storage cost alone, <strong>S3 is 5X cheaper than HDFS.</strong>\n\nBased on our experience managing petabytes of data, S3's human cost is virtually zero, whereas it usually takes a team of Hadoop engineers or vendor support to maintain HDFS. Once we factor in human cost, <strong>S3 is 10X cheaper than HDFS</strong> clusters on EC2 with comparable capacity.\n\n<h2>Elasticity</h2>\n\nCapacity planning is tough to get right, and very few organizations can accurately estimate their resource requirements upfront. In the on-premise world, this leads to either massive pain in the post-hoc provisioning of more resources or huge waste due to low utilization from over-provisioning upfront.\n\nOne of the nicest benefits of S3, or cloud storage in general, is its elasticity and pay-as-you-go pricing model: you are only charged what you put in, and if you need to put more data in, just dump them there. Under the hood, the cloud provider automatically provisions resources on demand.\n\nSimply put, <strong>S3 is elastic, HDFS is not.</strong>\n\n<h2>SLA (Availability and Durability)</h2>\n\nBased on our experience, S3's availability has been fantastic. Only twice in the last six years have we experienced S3 downtime and we have never experienced data loss from S3.\n\nAmazon claims 99.999999999% durability and 99.99% availability. Note that this is higher than the vast majority of organizations\u2019 in-house services. The official SLA from Amazon can be found here: <a href=\"https://aws.amazon.com/s3/sla/\">Service Level Agreement - Amazon Simple Storage Service (S3)</a>.\n\nFor HDFS, in contrast, it is difficult to estimate availability and durability. One could theoretically compute the two SLA attributes based on EC2's mean time between failures (MTTF), plus upgrade and maintenance downtimes. In reality, those are difficult to quantify. Our understanding working with customers is that the majority of Hadoop clusters have availability lower than 99.9%, i.e. at least 9 hours of downtime per year.\n\nWith cross-AZ replication that automatically replicates across different data centers, <strong>S3\u2019s availability and durability is far superior to HDFS\u2019.</strong>\n\n<h2>Performance per Dollar</h2>\n\nThe main problem with S3 is that the consumers no longer have data locality and all reads need to transfer data across the network, and S3 performance tuning itself is a black box.\n\nWhen using HDFS and getting perfect data locality, it is possible to get ~3GB/node local read throughput on some of the instance types (e.g. i2.8xl, roughly 90MB/s per core). <a href=\"https://databricks.com/blog/2017/05/24/databricks-runtime-3-0-beta-delivers-enterprise-grade-apache-spark.html\">DBIO</a>, our cloud I/O optimization module, provides optimized connectors to S3 and can sustain ~600MB/s read throughput on i2.8xl (roughly 20MB/s per core).\n\nThat is to say, on a per node basis, HDFS can yield 6X higher read throughput than S3. Thus, <strong>given that the S3 is 10x cheaper than HDFS, we find that S3 is almost 2x better compared to HDFS on performance per dollar.</strong>\n\nHowever, a big benefit with S3 is we can separate storage from compute, and as a result, we can just launch a larger cluster for a smaller period of time to increase throughput, up to allowable physical limits. This separation of compute and storage also allow for different Spark applications (such as a data engineering ETL job and an ad-hoc data science model training cluster) to run on their own clusters, preventing concurrency issues that affect multi-user fixed-sized Hadoop clusters. This separation (and the flexible accommodation of disparate workloads) not only lowers cost but also improves the user experience.\n\nOne advantage HDFS has over S3 is metadata performance: it is relatively fast to list thousands of files against HDFS namenode but can take a long time for S3. However, the <a href=\"https://databricks.com/blog/2016/12/15/scalable-partition-handling-for-cloud-native-architecture-in-apache-spark-2-1.html\">scalable partition handling feature</a> we implemented in Apache Spark 2.1 mitigates this issue with metadata performance in S3.\n\nStay tuned for announcements in the near future that completely eliminates this issue with DBIO.\n\n<h2>Transactional Writes and Data Integrity</h2>\n\nMost of the big data systems (e.g., Spark, Hive) rely on HDFS\u2019 atomic rename feature to support atomic writes: that is, the output of a job is observed by the readers in an \u201call or nothing\u201d fashion. This is important for data integrity because when a job fails, no partial data should be written out to corrupt the dataset.\n\n<strong>S3\u2019s lack of atomic directory renames has been a critical problem for guaranteeing data integrity.</strong> This has led to complicated application logic to guarantee data integrity, e.g. never append to an existing partition of data.\n\nToday, we are happy to announce the support for transactional writes in our DBIO artifact, which features high-performance connectors to S3 (and in the future other cloud storage systems) with transactional write support for data integrity. <a href=\"https://databricks.com/blog/2017/05/31/transactional-writes-cloud-storage.html\">See this blog post for more information</a>.\n\n<h2>Other Operational Concerns</h2>\n\nSo far, we have discussed durability, performance, and cost considerations, but there are several other areas where systems like S3 have lower operational costs and greater ease-of-use than HDFS:\n\n<ul>\n<li><strong>Encryption, access control, and auditing:</strong> S3 supports <a href=\"https://aws.amazon.com/s3/details/#encryption\">multiple types of encryption</a>, with both AWS- and customer-managed keys, and has easy-to-configure audit logging and access control capabilities. These features make it easy to meet regulatory compliance needs, such as PCI or <a href=\"https://databricks.com/company/newsroom/press-releases/databricks-announces-hipaa-compliance-apache-spark-based-platform-achieves-aws-public-sector-partner-status\">HIPAA compliance</a>.</li>\n<li><strong>Backups and disaster recovery:</strong> S3\u2019s opt-in <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\">versioning</a> feature automatically maintains backups of modified or deleted files, making it easy to recover from accidental data deletion. <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/crr.html\">Cross-region replication</a> can be used to enhance S3\u2019s already strong availability guarantees in order to withstand the complete outage of an AWS region.</li>\n<li><strong>Data lifecycle management:</strong> S3 can be configured to automatically migrate objects to cold storage after a configurable time period. In many organizations, data is read frequently when it is new and is read significantly less often over time. S3\u2019s lifecycle management policies can automatically perform migration of old objects to Infrequent Access storage in order to save on cost, or to Glacier to achieve even larger cost savings; the latter is useful for organizations where regulatory compliance mandates long-term storage of data.</li>\n</ul>\n\nSupporting these additional requirements on HDFS requires even more work on the part of system administrators and further increases operational cost and complexity.\n\n<h2>Conclusion</h2>\n\nIn this blog post we used S3 as the example to compare cloud storage vs HDFS:\n\n<table>\n  <thead>\n  <tr>\n    <th></th>\n    <th>S3</th>\n    <th>HDFS</th>\n    <th>S3 vs HDFS</th>\n  </tr>\n  </thead>\n  <tbody>\n  <tr>\n    <th>Elasticity</th>\n    <td>Yes</td>\n    <td>No</td>\n    <td>S3 is more elastic</td>\n  </tr>\n  <tr>\n    <th>Cost/TB/month</th>\n    <td>$23</td>\n    <td>$206</td>\n    <td>10X</td>\n  </tr>\n  <tr>\n    <th>Availability</th>\n    <td>99.99%</td>\n    <td>99.9% (estimated)</td>\n    <td>10X</td>\n  </tr>\n  <tr>\n    <th>Durability</th>\n    <td>99.999999999%</td>\n    <td>99.9999% (estimated)</td>\n    <td>10X+</td>\n  </tr>\n  <tr>\n    <th>Transactional writes</th>\n    <td>Yes with DBIO</td>\n    <td>Yes</td>\n    <td>Comparable</td>\n  </tr>\n </tbody>\n</table>\n\nTo summarize, S3 and cloud storage provide elasticity, with an order of magnitude better availability and durability and 2X better performance, at 10X lower cost than traditional HDFS data storage clusters.\n\nHadoop and HDFS commoditized big data storage by making it cheap to store and distribute a large amount of data. However, in a cloud native architecture, the benefit of HDFS is minimal and not worth the operational complexity. That is why many organizations do not operate HDFS in the cloud, but instead use S3 as the storage backend.\n\nWith Databricks\u2019 DBIO, our customers can sit back and enjoy the merits of performant connectors to cloud storage without sacrificing data integrity."}
{"status": "publish", "description": null, "creator": "bill", "link": "https://databricks.com/blog/2017/05/31/transactional-writes-cloud-storage.html", "authors": null, "id": 11028, "categories": ["Engineering Blog", "Platform"], "dates": {"publishedOn": "2017-05-31", "tz": "UTC", "createdOn": "2017-05-31"}, "title": "Transactional Writes to Cloud Storage on Databricks", "slug": "transactional-writes-cloud-storage", "content": "<em><a href=\"https://spark-summit.org/2017\">Spark Summit</a> will be next week, June 5-7, at Moscone in San Francisco. Register today.</em>\n\n<hr />\n\nIn another <a href=\"https://databricks.com/blog/2017/05/31/top-5-reasons-for-choosing-s3-over-hdfs.html\">blog post published today</a>, we showed the top five reasons for choosing S3 over HDFS. With the dominance of simple and effective cloud storage systems such as Amazon S3, the assumptions of on-premise systems like Apache Hadoop are becoming, sometimes painfully, clear. Apache Spark users require both fast and transactionally correct writes to cloud storage systems. This blog post will introduce how Databricks allows customers to achieve both by comparing the performance and by correctness pitfalls of current Hadoop commit protocols with Databricks' I/O (DBIO) transactional commit protocol.\n\n<h2>Why Does Spark Need Transactional Writes?</h2>\n\nLarge-scale data processing frameworks like Apache Spark implement fault-tolerance by breaking up the work required to execute a job into retriable <em>tasks</em>. Since tasks may occasionally fail, Spark must ensure that only the outputs of successful tasks and jobs are made visible. Formally, this is achieved using a <em>commit protocol</em>, which specifies how results should be written at the end of a job.\n\nThe <em>job commit</em> phase of a Spark job ensures that only the output of successful jobs are visible to readers. In our experience, job commit is a large source of performance and correctness issues when Spark is used in a cloud-native setting, for instance, writing directly to storage services like S3.\n\nTo better understand why job commit is necessary, let\u2019s compare two different failure scenarios if Spark were to not use a commit protocol.\n\n<ol>\n<li>If a task fails, it could leave partially written data on S3 (or other storage). The Spark scheduler will re-attempt the task, which could result in duplicated output data.</li>\n<li>If an entire job fails, it could leave partial results from individual tasks on S3.</li>\n</ol>\n\nEither of these scenarios can be extremely detrimental to a business. To avoid these data corruption scenarios, Spark relies on commit protocol classes from Hadoop that first stage task output into temporary locations, only moving the data to its final location upon task or job completion. As we will show, these Hadoop protocols were not designed for the cloud-native setting and force the user to choose between performance and correctness.\n\n<h2>Comparing Existing Commit Protocols</h2>\n\nBefore introducing Databricks I/O (DBIO) transactional commit, let\u2019s first evaluate the existing Hadoop commit protocols. Commit protocols can be evaluated on two dimensions:\n\n<ul>\n<li><strong>Performance</strong> - how fast is the protocol at committing files? Naturally, you want your jobs to run as quickly as possible.</li>\n<li><strong>Transactionality</strong> - Can a job complete with only partial or corrupt results? Ideally, job output should be made visible <strong>transactionally</strong> (i.e., all or nothing). If the job fails, readers should not observe corrupt or partial outputs.</li>\n</ul>\n\n<h3>Performance Test</h3>\n\nSpark ships with two default Hadoop commit algorithms \u2014 version 1, which moves staged task output files to their final locations at the end of the job, and version 2, which moves files as individual job tasks complete. Let's compare their performance. We use a Spark 2.1 cluster on <a href=\"http://databricks.com/try\">Databricks Community Edition</a> for these <a href=\"https://docs.databricks.com/_static/notebooks/dbio-transactional-commit.html\">test runs</a>:\n\n<pre><code class=\"scala\">// Append 10m rows with the specified Hadoop FileOutputCommitter version\nspark.range(10e6.toLong)\n  .repartition(100).write.mode(\"append\")\n  .option(\"mapreduce.fileoutputcommitter.algorithm.version\", \"&lt;version&gt;\")\n  .parquet(\"/tmp/test-&lt;version&gt;\")\n\n// Compare the total job run time using v1 vs v2\ndisplay(Seq((\"Hadoop Commit V1\", &lt;v1Time&gt;), (\"Hadoop Commit V2\", &lt;v2Time&gt;)).toDF(\"algorithm\", \"time (s)\"))\n</code></pre>\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/05/hadoop-commit-1-vs-2-performance-test-results.png\" alt=\"Test results showing performance between Hadoop Commit V1 vs Hadoop Commit V2\" width=\"581\" height=\"189\" class=\"aligncenter size-full wp-image-11032\" />\n\nBecause it starts moving files in parallel as soon as tasks complete, the v2 Hadoop commit protocol is almost five times faster than v1. This is why in the <a href=\"https://issues.apache.org/jira/browse/MAPREDUCE-6336\">latest Hadoop release</a>, v2 is the default commit protocol.\n\n<h3>Transactionality Test</h3>\n\nNow let's look at transactionality. We evaluate this by simulating a job failure caused by a persistently failing task, which occurs commonly in practice, for example, this might occur if there are bad records in a particular file that cannot be parsed. This can be done as follows:\n\n<pre><code class=\"scala\">// Append more rows to the previous output directory\nspark.range(10000).repartition(7).map { i =&gt;\n  if (i == 9999) { Thread.sleep(5000); throw new RuntimeException(\"oops!\") }\n  else i\n}.write.option(\"mapreduce.fileoutputcommitter.algorithm.version\", \"&lt;version&gt;\")\n  .mode(\"append\").parquet(\"/tmp/test-&lt;version&gt;\")\n\n// Compare the number of newly added rows from the failed jobs\nval newRowsV1 = spark.read.parquet(\"/tmp/test-1\").count() - 10000000\nval newRowsV2 = spark.read.parquet(\"/tmp/test-2\").count() - 10000000\ndisplay(Seq((\"Hadoop Commit V1\", newRowsV1), (\"Hadoop Commit V2\", newRowsV2)).toDF(\"algorithm\", \"corrupted rows\"))\n</code></pre>\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/05/hadoop-commit-1-vs-2-transactionality-test-results.png\" alt=\"Test results showing transactionality between Hadoop Commit V1 vs Hadoop Commit V2\" width=\"580\" height=\"191\" class=\"aligncenter size-full wp-image-11033\" />\n\nWe see empirically that <strong>while v2 is faster, it also leaves behind partial results on job failures</strong>, breaking transactionality requirements. In practice, this means that with chained ETL jobs, a job failure \u2014 even if retried successfully \u2014 could duplicate some of the input data for downstream jobs. This requires careful management when using chained ETL jobs.\n\n<h2>No Compromises with DBIO Transactional Commit</h2>\n\n<blockquote>\n  <strong>Note:</strong> This feature is available starting in Databricks Spark 2.1-db4\n</blockquote>\n\nAll Hadoop users face this performance-reliability tradeoff for their jobs when writing to cloud storage, whether they are using Spark or not. Although v1 is more transactional, it\u2019s extremely slow because moving files in S3 is expensive. This tradeoff is not fundamental, however, and so at Databricks, we built a new transactional commit protocol for DBIO that coordinates with a highly available service in Databricks. The basic idea is as follows:\n\nWhen a user writes a file in a job, DBIO will perform the following actions for you.\n\n<ul>\n<li>Tag files written with the unique transaction id.</li>\n<li>Write files directly to their final location.</li>\n<li>Mark the transaction as committed when the jobs commits.</li>\n</ul>\n\nWhen a user goes to read the files, DBIO will perform the following actions for you.\n\n<ul>\n<li>Check to see if it has a transaction id as well as a status and either ignore files if the transaction has not completed or read in your data.</li>\n</ul>\n\nThis simple idea greatly improves performance <em>without</em> trading off reliability. We run the same performance test from above and compare with the default Hadoop commit algorithms:\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/05/dbio-transactional-commit-comparison-results.png\" alt=\"DBIO vs Hadoop performance test results\" width=\"585\" height=\"190\" class=\"aligncenter size-full wp-image-11035\" />\n\nIn this performance test, Spark running on Databricks will beat both Hadoop versions of commit protocols. In fact, this comparison holds true across all types of ETL workloads. We also perform a theoretical correctness analysis of each protocol: does the protocol guarantee correct output in the presence of different types of failures?\n\n<table>\n  <thead>\n    <tr>\n      <th></th>\n      <th>No commit protocol</th>\n      <th>Hadoop Commit V1</th>\n      <th>Hadoop Commit V2</th>\n      <th>DBIO Transactional Commit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Task failure</th>\n      <td><span class=\"sr-only\">No</span></td>\n      <td class=\"h-color-green\" style=\"text-align: center; vertical-align: middle;\">[db_icon name=\"checkmark\"]<span class=\"sr-only\">Yes</span></td>\n      <td class=\"h-color-green\" style=\"text-align: center; vertical-align: middle;\">[db_icon name=\"checkmark\"]<span class=\"sr-only\">Yes</span></td>\n      <td class=\"h-color-green\" style=\"text-align: center; vertical-align: middle;\">[db_icon name=\"checkmark\"]<span class=\"sr-only\">Yes</span></td>\n    </tr>\n    <tr>\n      <th>Job failure (e.g. persistent task failure)</th>\n      <td><span class=\"sr-only\">No</span></td>\n      <td class=\"h-color-green\" style=\"text-align: center; vertical-align: middle;\">[db_icon name=\"checkmark\"]<span class=\"sr-only\">Yes</span></td>\n      <td><span class=\"sr-only\">No</span></td>\n      <td class=\"h-color-green\" style=\"text-align: center; vertical-align: middle;\">[db_icon name=\"checkmark\"]<span class=\"sr-only\">Yes</span></td>\n    </tr>\n    <tr>\n      <th>Driver failure during commit</th>\n      <td><span class=\"sr-only\">No</span></td>\n      <td><span class=\"sr-only\">No</span></td>\n      <td><span class=\"sr-only\">No</span></td>\n      <td class=\"h-color-green\" style=\"text-align: center; vertical-align: middle;\">[db_icon name=\"checkmark\"]<span class=\"sr-only\">Yes</span></td>\n    </tr>\n  </tbody>\n</table>\n\nAs the table shows, Databricks\u2019 new transactional commit protocol provides strong guarantees in the face of different types of failures. Moreover, by enforcing correctness, this brings several additional benefits to Spark users.\n\n<strong>Safe task speculation</strong> - Task speculation allows for Spark to speculatively launch tasks when certain tasks are observed to be executing unusually slowly. With current Hadoop commit protocols, Spark task speculation is not safe to enable when writing to S3 due to the possibility of file collisions. With transactional commit, you can safely enable task speculation with <code>\"spark.speculation true\"</code> in the cluster Spark config. Speculation reduces the impact of straggler tasks on job completion, <a href=\"https://www.usenix.org/legacy/events/osdi08/tech/full_papers/zaharia/zaharia_html/\">greatly improving performance</a> in some cases.\n\n<strong>Atomic file overwrite</strong> - It is sometimes useful to atomically overwrite a set of existing files. Today, Spark implements overwrite by first deleting the dataset, then executing the job producing the new data. This interrupts all current readers and is not fault-tolerant. With transactional commit, it is possible to \"logically delete\" files atomically by marking them as deleted at commit time. Atomic overwrite can be toggled by setting <code>\"spark.databricks.io.directoryCommit.enableLogicalDelete true|false\"</code>. This improves user experience across those that are accessing the same datasets at the same time.\n\n<strong>Enhanced consistency</strong> - Our transactional commit protocol, in conjunction with other Databricks services, helps mitigate <a href=\"https://aws.amazon.com/s3/faqs/\">S3 eventual consistency</a> issues that may arise with chained ETL jobs.\n\n<h2>Enabling DBIO Transactional Commit in Databricks</h2>\n\nStarting with our Spark 2.1-db4 cluster image, the DBIO commit protocol (<a href=\"https://docs.databricks.com/spark/latest/spark-sql/dbio-commit.html\">documentation</a>) can be enabled with the following SQL configuration:\n\n<pre><code class=\"sql\">%sql set spark.sql.sources.commitProtocolClass=com.databricks.io.CommitProtocol\n</code></pre>\n\nThis can also be set at cluster creation by setting the same cluster configuration. We have also enabled DBIO transactional commit by default in <a href=\"https://databricks.com/blog/2017/05/24/databricks-runtime-3-0-beta-delivers-enterprise-grade-apache-spark.html\">Databricks Runtime 3.0 Beta</a> \u2014 bringing fast and correct ETL to all Databricks customers. You can read more about this feature in <a href=\"https://docs.databricks.com/spark/latest/spark-sql/dbio-commit.html\">our documentation</a>.\n\n<h2>Conclusion</h2>\n\nTo recap, we showed that existing Hadoop commit protocols force performance-integrity tradeoffs when used in the cloud-native setting. In contrast, DBIO transactional commit offers both the best performance and strong correctness guarantees.\n\n<a href=\"http://databricks.com/try\">Give it a try on Databricks.</a>"}
{"status": "publish", "description": null, "creator": "jules_damji", "link": "https://databricks.com/blog/2017/06/01/apache-spark-cluster-monitoring-with-databricks-and-datadog.html", "authors": null, "id": 11067, "categories": ["Company Blog", "Partners", "Product"], "dates": {"publishedOn": "2017-06-01", "tz": "UTC", "createdOn": "2017-06-01"}, "title": "Apache Spark Cluster Monitoring with Databricks and Datadog", "slug": "apache-spark-cluster-monitoring-with-databricks-and-datadog", "content": "<em>This blog post is a joint effort between Caryl Yuhas, Databricks\u2019 Solutions Architect, and Ilan Rabinovitch, Datadog\u2019s \u200eDirector of Technical Community and Evangelism.</em>\n\n<hr />\n\nMonitoring the health of any large Apache Spark cluster is an imperative necessity among engineers. They want visibility into clusters\u2019 internals; they want system metrics from individual nodes\u2019 CPU, disk usage, and Spark metrics across each active stage or cached partition.\n\n<a href=\"https://www.datadoghq.com/\">Datadog</a> as a SaaS-based monitoring and analytics platform affords engineers all the monitoring capabilities they desire. And in this blog, we\u2019ll show how easy it is to use Datadog to monitor the system and garner Spark metrics from Databricks' Spark clusters.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/05/image2-1.png\" alt=\"Datadog Dashboard\" width=\"1999\" height=\"500\" class=\"aligncenter size-full wp-image-11069\" />\n\nWe will walk through two ways to set up Datadog monitoring in Databricks:\n\n<ol>\n<li><strong>Automated Datadog Monitoring:</strong> [<a href=\"https://docs.databricks.com/user-guide/faq/datadog.html#datadog-init-script\">Import Notebook</a>] A one-click way to automate Datadog monitoring for all of your Databricks nodes and clusters. With just one command, you can configure Databricks to start a Datadog agent and stream both system and Spark metrics to your Datadog dashboard every time you launch a cluster.</p></li>\n<li><p><strong>Datadog Setup Walkthrough:</strong> [<a href=\"https://docs.databricks.com/user-guide/faq/datadog.html#initialize-data-dog-agent\">Import Notebook</a>] A step-by-step process for installing the Datadog agent on an existing Databricks cluster to start collecting Spark-specific metrics. You can follow along and import the notebook if you want to test this integration on one of your own clusters.</p></li>\n</ol>\n\n<h2>Automated Datadog Monitoring</h2>\n\n<p><img src=\"https://databricks.com/wp-content/uploads/2017/05/image1-3.png\" alt=\"Databricks cluster and Datadog\" width=\"1530\" height=\"666\" class=\"aligncenter size-full wp-image-11068\" />\n\nIf you\u2019re looking for a way to set up monitoring for your Databricks\u2019 clusters as quickly as possible, our <a href=\"https://docs.databricks.com/user-guide/faq/datadog.html#datadog-init-script\">Datadog Init Scripts</a> are a great option. When you import and run the first notebook above, it will create one init script that automatically installs a Datadog agent on every machine you spin up in Databricks, and one init script that configures each cluster to send Spark metrics. To use this script, all you need to do is:\n\n<ol>\n<li>Enter your own Datadog API key in the DD_API_KEY parameter (Line 4).</li>\n<li>Run the notebook once to save the init script as a global configuration.</li>\n</ol>\n\nThat\u2019s all there is to it! Once you run the notebook, each new cluster will begin streaming metrics directly to your dashboards in Datadog.\n\n<h2>What Exactly Is the <em>init</em> Script Doing?</h2>\n\nThe first script, <em>datadogInstall.sh</em>, simply creates an agent on every machine in your cluster according to the Datadog installation instructions (see Step 1 in the Datadog Setup Walkthrough section below).\n\nThe second script configures your cluster to report Spark-specific metrics back to Datadog. For more detail on this configuration, read the next section.\n\n<h2>Datadog Setup Walkthrough</h2>\n\nNow let\u2019s walk through the setup step by step. You can follow along in <a href=\"https://docs.databricks.com/user-guide/faq/datadog.html#initialize-data-dog-agent\">this notebook</a>. This can be used to set up a Datadog agent on an existing cluster.\n\n<ol>\n<li><strong>One-line Install</strong>. Datadog makes it very simple \u2014 a one line curl command, to be exact \u2014 to install an agent on your system. In this case, we\u2019ll use the Ubuntu package install:\n\n<pre><code class=\"sh\">%sh DD_API_KEY=[ENTER_YOUR_API_KEY_HERE] bash -c \"$(curl -L https://raw.githubusercontent.com/DataDog/dd-agent/master/packaging/datadog-agent/source/install_agent.sh)\"\n</code></pre>\n\nYou\u2019ll find the latest version of this command in your <a href=\"https://app.datadoghq.com/account/login?next=%2Faccount%2Fsettings#agent/ubuntu\">Datadog Setup Guide</a>.\n\n<blockquote>\n  <strong>Note:</strong> Make sure you check the Datadog site for the latest install command and use the API key specified in your account.\n</blockquote></li>\n<li><strong>Configure for Spark.</strong> In order to start monitoring the Spark metrics on a cluster, a Spark config file must be written into the driver of the cluster. This config file requires the hostname and port of the driver so that the Datadog agent can point to it and listen for the Spark metrics. The hostname and port can be found in the master-params file as follows:\n\n<pre><code class=\"sh\">%sh cat /tmp/master-params\n10.0.177.85 44752\n</code></pre>\n\nThese gets entered into the spark_url parameter of the config as shown below:\n\n<pre><code class=\"sh\">echo \"init_config:\nInstances:\n- spark_url: http://10.0.177.85:44752\n spark_cluster_mode: spark_standalone_mode\n cluster_name: 10.0.177.85\" &gt; /etc/dd-agent/conf.d/spark.yaml\n</code></pre></li>\n<li><strong>Restart Agent.</strong> Once this config file is updated, the agent simply needs to be restarted in order to complete the Spark integration.\n\n<pre><code class=\"sh\">/etc/init.d/datadog-agent restart\n</code></pre></li>\n<li>Check Connection. A successful connection can be confirmed by calling the Datadog info command and looking for the following Spark check:\n\n<pre><code class=\"sh\">%sh sudo /etc/init.d/datadog-agent info\nspark (5.12.1:1.0.0)\n --------------------\n   - instance #0 [OK]\n   - Collected 25 metrics, 0 events &amp; 2 service checks\n</code></pre></li>\n</ol>\n\n<h2>Conclusion</h2>\n\nWhile this post provides a detailed tutorial and setup script for Datadog, Databricks also provides users with the flexibility to connect to clusters and machines from other services. In particular, init scripts allow you to execute shell scripts on both the drivers and workers. In general, the init scripts from the post above can be adapted and customized to install and connect your monitoring agent of choice.\n\nTry the <a href=\"https://docs.databricks.com/user-guide/faq/datadog.html#datadog-init-script\">init notebook</a> and <a href=\"https://docs.databricks.com/user-guide/faq/datadog.html#initialize-data-dog-agent\">setup notebook</a> on <a href=\"https://www.datadoghq.com\">Datadog</a> and <a href=\"https://databricks.com/try\">Databricks</a> today."}
{"status": "publish", "description": null, "creator": "scott", "link": "https://databricks.com/blog/2017/06/01/almost-time-spark-summit-2017-san-francisco.html", "authors": null, "id": 11091, "categories": ["Company Blog", "Events"], "dates": {"publishedOn": "2017-06-01", "tz": "UTC", "createdOn": "2017-06-01"}, "title": "It's Almost Time for Spark Summit 2017 in\u00a0San\u00a0Francisco", "slug": "almost-time-spark-summit-2017-san-francisco", "content": "<img src=\"https://databricks.com/wp-content/uploads/2017/06/moscone-west-spark-summit-2017.jpg\" alt=\"Spark Summit 2017 will be held at Moscone West in San Francisco June 5-7, 2017\" width=\"715\" height=\"498\" class=\"aligncenter size-full wp-image-11109\" />\n\nGet ready! In less than two weeks, thousands of developers, data scientists, analysts, researchers and business executives from around the world will gather at the <a href=\"https://spark-summit.org/2017/venue\">Moscone West Convention Center in San Francisco</a>, June 5-7, for the 10th edition of Spark Summit.\n\n<blockquote>\n  Save 15% by using promo code <strong>DATABRICKS</strong>. <a href=\"http://spark-summit.org/2017-register\">Register today</a>.\n</blockquote>\n\nCo-chaired by Reynold Xin and Edd Wilder-James, <a href=\"https://spark-summit.org/2017\">Spark Summit 2017</a> features <a href=\"https://spark-summit.org/2017/schedule\">more than 175 sessions</a> dedicated to all things Apache Spark, with an emphasis on the latest developments in data science, deep learning, machine learning and real-time streaming applications. From deep dive technical tutorials and cutting-edge research projects to real-world case studies, it\u2019s a comprehensive look at how Spark is being used across a variety of industries and applications to solve tough, big data challenges at scale.\n\nDatabricks will be making a number of exciting product announcements on the main stage at Spark Summit (wish we could say more now, but you\u2019ll need to wait until\u00a0the big event). In addition, our team will be delivering several community talks to help you improve your use of Spark and you\u2019ll get to hear first hand from customers that are leveraging Databricks to accelerate business outcomes.\n\nIf you haven\u2019t already registered, <a href=\"http://spark-summit.org/2017-register\">do so now</a>! Tickets are selling quickly and there\u2019s no better place to meet with, and learn from, key members of the Spark ecosystem.\n\nHere are some of the must-see highlights:\n\n<h2>Keynotes and Demos</h2>\n\n<ul>\n<li><a href=\"https://spark-summit.org/2017/events/expanding-apache-spark-use-cases-in-22-and-beyond/\">Spark creator and Databricks Chief Technologist Matei Zaharia</a> will kick off Developer Day with a keynote about expanding Apache Spark use cases in 2.2 and beyond, that looks at Databricks\u2019 Structured Streaming API and new machine learning libraries, followed by a demo by Databricks software engineers Tim Hunter and Michael Armbrust.</li>\n<li><a href=\"https://spark-summit.org/2017/events/snorkel-dark-data-and-machine-learning/\">Stanford University associate professor Christopher Re</a> will describe the open source machine learning system Snorkel that aims to make processing Dark Data easier, and will provide a set of tutorials to help you write Snorkel applications that use Spark.</li>\n<li><a href=\"https://spark-summit.org/2017/events/unleashing-data-intelligence-with-intel-and-apache-spark/\">Intel VP and GM Michael Greene</a> will discuss BigDL, the recently released open source distributed deep learning framework, and plans for expanding the BigDL ecosystem.</li>\n<li>O\u2019Reilly Media chief data scientist Ben Lorica will host a <a href=\"https://spark-summit.org/2017/events/riselab-fireside-chat/\">fireside chat with Databricks executive chairman and UC Berkeley professor Ion Stoica</a> about Berkeley\u2019s new RISELab, the successor to AMPlab.</li>\n<li><a href=\"https://spark-summit.org/2017/events/combating-abusive-language-in-chat-with-apache-spark/\">Riot Games senior data scientist Wes Kerr</a> will share how the gaming company\u2019s Player Behavior Team uses Spark to better understand and combat abusive language.</li>\n<li><a href=\"https://spark-summit.org/2017/events/databricks-keynote/\">Databricks CEO Ali Ghodsi</a> will share some big news that will be demoed by Databricks engineer Greg Owen.</li>\n<li><a href=\"https://spark-summit.org/2017/events/hotelscoms-journey-to-becoming-an-algorithmic-business-exponential-growth-in-data-science-whilst-migrating-to-spark-cloud-all-at-the-same-time/\">Hotels.com VP and chief data science officer Matt Fryer</a> will talk about the online travel company\u2019s journey to becoming an algorithmic business using Spark.</li>\n<li><a href=\"https://spark-summit.org/2017/events/cutting-edge-predictive-analytics/\">Author and analytics guru Eric Siegel</a> will share insights on how to get predictive analytics right.</li>\n</ul>\n\n<h2>Community Talks</h2>\n\n<ul>\n<li><a href=\"https://spark-summit.org/2017/events/apache-spark-mllibs-past-trajectory-and-new-directions/\">Apache Spark MLlib\u2019s Past Trajectory and New Directions</a> (Joseph Bradley, Databricks)</li>\n<li><a href=\"https://spark-summit.org/2017/events/easy-scalable-fault-tolerant-stream-processing-with-structured-streaming-in-apache-spark/\">Easy, Scalable, Fault-Tolerant Stream Processing with Structured Streaming in Apache Spark</a> (Michael Armbrust and Tathagata Das, Databricks)</li>\n<li><a href=\"https://spark-summit.org/2017/events/embracing-a-taxonomy-of-types-to-simplify-machine-learning/\">Embracing a Taxonomy of Types to Simplify Machine Learning</a> (Leah McGuire, Salesforce)</li>\n<li><a href=\"https://spark-summit.org/2017/events/herding-cats-migrating-dozens-of-oddball-analytics-systems-to-apache-spark/\">Herding Cats: Migrating Dozens of Oddball Analytics Systems to Apache Spark</a> (Jon Cavanaugh, HP)</li>\n<li><a href=\"https://spark-summit.org/2017/events/leveraging-spark-in-ecommerce-platform-to-democratize-data/\">Leveraging Spark in an E-commerce Platform to Democratize Data</a> (Shafaq Abdullah, Honest Company)</li>\n<li><a href=\"https://spark-summit.org/2017/events/real-time-machine-learning-analytics-using-structured-streaming-and-kinesis-firehose/\">Real-Time Machine Learning Analytics Using Structured Streaming and Kinesis Firehose</a> (Caryl Yuhas and Myles Baker, Databricks)</li>\n<li><a href=\"https://spark-summit.org/2017/events/spark-graphx-and-blockchains-building-a-behavioral-analytics-platform-for-forensics-fraud-and-finance/\">Spark, GraphX and Blockchains: Building a Behavioral Analytics Platform for Forensics, Fraud and Finance</a> (Bryan Cheng &amp; Karen Hsu, BlockCypher)</li>\n<li><a href=\"https://spark-summit.org/2017/events/ssr-structured-streaming-on-r-for-machine-learning/\">SSR: Structured Streaming on R for Machine Learning</a> (Felix Cheung, Microsoft)</li>\n</ul>\n\n<h2>Connect with Databricks and the Spark Community</h2>\n\nThere will also be office hours with Spark committers, an expanded Expo Hall with dozens of exhibitors, and a ton of networking opportunities -- including an <a href=\"https://www.meetup.com/spark-users/events/239467755/\">Apache Spark Meetup</a> and the JOIN closing night party which will include food, drinks, games, music and more.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/06/JOIN-OG.jpg\" alt=\"Don&#039;t miss the Spark Summit 2017 JOIN closing night party!\" width=\"1200\" height=\"630\" class=\"aligncenter size-full wp-image-11108\" />\n\n<h2>Sign Up Today</h2>\n\n<a href=\"https://spark-summit.org/2017/schedule\">See the full schedule</a> and <a href=\"http://spark-summit.org/2017-register\">sign up now</a> to join us at Spark Summit 2017. Use promo code <strong>DATABRICKS</strong> to save 15% off."}
{"status": "publish", "description": null, "creator": "jules_damji", "link": "https://databricks.com/blog/2017/06/02/integrating-apache-spark-cucumber-behavioral-driven-development.html", "authors": null, "id": 11115, "categories": ["Company Blog", "Partners", "Product"], "dates": {"publishedOn": "2017-06-02", "tz": "UTC", "createdOn": "2017-06-02"}, "title": "Integrating Apache Spark with Cucumber for Behavioral-Driven Development", "slug": "integrating-apache-spark-cucumber-behavioral-driven-development", "content": "<em>This is a guest blog from <a href=\"https://www.fisglobal.com/\">FIS Global</a></em>\n\nOne of the most difficult scenarios in data processing is ensuring that the data is correct and expected. We can take a variety of testing methods to solve this, but often these methods limit the amount of collaboration in a team and don\u2019t directly answer the question, \u201cHow do I prove everything works?\u201d\n\nUsing Behavior-Driven Development (BDD) patterns to develop our transformations can enable the entire team to participate and focus on the results. The Cucumber project (<a href=\"https://cucumber.io/\">Cucumber.io</a>) provides an easy to use framework for implementing BDD methods into most projects.\n\nWhen combining <a href=\"https://spark.apache.org\">Apache Spark</a> for data processing and Cucumber (Cucumber.io), we make a compelling combination that maintains the scale of a system and can prove that the data is being handled correctly. This not only enables the development team but can bring in other means to successfully test scenarios that have been typically very hard to prove such as Machine Learning.\n\n<h2>Why Cucumber and Apache Spark?</h2>\n\nThe majority of software today is done collaboratively in an agile fashion. This means we get a healthy mix of professionals with varying perspectives trying to play the team sport of building software. One of the biggest problems projects suffer from is low-quality communication between engineers and the domain experts.\n\nCucumber allows us to write a portion of our software in a simple, language-based approach that enables all team members to easily read the unit tests. Our focus is on detailing the results we want the system to return. Non-Technical members of the team can easily create, read, and validate the testing of the system.\n\nOften Apache Spark is one component among many in processing data and this can encourage multiple testing frameworks. Cucumber can help us provides a consistent unit testing strategy when the project may extend past Apache Spark for data processing. Instead of mixing the different unit testing strategies between sub-projects, we create one readable agile acceptance framework. This is creating a form of \u2018Automated Acceptance Testing\u2019.\n\nBest of all, we are able to create \u2018living documentation\u2019 produced during development. Rather than a separate Documentation process, the Unit Tests form a readable document that can be made readable to external parties. Each time the code is updated, the Documentation is updated. It is a true win-win.\n\n<h2>Our Recipe for Success</h2>\n\nIn order to be successful, we need a recipe.\n\n<strong>Successful BDD Data Transformation Project</strong>\n1 Cup of Apache Spark\n1 Cup of Cucumber.io\n2 Cups of IntelliJ (Substitute with Eclipse if you find IntelliJ too salty)\n\u00bd Cup of Databricks.\n\nFirst, add to your Java-Based Project Maven File.(Hint: <a href=\"https://cucumber.io/docs/reference/jvm#java\">https://cucumber.io/docs/reference/jvm#java</a>)\n\n<pre><code class=\"XML\"><br />info.cukes\ncucumber-java8\n1.2.5\ntest\n\n</code></pre>\n\nWe also need to setup the Junit Runner to recognize Cucumber.\n\n<pre><code class=\"java\">package mypackage;\n\nimport cucumber.api.CucumberOptions;\nimport cucumber.api.junit.Cucumber;\nimport org.junit.runner.RunWith;\n\n@RunWith(Cucumber.class)\n@CucumberOptions(plugin = {\"pretty\", \"html:target/cucumber\"})\npublic class RunCukesTest {\n}\n</code></pre>\n\nFor added fun, dependency Injection can add some spice to your recipe. While Scala is available as a project to use for Spark, We have found that the Java project works even better for Cucumber. The JVM allows us to interact with each language\u2019s interface.\n\nWe need to create a <em>.feature</em> file, this is a file written in an executable language called Gherkin. This will have a simple format:\n\n<strong>Feature:</strong> Eating something good\n<strong>Scenario:</strong> Eat some ice cream\n<strong>Given an</strong> ice cream cone\n<strong>When</strong> I eat the ice cream cone\n<strong>Then</strong> I should be happy\n\nWe can notice that there are some keywords at work here: Given, When, Then, Feature, and Scenario. These keywords can be used in any way, the standardization around the few keywords is to enable readability. The feature file really tells the reader what is being done.\n\nYou will want to mix in step definitions next. The step definitions are the how or basically the Java code. A cool benefit is that Cucumber can give you the methods to put into the step definitions based on the feature file.\n\nThe final and most important step is to actually write a data transformation. For that, we need our killer ingredient. Apache Spark.\n\nWhat makes Apache Spark killer in our data processing is what it changes fundamentally. Up until now, data processing was a batch process for us. We would put the data in, Wait a while, and then have it processed. At the end of the process, which could be some time of waiting, we could validate the results. We have many types of \u2018unit testing\u2019 around this approach, but all were very weak and often ignored. Quality was assumed in the system and not guaranteed.\n\nWith <em>Apache Spark</em>, We are able to perform the data processing and verify it immediately. The fact that we have guaranteed speed pushes the boundary. We are able to wrap the data processing in uniform testing frameworks, guarantee speed, and know the results immediately. We can now prove and guarantee quality. When you extend this to the utility of Spark to do machine learning tasks, which has been traditionally very hard and timely to prove, the ability to move fast accelerates.\n\n<h2>Data Processing Scenario</h2>\n\nWe choose to use a rather simple, but perhaps common scenario. When data producers generate data, We often care about when the data was generated. For our purposes, We will assume that the time is always recorded in Epoch Time (Unix time) and the machine generating it is perfectly in tune with all known time synchronizations.\n\nOur consumers, users inside our walls, care about having time in a more readable fashion. For simplicity sake, we will make the assumption that they always want the time to occur in Pacific Daylight Time.\n\n<pre><code class=\"scala\">class ExtractionClass(sparkSession: SparkSession) {\nval TIMESTAMP_FORMAT = \"yyyy-MM-dd HH:mm:ss\"\n\ndef RunExtractJob(sourceFilePath: String, destinationFilePath: String): Unit = {\nval sourceDataFrame: DataFrame = GetJsonDataFrame(sourceFilePath)\nval extractedDataFrame: DataFrame = ExtractDataFrame(sourceDataFrame)\nSaveJsonDataFrame(extractedDataFrame, destinationFilePath)\n}\n\ndef GetJsonDataFrame(filePath: String): DataFrame = {\nsparkSession.read.json(filePath)\n}\n\ndef ExtractDataFrame(dataFrame: DataFrame): DataFrame = {\nimport sparkSession.implicits._\n\ndataFrame\n.withColumn(\"timestampGmt\", from_unixtime($\"unixTimestamp\"))\n.withColumn(\"timestampLtz\",\ndate_format(($\"unixTimestamp\" + ($\"timezoneOffset\" * 60 * 60)).cast(TimestampType), TIMESTAMP_FORMAT))\n}\n\ndef SaveJsonDataFrame(dataFrame: DataFrame, filePath: String): Unit = {\ndataFrame.write.json(filePath)\n}\n}\n</code></pre>\n\n<h2>When, Given, Then, AND</h2>\n\nGherkin is our language parser that provides a lightweight structure for documenting \u2018executable specifications\u2019. Its primary goal is readability. For this scenario, we will write the following unit test:\n\n<pre><code class=\"java\">@Extraction @TempFileCleanup @ApacheSpark\nFeature: Json Logs Extract Process\n\nBackground: general system setup\nGiven the system is in UTC time\n\nScenario: Basic extraction of Epoch time into readable local time zones\nGiven there is a file \"srcFolder/example.json\" with the following lines:\n| {\"logId\":1,\"unixTimestamp\":1459482142,\"timezoneOffset\":-6} |\n| {\"logId\":2,\"unixTimestamp\":1459482142,\"timezoneOffset\":-2} |\nWhen the method RunExtractJob gets called with\n| SourceFolder | srcFolder/* |\n| DestinationFolder | dstFolder |\nThen there will be a \"_SUCCESS\" file in the \"dstFolder\" folder\nAnd the folder \"dstFolder\" will have json files with exactly the following DataFrame rows:\n| logId | unixTimestamp | timezoneOffset | timestampGmt | timestampLtz |\n| 1 | 1459482142 | -6 | 2016-04-01 03:42:22 | 2016-03-31 21:42:22 |\n| 2 | 1459482142 | -2 | 2016-04-01 03:42:22 | 2016-04-01 01:42:22 |\n</code></pre>\n\n<h2>Stepping It Up</h2>\n\nThis is where the real magic occurs. Our Gherkin file is written and is clearly explainable to our team. So we need to execute it and find out how our plainly understandable file will execute a unit test. So if we run it, We will get back some output:\n\n<pre><code class=\"java\">@Given(\"^the system is in UTC time$\")\npublic void the_system_is_in_UTC_time() throws Throwable {\n// Write code here that turns the phrase above into concrete actions\nthrow new PendingException();\n}\n</code></pre>\n\nThe Cucumber framework has auto-generated the method we need to implement. Now we need to hook our feature file together with the actual calls that are running the unit tests. So let\u2019s fill it in.\n\n<pre><code class=\"java\">public class ExtractionStepDefinitions {\n@Given(\"^the system is in UTC time$\")\npublic void theSystemIsInGMTTime() throws Throwable {\nTimeZone.setDefault(TimeZone.getTimeZone(\"UTC\"));\n}\n\n@Given(\"^there is a file \\\"([^\\\"]*)\\\" with the following lines:$\")\npublic void thereIsAFileWithTheFollowingLines(String propertiesPath, List lines) throws Throwable {\nFile file = new File(Helpers.getTempPath(propertiesPath));\nfile.getParentFile().mkdirs();\nPrintWriter writer = new PrintWriter(file.getAbsolutePath(), \"UTF-8\");\nfor (String str : lines) {\nwriter.println(str.trim());\n}\nwriter.close();\n}\n\n@When(\"^the method (RunExtractJob|RunExtractJobV2) gets called with$\")\npublic void theMethodRunExtractJobGetsCalledWith(String jobName, Map&lt;String, String&gt; arguments) throws Throwable {\nif(jobName.compareTo(\"RunExtractJob\")==0) {\nnew ExtractionClass(Helpers.testSparkSession).RunExtractJob(\nHelpers.getTempPath(arguments.get(\"SourceFolder\")),\nHelpers.getTempPath(arguments.get(\"DestinationFolder\")));\n}\nelse if(jobName.compareTo(\"RunExtractJobV2\")==0) {\nnew ExtractionClassV2(Helpers.testSparkSession).RunExtractJob(\nHelpers.getTempPath(arguments.get(\"SourceFolder\")),\nHelpers.getTempPath(arguments.get(\"DestinationFolder\")),\narguments.get(\"TimezoneOffset\"));\n} else { throw new PendingException(); }\n\u2026\n</code></pre>\n\nThis seems like extra work over simply implementing our unit test in JUnit. But if we had implemented in JUnit alone, our tests would be fully readable only to the developers of the system. The hooks were truly not a lot of extra work, and Cucumber actually helped out by creating the stubs needed to complete the work.\n\nWhat we did do is create a means to document our system for the users of the system in an agile fashion. We can remove the extra work to document elsewhere because we already did the work. We could also do variations of the process and ask our expert quality folks to write our Gherkin file and then fill in the step hooks later.\n\n<h2>Using Cucumber with Databricks</h2>\n\nNow let\u2019s extend this scenario into Databricks. Databricks is an excellent platform for the Data Scientist through its easy-to-use notebook environment. The true value shines through in having a platform built by the team that created Apache Spark.\n\nOur Data Scientists can spend a lot of time prepping the data. They are applying the business rules of the company and cleaning the data for preparation. We begin losing the value of these statistical masters by having them mired inside the minutia. The data scientist should be focusing in on unlocking insights from the data, but often we have business-specific logic that represents how the data was formed. We want actionable insights, not encouraging a \u2018spreadmart\u2019 of varying observations based on the preparation technique used.\n\nNow bring forward our above scenario. We can bring data alignment to our scientist by codifying the preparation rules into a compilable jar. This logic can easily be wrapped around by Cucumber. The benefit of unit testing is fantastic, but we now have a new pair of eyes looking at our code. These eyes may not be able to read the arcane Java as their preferred language. Because we used a ubiquitous language, The data scientist can now look at the way the module was built and tested. They can build and validate our scenarios!\n\nThrough the Databricks notebooks, we can load and prepare the data through our compiled jars and switch to a different preferred language for the scientists. Since Databricks allows us to attach jars to a cluster, we can ensure that the business logic is well tested and understood and then extend it into Databricks. This allows an agile process to discover new data value, while ensuring complex business logic is well tested.\n\nSteps:\n\n<ol>\n<li>Create local jar using <code>mvn package</code> which has the great benefit of running the Cucumber tests to ensure data quality.</li>\n<li>Upload jar to Databricks as a library and attach the library to the cluster (<a href=\"https://databricks.com/blog/2015/07/28/using-3rd-party-libraries-in-databricks-apache-spark-packages-and-maven-libraries.html\">https://databricks.com/blog/2015/07/28/using-3rd-party-libraries-in-databricks-apache-spark-packages-and-maven-libraries.html</a>).</li>\n<li>Import class, access public methods you are interested in using, then create DataFrame view to access from any language.</li>\n</ol>\n\n<img class=\"aligncenter size-full wp-image-11122\" src=\"https://databricks.com/wp-content/uploads/2017/06/image2.png\" alt=\"\" width=\"1999\" height=\"856\" />\n\n<ol>\n<li>Do your data science thing in the language of your choice knowing that the data sources are prepped tested.</li>\n</ol>\n\n<img class=\"aligncenter size-full wp-image-11121\" src=\"https://databricks.com/wp-content/uploads/2017/06/image1.png\" alt=\"\" width=\"1999\" height=\"587\" />\n\n<h2>Lessons Learned</h2>\n\nDatabricks is a powerful unifying platform for data processing that fosters a collaborative environment during the development process. We showed how we can not only unify the expensive data preparation work of the Data Scientist but also enable the preparation work to be easily validated.\n\nWe are also able to unify multiple data processing components that support Apache Spark under one readable unit testing framework and produce \u2018Living Documentation\u2019 about how the system works. Everyone from the developer to the tester all the way up to the executive stakeholders can now read and collaborate on the system tests and understand its behavior.\n\nThe true value comes when documentation is created at development time and not done as a separate process. Your project becomes truly oriented around the results in a fully-agile fashion.\n\nAre you interested in hearing more about Spark and BDD testing? <a href=\"https://spark-summit.org/2017/events/behavior-driven-development-bdd-testing-with-apache-spark/\">Come to our session at the Spark Summit 2017!</a>"}
{"status": "publish", "description": null, "creator": "michael", "link": "https://databricks.com/blog/2017/06/06/simple-super-fast-streaming-engine-apache-spark.html", "authors": null, "id": 11142, "categories": ["Apache Spark", "Engineering Blog", "Streaming"], "dates": {"publishedOn": "2017-06-06", "tz": "UTC", "createdOn": "2017-06-06"}, "title": "Making Apache Spark the Fastest Open Source Streaming Engine", "slug": "simple-super-fast-streaming-engine-apache-spark", "content": "We started building <a href=\"https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html\">Structured Streaming</a> in Apache Spark one year ago as a new, simpler way to develop <a href=\"https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html\">continuous applications</a>. Not only does this new way make it easy to build end-to-end streaming applications by exposing a single API to write streaming queries as you would write batch queries, but it also handles streaming complexities by ensuring exactly-once-semantics, doing incremental stateful aggregations, and providing data consistency across sources and sinks.\n\n<h2>Best-in-Class Performance</h2>\n\n<a href=\"https://www.youtube.com/watch?v=xwQwKW-cerE\">As we showed this morning at Spark Summit 2017</a>, Structured Streaming is not only the simplest-to-use streaming engine, but for many workloads is also the fastest!\n\n<iframe width=\"600\" height=\"300\" src=\"https://www.youtube.com/embed/qAZ5XUz32yM\" frameborder=\"0\" allowfullscreen></iframe>\n\nBy leveraging all of the work done on the <a href=\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\">Catalyst query optimizer</a> and the <a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\">Tungsten execution engine</a>, Structured Streaming brings the efficiency of Spark SQL to real-time streaming.  In our benchmarks, we showed <strong>5x</strong> or better throughput than other popular streaming engines on the widely used Yahoo! Streaming Benchmark.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/06/spark-yahoo-streaming-benchmark.png\" alt=\"Apache Spark achieves 5x higher throughput vs competition on the Yahoo Streaming Benchmark\" width=\"329\" height=\"345\" class=\"aligncenter size-full wp-image-11200\" />\n\nThe above shows a comparison when running <a href=\"https://data-artisans.com/blog/extending-the-yahoo-streaming-benchmark\">a modified version of the benchmark</a> that generates the data in the framework. We ran on a similar setup, using 10 r3.xlarge machines (40 cores) running Spark 2.2.0-RC3. To let you reproduce these results, we will shortly release a blog with full source code runnable on Databricks. <em>Note that for Kafka Streams, the data is still read from persistent storage as this is the only mode that is supported.</em>\n\n<h2>Best-in-Class Latency</h2>\n\nOf course, throughput is only one metric for evaluating a streaming engine.  Latency is also important for time-sensitive applications.  Up until now, the minimum possible latency has been bounded by the microbatch-based architecture of Spark Streaming.\n\nHowever, from the beginning, we carefully designed the API of Structured Streaming to be agnostic to the underlying execution engine, eliminating the concept of batching in the API. At Databricks, we have also been working to remove batching in the engine. Today, we are excited to propose a new extension, <strong><a href=\"https://issues.apache.org/jira/browse/SPARK-20928\">continuous processing</a></strong>, that also eliminates micro-batches from execution. As we demonstrated at Spark Summit this morning, this new execution mode lets users achieve <strong>sub-millisecond end-to-end latency</strong> for many important workloads -- with no change to their Spark application.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/06/db-runtime-latency.png\" alt=\"The Databricks Runtime execution engine achieves extremely low latency with continuous processing\" width=\"487\" height=\"316\" class=\"aligncenter size-full wp-image-11199\" />\n\nWe have already built a working first version of continuous processing, and look forward to working with the community to contribute this extension to Apache Spark.\n\n<h2>Efficient Streaming in the Cloud</h2>\n\nDatabricks customers can access the latest and greatest streaming features through the <a href=\"https://databricks.com/blog/2017/05/24/databricks-runtime-3-0-beta-delivers-enterprise-grade-apache-spark.html\">Databricks Runtime 3.0 beta</a>, which includes the following new features from Apache Spark:\n\n<ul>\n<li>Support for arbitrary complex <a href=\"https://youtu.be/JAb4FIheP28\">stateful processing using [flat]MapGroupsWithState</a>, allowing developers to write customized stateful aggregations such as sessionization or joining two streams.</li>\n<li>Support for <a href=\"https://databricks.com/blog/2017/04/26/processing-data-in-apache-kafka-with-structured-streaming-in-apache-spark-2-2.html\">reading and writing data in streaming or batch to/from Apache Kafka</a>, giving developers ability to publish transformed streams to subsequent stages in a complex data pipeline upstream or update dashboards in real time.</li>\n<li>Support for <a href=\"https://databricks.com/blog/2017/05/18/taking-apache-sparks-structured-structured-streaming-to-production.html\">production monitoring and alert management</a>, providing engineers ways to survey metrics, inspect query progress, and write advanced monitoring applications with third-party alerting platforms.</li>\n</ul>\n\nIn addition to the upstream improvements, Databricks Runtime 3.0 has optimized Structured Streaming specifically for the cloud deployments, including the following enhancements for running cloud workloads:\n\n<ul>\n<li>Drastically reduce costs by combining the <a href=\"https://databricks.com/blog/2017/05/22/running-streaming-jobs-day-10x-cost-savings.html\">Once Trigger mode with the Databricks Job Scheduler</a>.</li>\n<li>Easily monitor production streaming jobs with <a href=\"https://databricks.com/blog/2017/05/18/taking-apache-sparks-structured-structured-streaming-to-production.html\">integrated throughput and latency metrics</a>.</li>\n<li>Additionally support another source of streaming data from <a href=\"https://docs.databricks.com/spark/latest/structured-streaming/kinesis.html\">Amazon Kinesis</a>.</li>\n</ul>\n\n<h2>Ready for Production</h2>\n\nFinally, we are excited to announce that we at Databricks now consider Structured Streaming to be production ready and it is fully supported.  At Databricks, our customers have already been using Structured Streaming and <strong>in the last month alone processed over 3 trillion records</strong>.\n\n<h2>Read More</h2>\n\nTo explain how we and our customers employ Structured Streaming at scale, we have penned a half dozen blogs that cover many of the key aspects of Structured Streaming:\n\n<ul>\n<li><a href=\"https://databricks.com/blog/2017/01/19/real-time-streaming-etl-structured-streaming-apache-spark-2-1.html\">Real-time Streaming ETL with Structured Streaming in Apache Spark 2.1</a></li>\n<li><a href=\"https://databricks.com/blog/2017/02/23/working-complex-data-formats-structured-streaming-apache-spark-2-1.html\">Working with Complex Data Formats with Structured Streaming in Apache Spark 2.1</a></li>\n<li><a href=\"https://databricks.com/blog/2017/04/26/processing-data-in-apache-kafka-with-structured-streaming-in-apache-spark-2-2.html\">Processing Data in Apache Kafka with Structured Streaming in Apache Spark 2.2</a></li>\n<li><a href=\"https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html\">Event-time Aggregation and Watermarking in Apache Spark\u2019s Structured Streaming</a></li>\n<li><a href=\"https://databricks.com/blog/2017/05/18/taking-apache-sparks-structured-structured-streaming-to-production.html\">Taking Apache Spark\u2019s Structured Structured Streaming to Production</a></li>\n<li><a href=\"https://databricks.com/blog/2017/05/22/running-streaming-jobs-day-10x-cost-savings.html\">Once Trigger mode with the Databricks Job Scheduler</a></li>\n</ul>"}
{"status": "publish", "description": null, "creator": "rxin", "link": "https://databricks.com/blog/2017/06/06/databricks-vision-simplify-large-scale-deep-learning.html", "authors": null, "id": 11148, "categories": ["Apache Spark", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2017-06-06", "tz": "UTC", "createdOn": "2017-06-06"}, "title": "A Vision for Making Deep Learning Simple", "slug": "databricks-vision-simplify-large-scale-deep-learning", "content": "[dbce_cta href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/5669198905533692/3647723071348946/3983381308530741/latest.html\"]Try this notebook on Databricks[/dbce_cta]\n\nWhen MapReduce was introduced 15 years ago, it showed the world a glimpse into the future. For the first time, engineers at Silicon Valley tech companies could analyze the entire Internet. MapReduce, however, provided low-level APIs that were incredibly difficult to use, and as a result, this \"superpower\" was a luxury \u2014 only a small fraction of highly sophisticated engineers with lots of resources could afford to use it.\n\nToday, deep learning has reached its \u201cMapReduce\u201d point: it has demonstrated is potential; it is the \u201csuperpower\u201d of Artificial Intelligence. Its accomplishments were unthinkable a few years ago: self-driving cars and AlphaGo would have been considered miracles.\n\nYet leveraging the superpower of deep learning today is as challenging as big data was yesterday: deep learning frameworks have steep learning curves because of low-level APIs; scaling out over distributed hardware requires significant manual work; and even with the combination of time and resources, achieving success requires tedious fiddling and experimenting with parameters. Deep learning is often referred to as \u201cblack magic.\u201d\n\nSeven years ago, a group of us started the Spark project with the singular goal to \u201cdemocratize\u201d the \u201csuperpower\u201d of big data, by offering high-level APIs and a unified engine to do machine learning, ETL, streaming and interactive SQL. Today, Apache Spark makes big data accessible to everyone from software engineers to SQL analysts.\n\nContinuing with that vision of democratization, we are excited to announce <strong><a href=\"https://github.com/databricks/spark-deep-learning\">Deep Learning Pipelines</a></strong>, a new open-source library aimed at enabling everyone to easily integrate scalable deep learning into their workflows, from machine learning practitioners to business analysts.\n\n<iframe width=\"600\" height=\"300\" src=\"https://www.youtube.com/embed/qAZ5XUz32yM\" frameborder=\"0\" allowfullscreen></iframe>\n\nDeep Learning Pipelines builds on Apache Spark\u2019s <a href=\"https://spark.apache.org/docs/latest/ml-pipeline.html\">ML Pipelines</a> for training, and with Spark DataFrames and SQL for deploying models. It includes high-level APIs for common aspects of deep learning so they can be done efficiently in a few lines of code:\n\n<ul>\n<li>Image loading</li>\n<li>Applying pre-trained models as transformers in a Spark ML pipeline</li>\n<li>Transfer learning</li>\n<li>Distributed hyperparameter tuning</li>\n<li>Deploying models in DataFrames and SQL</li>\n</ul>\n\nIn the rest of the post, we describe each of these features in detail with examples. To try out these and further examples on Databricks, check out the notebook <a href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/5669198905533692/3647723071348946/3983381308530741/latest.html\">Deep Learning Pipelines on Databricks</a>.\n\n<h2>Image Loading</h2>\n\nThe first step to applying deep learning on images is the ability to load the images. Deep Learning Pipelines includes utility functions that can load millions of images into a DataFrame and decode them automatically in a distributed fashion, allowing manipulation at scale.\n\n<pre><code class=\"python\">df = imageIO.readImages(\"/data/myimages\")\n</code></pre>\n\nWe are also working on adding support for more data types, such as text and time series.\n\n<h2>Applying Pre-trained Models for Scalable Prediction</h2>\n\nDeep Learning Pipelines supports running pre-trained models in a distributed manner with Spark, available in both batch and <a href=\"https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html\">streaming data processing</a>. It houses some of the most popular models, enabling users to start using deep learning without the costly step of training a model. For example, the following code creates a Spark prediction pipeline using InceptionV3, a state-of-the-art convolutional neural network (CNN) model for image classification, and predicts what objects are in the images that we just loaded. This prediction, of course, is done in parallel with all the benefits that come with Spark:\n\n<pre><code class=\"python\">from sparkdl import readImages, DeepImagePredictor\npredictor = DeepImagePredictor(inputCol=\"image\", outputCol=\"predicted_labels\", modelName=\"InceptionV3\")\npredictions_df = predictor.transform(df)\n</code></pre>\n\nIn addition to using the built-in models, users can plug in <a href=\"https://keras.io/models/about-keras-models/\">Keras models</a> and TensorFlow Graphs in a Spark prediction pipeline. This turns any single-node models on single-node tools into one that can be applied in a distributed fashion, on a large amount of data.\n\nOn Databricks\u2019 <a href=\"https://databricks.com/product/databricks\">Unified Analytics Platform</a>, if you choose a GPU-based cluster, the computation intensive parts will automatically run on GPUs for best efficiency.\n\n<h2>Transfer Learning</h2>\n\nPre-trained models are extremely useful when they are suitable for the task at hand, but they are often not optimized for the specific dataset users are tackling. As an example, InceptionV3 is a model optimized for image classification on a broad set of 1000 categories, but our domain might be dog breed classification. A commonly used technique in deep learning is transfer learning, which adapts a model trained for a similar task to the task at hand. Compared with training a new model from ground-up, transfer learning requires substantially less data and resources. This is why transfer learning has become the go-to method in many real world use cases, such as <a href=\"https://www.nature.com/nature/journal/v542/n7639/full/nature21056.html\">cancer detection</a>.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/06/image1-2.png\" alt=\"Deep Learning Pipeline\" width=\"1999\" height=\"964\" class=\"aligncenter size-full wp-image-11189\" />\n\nDeep Learning Pipelines enables fast transfer learning with the concept of a <em>Featurizer</em>. The following example combines the InceptionV3 model and logistic regression in Spark to adapt InceptionV3 to our specific domain. The DeepImageFeaturizer automatically peels off the last layer of a pre-trained neural network and uses the output from all the previous layers as features for the logistic regression algorithm. Since logistic regression is a simple and fast algorithm, this transfer learning training can converge quickly using far fewer images than are typically required to train a deep learning model from ground-up.\n\n<pre><code class=\"python\">from sparkdl import DeepImageFeaturizer \nfrom pyspark.ml.classification import LogisticRegression\n\nfeaturizer = DeepImageFeaturizer(modelName=\"InceptionV3\")\nlr = LogisticRegression()\np = Pipeline(stages=[featurizer, lr])\n\n# train_images_df = ... # load a dataset of images and labels\nmodel = p.fit(train_images_df)\n</code></pre>\n\n<h2>Distributed Hyperparameter Tuning</h2>\n\nGetting the best results in deep learning requires experimenting with different values for training parameters, an important step called hyperparameter tuning. Since Deep Learning Pipelines enables exposing deep learning training as a step in Spark\u2019s machine learning pipelines, users can rely on the hyperparameter tuning infrastructure already built into Spark.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/06/image2-1.png\" alt=\"\" width=\"1999\" height=\"760\" class=\"aligncenter size-full wp-image-11190\"/>\n\nThe following code plugs in a Keras Estimator and performs hyperparameter tuning using grid search with cross validation:\n\n<pre><code class=\"python\">myEstimator = KerasImageFileEstimator(inputCol='input',\n                                      outputCol='output',\n                                      modelFile='/my_models/model.h5',\n                                      imageLoader=_loadProcessKeras)\n\nkerasParams1 = {'batch_size':10, epochs:10}\nkerasParams2 = {'batch_size':5, epochs:20}\n\nmyParamMaps =\n  ParamGridBuilder() \\\n    .addGrid(myEstimator.kerasParams, [kerasParams1, kerasParams2]) \\\n    .build()\n\ncv = CrossValidator(myEstimator, myEvaluator, myParamMaps)\ncvModel = cv.fit()\nkerasTransformer = cvModel.bestModel  # of type KerasTransformer\n</code></pre>\n\n<h2>Deploying Models in SQL</h2>\n\nOnce a data scientist builds the desired model, Deep Learning Pipelines makes it simple to expose it as a function in SQL, so anyone in their organization can use it \u2013 data engineers, data scientists, business analysts, anybody.\n\n<pre><code class=\"python\">sparkdl.registerKerasUDF(\"img_classify\", \"/mymodels/dogmodel.h5\")\n</code></pre>\n\nNext, any user in the organization can apply prediction in SQL:\n\n<pre><code class=\"sql\">SELECT image, img_classify(image) label FROM images \nWHERE contains(label, \u201cChihuahua\u201d)\n</code></pre>\n\nSimilar functionality is also available in the DataFrame programmatic API across all supported languages (Python, Scala, Java, R). Similar to scalable prediction, this feature works in both batch and <a href=\"https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html\">structured streaming</a>.\n\n<h2>Conclusion</h2>\n\nIn this blog post, we introduced <a href=\"https://github.com/databricks/spark-deep-learning\">Deep Learning Pipelines</a>, a new library that makes deep learning drastically easier to use and scale. While this is just the beginning, we believe Deep Learning Pipelines has the potential to accomplish what Spark did to big data: make the deep learning \"superpower\" approachable for everybody.\n\nFuture posts in the series will cover the various tools in the library in more detail: image manipulation at scale, transfer learning, prediction at scale, and making deep learning available in SQL.\n\nTo learn more about the library, check out the <a href=\"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/5669198905533692/3647723071348946/3983381308530741/latest.html\">Databricks notebook</a> as well as the <a href=\"https://github.com/databricks/spark-deep-learning\">github repository</a>. We encourage you to give us feedback. Or even better, be a contributor and help bring the power of scalable deep learning to everyone."}
{"status": "publish", "description": null, "creator": "bill", "link": "https://databricks.com/blog/2017/06/05/sharing-knowledge-community-preview-apache-spark-definitive-guide.html", "authors": null, "id": 11154, "categories": ["Announcements", "Company Blog"], "dates": {"publishedOn": "2017-06-05", "tz": "UTC", "createdOn": "2017-06-05"}, "title": "Sharing Knowledge with the Community in a Preview of Apache Spark: The Definitive Guide", "slug": "sharing-knowledge-community-preview-apache-spark-definitive-guide", "content": "<a href=\"https://spark.apache.org\">Apache Spark</a> has seen immense growth over the past several years. The size and scale of this Spark Summit is a true reflection of innovation after innovation that has made itself into the Apache Spark project. Hundreds of contributors working collectively have made Spark an amazing piece of the technology powering thousands of organizations, and Databricks has initiated many key efforts in Spark including <a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\">Project Tungsten</a>, <a href=\"https://spark-summit.org/east-2017/events/parallelizing-existing-r-packages-with-sparkr/\">SparkR</a>, <a href=\"http://spark.apache.org/sql/\">Spark SQL</a> and <a href=\"https://spark.apache.org/docs/latest/sql-programming-guide.html\">DataFrame APIs</a>, and <a href=\"https://www.youtube.com/watch?v=IJmFTXvUZgY&amp;feature=youtu.be&amp;list=PLTPXxbhUt-YVEyOqTmZ_X_tpzOlJLiU2k\">Structured Streaming</a> and we continue to contribute heavily to the project both with code and fostering the community.\n\nWhile the blistering pace of innovation moves the project forward, it makes keeping up to date with all these improvements challenging. To solve this problem, we are happy to introduce <a href=\"http://shop.oreilly.com/product/0636920034957.do\">Spark: The Definitive Guide</a>. In partnership with O\u2019Reilly Media, we will publish this new comprehensive book on Spark later this year. <a href=\"http://go.databricks.com/definitive-guide-apache-spark\"><strong>To celebrate the largest Spark Summit ever, we are releasing several chapters for free to the community</strong></a>. Additionally, if you use discount code <strong>AUTHD</strong> on the <a href=\"http://shop.oreilly.com/product/0636920034957.do\">O'Reilly site</a>, you can get 50% off the ebook and 40% off of the print edition!\n\n[caption id=\"attachment_11157\" align=\"aligncenter\" width=\"500\"]<img src=\"https://databricks.com/wp-content/uploads/2017/06/image1-1.png\" alt=\"Early Release of Spark: The Definitive Guide\" width=\"500\" height=\"656\" class=\"size-full wp-image-11157\" /> source: O'Reilly[/caption]\n\nWe have strived to write an informative book on Spark, focusing on condensing the community's development knowledge of Apache Spark, for you.\n\n<h2>The Parts of the Book</h2>\n\nThe first few chapters are the \u201cGentle Introduction to Spark\u201d; the intended audience is anyone from a SQL Analyst to a Data Engineer. This section covers the simple concepts that everyone should understand about Apache Spark as well as provides a tour of different aspects of Spark\u2019s ecosystem.\n\nThe second part of the book dives into Spark\u2019s Structured APIs, powered by the Catalyst engine. You\u2019ll see everything from data sources, transformations, DataFrame and Dataset transformations and everything in between. This includes examples in SQL, Python, and Scala for people to follow.\n\nTo show the foundation of what DataFrames are actually built on, the third part of the book discusses Spark\u2019s low-level APIs including RDDs for those that need some advanced functionality or need to work with legacy code built on RDDs.\n\nThe fourth part of the book is a deep dive into how Spark actually runs on a cluster and discusses some options for optimizations, monitoring, tuning and debugging.\n\nFinally, the fifth and sixth parts are deep dives into Structured Streaming and Machine Learning respectively. We discuss what makes Structured Streaming such a powerful paradigm and the number of tools and algorithms that Spark makes available to end users through MLlib, Spark\u2019s Machine Learning Library. We even include sections on Graph Analysis with <a href=\"http://graphframes.github.io/\">GraphFrames</a> and Deep Learning with <a href=\"https://github.com/databricks/tensorframes\">TensorFrames</a>.\n\nThe last part of the book discusses the ecosystem more generally. We discuss how Spark works with different languages, the ecosystem, and the vast community around Spark.\n\n<h2>Getting Started</h2>\n\nFor your preliminary viewing of the book, we are providing a preview copy of the contents of the book for anyone to <a href=\"http://go.databricks.com/definitive-guide-apache-spark\">download and read</a>, free of charge. This sample is the unedited sample of the current Definitive Guide.\n\nWe also plan on adding much of this content to the <a href=\"https://docs.databricks.com/\">Databricks Documentation</a> so that Databricks customers can always have an up to date reference. We already include extensive notebook examples that you can use to get started with right away; however, we will continue to add to this as we finish the book."}
{"status": "publish", "description": null, "creator": "jules_damji", "link": "https://databricks.com/blog/2017/06/07/databricks-serverless-next-generation-resource-management-for-apache-spark.html", "authors": null, "id": 11234, "categories": ["Announcements", "Company Blog", "Product"], "dates": {"publishedOn": "2017-06-07", "tz": "UTC", "createdOn": "2017-06-07"}, "title": "Databricks Serverless: Next Generation Resource Management for Apache Spark", "slug": "databricks-serverless-next-generation-resource-management-for-apache-spark", "content": "As the amount of data in an organization grows, more and more engineers, analysts and data scientists need to analyze this data using tools like Apache Spark. Today, IT teams constantly struggle to find a way to allocate big data infrastructure, budget among different users, and optimize performance. End-users like data scientists and analysts also spend enormous amounts of time tuning their big data infrastructure for optimum performance, which is neither their core expertise nor their primary goal of deriving insights from data.\n\nTo remove these operational complexities for users, the next generation of cloud computing is headed toward serverless computing. Products like BigQuery offer serverless interfaces that require zero infrastructure management for users. But all these existing products only address simple, stateless SQL use cases.\n\nToday, we are excited to announce <strong>Databricks Serverless</strong>, a new initiative to offer serverless computing for complex data science and Apache Spark workloads. Databricks Serverless is the first product to offer a serverless API for Apache Spark, greatly simplifying and unifying data science and big data workloads for both end-users and DevOps.\n\n<iframe src=\"https://www.youtube.com/embed/oFOgKB4OlBM\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\nSpecifically, in Databricks Serverless, we set out to achieve the following goals:\n\n<ol>\n<li>Remove all operational complexities for both big data and interactive data science.</li>\n<li>Reduce operational cost by an order of magnitude by letting organizations maximize their resource utilization in a shared environment spanning all their workloads.</li>\n<li>Minimize query latencies for interactive analysis.</li>\n<li>Achieve all of the above without compromising on reliability.</li>\n</ol>\n\nAt <a href=\"https://spark-summit.org/2017/\">Spark Summit</a> today, we have launched our first phase of Databricks Serverless, called Serverless Pools, which allow customers to run a pool for serverless workloads in their own AWS account. Hundreds of users can share a pool, while DevOps can control the resource cost of their whole workload in a single place. In future phases, we will also provide services to run serverless workloads outside the customer's AWS environment.\n\n<h2>What Are Serverless Pools?</h2>\n\nDatabricks Serverless pools are automatically managed pools of cloud resources that are auto-configured and auto-scaled for interactive Spark workloads. Administrators only need to provide the minimum and maximum number of instances they want in their pool, for the purpose of budget control. End-users then program their workloads using Spark APIs in SQL or Python, and Databricks will automatically and efficiently run these workloads.\n\nThe three key benefits of serverless pools are:\n\n<ul>\n<li><strong>Auto-configuration:</strong> The Spark version deployed in serverless pools is automatically optimized for interactive SQL and Python workloads.</li>\n<li><strong>Spark-aware elasticity:</strong> Databricks automatically scales the compute and local storage resources in the serverless pools in response to Apache Spark\u2019s changing resource requirements for user jobs.</li>\n<li><strong>Reliable fine-grained sharing:</strong> Serverless pools embed preemption and fault isolation into Spark, enabling a pool's resources to be shared among many users in a fine-grained manner without compromising on reliability.</li>\n</ul>\n\n<h2>Why Serverless Pools?</h2>\n\nThere are multiple existing resource managers for Apache Spark, but none of them provides the high concurrency and automatic elasticity of serverless pools. Existing cluster managers, such as YARN, and cloud services, such as EMR, suffer from the following issues:\n\n<ol>\n<li><strong>Complex configuration</strong>: Each user needs to configure their Spark application by specifying its resource demands (e.g. memory size for containers). The wrong configuration will result in poor performance.</li>\n<li><strong>Low utilization</strong>: Applications will often consume fewer resources than they have been allocated in the system, resulting in a wasted resources and higher cost. Reallocation is only done at coarse-grained time scales.</li>\n<li><strong>High query latencies</strong>: Users working on interactive data science need their queries to return quickly so that they can plan their next steps in exploring their data sets. If each query requires a few minutes of overhead to submit a job or spin up a new cluster, these latency-sensitive users will have a very poor user experience.</li>\n</ol>\n\nDatabricks Serverless pools combine elasticity and fine-grained resource sharing to tremendously simplify infrastructure management for both admins and end-users:\n\n<ol>\n<li>IT admins can easily manage costs and performance across many users and teams through one setting, without having to configure multiple Spark clusters or YARN jobs.</li>\n<li>Users can focus on their queries, writing stateful data processing code in collaborative environments such as notebooks, without having to think about infrastructure. They simply connect their notebook or job to a serverless pool.</li>\n</ol>\n\n<img class=\"aligncenter size-full wp-image-11237\" src=\"https://databricks.com/wp-content/uploads/2017/06/image2-2.png\" alt=\"\" width=\"1520\" height=\"830\" />\n\nNext, we look at the three key properties of serverless pools in detail.\n\n<h2>Auto-Configuration</h2>\n\nTypically, configuring a Spark cluster involves the following stages:\n\n<ol>\n<li>IT admins are tasked with provisioning clusters and managing budgets. They look at all the usage requirements and the cost options available, including things like choosing the right instance types, reserving instances, selecting a spot bid, etc.</li>\n<li>Data engineers and Spark experts then jump in and play around with hundreds of Spark configurations (off-heap memory, serialization formats, etc.) to fine-tune their Spark jobs for good performance.</li>\n<li>If the cluster is going to be used for machine learning workloads, data scientists then spend additional time optimizing the clusters for <em>their</em> algorithms and utilization needs.</li>\n</ol>\n\nServerless pools drastically simplifies stage 1 and eliminates stage 2 and stage 3, by allowing admins to create a single pool with key AWS parameters such as spot bidding.\n\n<h2>Spark-Aware Elasticity</h2>\n\nAs mentioned earlier, predicting the correct amount of resources for a cluster is one of the hardest tasks for admins and users as they don\u2019t know the usage requirements. This results in a lot of trial and error for users. With serverless pools, users can just specify the range of desired instances and the serverless pools elastically scales the compute and local storage based on individual Spark job\u2019s resource requirements.\n\n<strong>Autoscaling Compute</strong>: The compute resources in a serverless pool are autoscaled based on Spark tasks queued up in the cluster. This is different from the coarse-grained autoscaling found in traditional resource managers. The Spark-native approach to scaling helps in best resource utilization thereby bringing the infrastructure costs down significantly. Furthermore, serverless pools combine this autoscaling with a mix of on-demand and spot instances to further optimize costs. Read more in <a href=\"https://docs.databricks.com/user-guide/clusters/sizing.html#autoscaling-overview\">our autoscaling documentation</a>.\n\n<strong>Autoscaling Storage</strong>: Apart from compute and memory, Spark requires disk space for supporting data shuffles and spilling from memory. Having the right amount of disk space is critical to get Spark jobs working without any failures, and data engineers and scientists typically struggle to get this right. Serverless pools use logical volume management to address this issue. As the local storage of worker instances fills up, serverless pools automatically provision additional EBS volumes for the instances and the running Spark jobs seamlessly use the additional space. No more \"out of disk space\" failures ever!\n\n<h2>Reliable Fine-Grained Sharing</h2>\n\nSince serverless pools allow for fine-grained sharing of resources between multiple users, dynamic workload management and isolation are essential for predictable performance.\n\n<strong>Preemption</strong>: When multiple users are sharing a cluster, it is very common to have a single job from a user monopolize all the cluster resources, thereby slowing all other jobs on the cluster. Spark\u2019s fair scheduler pool can help address such issues for a small number of users with similar workloads. As the number of users on a cluster increase, however, it becomes more and more likely that a large Spark job will hog all the cluster resources. The problem can be more aggravated when multiple data personas are running different types of workloads on the same cluster. For example, a data engineer running a large ETL job will often prevent a data analyst from running short, interactive queries. To combat such problems, the serverless pool will proactively preempt Spark tasks from over-committed users to ensure all users get their fair share of cluster time. This gives each user a highly interactive experience while still minimizing overall resource costs.\n\n<strong>Fault Isolation</strong>: Another common problem when multiple users share a cluster and do interactive analysis in notebooks is that one user's faulty code can crash the Spark driver, bringing down the cluster for all users. In such scenarios, the Databricks resource manager provides fault isolation by sandboxing the driver processes belonging to different notebooks from one another so that a user can safely run commands that might otherwise crash the driver without worrying about affecting the experience of other users.\n\n<h2>Performance Evaluation</h2>\n\nWe did some benchmarking to understand how the serverless pools fare when there is a concurrent and heterogeneous load. Here is the setup: many data scientists are running Spark queries on a cluster. These are short-running interactive jobs that last at most a few minutes. What happens when we introduce a large ETL workload to the same cluster?\n\n<strong>20 Users on Standard Cluster</strong>\n\nFor standard Spark clusters, when ETL jobs are added, average response times increase from 5 minutes (red line) to 15 (orange line), and in the worst case more than 40 minutes.\n\n<img class=\"aligncenter size-full wp-image-11238\" src=\"https://databricks.com/wp-content/uploads/2017/06/image3-1.png\" alt=\"\" width=\"812\" height=\"562\" />\n\n<strong>20 Users on Serverless Pool</strong>\n\nWith a serverless pool, the interactive queries get a little slower when the ETL jobs start, but the Databricks scheduler is able to guarantee performance isolation and limit their impact. The ETL jobs runs in the background, efficiently utilizing idle resources. Users get excellent performance for both workloads without having to run a second cluster.\n\n<img class=\"aligncenter size-full wp-image-11240\" src=\"https://databricks.com/wp-content/uploads/2017/06/image5.png\" alt=\"\" width=\"812\" height=\"562\" />\n\n<strong>Comparison with Other Systems</strong>\n\nWe also tested the performance of larger, concurrent TPC-DS workloads on three environments: (1) Presto on EMR, (2) Apache Spark on EMR and (3) Databricks Serverless.\n\nWhen there were 5 users each running a TPC-DS workload concurrently on the cluster, the average query latencies for Serverless pools were an order of magnitude lower than Presto.\nWith 20 users and a background ETL job on the cluster, the difference is even larger, to 12x faster than Presto and 7x faster than Spark on EMR.\n\n<div><img class=\"wp-image-11239 alignleft\" src=\"https://databricks.com/wp-content/uploads/2017/06/5-Users-4.png\" alt=\"\" width=\"320px\" height=\"240px\" /><img class=\"wp-image-11236 alignleft\" src=\"https://databricks.com/wp-content/uploads/2017/06/20-Users-Background-Job-4.png\" alt=\"\" width=\"320px\" height=\"240px\" /></div>\n\n<h2>Conclusion</h2>\n\nServerless pools are the first step in our mission to eliminate all operational complexities involved with big data. They take all of the guesswork out of cluster management -- just set the minimum and maximum size of a pool and it will automatically scale within those bounds to adapt to the load being placed on it. They also provide a zero-management experience for users -- just connect to a pool and start running code from notebooks or jobs. We are excited that Databricks Serverless is the first platform to offer all of these serverless computing features for the full power of Apache Spark.\n\nYou can try Databricks Serverless in beta form today by signing up for a <a href=\"https://databricks.com/try-databricks\">free Databricks trial</a>."}
{"status": "publish", "description": null, "creator": "jakebellacera", "link": "https://databricks.com/blog/2017/06/09/10th-spark-summit-sets-another-record-attendance.html", "authors": null, "id": 11323, "categories": ["Company Blog", "Events"], "dates": {"publishedOn": "2017-06-09", "tz": "UTC", "createdOn": "2017-06-09"}, "title": "10th Spark Summit Sets Another Record of Attendance", "slug": "10th-spark-summit-sets-another-record-attendance", "content": "<img class=\"aligncenter size-full wp-image-11332\" src=\"https://databricks.com/wp-content/uploads/2017/06/spark-summit-2017-stage.jpg\" alt=\"The stage for Spark Summit 2017\" width=\"1000\" height=\"327\" />\n\nWe have assembled a selected collage of highlights from Databricks\u2019 speakers at our 10th Spark Summit, a milestone for Apache Spark community and users. Shortly, the coverage of all sessions and slides will be available on the <a href=\"https://spark-summit.org/2017/\">Spark Summit 2017 website</a>.\n\n<h2>Day One: Developer Day</h2>\n\n<h3><strong>Expanding Apache Spark Use Cases in 2.2 and Beyond</strong></h3>\n\nApache Spark has come a long way, and it is tackling new frontiers through innovations to embrace new workloads. In his keynote, the creator of Apache Spark Matei Zaharia shares the rate of Spark\u2019s adoption in the community; Spark\u2019s philosophy and some notable use cases; and reveals two new open source projects from Databricks to handle new workloads: deep learning and structured streaming general availability and performance.\n\nSoftware engineer Tim Hunter demonstrates how to use deep learning pipelines on the Databricks, and Michael Armbrust, team lead in the streaming team, shows Structured Streaming\u2019s performance and continuous processing on Databricks\u2019 in a live demo.\n\n<iframe src=\"https://www.youtube.com/embed/qAZ5XUz32yM\" width=\"600\" height=\"300\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\n<a href=\"https://www.slideshare.net/databricks/expanding-apache-spark-use-cases-in-22-and-beyond-with-matei-zaharia-and-demos-by-michael-armbrust-and-tim-hunter\">View the slides for this talk</a>\n\n&nbsp;\n\n<h3><strong>A Deep Dive into Spark SQL's Catalyst Optimizer</strong></h3>\n\nKicking off the Developer track on the first day, Yin Huai takes us into the internals of how SQL optimizes your query\u2014whether written in SQL, Dataframe or Dataset\u2014the outcome is the same: optimized code for execution. But what facilitates this optimizations are Structured APIs, which signal the optimizer what needs to be accomplished not how. As a result, the Catalyst optimizer can rearrange higher-level operations such as project, aggregate, join or filter for optimal execution.\n\n<img class=\"wp-image-11328 alignleft\" src=\"https://databricks.com/wp-content/uploads/2017/06/how-catalyst-works-overview.jpg\" alt=\"Diagram showing how Catalyst works\" width=\"400\" height=\"225\" />\n\n<a href=\"https://www.slideshare.net/databricks/a-deep-dive-into-spark-sqls-catalyst-optimizer-with-yin-huai\">View the slides for this talk</a>\n\n&nbsp;\n\n<h3><strong>Challenging Web-Scale Graph Analytics with Apache Spark</strong></h3>\n\nXiangrui Meng informs us that the rise of social networks and internet of things demand complex web-scale graphs with billions of vertices and edges. One way to address this problem, Meng said, is to use GraphFrames, which implements graph queries and pattern matching to enable and simplify Spark SQL like queries for graph analytics.\n\n<img class=\"wp-image-11327 alignleft\" src=\"https://databricks.com/wp-content/uploads/2017/06/graphframes-overview-slide.png\" alt=\"A slide showing the overview of GraphFrames\" width=\"399\" height=\"225\" />\n\n<a href=\"https://www.slideshare.net/databricks/challenging-webscale-graph-analytics-with-apache-spark\">View the slides for this talk</a>\n\n&nbsp;\n\n<h3><strong>Apache Spark MLlib's Past Trajectory and New Directions</strong></h3>\n\nJoseph Bradley frames his talk in three basic questions: What? So What? And Now What?.\nBy giving us a trajectory of Spark\u2019s MLlib evolution, he touches on some key milestones why MLlib has become a popular framework among machine learning practitioners. Through its continued growth and expanding community of contributors, MLlib\u2019s major projects have covered most of the popular use cases\u2019 algorithms.\n\n<img class=\"wp-image-11329 alignleft\" src=\"https://databricks.com/wp-content/uploads/2017/06/mllib-overview-slide.jpg\" alt=\"Slide showing an overview of Joseph Bradley's talk on MLlib\" width=\"400\" height=\"225\" />\n\n<a href=\"https://www.slideshare.net/databricks/apache-sparks-mllibs-past-trajectory-and-new-directions\">View the slides for this talk</a>\n\n&nbsp;\n\n<h3><strong>How to </strong>Productionize<strong> Your Machine Learning Models Using Apache Spark MLlib 2.x</strong></h3>\n\nData scientists like to understand and explore data by transforming massive datasets and by building large-scale machine learning models with Apache Spark, a popular tool. But deploying them in production can prove challenging. Richard Garris shows how you can deploy this models by examining some actual case studies.\n\n<iframe src=\"https://www.youtube.com/embed/r740xbIpb54\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\n<a href=\"https://www.slideshare.net/databricks/how-to-productionize-your-machine-learning-models-using-apache-spark-mllib-2x-with-richard-garris\">View the slides for this talk</a>\n\n&nbsp;\n\n<h3><strong>Cost-Based Optimizer in Apache Spark 2.2</strong></h3>\n\nIn this deep-dive technical track Messrs Sameer Agarwal and Wenchen Fan (Databricks) and Ron Hu and Zhenhua Wang (Huawei) explore details in how Apache Spark 2.2 Cost-based Optimizer works and how statistical collections schemes are used to formulate an execution plan to generate compact code for execution.\n\n<img class=\"alignleft wp-image-11326\" src=\"https://databricks.com/wp-content/uploads/2017/06/cost-based-optimizer-apache-spark.jpg\" alt=\"Overview of the cost based optimizer in Apache Spark\" width=\"400\" height=\"225\" />\n\n<a href=\"https://www.slideshare.net/databricks/costbased-optimizer-in-apache-spark-22\">View the slides for this talk</a>\n\n&nbsp;\n\n<h3><strong>Real-Time Machine Learning Analytics Using Structured Streaming and Kinesis Firehose</strong></h3>\n\nOne of the most popular Apache Spark use cases is to build real-time advanced analytics applications. Databricks Solutions Architects Caryl Yuhas and Myles Baker walk through how to build a sample application and share valuable tips and tricks learned from working with many applications during the entirety of the talk.\n\n<iframe src=\"https://www.youtube.com/embed/qyso00ZfQ9M\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\n<a href=\"https://www.slideshare.net/databricks/realtime-machine-learning-analytics-using-structured-streaming-and-kinesis-firehose\">View the slides for this talk</a>\n\n&nbsp;\n\n<h3><strong>Easy, Scalable, Fault-Tolerant Stream Processing with Structured Streaming in Apache Spark</strong></h3>\n\nIn this follow up deep-dive session to Matei Zaharia\u2019s keynote and Michael Armbrust\u2019s demo on the performance of structured streaming, Michael Armbrust and Tathagata Das expound on all aspects of Structured Streaming in its intimate technical details, in particular how complex data, complex workloads, and complex systems can be used for its sinks and sources in a transaction-oriented manner.\n\nIn short, with structured streaming you as developer should not worry about underlying streaming complexities. Instead, you as developer should focus on writing simple queries and let Spark continuously update the answer.\n\n<img class=\"alignleft wp-image-11325\" src=\"https://databricks.com/wp-content/uploads/2017/06/complexities-in-stream-processing.jpg\" alt=\"Slide featuring an overview of the complexities in stream processing\" width=\"400\" height=\"225\" />\n\n<a href=\"https://www.slideshare.net/databricks/easy-scalable-fault-tolerant-stream-processing-with-structured-streaming-spark-summit-2017\">View the slides for this talk</a>\n\n&nbsp;\n\n<h3><strong>Building Robust ETL Pipelines with Apache Spark</strong></h3>\n\nWhile stable and robust ETL pipelines are critical components in any enterprise data processing, equally important are their reliability and resiliency. Xiao Li offers best and safe practices to attain both. By using features in Spark 2.2 (and future Spark 2.3), he shares a few tips and tricks: how to deal with dirty or bad records; how to handle multi-line JSON/CSV support, and to you use high-order functions in SQL.\n\n<img class=\"alignleft wp-image-11330\" src=\"https://databricks.com/wp-content/uploads/2017/06/spark-sql-etl-overview.png\" alt=\"Slide showing the overview of Spark SQL being used for ETL\" width=\"400\" height=\"214\" />\n\n<a href=\"https://www.slideshare.net/databricks/building-robust-etl-pipelines-with-apache-spark\">View the slides for this talk</a>\n\n&nbsp;\n\n<h2>Day Two: Enterprise Day</h2>\n\n<h3><strong>Accelerating Innovation with Unified Analytics Platform</strong></h3>\n\nCEO and Co-Founder of Databricks Ali Ghodsi kicks off day two with his keynote. He tells how the continued growth of Apache Spark has resulted into a myriad of innovative uses cases, from churn analytics to genome sequencing. As result, these applications are difficult to develop as they often involve siloed teams of different domain experts; their complex workflows take too long from data access to insight; and the infrastructure is costly and difficult to manage.\n\n<iframe src=\"https://www.youtube.com/embed/oFOgKB4OlBM\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\nTo address these problems, Ali explains the three pillars of Databricks\u2019 Unified Analytics Platform: people, process and platforms (or systems).\n\nHe then shares three customer stories from HP, and Shell that highlight how the Databricks Unified Analytics Platform addresses these challenges to help enterprises accelerate innovation by unifying data science, engineering, and business.\n\nTo close out his keynote, Ali announces Databricks Serverless that eliminates the complexity of infrastructure by allowing large groups of users to run workloads on a single, automatically managed pool of resources \u2014 and increase the deployment speed of Spark by up to 10x at a lower cost.\n\nSoftware engineer Greg Owen demonstrates Databricks Serverless capabilities such as fine-grained security, performance benefits, and auto-scaling. Greg shows how resources are automatically managed to ensure that all users are isolated and not impacted by others while they share the serverless pool.\n\n<a href=\"https://www.slideshare.net/databricks/accelerating-innovation-with-unified-analytics-with-ali-ghodsi\">View the slides for this talk</a>\n\n<h3><strong>Machine Learning Innovation Fireside Chat</strong></h3>\n\nBen Lorica sits down with Ion Stoica (co-founder and Executive Chairman of Databricks; co-founder of <a href=\"https://rise.cs.berkeley.edu/\">UC Berkeley RISELab</a>) and Matei Zaharia (co-founder and Chief Technologist of Databricks; assistant professor at <a href=\"http://cs.stanford.edu/\">Stanford CS</a>).\n\n<img class=\"size-full wp-image-11324 alignleft\" src=\"https://databricks.com/wp-content/uploads/2017/06/ben-lorica-ion-stocia-matei-zaharia-spark-summit-2017-panel.jpg\" alt=\"Spark Summit 2017 panel featuring Ben Lorica, Ion Stocia, Matei Zaharia\" width=\"1000\" height=\"667\" />\n\nMachine Learning in the Apache Spark ecosystem is advancing at a rapid pace. New projects are emerging to help take it to the next level. Ion Stoica and Matei Zaharia discuss how their two respective new initiatives from RISELab and <a href=\"http://dawn.cs.stanford.edu/projects/\">DAWN</a> are solving and approaching new demands of real-time intelligent decision making.\n\n<a href=\"https://www.youtube.com/watch?v=hEJtwezHjk8\">Watch the full recording of the panel here</a>\n\n<h3><strong>Transactional I/O on Cloud Storage in Databricks</strong></h3>\n\nEric Liang, a software engineer at Databricks, discusses the three dimensions to evaluate HDFS to S3: cost, SLAs (availability and durability), and performance. He then provides a deep dive on the challenges in writing to Cloud storage with Apache Spark and shares transactional commit benchmarks on Databricks I/O (DBIO) compared to Hadoop.\n\n<a href=\"https://www.slideshare.net/databricks/transactional-writes-to-cloud-storage-with-eric-liang\">View the slides for this talk</a>\n\n<h3><strong>From Pipelines to Refineries: Building Complex Data Applications with Apache Spark</strong></h3>\n\nApache Spark provides strong building blocks for batch processes, streams and ad-hoc interactive analysis. However, users face challenges when putting together a single coherent pipeline that could involve hundreds of transformation steps, especially when confronted by the need of rapid iterations. Databricks software engineer Tim Hunter explores these issues through the lens of functional programming.\n\n<a href=\"https://www.slideshare.net/databricks/from-pipelines-to-refineries-scaling-big-data-applications\">View the slides for this talk</a>\n\n<h3><strong>Identify Disease-Associated Genetic Variants Via 3D Genomics Structure and Regulatory Landscapes Using Deep Learning Frameworks</strong></h3>\n\nData analytics is exploding in the life sciences industry, particularly in the field of genomics where biotechnology firms are trying to glean insights from genomic data to accelerate drug innovation. This fascinating talk featuring Dr. Yi-Hsiang Hsu, Director of the GeriOMICS Center at the Beth Israel Deaconess Medical Center, and YongSheng Huang, a resident solutions architect at Databricks, demonstrated how Apache Spark can be used to apply deep learning to predict functions of disease-associated variants to impact development of new drug intervention.\n\n<a href=\"https://spark-summit.org/2017/events/identify-disease-associated-genetic-variants-via-3d-genomics-structure-and-regulatory-landscapes-using-deep-learning-frameworks/\">View the slides for this talk</a>\n\n<h3><strong>Apache SparkR Under the Hood: How to Debug your SparkR Applications</strong></h3>\n\nOn the final day of the data science track, Hossein Falaki a software engineer and data scientist at Databricks, dives deep into the inner-workings of SparkR, its architecture, performance issues, and more. He then walked through real SparkR use cases to show how common errors can be eliminated based on his experience.\n\n<a href=\"https://www.slideshare.net/databricks/spark-r-under-the-hood-with-hossein-falaki\">View the slides for this talk</a>\n\n<h2>Voices from the Apache Spark Ecosystem</h2>\n\n<h3><strong>Using AI For Providing Insights and Recommendations on Activity Data</strong></h3>\n\nIn front of a packed room, Alexis Roos and Sammy Nammari from Salesforce share how Databricks and Apache Spark power Einstein, an artificial intelligence capability built into the Salesforce platform. In a live demo within Databricks, they show how they combine activity data with contextual and CRM data to provide real-time insights and recommendations to their customers.\n\n<a href=\"https://spark-summit.org/2017/events/apache-sparkr-under-the-hood-how-to-debug-your-sparkr-applications/\">View the slides for this talk</a>\n\n<h3><strong>Applying Machine Learning to Construction</strong></h3>\n\nAutodesk is a leader in architecture, engineering and construction software. They developed a set of cloud products for construction that enables almost anytime, anywhere access to project-related data throughout the building construction lifecycle. Charis Kaskiris and Subham Goel discuss how Databricks and Apache Spark have allowed their data science team to work more efficiently by unifying data management, machine learning, and insight generation to empower intelligent construction.\n\n<a href=\"https://spark-summit.org/2017/events/applying-machine-learning-to-construction/\">View the slides for this talk</a>\n\n<iframe src=\"https://www.youtube.com/embed/pV8oLBxBcPQ\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\n<h3><strong>Herding Cats: Migrating Dozens of Oddball Analytics Systems to Apache Spark</strong></h3>\n\nTrying to manage and derive insights from multitudes of large sets of data distributed across various data sources and legacy systems is a common challenge many companies face. John Cavanaugh, master architect and strategist at HP, speaks of their experience finding, cataloging, and eventually eliminating these siloed systems by migrating everything to Databricks \u2014 allowing them to fully leverage the power of Apache Spark and Databricks to analyze the data.\n\n<iframe src=\"https://www.youtube.com/embed/8baBvi_NMNo\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\n<a href=\"https://spark-summit.org/2017/events/herding-cats-migrating-dozens-of-oddball-analytics-systems-to-apache-spark/\">View the slides for this talk</a>\n\n&nbsp;\n\n<h2><strong>What\u2019s Next</strong></h2>\n\nIn the coming days, all of the keynotes, sessions and slides will available on the <a href=\"https://spark-summit.org/2017/\">Spark Summit 2017 website</a>."}
{"status": "publish", "description": null, "creator": "Wayne Chan", "link": "https://databricks.com/blog/2017/06/14/analyzing-metro-operations-using-apache-spark.html", "authors": null, "id": 11347, "categories": ["Company Blog", "Customers", "Product"], "dates": {"publishedOn": "2017-06-14", "tz": "UTC", "createdOn": "2017-06-14"}, "title": "Analysing Metro Operations Using Apache Spark on Databricks", "slug": "analyzing-metro-operations-using-apache-spark", "content": "<i>This is a guest blog from EY Advisory Data &amp; Analytics team, who have been working with Sporveien in Oslo building a platform for metro analytics using Apache Spark on Databricks.</i>\n\n<a href=\"https://www.sporveien.com/inter/forside\">Sporveien</a> is the operator of the <a href=\"https://en.wikipedia.org/wiki/Oslo_Metro\">Oslo Metro</a>, a municipally owned Metro network supplying the greater Oslo area with public transportation since 1898. Today, the Metro transports more than 200,000 passengers on a daily basis and is a critical part of the city's infrastructure.\n\nSporveien is an integrated operator, meaning they are responsible for all aspects of keeping the service running. Metro operations, train maintenance, infrastructure maintenance and even construction of new infrastructure falls under Sporveien\u2019s domain. As a result, the company employs a large workforce of experienced metro drivers, operators, mechanics and engineers to ensure smooth operation of the metro.\n\nOver the next few years, with a growing population and political shifts towards green transportation, the Metro will need to be able transport even more people than today. Sporveien\u2019s stated goal for the coming years is to enable this expansion while keeping costs down and, just as importantly, punctuality high.\n\nPunctuality \u2014 the percentage of Metro departures delivered on time \u2014 is a key metric being closely monitored by Sporveien, as any deviation in punctuality can cause big problems for Sporveien\u2019s end customers \u2014 the citizens of Oslo. The Metro is a closed loop system running on a tight schedule, and even small deviations from the schedule can become amplified throughout the network and end up causing large delays.\n\nThe Metro is being monitored continuously by a signalling system following every movement of every train, logging around 170,000 train movements and 3 million signal readings every day. Due to the volume and complexity of the signalling data, Sporveien was, until recently, not able to fully utilize this information source to get a deeper understanding of how the different aspects of Metro operations affect overall performance.\n\nWhen we started looking at this data and the hypotheses we wanted to investigate, we realized this was a great case for Apache Spark. The combination of high data volumes with a need for near real time processing, as well as the need for extensive data cleaning and transformation to be able to extract value all pointed to the conclusion that a modern data analysis platform would be necessary. Wanting to start performing analyses and operationalizing them quickly without getting bogged down in the complexities of setting up and administering a Spark cluster, we decided to use Databricks for both analysis and production deployment. Sporveien had already chosen AWS as their preferred cloud stack, so that Databricks was tightly integrated was an added advantage.\n\n<h2>Our Data Pipeline: S3, Spark, Redshift and Tableau</h2>\n\nIn order to get insights from the signaling data, it is extracted from the source system and pushed to a Kinesis queue, before being loaded to S3 using a simple AWS Lambda function. Then the Spark processing sets off. First the data is cleaned and transformed to calculate precise timings of the actual metro departures. It is then combined with detailed scheduling data from the planning system using a multi-step matching routine which ensures that any single actual departure is only matched with a single planned departure. The departure data is then further enriched with passenger flow data fetched from the automated passenger counting system installed in each train. Finally, the enriched and cleaned data is dumped to our data warehouse in Redshift.\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/06/Sporveien-Data-Pipeline.png\"><img class=\"alignnone size-large wp-image-11377\" src=\"https://databricks.com/wp-content/uploads/2017/06/Sporveien-Data-Pipeline-1024x439.png\" alt=\"Sporveien Data Pipeline with Databricks\" width=\"1024\" height=\"439\" /></a>\n\nThis whole process runs every five minutes throughout the day on real time data, using heuristics for some of the more complex calculations. To ensure precise results over time, the whole day\u2019s data is recalculated during the night using more detailed calculations. We like to think of this as our version of lambda architecture, where we are able to use the same stack and the same logic for both real time and batch processing.\n\nOur data pipelines and our Redshift cluster have a myriad of uses within Sporveien, including operational analysis of operations performance and deviations, KPI reporting, real time dashboards, and recently also feeding analyzed sensor data back into production systems to enable condition based maintenance of some of Sporveien\u2019s railway infrastructure.\n\nThe signalling data is one of several data pipelines we have running in production. At the time of writing this, we have around 40 Databricks notebooks running nightly or continuously, coordinated using <a href=\"https://databricks.com/blog/2016/08/30/notebook-workflows-the-easiest-way-to-implement-apache-spark-pipelines.html\">Databricks Notebook Workflows</a> and scheduled in parallel using the Jobs feature. We have also set up an integration to Sporveien\u2019s Slack instance to make sure that everyone on the analytics team is aware of what\u2019s happening with our production jobs.\n\nIn all this, the Spark framework has proven to be an immensely powerful tool kit. An obvious advantage is being able to spread computing over several nodes \u2014- for instance we run our real-time jobs on a small cluster, and then launch a bigger one for recalculations at night. But equally important, we\u2019ve found that <a href=\"http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html\">PySpark SQL</a> is a surprisingly succinct and communicative way of expressing data transformation logic. We get to write transformation logic that is understandable and auditable even to non-analysts, something that increases the trust in the results greatly.\n\n<h2>The Challenge: How to do Continuous Integration on Databricks</h2>\n\nAs we\u2019re continuously extending our analyses and refining our algorithms, we\u2019ve found ourselves needing to deploy updated transformation logic to our development and production repositories quite often.\n\nWe use Databricks for all aspects of writing and running Spark code, and were early adopters of the GitHub integration as we have the rest of our codebase in GitHub as well. However, we found the built in Github integration to be too simplistic to work for our needs. Since we are using Databricks Notebook Workflows, we have many dependencies between different notebooks, and as such we wanted to be able to perform commits across notebooks, being in control of all changes and what to deploy when. We also found it cumbersome trying to do larger deployments to production.\n\nWe have been working with Github Flow as our deployment workflow in previous projects, using branches for feature development and pull requests for both code reviews and automated deployments, and felt this could work well in this setting as well \u2014 if we only had the necessary CI setup in place. With this motivation, we spoke to Databricks to see what could be done, and agreed that an API for accessing the workspace would make a lot of what we wanted possible. A few months later we were very happy to be invited to try out the new <a href=\"https://docs.databricks.com/api/latest/workspace.html\">Workspace API</a>, and quickly confirmed that this was what we needed to be able to get a complete code versioning and CI setup going.\n\n<h2>Bricker</h2>\n\nThe Workspace API makes it easy to list, download, upload and delete notebooks in Databricks. If we could get this working together, we would be able to sync a whole folder structure between our local machines and Databricks. And, of course, as soon as we had the notebooks locally, we would have all our usual Git tools at our disposal, making it simple to stage, diff, commit, etc.\n\nSo to make this work, we built a small Python CLI tool we\u2019ve called <a href=\"https://github.com/sporveien/bricker \"><b>bricker</b></a>. Bricker allows us to sync a local folder structure to Databricks just by entering the command <b>bricker up</b>, and similarly to sync from Databricks to the local filesystem using <b>bricker down</b>. Bricker uses your current checked out Git branch name to determine the matching folder name in Databricks.\n\nWe have also added a feature to create a preconfigured cluster with the right IAM roles, Spark configs etc. using <b>bricker create_cluster</b> to save some time as we found ourselves often creating the same types of clusters.\n\nWe thought more people might be needing something similar, so we\u2019ve open sourced the tool and published it on PyPI. You can find install instructions <a href=\"https://github.com/sporveien/bricker\">here</a>.\n\n<h2>Our Analysis Development Lifecycle: GitHub, Bricker, CircleCL and Databricks</h2>\n\nUsing bricker, our analysis lifecycle now works like this: We start a new feature by branching from the dev branch on our laptops, and <b>bricker up </b>to get a new cloned folder in Databricks where we develop and test new and updated analyses. When we\u2019re ready to merge, we use <b>bricker down</b> and commit to our feature branch, before pushing to Github and creating a pull request.\n\nWe\u2019ve set up the cloud CI solution Circle CI to trigger on pull request merges, and so as soon as the pull request is merged into dev or production, CI fetches the merged code and runs <b>bricker up </b>to automatically deploy the merge to our Databricks production folder. With this, we\u2019ve been able to reduce our entire production deployment into a one click approval in Github, allowing the entire team to iterate on analyses quickly and safely without worrying\u00a0about broken dependencies, and to deploy into production as often as we need.\n\n<a href=\"https://databricks.com/wp-content/uploads/2017/06/Sporveien-Deployment-Pipeline.png\"><img class=\"alignnone size-full wp-image-11378\" src=\"https://databricks.com/wp-content/uploads/2017/06/Sporveien-Deployment-Pipeline.png\" alt=\"Sporveien Deployment Pipeline with Databricks\" width=\"858\" height=\"392\" /></a>\n\n<h3>Impact</h3>\n\nUsing this setup, we have been able to very rapidly establish a robust and full featured analytics solution in Sporveien. We have spent less time thinking about infrastructure and more about creating analyses and data products to help Sporveien better understand how different aspects of the Metro operations affect punctuality.\n\nIn the last few years, Sporveien has really embraced data-driven decision making. We are using detailed analyses and visualizations of different aspects of performance in every setting from strategic planning to operational standups in the garages and with operations teams.\n\nLooking forward, there is still a lot of exciting work to be done at Sporveien. We have only seen the beginning of what insights can be had by understanding and analysing the ever growing amounts of data generated by all kinds of operational systems. As we get more sensors and higher resolution measurements, we are now looking at applying machine learning to predict when railway infrastructure components will break down.\n\nBy continuously optimizing operations and improving performance, Sporveien is doing everything to be able to offer the citizens of Oslo a reliable and punctual service both today and in the many years to come."}
{"status": "publish", "description": null, "creator": "jules_damji", "link": "https://databricks.com/blog/2017/06/13/five-spark-sql-utility-functions-extract-explore-complex-data-types.html", "authors": null, "id": 11351, "categories": ["Apache Spark", "Engineering Blog", "Streaming"], "dates": {"publishedOn": "2017-06-13", "tz": "UTC", "createdOn": "2017-06-13"}, "title": "Five Spark SQL Utility Functions to Extract and Explore Complex Data Types", "slug": "five-spark-sql-utility-functions-extract-explore-complex-data-types", "content": "[dbce_cta href=\"https://docs.databricks.com/_static/notebooks/complex-nested-structured.html\"]Try this notebook on Databricks[/dbce_cta]\n\nFor developers, often the <strong><em>how</em></strong> is as important as the <strong><em>why</em></strong>. While our in-depth blog explains the <a href=\"https://databricks.com/blog/2017/02/23/working-complex-data-formats-structured-streaming-apache-spark-2-1.html\">concepts and motivations</a> of <em>why</em> handling complex data types and formats are important, and equally explains their utility in processing complex data structures, this blog post is a preamble to the <em>how</em> as a <a href=\"https://docs.databricks.com/_static/notebooks/complex-nested-structured.html\">notebook tutorial</a>.\n\nIn this tutorial, I show and share ways in which you can explore and employ five Spark SQL utility functions and APIs. Introduced in Apache Spark 2.x as part of <a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\"><em>org.apache.spark.sql.functions</em>,</a> they enable developers to easily work with complex data or nested data types.\n\nIn particular, they come in handy while doing Streaming ETL, in which data are JSON objects with complex and nested structures: Map and Structs embedded as JSON. This notebook tutorial focuses on the following Spark SQL functions:\n\n<ul>\n<li><a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\"><em>get_json_object()</em></a></li>\n<li><a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\"><em>from_json()</em></a></li>\n<li><a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\"><em>to_json()</em></a></li>\n<li><a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\"><em>explode()</em></a></li>\n<li><a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\"><em>selectExpr()</em></a></li>\n</ul>\n\nTo give you a glimpse, consider this nested schema that defines what your IoT events may look like coming down an Apache Kafka stream or deposited in a data source of your choice.\n\n[code_tabs]\n\n<pre><code class=\"scala\"><br />import org.apache.spark.sql.types._\nval schema = new StructType()\n  .add(\"dc_id\", StringType)                               // data center where data was posted to Kafka cluster\n  .add(\"source\",                                          // info about the source of alarm\n    MapType(                                              // define this as a Map(Key-&gt;value)\n      StringType,\n      new StructType()\n      .add(\"description\", StringType)\n      .add(\"ip\", StringType)\n      .add(\"id\", LongType)\n      .add(\"temp\", LongType)\n      .add(\"c02_level\", LongType)\n      .add(\"geo\", \n         new StructType()\n          .add(\"lat\", DoubleType)\n          .add(\"long\", DoubleType)\n        )\n      )\n    )\n</code></pre>\n\n<pre><code class=\"python\"><br />from pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\nschema = StructType() \\\n          .add(\"dc_id\", StringType()) \\                           # data center where data was posted to Kafka cluster\n          .add(\"source\", MapType(StringType(), StructType() \\     # info about the source of alarm\n                        .add(\"description\", StringType()) \\       # define this as a Map(Key-&gt;value)\n                        .add(\"ip\", StringType()) \\\n                        .add(\"id\", LongType()) \\\n                        .add(\"temp\", LongType()) \\\n                        .add(\"c02_level\", LongType()) \\\n                        .add(\"geo\", StructType() \\\n                              .add(\"lat\", DoubleType()) \\\n                              .add(\"long\", DoubleType()))))\n</code></pre>\n\n[/code_tabs]\n\nAnd its corresponding sample DataFrame/Dataset data may look as follows:\n\n[code_tabs]\n\n<pre><code class=\"scala\"><br />val dataDS = Seq(\"\"\"\n{\n\"dc_id\": \"dc-101\",\n\"source\": {\n    \"sensor-igauge\": {\n      \"id\": 10,\n      \"ip\": \"68.28.91.22\",\n      \"description\": \"Sensor attached to the container ceilings\",\n      \"temp\":35,\n      \"c02_level\": 1475,\n      \"geo\": {\"lat\":38.00, \"long\":97.00}                        \n    },\n    \"sensor-ipad\": {\n      \"id\": 13,\n      \"ip\": \"67.185.72.1\",\n      \"description\": \"Sensor ipad attached to carbon cylinders\",\n      \"temp\": 34,\n      \"c02_level\": 1370,\n      \"geo\": {\"lat\":47.41, \"long\":-122.00}\n    },\n    \"sensor-inest\": {\n      \"id\": 8,\n      \"ip\": \"208.109.163.218\",\n      \"description\": \"Sensor attached to the factory ceilings\",\n      \"temp\": 40,\n      \"c02_level\": 1346,\n      \"geo\": {\"lat\":33.61, \"long\":-111.89}\n    },\n    \"sensor-istick\": {\n      \"id\": 5,\n      \"ip\": \"204.116.105.67\",\n      \"description\": \"Sensor embedded in exhaust pipes in the ceilings\",\n      \"temp\": 40,\n      \"c02_level\": 1574,\n      \"geo\": {\"lat\":35.93, \"long\":-85.46}\n    }\n  }\n}\"\"\").toDS()\n</code></pre>\n\n<pre><code class=\"python\"># Convenience function for turning JSON strings into DataFrames.\ndef jsonToDataFrame(json, schema=None):\n  # SparkSessions are available with Spark 2.0+\n  reader = spark.read\n  if schema:\n    reader.schema(schema)\n  return reader.json(sc.parallelize([json]))\n\ndataDF = jsonToDataFrame( \"\"\"{\n\n    \"dc_id\": \"dc-101\",\n    \"source\": {\n        \"sensor-igauge\": {\n        \"id\": 10,\n        \"ip\": \"68.28.91.22\",\n        \"description\": \"Sensor attached to the container ceilings\",\n        \"temp\":35,\n        \"c02_level\": 1475,\n        \"geo\": {\"lat\":38.00, \"long\":97.00}                        \n      },\n      \"sensor-ipad\": {\n        \"id\": 13,\n        \"ip\": \"67.185.72.1\",\n        \"description\": \"Sensor ipad attached to carbon cylinders\",\n        \"temp\": 34,\n        \"c02_level\": 1370,\n        \"geo\": {\"lat\":47.41, \"long\":-122.00}\n      },\n      \"sensor-inest\": {\n        \"id\": 8,\n        \"ip\": \"208.109.163.218\",\n        \"description\": \"Sensor attached to the factory ceilings\",\n        \"temp\": 40,\n        \"c02_level\": 1346,\n        \"geo\": {\"lat\":33.61, \"long\":-111.89}\n      },\n      \"sensor-istick\": {\n        \"id\": 5,\n        \"ip\": \"204.116.105.67\",\n        \"description\": \"Sensor embedded in exhaust pipes in the ceilings\",\n        \"temp\": 40,\n        \"c02_level\": 1574,\n        \"geo\": {\"lat\":35.93, \"long\":-85.46}\n      }\n    }\n  }\"\"\", schema)\n\n</code></pre>\n\n[/code_tabs]\n\nIf you examine the respective schemas in Scala or Python notebook, you can see the nested structures:\n\n[code_tabs]\n\n<pre><code class=\"scala\">df.printSchema\nroot\n |-- dc_id: string (nullable = true)\n |-- source: map (nullable = true)\n |    |-- key: string\n |    |-- value: struct (valueContainsNull = true)\n |    |    |-- description: string (nullable = true)\n |    |    |-- ip: string (nullable = true)\n |    |    |-- id: long (nullable = true)\n |    |    |-- temp: long (nullable = true)\n |    |    |-- c02_level: long (nullable = true)\n |    |    |-- geo: struct (nullable = true)\n |    |    |    |-- lat: double (nullable = true)\n |    |    |    |-- long: double (nullable = true)\n</code></pre>\n\n<pre><code class=\"python\">dataDF.printSchema()\n\nroot\n |-- dc_id: string (nullable = true)\n |-- source: map (nullable = true)\n |    |-- key: string\n |    |-- value: struct (valueContainsNull = true)\n |    |    |-- description: string (nullable = true)\n |    |    |-- ip: string (nullable = true)\n |    |    |-- id: long (nullable = true)\n |    |    |-- temp: long (nullable = true)\n |    |    |-- c02_level: long (nullable = true)\n |    |    |-- geo: struct (nullable = true)\n |    |    |    |-- lat: double (nullable = true)\n |    |    |    |-- long: double (nullable = true)\n\n</code></pre>\n\n[/code_tabs]\n\nI use a sample of these JSON event data from IoT and Nest devices to illustrate how to use these functions. The takeaway from this tutorial is that there are myriad ways to slice and dice nested JSON structures with Spark SQL utility functions, namely the aforementioned list.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/06/image1-4.png\" alt=\"\" width=\"1999\" height=\"1251\" class=\"aligncenter size-full wp-image-11356\" /> \n<em><a href=\"https://developers.nest.com/documentation/api-reference\">nest image source</a></em>\n\nSince I would be repeating here what I already demonstrated in the notebook, I encourage that you explore the <a href=\"https://docs.databricks.com/_static/notebooks/complex-nested-structured.html\">accompanying notebook</a>, import it into your Databricks workspace, and have a go at it.\n\n<h2>What's Next?</h2>\n\nIn a follow-up tutorial on <a href=\"https://databricks.com/blog/2017/05/24/working-with-nested-data-using-higher-order-functions-in-sql-on-databricks.html\">Higher Order Functions</a>, I'll explore how to use these powerful SQL functions to manipulate structured data.\n\nIf you don\u2019t have a Databricks account, get one <a href=\"https://databricks.com/try\">Databricks today</a>."}
{"status": "publish", "description": null, "creator": "jules_damji", "link": "https://databricks.com/blog/2017/06/20/managing-securing-credentials-databricks-apache-spark-jobs.html", "authors": null, "id": 11414, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2017-06-20", "tz": "UTC", "createdOn": "2017-06-20"}, "title": "Managing and Securing Credentials in Databricks for Apache Spark Jobs", "slug": "managing-securing-credentials-databricks-apache-spark-jobs", "content": "Since <a href=\"https://spark.apache.org/\">Apache Spark</a> separates compute from storage, every Spark Job requires a set of credentials to connect to disparate data sources.  Storing those credentials in the clear can be a security risk if not stringently administered.  To mitigate that risk, Databricks makes it easy and secure to connect to S3 with either <a href=\"https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html\">Access Keys via DBFS</a> or by using <a href=\"https://docs.databricks.com/user-guide/cloud-configurations/aws/iam-roles.html\">IAM Roles</a>.  For all other data sources (Kafka, Cassandra, RDBMS, etc.), the sensitive credentials must be managed by some other means.\n\nThis blog post will describe how to leverage an IAM Role to map to any set of credentials. It will leverage the AWS\u2019s Key Management Service (KMS) to encrypt and decrypt the credentials so that your credentials are never in the clear at rest or in flight.  When a Databricks Cluster is created using the IAM Role, it will have privileges to both read the encrypted credentials from an S3 bucket and decrypt the ciphertext with a KMS key.\n\nIn this example, an ETL Job will read data from a relational database using JDBC and write to an S3 bucket. It uses an IAM Role, <strong><em>prod-etl</em></strong>, that is allowed to read configuration for production ETL jobs from the S3 bucket <strong><em>prod-etl-config</em></strong>, to decrypt the ciphertext using the <strong><em>prod-data-credentials</em></strong> KMS key, and to write data to the <strong><em>prod-datamart</em></strong> S3 bucket. Similarly, there can be equivalent buckets, roles, and keys for Dev and Test environments. Databricks allows you to restrict which users are allowed to use which IAM Roles.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/06/image1-6.png\" alt=\"Managing Credentials in Databricks\" width=\"596\" height=\"539\" class=\"aligncenter size-full wp-image-11419\" />\n\n<h2>Job Configuration File</h2>\n\nThe configuration for this ETL Job resides in an S3 bucket and resembles the following:\n\n<pre><code class=\"json\">{\n  \"Name\": \"Stage Orders Data Mart\",\n  \"Connections\": [\n    {\n      \"Name\": \"Orders Database\",\n      \"Type\": \"jdbc\",\n      \"Properties\": {\n        \"database\": \"orders\",\n        \"hostname\": \"orders-cluster.cluster-cmeifwaki1jl.us-west-2.rds.amazonaws.com\",\n        \"username\": \"etluser\",\n        \"password.encrypted\": \"AYADeMYi2FpjU5mDrWdouDl9JWcAgQACABVhd3MtY3J5cHRvLXB1YmxpYy1rZXkAREF2Sys1Vjl0UmNoODBJdkp4dHljaWNFdlJTQS9BZ2h2WVpqNE5md3hYYnUrYjRMYjlYQUxidmNjUU1iWXhWNVNZdz09AA9jb25uZWN0aW9uLW5hbWUAD09yZGVycyBEYXRhYmFzZQABAAdhd3Mta21zAEthcm46YXdzOmttczp1cy13ZXN0LTI6OTk3ODE5MDEyMzA3OmtleS80MjcyMmMzMi1kYjc5LTRiOTItOWRiOC01ODFjY2RiZjhhNjkApwEBAQB4NOkLYxq\\/N2xFyhCyrixGzcdEzBAjkxwIZTwQwQPPa48AAAB+MHwGCSqGSIb3DQEHBqBvMG0CAQAwaAYJKoZIhvcNAQcBMB4GCWCGSAFlAwQBLjARBAxn7MgQJz4LCuu35e0CARCAO2teqEsw\\/P\\/ps2VfQiMs9+lKEho7W1LtDOM+bqBWRIEnJBf43oVgCVkNEXDuFe4bwziNQufqdH2rkj\\/SAgAAAAAMAAAQAHDvzh\\/UQ6kq3g9+twLRZUllS3VL8j\\/II1+fmcD\\/\\/\\/\\/\\/AAAAAcCptPeAQLNjaQUfUQAAAAv8BUBF4r\\/YTpJ08etxFXYM+c4m7HO9\\/iJBoe0AZjBkAjBeWc2i9N\\/LUO\\/HMBN5oJXQbXozBibjA7es5FslnsiZJUPsJku3pvxbCjSAPdNxiMACMFM2uq2qETaz1h0BI+I9FT70EhOVRx51JpgzoLFKTPaqrW41Fr3GN7PQuXpSFdDOhA==\",\n        \"password.encrypted.keyARN\": \"arn:aws:kms:us-west-2:997819012307:key\\/42722c32-db79-4b92-9db8-581ccdbf8a69\",\n        \"port\": \"3306\",\n        \"driver\": \"mysql\"\n      }\n    },\n    {\n      \"Name\": \"Orders Datamart\",\n      \"Type\": \"file\",\n      \"Properties\": {\n        \"path\": \"s3a:\\/\\/prod-datamart\\/orders\"\n      }\n    }\n  ]\n}\n</code></pre>\n\nEvery part of the configuration is in clear text except for the password which is represented as encrypted ciphertext.  When the ETL Job is run, it accepts a path for the configuration file as an input:\n\n<code>s3a://prod-etl-config/StageOrdersDataMart.json</code>\n\nIf this configuration were meant to connect to Test data sources, a config file corresponding to the Test environment would be referenced instead.\n\n<h2>ETL Notebook</h2>\n\nThe <a href=\"https://docs.databricks.com/_static/notebooks/blog-credentials-etl-job.html\">notebook for the ETL Job</a>  does the following:\n\n<ol>\n<li>Receives Job Parameters (config file &amp; job name)</li>\n<li>Downloads Config File</li>\n<li>Checks that the Config is valid</li>\n<li>Parses the file into a <em>JobConfig</em> object</li>\n<li>Uses that <em>JobConfig</em> object to decrypt and apply credentials to both the <em>DataFrameReader</em> and <em>DataFrameWriter</em></li>\n</ol>\n\n<h2>Ciphertext Decryption</h2>\n\nThe actual decryption occurs within the Connection class in a private method. The API accepts either a <em>DataFrameReader</em> or <em>DataFrameWriter</em> object and applies the decrypted values as options to them. At no point are the decrypted values printed or logged.  This class could be built into its own library from a source code repository that is restricted so that only a small number of developers are allowed to modify it. This <a href=\"https://docs.databricks.com/_static/notebooks/blog-credentials-job-config.html\">notebook shows the <em>Connection</em> code</a>.\n\nThis code was inspired by the <a href=\"https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/java-example-code.html\">AWS Encryption SDK for Java Example Code</a>. You may want to experiment with the <a href=\"http://docs.aws.amazon.com/kms/latest/developerguide/encryption-context.html\">encryption context</a> to suit your particular use case. In order for the Encryption SDK to work, you need to <a href=\"https://docs.databricks.com/user-guide/libraries.html#libraries-from-maven-pypi-or-spark-packages\">create and attach the following libraries from maven</a>.\n\n<ul>\n<li><a href=\"https://mvnrepository.com/artifact/com.amazonaws/aws-encryption-sdk-java/0.0.1\">AWS Encryption SDK Library</a></li>\n<li><a href=\"https://mvnrepository.com/artifact/org.bouncycastle/bcprov-jdk15on/1.56\">BouncyCastle Provider Library</a></li>\n</ul>\n\n<h2>Encryption</h2>\n\nThe AWS Encryption SDK must be used to encrypt the clear text password before it can be included in the configuration file.  A method to do so has been included as part of the <em>Connection</em> class.  This can be done within a <a href=\"https://docs.databricks.com/_static/notebooks/blog-credentials-save-config.html\">Databricks notebook</a>. After the config file has been created and written to S3, the <a href=\"https://docs.databricks.com/_static/notebooks/blog-credentials-save-config.html\">notebook</a> to create \nit can be deleted.\n\n<h2>AWS Setup</h2>\n\n<ol>\n<li>Create an IAM Role, <strong><em>prod-etl</em></strong></li>\n<li>Add policies to IAM Role to List, Get, Put, Delete to the following S3 buckets:\n\n<ul>\n<li>Parquet Datamart Bucket: <strong><em>prod-datamart</em></strong></li>\n<li>ETL Configuration Bucket: <strong><em>prod-etl-config</em></strong></li>\n</ul></li>\n<li><a href=\"https://docs.databricks.com/user-guide/cloud-configurations/aws/iam-roles.html\">Register IAM Role with Databricks</a></li>\n<li>Create a <a href=\"http://docs.aws.amazon.com/kms/latest/developerguide/create-keys.html\">Customer Master Key</a> (CMK) via the AWS Console\n\n<ul>\n<li>Be sure to select the correct region BEFORE clicking \u201cCreate Key\u201d</li>\n<li>Add the <strong><em>prod-etl</em></strong> IAM Role as a Key User</li>\n<li>Record the ARN of the CMK as you will need it to encrypt your credentials</li>\n</ul></li>\n</ol>\n\n<h2>Conclusion</h2>\n\nThis blog post illustrated how you can leverage an IAM Role and a KMS Key to encrypt sensitive credentials at rest and decrypt them in memory.  This provides a way to leverage the access controls in Databricks that restricts which users can create clusters with which IAM roles. This prevents sensitive credentials from being left in the clear within either notebooks or a repository of source code. Finally, this scheme provides a way to have a clear separation between Dev, Test, and Production environment configurations.\n\nTry these three notebooks on your Databricks. If you don\u2019t have a Databricks account, <a href=\"https://databricks.com/try\">get one today</a>.\n\n<ul>\n<li><a href=\"https://docs.databricks.com/_static/notebooks/blog-credentials-etl-job.html\">Credentials for ETL</a></li>\n<li><a href=\"https://docs.databricks.com/_static/notebooks/blog-credentials-job-config.html\">Credentials for Job Configuration</a></li>\n<li><a href=\"https://docs.databricks.com/_static/notebooks/blog-credentials-save-config.html\">Credentials for Saving Job Configuration</a></li>\n</ul>"}
{"status": "publish", "description": null, "creator": "jules_damji", "link": "https://databricks.com/blog/2017/06/23/parallelizing-large-simulations-apache-sparkr-databricks.html", "authors": null, "id": 11442, "categories": ["Company Blog", "Customers", "Product"], "dates": {"publishedOn": "2017-06-23", "tz": "UTC", "createdOn": "2017-06-23"}, "title": "Parallelizing Large Simulations with Apache SparkR on Databricks", "slug": "parallelizing-large-simulations-apache-sparkr-databricks", "content": "<em>This blog post is a joint engineering effort between Shell\u2019s Data Science Team (<a href=\"mailto:wayne.w.jones@shell.com\">Wayne W. Jones</a> and <a href=\"mailto:dennis.vallinga@shell.com\">Dennis Vallinga</a>) and Databricks (<a href=\"mailto:hossein@databricks.com\">Hossein Falaki</a>).</em>\n\n<h2>Introduction</h2>\n\n<a href=\"https://databricks.com/blog/2016/07/26/introducing-apache-spark-2-0.html\">Apache Spark 2.0</a> introduced a new family of APIs in <a href=\"https://spark.apache.org/docs/latest/sparkr.html\">SparkR</a>, the R interface to Apache Spark to enable users to parallelize existing R functions. The new <a href=\"http://spark.apache.org/docs/latest/api/R/dapply.html\">dapply</a>, <a href=\"http://spark.apache.org/docs/latest/api/R/gapply.html\">gapply</a> and <a href=\"http://spark.apache.org/docs/latest/api/R/gapply.html\">spark.lapply</a> methods open exciting possibilities for R users. In this post, we present details on one use case jointly done by <a href=\"http://www.shell.us/\">Shell Oil Company</a> and <a href=\"http://databricks.com\">Databricks</a>.\n\n<h2>Use Case: Stocking Recommendation</h2>\n\nIn Shell, current stocking practices are often driven by a combination of vendor recommendations, prior operational experience and \u201cgut feeling.\u201d As such, a limited focus is directed towards incorporating historical data in these decisions, which can sometimes lead to excessive or insufficient stock being held at Shell\u2019s locations (e.g., an oil rig).\n\nThe prototype tool, Inventory Optimization Analytics solution, has proven that Shell can use advanced data analysis techniques on SAP inventory data to:\n\n<ul>\n<li>Optimize warehouse inventory levels</li>\n<li>Forecast safety stock levels</li>\n<li>Rationalize slow moving materials</li>\n<li>Review and re-assign non-stock and stock items on materials list</li>\n<li>Identify material criticality (e.g., via bill of materials linkage, past usage or lead time)</li>\n</ul>\n\nTo calculate the recommended stocking inventory level requirement for a material, the Data Science team has implemented a Markov Chain Monte Carlo (MCMC) bootstrapping statistical model in R. The model is applied to each and every material (typically 3000+) issued across 50+ Shell locations. Each individual material model involves simulating 10,000 MCMC iterations to capture the historical distribution of issues.  Cumulatively, the computational task is large but, fortunately, is one of an embarrassingly parallel nature because the model can be applied independently to each material.\n\n<h2>Existing Setup</h2>\n\nThe full model is currently executed on a 48-core, 192GB RAM standalone physical offline PC. The MCMC bootstrap model is a custom built set of functions which use a number of third-party R packages <code>(\u201cfExtremes\u201d, \u201cismev\u201d, \u201cdplyr\u201d, \u201ctidyr\u201d, \u201cstringr\u201d)</code>.\n\nThe script iterates through each of the Shell locations and distributes the historical material into roughly equally sized groups of materials across the 48 cores. Each core then iteratively applies the model to each individual material. We are grouping the materials because a simple loop for each material would create too much overhead (e.g. starting the R process etc.) as each calculation takes 2-5 seconds. The distribution of the material group jobs across the cores is implemented via the R <a href=\"https://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf\">parallel package</a>. When the last of the individual 48 core jobs complete, the script moves on to the next location and repeats the process. The script takes a total time of approximately 48 hours to calculate the recommended inventory levels for all Shell locations.\n\n<h2>Using Apache Spark on Databricks</h2>\n\nInstead of relying on a single large machine with many cores, Shell decided to use cluster computing to scale out. The new R API in Apache Spark was a good fit for this use case. Two versions of the workload were developed as prototypes to verify scalability and performance of SparkR.\n\n<h2>Prototype I: A proof of concept</h2>\n\nFor the first prototype, we tried to minimize the amount of code change as the goal was to quickly validate that the new SparkR API can handle the workload. We limited all the changes to the simulation step as following:\n\nFor each Shell location list element:\n\n<ol>\n<li>Parallelize input date as a Spark DataFrame</li>\n<li>Use <code>SparkR::gapply()</code> to perform parallel simulation for each of the chunks.</li>\n</ol>\n\nWith limited change to existing simulation code base, we could reduce the total simulation time to 3.97 hours on a 50 node Spark cluster on Databricks.\n\n<h2>Prototype II: Improving performance</h2>\n\nWhile the first prototype was quick to implement, it suffered from one obvious performance bottleneck: a Spark job is launched for every iteration of the simulation. The data is highly skewed and as a result, during each job most executors are waiting idle for the straglers to finish before they can take on more work from the next job. Further, at the beginning of each job, we spend time parallelizing the data as a Spark DataFrame while most of the CPU cores on the cluster are idle.\n\nTo solve these problems, we modified the pre-processing step to produce input and auxiliary date for all locations and material values up-front. Input data was parallelized as a large Spark DataFrame. Next, we used a single <code>SparkR::gapply()</code> call with two keys: location ID and material ID to perform the simulation\n\nWith these simple improvements, we could reduce the simulation time to 45 minutes on a 50 node Spark cluster on Databricks.\n\n<h2>Improvements to SparkR</h2>\n\nSparkR is one of the latest additions to Apache Spark, and the <code>apply</code> API family was the latest addition to SparkR at the time of this work. Through this experiment, we identified a number of limitations and bugs in SparkR and fixed them in Apache Spark.\n\n<ul>\n<li><a href=\"https://issues.apache.org/jira/browse/SPARK-17790\">[SPARK-17790]</a> Support for parallelizing R data.frame larger than 2GB</li>\n<li><a href=\"https://issues.apache.org/jira/browse/SPARK-17919\">[SPARK-17919]</a> Make timeout to RBackend configurable in SparkR</li>\n<li><a href=\"https://issues.apache.org/jira/browse/SPARK-17811\">[SPARK-17811]</a> SparkR cannot parallelize data.frame with NA or NULL in Date columns</li>\n</ul>\n\n<h2>What\u2019s Next?</h2>\n\nIf you are a SparkR developer and you want to explore SparkR, get an account on <a href=\"https://databricks.com/try-databricks\">Databricks today</a> and peruse through our <a href=\"https://docs.databricks.com/spark/latest/sparkr/overview.html\">SparkR documentation</a>."}
{"status": "publish", "description": null, "creator": "jules_damji", "link": "https://databricks.com/blog/2017/06/26/declarative-infrastructure-jsonnet-templating-language.html", "authors": null, "id": 11478, "categories": ["Apache Spark", "Ecosystem", "Engineering Blog", "Platform"], "dates": {"publishedOn": "2017-06-26", "tz": "UTC", "createdOn": "2017-06-26"}, "title": "Declarative Infrastructure with the Jsonnet Templating Language", "slug": "declarative-infrastructure-jsonnet-templating-language", "content": "<em>This blog post is part of our series of internal engineering blogs on Databricks platform, infrastructure management, integration, tooling, monitoring, and provisioning.</em>\n\nAt Databricks engineering, we are avid fans of <a href=\"https://kubernetes.io/\">Kubernetes</a>. Much of our platform infrastructure runs within Kubernetes, whether in AWS cloud or <a href=\"https://databricks.com/solutions/public-sector\">more regulated environments</a>.\n\nHowever, we have found that Kubernetes alone is not enough for managing a complex service infrastructure, which may encompass both resources created in Kubernetes (e.g., <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/pod/\">pods</a>, <a href=\"https://kubernetes.io/docs/concepts/services-networking/service/\">services</a>) and external resources such as <a href=\"http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\">IAM roles</a>. Management complexity comes from (1) need for visibility into the current state of the infrastructure (2) reasoning about how to make changes to the infrastructure.\n\nTo help reduce management complexity, it is desirable for infrastructure to be <a href=\"https://research.google.com/pubs/pub43438.html\"><em>declarative</em></a> (i.e., described by a set of configuration files or templates). The first advantage is that one can easily inspect the target state of infrastructure by reading the configuration files. Second, since the infrastructure is entirely described by the files, changes can be proposed, reviewed, and applied as part of a standard software development workflow.\n\n<h2>What's Missing?</h2>\n\nKubernetes <a href=\"https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/\">YAML configuration files</a> already implement declarative updates to objects in Kubernetes. The user only need edit an object's YAML file and then run <code>$ kubectl apply -f theobject.yaml</code> to sync the changes to Kubernetes. These YAML files may be checked into source control, and if needed the user can query Kubernetes to inspect the difference between the live version and the local file.\n\nHowever, one runs into pain points when attempting to apply this methodology to a larger production environment:\n\n<ol>\n<li>Common structures cannot be shared across YAML files. You typically set up a service not once but several times, perhaps in {dev, staging, prod} environments, across different geographic regions such as {us-west, us-east, asia-pacific}, and Cloud providers {AWS, Azure, GCE}.</p></li>\n<li><p>YAML files must often reference metadata about externally defined entities such as relational databases, networking configurations, or SSL certificates.</p></li>\n<li><p>Complex multi-tier service deployments may require the composition of many distinct resources or YAML files. Updating a deployment composed of many such files becomes burdensome.</p></li>\n</ol>\n\n<p>In this blog post, we describe how we use Google's <a href=\"http://jsonnet.org/\">Jsonnet configuration language</a> to solve these problems, walk through an example of templatizing a service deployment using Jsonnet, and propose a <a href=\"https://github.com/databricks/jsonnet-style-guide\">Jsonnet style guide</a> for the infrastructure templating use case. We've found that Jsonnet is easy to get started with and scales well to complex use cases.\n\n<h2>Jsonnet Usage at Databricks</h2>\n\nLate 2015 we started experimenting with Jsonnet as part of the effort for <a href=\"https://databricks.com/blog/2016/06/07/dce-ga.html\">Databricks Community Edition</a>, our free tier Apache Spark service for education. Since then, Jsonnet has exploded in popularity within Databricks engineering, with over 40,000 lines of Jsonnet in 1000+ distinct files checked into our main development repository. These templates expand to hundreds of thousands of lines of raw YAML once materialized.\n\nTeams at Databricks currently use Jsonnet to manage configurations for Kubernetes resources (including internal configurations of services running on Kubernetes), <a href=\"https://aws.amazon.com/cloudformation/\">AWS CloudFormation</a>, <a href=\"https://www.terraform.io/\">Terraform</a>, <a href=\"https://docs.databricks.com/user-guide/jobs.html\">Databricks Jobs</a>, and also for tasks such as <a href=\"https://en.wikipedia.org/wiki/Transport_Layer_Security\">TLS cert management</a> and defining <a href=\"https://prometheus.io/docs/practices/alerting/\">Prometheus alerts</a>. Over the past two years, Jsonnet has grown to become the de-facto standard configuration language within engineering.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/06/image5-1.png\" alt=\"\" width=\"929\" height=\"459\" class=\"aligncenter size-full wp-image-11483\" />\n\n<h2>Jsonnet Basics</h2>\n\n<a href=\"http://jsonnet.org/\">Jsonnet</a> is a configuration language that helps you define <a href=\"http://www.json.org/\">JSON</a> data. The basic idea is that some JSON fields may be left as variables or expressions that are evaluated at compilation time. For example, the JSON object <code>{\"count\": 4}</code> may be expressed as <code>{count: 2 + 2}</code> in Jsonnet. You can also declare hidden fields with \"::\" that can be referenced during compilation, e.g. <code>{x:: 2, y:: 2, count: $.x + $.y}</code> also evaluates to <code>{\"count\": 4}</code>.\n\nJsonnet objects may be <em>subclassed</em> by concatenating (\"+\") objects together to override field values, e.g. suppose we define the following.\n\n<pre><code class=\"json\">local base = {\n   x:: error \"you must define the value of 'x'\",\n   y:: 2,  // this field has a default value\n   count: $.x + $.y\n};\n\n</code></pre>\n\nThen, the Jsonnet expression <code>(base + {x:: 10})</code> will compile to <code>{\"count\": 12</code>}. In fact, the Jsonnet compiler requires you to override x since the default value raises an error. You can, therefore, think of base as defining an abstract base class in Jsonnet.\n\nJsonnet compilation is completely deterministic and cannot perform external I/O, making it ideal for defining configuration. We've found that Jsonnet strikes the right balance between restrictiveness and flexibility -- previously we generated configurations using Scala code, which erred too much on the side of flexibility, leading to many headaches.\n\nAt Databricks, we extended our <a href=\"https://kubernetes.io/docs/user-guide/kubectl-overview/\">kubectl</a> tool (dubbed kubecfg) so that it can take Jsonnet files directly as arguments. Internally it compiles the Jsonnet to plain JSON / YAML before sending the data to Kubernetes. Kubernetes then creates or updates objects as needed based on the uploaded configuration.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/06/image4-1.png\" alt=\"\" width=\"541\" height=\"172\" class=\"aligncenter size-full wp-image-11482\" />\n\n<h2>Composing Kubernetes Objects with Jsonnet</h2>\n\nTo better understand how Jsonnet can be used with Kubernetes, let's consider the task of deploying an idealized [1] single-tenant \"Databricks platform\" for an enterprise customer. Here we want to create two distinct but related service deployments: a Webapp (for the interactive workspace) and Cluster manager (for managing Spark clusters). In addition, the services need access to an <a href=\"https://aws.amazon.com/rds/\">AWS RDS database</a> to store persistent data.\n\n[1]: Note that we don't actually create individual Jsonnet files for each customer in reality -- that would create a frightening number of Jsonnet files and would in itself be a problem.\n\n<h3>Template for Defining Kubernetes Deployments</h3>\n\nFor these examples, we'll be using <a href=\"https://github.com/databricks/jsonnet-style-guide/blob/master/examples/databricks/service-deployment.jsonnet.TEMPLATE\">service-deployment.jsonnet.TEMPLATE</a>, a simplified version of one of our internal base templates that define a Kubernetes <a href=\"https://kubernetes.io/docs/concepts/services-networking/service/\">service</a> and <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/\">deployment</a> together as a pair. The instantiated service and deployment together constitute a standalone \"production service\" in Kubernetes that can receive network traffic. Note that the template has two required arguments in addition to several optional arguments, including optional config passed to the service binary itself:\n\n<a href=\"https://github.com/databricks/jsonnet-style-guide/blob/master/examples/databricks/service-deployment.jsonnet.TEMPLATE\">service-deployment.jsonnet.TEMPLATE</a>:\n\n<pre><code class=\"json\">// Template for a Kubernetes (service, deployment) pair.\n{\n  // Required arguments for this template\n  serviceName:: error \"serviceName must be specified\",\n  dockerImage:: error \"dockerImage must be specified\",\n\n  // Optional arguments for this template.\n  serviceConf:: {}\n  ...\n}\n</code></pre>\n\n<h3>Example 1: One File for Each Service Deployment</h3>\n\nGiven service-deployment.jsonnet.TEMPLATE, the simplest option we have is to create separate Jsonnet files for the Webapp and Cluster manager. In each file we must import the base template, subclass it, and fill in the required parameters.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/06/image1-8.png\" alt=\"\" width=\"431\" height=\"104\" class=\"aligncenter size-full wp-image-11479\" />\n\nWe specify the service name, the Docker image containing the service binary, and some service specific configurations including the RDS address. In this example, the RDS address is hard-coded, but later on we'll show how to import that metadata from e.g. the output file of running a CloudFormation script.\n\nThe following file describes the manager service (here service takes on its standard meaning, we'll refer to <em>Kubernetes services</em> explicitly as such going on). We also construct service-specific configurations within the Jsonnet template via the <code>serviceConf:: field</code>, which the template passes to the pod as an environment variable. We've found it useful to unify service and Kubernetes configs in this way:\n\n<a href=\"https://github.com/databricks/jsonnet-style-guide/blob/master/examples/databricks/simple/foocorp-manager.jsonnet\">simple/foocorp-manager.jsonnet</a>:\n\n<pre><code class=\"json\">local serviceDeployment = import \"../service-deployment.jsonnet.TEMPLATE\";\n\n// The cluster manager deployment for foocorp.\nserviceDeployment + {\n  serviceName:: \"foocorp-manager\",\n  dockerImage:: \"manager:2.42-rc1\",\n  serviceConf:: {\n    customerName: \"foocorp\",\n    database: \"user-db.databricks.us-west-2.rds.amazonaws.com\",\n  },\n}\n</code></pre>\n\nFor the webapp service to create clusters, it must specify the cluster manager\u2019s <a href=\"https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/\">Kubernetes DNS</a> address, which can be determined ahead of time from the Kubernetes service name:\n\n<a href=\"https://github.com/databricks/jsonnet-style-guide/blob/master/examples/databricks/simple/foocorp-webapp.jsonnet\">simple/foocorp-webapp.jsonnet</a>:\n\n<pre><code class=\"json\">local serviceDeployment = import \"../service-deployment.jsonnet.TEMPLATE\";\n\n// The webapp deployment for foocorp.\nserviceDeployment + {\n  serviceName:: \"foocorp-webapp\",\n  dockerImage:: \"webapp:2.42-rc1\",\n  serviceConf:: {\n    customerName: \"foocorp\",\n    database: \"user-db.databricks.us-west-2.rds.amazonaws.com\",\n    managerAddress: \"foocorp-manager.prod.svc.cluster.local\",\n  },\n}\n</code></pre>\n\nIf you have the <a href=\"https://github.com/databricks/jsonnet-style-guide\">example code repo</a> cloned, you can view the materialized outputs of these templates by running the jsonnet compiler on the files, e.g.:\n\n<code>$ jsonnet examples/databricks/simple/foocorp-webapp.jsonnet</code>\n\nSo what has this gotten us? We've managed to at least remove some of the standard Kubernetes boilerplate around defining services, reducing each deployment definition from over 100 lines to about 10. However, we can do better -- there are still duplicated parameters which would make this pattern hard to maintain if there were many distinct customers or more services required per customer.\n\n<h3>Example 2: Composing Together Deployments in a Single File</h3>\n\nSince both the Webapp and Cluster manager services are deployed together as a unit or <em>shard</em> per customer, it makes sense that there should just be one file per customer describing their unique requirements. It is indeed possible to define a single template, <a href=\"https://github.com/databricks/jsonnet-style-guide/blob/master/examples/databricks/shard-v1/shard.jsonnet.TEMPLATE\">shard.jsonnet.TEMPLATE</a>, that composes together both the Webapp and Cluster manager deployments without duplication of params.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/06/image6-1.png\" alt=\"\" width=\"376\" height=\"194\" class=\"aligncenter size-full wp-image-11484\" />\n\nThe trick here is that the template defines a Kubernetes \"List\" object, which can include multiple Kubernetes resources within one JSON object. We merge the sub-lists produced by the service-deployment templates using the Jsonnet <a href=\"http://jsonnet.org/docs/stdlib.html\">standard library function</a> <code>std.flattenArrays</code>:\n\n<a href=\"https://github.com/databricks/jsonnet-style-guide/blob/master/examples/databricks/shard-v1/shard.jsonnet.TEMPLATE\">shard-v1/shard.jsonnet.TEMPLATE</a>:\n\n<pre><code class=\"json\">local serviceDeployment = import \"../service-deployment.jsonnet.TEMPLATE\";\n\n{\n  // Required arguments\n  customerName:: error \"customerName must be defined\",\n  release:: error \"release must be defined\",\n\n  // Optional arguments\n  commonConf:: {\n    customerName: $.customerName,\n    database: \"user-db.databricks.us-west-2.rds.amazonaws.com\",\n  },\n\n  local webapp = serviceDeployment + ...\n  local manager = serviceDeployment + ...\n\n  kind: \"List\",\n  items: std.flattenArrays([webapp, manager]),\n}\n</code></pre>\n\nNow we can conveniently define FooCorp's entire deployment without any duplicated data, and using only one file:\n\n<a href=\"https://github.com/databricks/jsonnet-style-guide/blob/master/examples/databricks/shard-v1/foocorp-shard.jsonnet\">shard-v1/foocorp-shard.jsonnet</a>:\n\n<pre><code class=\"json\">local shardTemplate = import \"shard.jsonnet.TEMPLATE\";\n\nshardTemplate + {\n  customerName:: \"foocorp\",\n  release:: \"2.42-rc1\",\n}\n</code></pre>\n\n<h3>Example 3: Subclassing Templates for Multiple Environments</h3>\n\nTo see the flexibility of Jsonnet, consider how you might subclass the templates defined in the above examples to create shards that run in the development environment. In dev, deployments should use the dev database instead of the production one, and we also want to run with the latest bleeding-edge Docker images. To do this, we can further subclass the shard.jsonnet.TEMPLATE to different environments. This makes it possible to create specific production or development shards from these specialized templates:\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/06/image3-3.png\" alt=\"\" width=\"618\" height=\"222\" class=\"aligncenter size-full wp-image-11481\" />\n\nTo make the interface exposed by shard.jsonnet.TEMPLATE more explicit, instead of exporting an object we will export a function <code>newShard(name, release, env)</code>, which can be called to construct the shard object. The env object encapsulates the differences between environments (e.g. database URL).\n\n<a href=\"https://github.com/databricks/jsonnet-style-guide/blob/master/examples/databricks/shard-v2/shard.jsonnet.TEMPLATE\">shard-v2/shard.jsonnet.TEMPLATE</a>:\n\n<pre><code class=\"json\">local serviceDeployment = import \"../service-deployment.jsonnet.TEMPLATE\";\n\nlocal newShard(customerName, release, env) = {\n  local commonConf = {\n    customerName: customerName,\n    database: env.database,\n  },\n\n  local webapp = serviceDeployment + {\n    serviceName:: customerName + \"-webapp\",\n    dockerImage:: \"webapp:\" + release,\n    serviceConf:: commonConf + {\n      managerAddress: customerName + \"-manager.prod.svc.cluster.local\",\n    },\n  },\n\n  local manager = serviceDeployment + {\n    serviceName:: customerName + \"-manager\",\n    dockerImage:: \"manager:\" + release,\n    serviceConf:: commonConf,\n  },\n\n  apiVersion: \"v1\",\n  kind: \"List\",\n  items: std.flattenArrays([webapp.items, manager.items]),\n};\n\n// Export the function as a constructor for shards\n{\n  newShard:: newShard,\n}\n</code></pre>\n\nThe dev shard template now just needs to fill in the required env for dev and specify the bleeding-edge release by default. It obtains the env by importing a plain JSON file containing the needed database metadata:\n\n<a href=\"https://github.com/databricks/jsonnet-style-guide/blob/master/examples/databricks/shard-v2/dev-shard.jsonnet.TEMPLATE\">shard-v2/dev-shard.jsonnet.TEMPLATE</a>:\n\n<pre><code class=\"json\">local shardTemplate = import \"shard.jsonnet.TEMPLATE\";\nlocal devEnv = import \"dev-env.json\";\n\nlocal newDevShard(shardName, release=\"bleeding-edge\") = (\n  shardTemplate.newShard(shardName, release, env=devEnv)\n);\n\n{\n  newDevShard:: newDevShard,\n}\n</code></pre>\n\nThe full example code for the prod and dev shard templates can be found here: <a href=\"https://github.com/databricks/jsonnet-style-guide/tree/master/examples/databricks/shard-v2\">https://github.com/databricks/jsonnet-style-guide/tree/master/examples/databricks/shard-v2</a>\n\nNote that the use of functions for subclassing is optional -- we could have instead derived <a href=\"https://github.com/databricks/jsonnet-style-guide/blob/master/examples/databricks/shard-v2/dev-shard.jsonnet.TEMPLATE\">dev-shard.jsonnet.TEMPLATE</a> and <a href=\"https://github.com/databricks/jsonnet-style-guide/blob/master/examples/databricks/shard-v2/prod-shard.jsonnet.TEMPLATE\">prod-shard.jsonnet.TEMPLATE</a> using only field overrides. However, in our experience, the use of ad-hoc field overrides, though powerful, tends to lead to more brittle templates. We've found it useful to establish best practices to limit the use of such constructs in larger templates.\n\n<h2>Jsonnet Style Guide</h2>\n\nIn the above examples, we saw how Jsonnet templates can be used to remove duplication of configuration data between Kubernetes objects, compose multiple deployments together, and reference external entities. This greatly simplifies infrastructure management. In large projects, however, templates themselves may become a source of complexity. We've found this to be manageable with some best practices and investment in tooling around Jsonnet:\n\n<strong>Here are a few Jsonnet best practices we've found:</strong>\n\n<ol>\n<li>For large templates (>10 parameters), avoid directly overriding internal fields when subclassing. Rather, define explicit constructors for templates using Jsonnet functions, which helps with readability and encourages modularity.</li>\n<li>Check your Jsonnet configurations into source control.\n\n<ul>\n<li>Add pre-commit tests to make sure all checked in templates compile. This avoids inadvertent breakages when common templates are updated. You can also create \"unit tests\" using <a href=\"http://jsonnet.org/docs/stdlib.html#assertions_debugging\">Jsonnet assert expressions</a> that assert invariants over template variables.</li>\n<li>Consider also checking in the Jsonnet materialized JSON / YAML to make changes more visible during code review. Because common Jsonnet templates may be imported by many files, changing one file can affect the output of many. Fortunately, unintended changes can be detected at compile time if the materialized YAML is also included with the Jsonnet changes.</li>\n<li>Refactor often -- there's no risk of breakage. Since Jsonnet templates compile to concrete JSON / YAML files, it's possible to check the before and after outputs to ensure the refactoring was correct (i.e. zero net materialized diff).</li>\n</ul></li>\n<li>Push as much configuration to Jsonnet as possible. It may be tempting to introduce logic into services based on some environment flag (e.g. dev vs prod). We've found this to be an anti-pattern -- such configuration should be determined at template compile time rather than at runtime. This principle of <em>hermetic configuration</em> helps prevent surprises when services are deployed to new environments.</li>\n</ol>\n\n<strong>For more detailed recommendations, check out our newly posted <a href=\"https://github.com/databricks/jsonnet-style-guide\">Jsonnet style guide</a></strong>. We also want to welcome any comments or contributions there from the community.\n\n<h2>Conclusion</h2>\n\nIn this blog post, we've shared our experience at Databricks using Jsonnet to simplify infrastructure management. As a unifying templating language that enables composition across a broad range of infrastructure and services, Jsonnet brings us one step closer to a totally declarative infrastructure. We've found Jsonnet easy to use and flexible enough to serve as our primary configuration language, and suggest you try it too!\n\nIf you found this topic interesting, watch out for future blog posts on the tooling we've built around Jsonnet and Kubernetes. We're also <a href=\"https://databricks.com/company/careers\">hiring</a> for Cloud platform, infrastructure, and <a href=\"https://databricks.com/blog/2017/06/07/databricks-serverless-next-generation-resource-management-for-apache-spark.html\">Databricks Serverless</a>."}
{"status": "publish", "description": null, "creator": "jules_damji", "link": "https://databricks.com/blog/2017/06/27/4-sql-high-order-lambda-functions-examine-complex-structured-data-databricks.html", "authors": null, "id": 11499, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2017-06-27", "tz": "UTC", "createdOn": "2017-06-27"}, "title": "4 SQL High-Order and Lambda Functions to Examine Complex and Structured Data in Databricks", "slug": "4-sql-high-order-lambda-functions-examine-complex-structured-data-databricks", "content": "[dbce_cta href=\"https://docs.databricks.com/_static/notebooks/complex-higher-order-functions-python-tutorial.html\"]Try this notebook on Databricks[/dbce_cta]\n\nA couple of weeks ago, we published a <a href=\"https://databricks.com/blog/2017/06/13/five-spark-sql-utility-functions-extract-explore-complex-data-types.html\">short blog</a> and an accompanying <a href=\"https://docs.databricks.com/_static/notebooks/complex-nested-structured.html\">tutorial notebook</a> that demonstrated how to use five Spark SQL utility functions to explore and extract structured and nested data from IoT Devices. Keeping with the same theme, I want to show how you can put to a wide use of the four high-order functions introduced in SQL as part of <a href=\"https://databricks.com/blog/2017/05/24/databricks-runtime-3-0-beta-delivers-enterprise-grade-apache-spark.html\">Databricks Runtime Beta 3.0</a>.\n\nKnowing <em>why</em> offers insight but doesn\u2019t make you productive. Knowing <em>how</em> does. Whereas our in-depth blog explains the <a href=\"https://databricks.com/blog/2017/05/24/working-with-nested-data-using-higher-order-functions-in-sql-on-databricks.html\">concepts and motivations</a> of <em>why</em> handling complex data types such as arrays with high-order functions are important in SQL, this blog is a preamble to <em>how</em> as a notebook tutorial to use high-order functions in SQL in processing structured data and arrays in IoT device events.\n\nIn particular, you can put them to good use if you enjoy functional programming. Far more important, these high-order functions offer three benefits. For instance, you don\u2019t need to:\n\n<ul>\n<li>unpack arrays into individual rows and apply your function and repack them;</li>\n<li>depend on limited built-in functions; and</li>\n<li>write UDFs in Scala or Python.</li>\n</ul>\n\nAll can be done in SQL. For this tutorial, we will explore four SQL functions, and how you can use them to process array types:\n\n<ul>\n<li><a href=\"https://docs.databricks.com/spark/latest/spark-sql/higher-order-functions-lambda-functions.html#transform-array-t-function-t-u-array-u\"><em>transform()</em></a></li>\n<li><a href=\"https://docs.databricks.com/spark/latest/spark-sql/higher-order-functions-lambda-functions.html#filter-array-t-function-t-boolean-array-t\"><em>filter()</em></a></li>\n<li><a href=\"https://docs.databricks.com/spark/latest/spark-sql/higher-order-functions-lambda-functions.html#exists-array-t-function-t-v-boolean-boolean\"><em>exists()</em></a></li>\n<li><a href=\"https://docs.databricks.com/spark/latest/spark-sql/higher-order-functions-lambda-functions.html#aggregate-array-t-b-function-b-t-b-function-b-r-r\"><em>aggregate()</em></a></li>\n</ul>\n\nAgain, as in the <a href=\"https://docs.databricks.com/_static/notebooks/complex-nested-structured.html\">previous tutorial</a>, the takeaway from this tutorial is simple: There exist myriad ways to slice and dice nested JSON structures with Spark SQL utility functions, namely the aforementioned list. These dedicated high-order functions are primarily suited to manipulate arrays in SQL, making the code easier to write and more concise to express when processing table values with arrays or nested arrays.\n\nTo give you a glimpse, consider this nested schema that defines what your IoT events may look like coming down an Apache Kafka stream or deposited in a data source of your choice.\n\n<pre><code class=\"pyton\">from pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nschema = StructType() \\\n          .add(\"dc_id\", StringType()) \\\n          .add(\"source\", MapType(StringType(), StructType() \\\n                        .add(\"description\", StringType()) \\\n                        .add(\"ip\", StringType()) \\\n                        .add(\"id\", IntegerType()) \\\n                        .add(\"temp\", ArrayType(IntegerType())) \\\n                        .add(\"c02_level\", ArrayType(IntegerType())) \\\n                        .add(\"geo\", StructType() \\\n                              .add(\"lat\", DoubleType()) \\\n                              .add(\"long\", DoubleType()))))\n</code></pre>\n\nAnd its corresponding sample DataFrame/Dataset data may look as follows:\n\n<pre><code class=\"python\">dataDF = jsonToDataFrame( \"\"\"{\n    \"dc_id\": \"dc-101\",\n    \"source\": {\n        \"sensor-igauge\": {\n        \"id\": 10,\n        \"ip\": \"68.28.91.22\",\n        \"description\": \"Sensor attached to the container ceilings\",\n        \"temp\":[35,35,35,36,35,35,32,35,30,35,32,35],\n        \"c02_level\": [1475,1475,1473],\n        \"geo\": {\"lat\":38.00, \"long\":97.00}                        \n      },\n      \"sensor-ipad\": {\n        \"id\": 13,\n        \"ip\": \"67.185.72.1\",\n        \"description\": \"Sensor ipad attached to carbon cylinders\",\n        \"temp\": [45,45,45,46,45,45,42,35,40,45,42,45],\n        \"c02_level\": [1370,1370,1372],\n        \"geo\": {\"lat\":47.41, \"long\":-122.00}\n      },\n      \"sensor-inest\": {\n        \"id\": 8,\n        \"ip\": \"208.109.163.218\",\n        \"description\": \"Sensor attached to the factory ceilings\",\n        \"temp\": [40,40,40,40,40,43,42,40,40,45,42,45],\n        \"c02_level\": [1346,1346, 1343],\n        \"geo\": {\"lat\":33.61, \"long\":-111.89}\n      },\n      \"sensor-istick\": {\n        \"id\": 5,\n        \"ip\": \"204.116.105.67\",\n        \"description\": \"Sensor embedded in exhaust pipes in the ceilings\",\n        \"temp\":[30,30,30,30,40,43,42,40,40,35,42,35],\n        \"c02_level\": [1574,1570, 1576],\n        \"geo\": {\"lat\":35.93, \"long\":-85.46}\n      }\n    }\n  }\"\"\", schema)\n</code></pre>\n\nIf you examine the corresponding schema in our Python notebook, you will  see the nested structures: array of integers for <code>temp</code> and <code>c02-level</code>.\n\n<pre><code class=\"python\">root\n |-- dc_id: string (nullable = true)\n |-- source: map (nullable = true)\n |    |-- key: string\n |    |-- value: struct (valueContainsNull = true)\n |    |    |-- description: string (nullable = true)\n |    |    |-- ip: string (nullable = true)\n |    |    |-- id: integer (nullable = true)\n |    |    |-- temp: array (nullable = true)\n |    |    |    |-- element: integer (containsNull = true)\n |    |    |-- c02_level: array (nullable = true)\n |    |    |    |-- element: integer (containsNull = true)\n |    |    |-- geo: struct (nullable = true)\n |    |    |    |-- lat: double (nullable = true)\n |    |    |    |-- long: double (nullable = true)\n</code></pre>\n\nI use a sample of these JSON event data from IoT devices to illustrate how to use these SQL functions. Instead of repeating myself here what I already demonstrated in the notebook, I encourage that you explore the <a href=\"https://docs.databricks.com/_static/notebooks/complex-higher-order-functions-python-tutorial.html\">accompanying notebook</a>, import it into your Databricks workspace, and have a go at it.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/06/image2-5.png\" alt=\"\" width=\"1999\" height=\"789\" class=\"aligncenter size-full wp-image-11501\" />\n\n<h2>What\u2019s Next</h2>\n\nTry the <a href=\"https://docs.databricks.com/_static/notebooks/complex-higher-order-functions-python-tutorial.html\">accompanying tutorial</a> on Databricks. If you have not read our previous related blog and its tutorial on <a href=\"https://databricks.com/blog/2017/06/13/five-spark-sql-utility-functions-extract-explore-complex-data-types.html\">Spark SQL utility</a> functions, do read them. Also, If you don\u2019t have a Databricks account, get <a href=\"https://databricks.com/try-databricks\">one today</a>."}
{"status": "publish", "description": null, "creator": "jules_damji", "link": "https://databricks.com/blog/2017/07/13/serverless-continuous-delivery-with-databricks-and-aws-codepipeline.html", "authors": null, "id": 11639, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2017-07-13", "tz": "UTC", "createdOn": "2017-07-13"}, "title": "Serverless Continuous Delivery with Databricks and AWS CodePipeline", "slug": "serverless-continuous-delivery-with-databricks-and-aws-codepipeline", "content": "Two characteristics commonly mark many companies' success. First, they quickly adapt to new technology. Second, as a result, they gain technological leadership and, in turn, greater market share.  Organizations that can quickly turn insight into action maintain huge advantages over their competition. The key to exploiting analytics in an agile and iterative fashion is a robust continuous integration (CI) and continuous delivery (CD) system that streamlines the transition from development to production for complex data applications.\n\nDatabricks' interactive workspace serves as an ideal environment for collaborative development and interactive analysis. The platform supports all the necessary features to make the creation of a continuous delivery pipeline not only possible but simple. In this blog, we will walk through how to leverage <a href=\"https://databricks.com\">Databricks</a> along with <a href=\"https://aws.amazon.com/codepipeline/\">AWS CodePipeline</a> to deliver a full end-to-end pipeline with serverless CI/CD.\n\n<h2>Four Steps of the Pipeline</h2>\n\nLet\u2019s take a simple scenario. First, a developer working in a notebook makes a commit to the development branch in Github. This automatically triggers CodePipeline to execute four stages in sequence:\n\n<ul>\n<li>Source - Pull the branch from Github</li>\n<li>Staging - Jobs API will launch integration testing job(s) in Databricks* </li>\n<li>Approval - QA will be notified to review via email</li>\n<li>Deployment - Upon successful review, notebooks will be deployed into Databricks using the Workspace API, while libraries are uploaded to S3 for production use</li>\n</ul>\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/07/image3.png\" alt=\"\" width=\"1321\" height=\"750\" class=\"aligncenter size-full wp-image-11642\"/>\n\nSince this example consists of a user committing a Databricks notebook, a build stage isn\u2019t included.  However, CodePipeline supports some of the most popular build systems (Jenkins, Solano, CodeBuild) and their implementation is well documented.\n\n<h2>Source</h2>\n\nThe first step in configuring our pipeline is to specify the source. CodePipeline currently supports 3 sources: Github, S3, and CodeCommit. Since Databricks provides built-in integration to Github, it makes the automation of CodePipeline seamless. The same token used to link Databricks to Github can also be used to link CodePipeline to a specific branch in Github. Data Scientists and engineers can commit <a href=\"https://docs.databricks.com/user-guide/notebooks/github-version-control.html\">Notebooks into GitHub</a> directly from the Databricks workspace. CodePipeline continuously monitors the branch, so that when a Notebook is committed it will automatically trigger our pipeline to run.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/07/image8.png\" alt=\"\" width=\"1006\" height=\"744\" class=\"aligncenter size-full wp-image-11647\" />\n\n<h2>Stage</h2>\n\nAt this point, the code has been committed and it needs to go through integration testing.  A common way to test an Apache Spark job is by running it against a known dataset and comparing the output. A best practice is to set up variables in your notebook that are configured based on your run mode (i.e., prod, test, etc.) and can be set through <a href=\"https://docs.databricks.com/user-guide/notebooks/widgets.html\">Databricks widgets</a>. For example, I set up a notebook for test run mode with a source, target, and mode widgets.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/07/image1.png\" alt=\"\" width=\"1674\" height=\"340\" class=\"aligncenter size-full wp-image-11640\" />\n\nWhen the notebook is run for testing, the source is defined as a test dataset in S3, and the target is a test output location in S3. When the mode is set to \u2018test,\u2019 the notebook returns the S3 location where the data was written, which can be captured by a Databricks <a href=\"https://docs.databricks.com/user-guide/notebooks/notebook-workflows.html\">Utils Notebook run command</a>. This return value is then passed into a DataFrame compare function to provide Pass/Fail status.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/07/image2.png\" alt=\"\" width=\"1636\" height=\"514\" class=\"aligncenter size-full wp-image-11641\" />\n\nThis may sound complicated, but the <a href=\"https://docs.databricks.com/api/latest/jobs.html\">Jobs API</a> makes it really easy to call Jobs and pass in parameters. This stage can actually be executed using only 3 API calls: <a href=\"https://docs.databricks.com/api/latest/jobs.html\">runs now, runs list, and runs get</a>. Run now requires a Job ID and that\u2019s it (job parameters optional). Once the job is running it can  be polled with \u2018runs get.' Which brings me to the next point \u2014 this stage is executed via invoking AWS Lambda functions (I said this was serverless, right?).\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/07/image9.png\" alt=\"\" width=\"1250\" height=\"446\" class=\"aligncenter size-full wp-image-11648\" />\n\nThe only hurdle with this approach is dealing with Lambda\u2019s 300 second duration limit, but fortunately CodePipeline has already resolved this for us with Continuation Tokens. A Lambda function can return any of three statuses back to CodePipeline - Success, Failure, or Continuation.  When a continuation token is returned back to CodePipeline, it invokes the Lambda function again and includes an indicator that the function was re-invoked.\nThis just needs to be monitored so we don\u2019t relaunch an already running job.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/07/image5.png\" alt=\"\" width=\"1220\" height=\"498\" class=\"aligncenter size-full wp-image-11644\" />\n\nThrough the use of continuation tokens, a developer is no longer bound by the 300 second limit and can poll a job indefinitely.  However, it\u2019s still best to set a Timeout for the job.\n\n<h2>Approval</h2>\n\nCodePipeline provides an Approval stage that will send a notification to whomever needs to sign off on the commit before it continues through the pipeline. It allows the developer to send URL links to what needs to be reviewed.  So, along with location of the Github repo, you could also include the URL to the job output within Databricks.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/07/image6.png\" alt=\"\" width=\"1230\" height=\"566\" class=\"aligncenter size-full wp-image-11645\" />\n\nThis is tied to an SNS topic, so you\u2019ll need to first create a subscription in SNS. This essentially just assigns who to send the email to. The pipeline only reaches the approval stage if the testing job passed, so in order to get immediate feedback on failure, it\u2019s best to set alerts on failure for the job within Databricks.\n\n<h2>Deploy</h2>\n\nNow that the code has been approved, it needs to be deployed. Like integration testing, deployment will be executed by invoking a Lambda function. In CodePipeline, the source code is stored in S3 as a zipped artifact with the location passed from stage to stage. So, the first step is to extract the files. Recognizing the memory limits of lambda and knowing that most repos will consist of more than just notebooks and libraries, it\u2019s best to read through the files\u2019 names and only extract files that match the correct extension and file path.\n\nNotebooks will get deployed into a specified Databricks workspace using the Workspace API. The Workspace API provides basic file system functionality to do things like import/export notebooks, create directories, etc. This can be done with two API calls: Import and Mkdirs.  There are a couple of things to take note of when importing notebooks.  First, the content of the notebook is transmitted via base64 encoding, so the data file needs to be encoded:\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/07/image4.png\" alt=\"\" width=\"600\" height=\"150\" class=\"aligncenter size-full wp-image-11643\" />\n\nSecond, the import will fail if the base directory doesn\u2019t already exist. It will return an error response of 400 and an error message stating \u2018RESOURCE_DOES_NOT_EXIST\u2019. Just create the directory and import again.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/07/image7.png\" alt=\"\" width=\"1422\" height=\"426\" class=\"aligncenter size-full wp-image-11646\" />\n\nIn addition to importing libraries into the workspace, Databricks also supports calling libraries directly from <a href=\"https://docs.databricks.com/api/latest/libraries.html#install\">S3</a>. This provides additional options for teams that prefer to work within S3. For example, as we\u2019re reading through file extensions, one could write .jar and .egg files to a designated S3 bucket and key while notebooks are imported into the workspace. S3 versioning should be turned on for this bucket.\n\n<h2>Conclusion</h2>\n\nIn order to keep pace with customers\u2019 evolving needs, companies need to employ an agile development process that can provide value rapidly.  A robust Continuous Delivery pipeline can reduce delivery times while keeping consumers happy.  As demonstrated above, Databricks provides all the tools necessary to integrate with CodePipeline to build a robust, serverless, and cost effective continuous delivery model.\n\nTo learn more Databricks, start a <a href=\"https://databricks.com/try-databricks\">free trial today</a>."}
{"status": "publish", "description": null, "creator": "rxin", "link": "https://databricks.com/blog/2017/07/12/benchmarking-big-data-sql-platforms-in-the-cloud.html", "authors": null, "id": 11660, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2017-07-12", "tz": "UTC", "createdOn": "2017-07-12"}, "title": "Benchmarking Big Data SQL Platforms in the Cloud", "slug": "benchmarking-big-data-sql-platforms-in-the-cloud", "content": "Performance is often a key factor in choosing big data platforms. Given SQL is the lingua franca for big data analysis, we wanted to make sure we are offering one of the most performant SQL platforms in our <a href=\"https://databricks.com/product/databricks\">Unified Analytics Platform</a>.\n\nIn this blog post, we compare <a href=\"https://databricks.com/blog/2017/05/24/databricks-runtime-3-0-beta-delivers-enterprise-grade-apache-spark.html\">Databricks Runtime 3.0</a> (which includes Apache Spark and our DBIO accelerator module) with vanilla open source Apache Spark and Presto on in the cloud using the industry standard <a href=\"http://www.tpc.org/tpcds/\">TPC-DS v2.4</a> benchmark. In addition to the cloud setup, the Databricks Runtime is compared at 10TB scale to a recent <a href=\"https://blog.cloudera.com/blog/2017/04/apache-impala-leads-traditional-analytic-database/\">Cloudera benchmark</a> on Apache Impala using on-premises hardware. In this case, only 77 of the 104 TPC-DS queries are reported in the Impala results published by Cloudera.\n\nThe summary of results reveal that:\n\n<ol>\n<li>Databricks Runtime 3.0 outperforms vanilla Spark on AWS by 5X using the same hardware specs.</li>\n<li>Databricks outperforms Presto by 8X. While Presto could run only 62 out of 104 queries, Databricks ran all.</li>\n<li>Databricks not only outperforms the on-premise Impala by 3X on the queries picked in the Cloudera report, but also benefits from S3 storage elasticity, compared to fixed-physical disks on-premise.</li>\n</ol>\n\nTo reproduce this benchmark, you can get all the scripts from <a href=\"https://github.com/databricks/benchmarks/tree/master/tpc-ds-2.4\">here</a>.\n\n<h2>TPC-DS</h2>\n\nCreated by a third-party committee, TPC-DS is the de-facto industry standard benchmark for measuring the performance of decision support solutions. According to its <a href=\"http://www.tpc.org/tpcds/\">own homepage</a>, it defines decision support systems as those that examine large volumes of data, give answers to real-world business questions, execute SQL queries of various operational requirements and complexities (e.g., ad-hoc, reporting, iterative OLAP, data mining), and are characterized by high CPU and IO load.\n\nThis benchmark includes 104 queries that exercise a large part of the SQL 2003 standards - 99 queries of the TPC-DS benchmark, four of which with two variants (14, 23, 24, 39) and \u201cs_max\u201d query performing a full scan and aggregation of the biggest table, store_sales. As discussed in an <a href=\"https://databricks.com/blog/2016/07/26/introducing-apache-spark-2-0.html\">earlier blog post</a>, Spark SQL is one of the few open source SQL engines that are capable of running all TPC-DS queries without modification.\n\n<h2>Databricks Runtime vs Vanilla Apache Spark</h2>\n\nWe conducted this experiment using the latest Databricks Runtime 3.0 release and compared it with a Spark cluster setup on another popular cloud data platform for AWS. Databricks Runtime augments Spark with an IO layer (DBIO) that enables optimized access to cloud storage (in this case S3).\n\nCloud storage for optimal Spark performance is different from Spark on-prem HDFS, as the cloud storage IO semantics can introduce network latencies or file inconsistencies \u2014 in some cases unsuitable for big data software. But with Spark on Databricks, we eliminate both.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/07/image5-1.png\" alt=\"\" width=\"778\" height=\"443\" class=\"aligncenter size-full wp-image-11665\" />\n\nAs illustrated above, <strong>Spark on Databricks performed roughly 5X better in total runtime and 4X better</strong> in <a href=\"https://en.wikipedia.org/wiki/Geometric_mean\">geometric mean</a>. Next, we explain more details of the benchmark setup.\n\n<strong>Hardware Configuration</strong>: We used the following setup on Amazon EC2:\n\n<ul>\n<li>Machine type: 11 r3.xlarge nodes (10 workers and 1 driver)</li>\n<li>CPU core count: 44 virtual cores (22 physical cores)</li>\n<li>System memory: 335 GB</li>\n<li>Total local disk space for shuffle: 880 GB (benchmark data stored as Parquet on S3)</li>\n<li>Networking performance is described as \u201cModerate\u201d by Amazon</li>\n</ul>\n\n<strong>Dataset:</strong> TPC-DS 1,000 scale factor, on S3. We chose this instead of scale factor 10000 because Presto, compared in the next section, had severe issues scaling up.\n\n<strong>Query Rewrites:</strong> No query rewrite was done. Both Spark SQL flavors were capable of running all 104 queries.\n\n<strong>Configuration Tuning:</strong> We ran the benchmark using out-of-the-box configuration on Databricks, and with additional manual tuning on the AWS cluster. We initially ran this benchmark on the competing platform using its default configurations but found the performance to be below our expectations. We then did some manual tuning to match the configurations on Databricks so Spark on AWS would perform better. The additional configuration on the non-Databricks platform can be found <a href=\"https://github.com/databricks/benchmarks/blob/master/tpc-ds-2.4/emr-spark/emr-cluster-setup.sh\">here</a> and <a href=\"https://github.com/databricks/benchmarks/blob/master/tpc-ds-2.4/emr-spark/spark-defaults.json\">here</a>.\n\nTo further analyze the query results, we also divided the queries into three categories:\n\n<ol>\n<li><strong>Interactive queries:</strong> Queries in this category complete within 1 mins. In this category, Databricks Runtime 3.0 is 3X faster.</li>\n<li><strong>Reporting queries:</strong> Queries in this category complete within 3 mins. In this category, Databricks Runtime 3.0 is 4X faster.</li>\n<li><strong>Deep analytics queries:</strong> Long running queries that can take an hour or more. In this category, Databricks Runtime 3.0 is 5X faster.</li>\n</ol>\n\nBecause interactive queries were bottlenecked by the latency of metadata discovery, we observed only a 3X speedup, whereas the reporting and deep analytics queries benefited immensely from optimized DBIO. Future versions of DBIO will also improve the latency of metadata discovery substantially to improve interactive queries even more.\n\n<h2>Databricks Runtime vs Presto</h2>\n\nUsing the same hardware configuration, we also compared Databricks Runtime with Presto on AWS, using the same vendor to set up Presto clusters.\n\n<strong>Hardware Configuration:</strong> Same as above (11 r3.xlarge nodes)\n\n<strong>Dataset:</strong> TPC-DS 1,000 scale factor, on S3\n\n<strong>Query Rewrites:</strong> We had to rewrite some queries for Presto due to the lack of support for grouping function for a rollup. Even with some minor rewrites, only 62 queries could complete on Presto. The rest either crashed the system or returned no result. This explains why the total runtime on Presto is smaller than the total runtime for vanilla Spark from the previous section, as the total runtime for Presto does not take into account the failing queries.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/07/image7-1.png\" alt=\"\" width=\"926\" height=\"229\" class=\"aligncenter size-full wp-image-11667\" />\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/07/image3-1.png\" alt=\"\" width=\"916\" height=\"229\" class=\"aligncenter size-full wp-image-11663\" />\n\nAs illustrated above, Spark SQL on Databricks completed all 104 queries, versus the 62 by Presto. Comparing only the 62 queries Presto was able to run, Databricks Runtime performed 8X better in geometric mean than Presto. <strong>Databricks Runtime is 8X faster than Presto, with richer ANSI SQL support.</strong>\n\n<h2>Databricks in the Cloud vs Apache Impala On-prem</h2>\n\nApache Impala is another popular query engine in the big data space, used primarily by Cloudera customers. Cloudera publishes benchmark numbers for the Impala engine themselves. The <a href=\"https://blog.cloudera.com/blog/2017/04/apache-impala-leads-traditional-analytic-database/\">most recent benchmark</a> was published two months ago by Cloudera and ran only 77 queries out of the 104.\n\nIn this experiment, we asked ourselves: how does the Databricks Runtime in a cloud setup compares to the Impala results on physical hardware? What if we compare using out-of-the-box configuration of Databricks to Impala tuned by the engineering team behind the product, and the set of cherry-picked queries? Moreover, what is the performance of Spark on S3 to Impala with physical disks? This section presents the result from this experiment.\n\n<strong>Hardware Configuration:</strong>\n\n<table>\n  <thead>\n  <tr>\n    <th></th>\n    <th>Databricks Runtime</th>\n    <th>Cloudera Impala</th>\n  </tr>\n  </thead>\n  <tbody>\n  <tr>\n    <th>CPU core count</th>\n    <td>144 (288 AWS vCPUs)</td>\n    <td>280</td>\n  </tr>\n  <tr>\n    <th>Memory (GB)</th>\n    <td>2196</td>\n    <td>1792</td>\n  </tr>\n  <tr>\n    <th>Local Disk (TB)</th>\n    <td>68</td>\n    <td>112</td>\n  </tr>\n  <tr>\n    <th>Data storage</th>\n    <td>S3 (decoupled storage and compute)</td>\n    <td>HDFS (local disks)</td>\n  </tr>\n  <tr>\n    <th>Machine details</th>\n    <td>18 cloud i3.4xlarge</td>\n    <td>7 on-prem nodes</td>\n  </tr>\n </tbody>\n</table>\n\n<strong>Dataset:</strong> For Databricks, TPC-DS 10000 scale factor, on S3. For Impala, on HDFS.\n\n<strong>Query Rewrites:</strong> None, but the set of 77 queries selected by the Cloudera team excluded some of the most demanding queries in TPC-DS.\n\n<strong>Configuration Tuning:</strong> None on Databricks; we ran with out-of-the-box configuration. Unknown for what was done in the Cloudera benchmark, as it was not reported (look at the comments).\n\nAll 104 queries completed at the 10000 scale factor in 19990 seconds. The following chart compares the runtime for the 77 queries picked by Cloudera in their report:\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/07/image6-1.png\" alt=\"\" width=\"926\" height=\"229\" class=\"aligncenter size-full wp-image-11666\" />\n\nIn the case where we take the number of CPUs as a normalization factor, Databricks Runtime, using commodity hardware in the cloud, is 3X more efficient than Impala:\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/07/image2-1.png\" alt=\"\" width=\"936\" height=\"258\" class=\"aligncenter size-full wp-image-11662\" />\n\nDatabricks Runtime achieves better performance to the numbers published by Cloudera on Impala, on queries picked by Impala\u2019s engineering team, using a cluster with only half of the physical CPU cores. One important factor these numbers alone don\u2019t highlight is that Databricks experiment was run against data in S3, using decoupled storage and compute, which adds elasticity and ease of management compared to local disks, as done in the Impala benchmark.\n\nIn an <a href=\"https://databricks.com/blog/2017/05/31/top-5-reasons-for-choosing-s3-over-hdfs.html\">earlier blog post comparing S3 vs HDFS</a>, we came to the conclusion that S3 has a much lower total cost of ownership, while HDFS might have better performance on a per node basis. This benchmark result has shown that with our optimizations, it is possible to have the best of both worlds: flexibility and lower TCO of the cloud, with better than on-prem performance, and with a broader set of ANSI SQL support.\n\n<h2>Conclusion</h2>\n\nThis blog post reports the benchmark we have conducted comparing Databricks Runtime 3.0 with other big data engines, including vanilla Apache Spark and Presto in the cloud. Even after improving the Spark configuration in Spark on AWS, Databricks Runtime outperforms vanilla Spark by 5X using the same hardware specs.\n\nWhen compared to Presto, Databricks Runtime performed 8X better, while being able to run all queries. Presto could run only 62 out of the 104 queries, while Spark was able to run the 104 unmodified in both vanilla open source version and in Databricks.\n\nAdditionally to the cloud results, we have compared our platform to a recent Impala 10TB scale result set by Cloudera. While the results were from an on-prem cluster, Databricks Runtime outperforms the on-premises Impala by 3X on the queries picked in the report to the same number of CPU cores. Databricks Runtime test uses S3 as storage with the additional cloud elasticity leading to lower TCO than on-prem.\n\nTo leverage the latest performance optimizations in Databricks Runtime 3.0, <a href=\"https://databricks.com/try-databricks\">sign up for a Databricks account</a>."}
{"status": "publish", "description": null, "creator": "michael", "link": "https://databricks.com/blog/2017/07/11/introducing-apache-spark-2-2.html", "authors": null, "id": 11696, "categories": ["Apache Spark", "Engineering Blog"], "dates": {"publishedOn": "2017-07-11", "tz": "UTC", "createdOn": "2017-07-11"}, "title": "Introducing Apache Spark 2.2", "slug": "introducing-apache-spark-2-2", "content": "Today we are happy to announce the availability of <a href=\"http://spark.apache.org/releases/spark-release-2-2-0.html\">Apache Spark 2.2.0</a> on Databricks as part of the Databricks Runtime 3.0.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/07/Pasted-image-at-2017_07_11-02_09-PM.png\" alt=\"\" width=\"411\" height=\"95\" class=\"aligncenter size-full wp-image-11701\" />\n\nThis release marks a major milestone for Structured Streaming by marking it as production ready and removing the experimental tag. In this release, we also support for arbitrary stateful operations in a stream, and Apache Kafka 0.10 support for both reading and writing using the streaming and batch APIs. In addition to extending new functionality to SparkR, Python, MLlib, and GraphX, the release focuses on usability, stability, and refinement, resolving over 1100 tickets.\n\nThis blog post discusses some of the high-level changes, improvements and bug fixes:\n\n<ul>\n<li>Production ready Structured Streaming</li>\n<li>Expanding SQL functionalities</li>\n<li>New distributed machine learning algorithms in R</li>\n<li>Additional Algorithms in MLlib and GraphX</li>\n</ul>\n\n<h2>Structured Streaming</h2>\n\nIntroduced in Spark 2.0, <a href=\"https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html\">Structured Streaming</a> is a high-level API for building <a href=\"https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html\">continuous applications</a>. Our goal is to make it easier to build end-to-end streaming applications, which integrate with storage, serving systems, and batch jobs in a consistent and fault-tolerant way.\n\nThe third release in 2.x line, Spark 2.2 declares Structured Streaming as production ready, <strong>meaning removing the experimental tag</strong>, with additional high-level changes:\n\n<ul>\n<li><strong>Kafka Source and Sink:</strong> Support for <a href=\"https://databricks.com/blog/2017/04/26/processing-data-in-apache-kafka-with-structured-streaming-in-apache-spark-2-2.html\">reading and writing data in streaming or batch to and from Apache Kafka</a></li>\n<li><strong>Kafka Improvements:</strong> Cached producer for lower latency Kafka to Kafka streams</li>\n<li><strong>Additional Stateful APIs:</strong> Support for complex stateful processing and timeouts using <em>[flat]MapGroupsWithState</em></li>\n<li><strong>Run Once Triggers:</strong> Allows to <a href=\"https://databricks.com/blog/2017/05/22/running-streaming-jobs-day-10x-cost-savings.html\">trigger only one-time execution</a>, hence lowering the cost of clusters</li>\n</ul>\n\nAt Databricks, we religiously believe in dogfooding. Using a release candidate version of Spark 2.2, we have ported some of our <a href=\"https://spark-summit.org/2017/events/easy-scalable-fault-tolerant-stream-processing-with-structured-streaming-in-apache-spark/\">internal data pipelines</a> as well as worked with some of our customers to port their <a href=\"https://databricks.com/blog/2017/05/18/taking-apache-sparks-structured-structured-streaming-to-production.html\">production pipelines using Structured Streaming</a>.\n\n<h2>SQL and Core APIs</h2>\n\nSince Spark 2.0 release, Spark is now one of the most feature-rich and standard-compliant SQL query engine in the Big Data space. It can connect to a variety of data sources and perform SQL-2003 feature sets such as <a href=\"https://databricks.com/blog/2016/06/17/sql-subqueries-in-apache-spark-2-0.html\">analytic functions and subqueries</a>. Spark 2.2 adds a number of SQL functionalities:\n\n<ul>\n<li><strong>API Updates:</strong> Unify CREATE TABLE syntax for data source and hive serde tables and add broadcast hints such as BROADCAST, BROADCASTJOIN, and MAPJOIN for SQL Queries</li>\n<li><strong>Overall Performance and stability:</strong>\n\n<ul>\n<li>Cost-based optimizer cardinality estimation for filter, join, aggregate, project and limit/sample operators and Cost-based join re-ordering</li>\n<li>TPC-DS performance improvements using star-schema heuristics</li>\n<li>File listing/IO improvements for CSV and JSON</li>\n<li>Partial aggregation support of HiveUDAFFunction</li>\n<li>Introduce a JVM object based aggregate operator</li>\n</ul></li>\n<li><strong>Other notable changes:</strong>\n\n<ul>\n<li>Support for parsing multi-line JSON and CSV files</li>\n<li>Analyze Table Command on partitioned tables</li>\n<li>Drop Staging Directories and Data Files after completion of Insertion/CTAS against Hive-serde Tables</li>\n</ul></li>\n</ul>\n\n<h2>MLlib, SparkR, and Python</h2>\n\nThe last major set of changes in Spark 2.2 focuses on advanced analytics and Python. Now you can install <a href=\"https://pypi.python.org/pypi/pyspark\">PySpark from PyPI</a> package using pip install. To boost advanced analytics, a few new algorithms were added to MLlib and GraphX:\n\n<ul>\n<li>Locality Sensitive Hashing</li>\n<li>Multiclass Logistic Regression</li>\n<li>Personalized PageRank</li>\n</ul>\n\nSpark 2.2 also adds support for the following distributed algorithms in SparkR:\n\n<ul>\n<li>ALS</li>\n<li>Isotonic Regression</li>\n<li>Multilayer Perceptron Classifier</li>\n<li>Random Forest</li>\n<li>Gaussian Mixture Model</li>\n<li>LDA</li>\n<li>Multiclass Logistic Regression</li>\n<li>Gradient Boosted Trees</li>\n<li>Structured Streaming API for R</li>\n<li>column functions <em>to_json</em>, <em>from_json</em> for R</li>\n<li>Multi-column approxQuantile in R</li>\n</ul>\n\nWith the addition of these algorithms, SparkR has become the most comprehensive library for distributed machine learning on R.\n\nWhile this blog post only covered some of the major features in this release, you can read the official <a href=\"http://spark.apache.org/releases/spark-release-2-2-0.html\">release notes</a> to see the complete list of changes.\n\nIf you want to try out these new features, you can use Spark 2.2 in Databricks Runtime 3.0. Sign up for a <a href=\"http://databricks.com/try\">free trial account here</a>."}
{"status": "publish", "description": null, "creator": "jules_damji", "link": "https://databricks.com/blog/2017/07/19/integrating-apache-airflow-with-databricks.html", "authors": null, "id": 11719, "categories": ["Apache Spark", "Ecosystem", "Engineering Blog", "Platform"], "dates": {"publishedOn": "2017-07-19", "tz": "UTC", "createdOn": "2017-07-19"}, "title": "Integrating Apache Airflow with Databricks", "slug": "integrating-apache-airflow-with-databricks", "content": "<em>This blog post is part of our series of internal engineering blogs on Databricks platform, infrastructure management, integration, tooling, monitoring, and provisioning.</em>\n\n<hr />\n\nToday, we are excited to announce native Databricks integration in <a href=\"https://airflow.incubator.apache.org/\">Apache Airflow</a>, a popular open source workflow scheduler. This blog post illustrates how you can set up Airflow and use it to trigger Databricks jobs.\n\nOne very popular feature of Databricks' <a href=\"https://databricks.com/unified-analytics-platform\">Unified Analytics Platform</a> (UAP) is the ability to convert a data science notebook directly into production jobs that can be run regularly. While this feature unifies the workflow from exploratory data science to production data engineering, some data engineering jobs can contain complex dependencies that are difficult to capture in notebooks. To support these complex use cases, we provide REST APIs so jobs based on notebooks and libraries can be triggered by external systems. Of these, one of the most common schedulers used by our customers is Airflow. We are happy to share that we have also extended Airflow to support Databricks out of the box.\n\n<h2>Airflow Basics</h2>\n\nAirflow is a generic workflow scheduler with dependency management. Besides its ability to schedule periodic jobs, Airflow lets you express explicit dependencies between different stages in your data pipeline.\n\nEach ETL pipeline is represented as a directed acyclic graph (DAG) of tasks (not to be mistaken with Spark\u2019s own DAG scheduler and tasks). Dependencies are encoded into the DAG by its edges \u2014 for any given edge, the downstream task is only scheduled if the upstream task completed successfully. For example, in the example, DAG below, task <strong>B</strong> and <strong>C</strong> will only be triggered after task <strong>A</strong> completes successfully. Task <strong>D</strong> will then be triggered when task <strong>B</strong> and <strong>C</strong> both complete successfully.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/07/image1-2.png\" alt=\"\" width=\"431\" height=\"201\" class=\"aligncenter size-full wp-image-11720\" />\n\nThe tasks in Airflow are instances of \u201coperator\u201d class and are implemented as small Python scripts. Since they are simply Python scripts, operators in Airflow can perform many tasks: they can poll for some precondition to be true (also called a sensor) before succeeding, perform ETL directly, or trigger external systems like Databricks.\n\n<blockquote>\n  For more information on Airflow, please take a look at their <a href=\"http://airflow.readthedocs.io/en/latest/\">documentation</a>.\n</blockquote>\n\n<h2>Native Databricks Integration in Airflow</h2>\n\nWe implemented an Airflow operator called <a href=\"http://airflow.readthedocs.io/en/latest/integration.html#databrickssubmitrunoperator\"><em>DatabricksSubmitRunOperator</em></a>, enabling a smoother integration between Airflow and Databricks. Through this operator, we can hit the Databricks <a href=\"https://docs.databricks.com/api/latest/jobs.html\">Runs Submit API</a> endpoint, which can externally trigger a single run of a jar, python script, or notebook. After making the initial request to submit the run, the operator will continue to poll for the result of the run. When it completes successfully, the operator will return allowing for downstream tasks to run.\n\nWe\u2019ve contributed the <em>DatabricksSubmitRunOperator</em> upstream to the open-source Airflow project. However, the integrations will not be cut into a release branch until Airflow 1.9.0 is released. Until then, to use this operator you can install Databricks\u2019 fork of Airflow, which is essentially Airflow version 1.8.1 with our <em>DatabricksSubmitRunOperator</em> patch applied.\n\n<pre><code>pip install --upgrade \"git+git://github.com/databricks/incubator-airflow.git@1.8.1-db1#egg=apache-airflow[databricks]\"\n</code></pre>\n\n<h2>Airflow with Databricks Tutorial</h2>\n\nIn this tutorial, we\u2019ll set up a toy Airflow 1.8.1 deployment which runs on your local machine and also deploy an example DAG which triggers runs in Databricks.\n\nThe first thing we will do is initialize the sqlite database. Airflow will use it to track miscellaneous metadata. In a production Airflow deployment, you\u2019ll want to edit the configuration to point Airflow to a <a href=\"https://airflow.incubator.apache.org/configuration.html#setting-configuration-options\">MySQL or Postgres database</a> but for our toy example, we\u2019ll simply use the default sqlite database. To perform the initialization run:\n\n<pre><code>airflow initdb\n</code></pre>\n\nThe SQLite database and default configuration for your Airflow deployment will be initialized in <code>~/airflow</code>.\n\nIn the next step, we\u2019ll write a DAG that runs two Databricks jobs with one linear dependency. The first Databricks job will trigger a notebook located at <code>/Users/airflow@example.com/PrepareData</code>, and the second will run a jar located at <code>dbfs:/lib/etl-0.1.jar</code>. To save time, we\u2019ve already gone ahead and written the DAG for you <a href=\"https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/example_dags/example_databricks_operator.py\">here</a>.\n\nFrom a mile high view, the script DAG essentially constructs two <em>DatabricksSubmitRunOperator</em> tasks and then sets the dependency at the end with the <a href=\"https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/example_dags/example_databricks_operator.py#L82\">set_dowstream method</a>. A skeleton version of the code looks something like this:\n\n<pre><code class=\"python\">notebook_task = DatabricksSubmitRunOperator(\n    task_id='notebook_task',\n    \u2026)\n\nspark_jar_task = DatabricksSubmitRunOperator(\n    task_id='spark_jar_task',\n    \u2026)\nnotebook_task.set_downstream(spark_jar_task)\n</code></pre>\n\nIn reality, there are some other details we need to fill in to get a working DAG file. The first step is to set some default arguments which will be applied to each task in our DAG.\n\n<pre><code class=\"json\">args = {\n    'owner': 'airflow',\n    'email': ['airflow@example.com'],\n    'depends_on_past': False,\n    'start_date': airflow.utils.dates.days_ago(2)\n}\n</code></pre>\n\nThe two interesting arguments here are <a href=\"https://airflow.incubator.apache.org/code.html?highlight=depends_on_past#baseoperator\">depends_on_past</a> and <a href=\"https://airflow.incubator.apache.org/code.html?highlight=start_date#baseoperator\">start_date</a>. If depends_on_past is true, it signals Airflow that a task should not be triggered unless the previous instance of a task completed successfully. The start_date argument determines when the first task instance will be scheduled.\n\nThe next section of our DAG script actually instantiates the <a href=\"https://airflow.incubator.apache.org/code.html?highlight=dag#airflow.models.DAG\">DAG</a>.\n\n<pre><code class=\"python\">dag = DAG(\n    dag_id='example_databricks_operator', default_args=args,\n    schedule_interval='@daily')\n</code></pre>\n\nIn this DAG, we give it a unique ID, attach the default arguments we declared earlier, and give it a daily schedule. Next, we\u2019ll specify the specifications of the cluster that will run our tasks.\n\n<pre><code class=\"json\">new_cluster = {\n    'spark_version': '2.1.0-db3-scala2.11',\n    'node_type_id': 'r3.xlarge',\n    'aws_attributes': {\n        'availability': 'ON_DEMAND'\n    },\n    'num_workers': 8\n}\n</code></pre>\n\nThe schema of this specification matches the new cluster field of the <a href=\"https://docs.databricks.com/api/latest/jobs.html#runs-submit\">Runs Submit</a> endpoint. For your example DAG, you may want to decrease the number of workers or change the instance size to something smaller.\n\nFinally, we\u2019ll instantiate the <em>DatabricksSubmitRunOperator</em> and register it with our DAG.\n\n<pre><code class=\"json\">notebook_task_params = {\n    'new_cluster': new_cluster,\n    'notebook_task': {\n        'notebook_path': '/Users/airflow@example.com/PrepareData',\n    },\n}\n</code></pre>\n\n<pre><code class=\"python\"># Example of using the JSON parameter to initialize the operator.\nnotebook_task = DatabricksSubmitRunOperator(\n    task_id='notebook_task',\n    dag=dag,\n    json=notebook_task_params)\n</code></pre>\n\nIn this piece of code, the JSON parameter takes a python dictionary that matches the <a href=\"https://docs.databricks.com/api/latest/jobs.html#runs-submit\">Runs Submit</a> endpoint.\n\nTo add another task downstream of this one, we do instantiate the <em>DatabricksSubmitRunOperator</em> again and use the special <a href=\"https://airflow.incubator.apache.org/code.html?highlight=dag#airflow.models.DAG.set_dependency\">set_downstream</a> method on the notebook_task operator instance to register the dependency.\n\n<pre><code class=\"python\"># Example of using the named parameters of DatabricksSubmitRunOperator\n# to initialize the operator.\nspark_jar_task = DatabricksSubmitRunOperator(\n    task_id='spark_jar_task',\n    dag=dag,\n    new_cluster=new_cluster,\n    spark_jar_task={\n        'main_class_name': 'com.example.ProcessData'\n    },\n    libraries=[\n        {\n            'jar': 'dbfs:/lib/etl-0.1.jar'\n        }\n    ]\n)\n\nnotebook_task.set_downstream(spark_jar_task)\n</code></pre>\n\nThis task runs a jar located at <code>dbfs:/lib/etl-0.1.jar.</code>\n\nNotice that in the notebook_task, we used the JSON parameter to specify the full specification for the submit run endpoint and that in the spark_jar_task, we flattened the top level keys of the submit run endpoint into parameters for the <em>DatabricksSubmitRunOperator</em>. Although both ways of instantiating the operator are equivalent, the latter method does not allow you to use any new top level fields like <a href=\"https://docs.databricks.com/api/latest/jobs.html#request-structure\">spark_python_task</a> or <a href=\"https://docs.databricks.com/api/latest/jobs.html#request-structure\">spark_submit_task</a>. For more detailed information about the full API of <em>DatabricksSubmitRunOperator</em>, please look at the <a href=\"https://airflow.readthedocs.io/en/latest/integration.html#databrickssubmitrunoperator\">documentation here</a>.\n\nNow that we have our DAG, to install it in Airflow create a directory in <code>~/airflow</code> called <code>~/airflow/dags</code> and copy the DAG into that directory.\n\nAt this point, Airflow should be able to pick up the DAG.\n\n<pre><code>$ airflow list_dags                                                           [10:27:13]\n[2017-07-06 10:27:23,868] {__init__.py:57} INFO - Using executor SequentialExecutor\n[2017-07-06 10:27:24,238] {models.py:168} INFO - Filling up the DagBag from /Users/andrew/airflow/dags\n\n\n-------------------------------------------------------------------\nDAGS\n-------------------------------------------------------------------\nexample_bash_operator\nexample_branch_dop_operator_v3\nexample_branch_operator\nexample_databricks_operator\n</code></pre>\n\nWe can also visualize the DAG in the web UI. To start it up, run airflow webserver and connect to localhost:8080. Clicking into the \u201cexample_databricks_operator,\u201d you\u2019ll see many visualizations of your DAG. Here is the example:\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/07/image2-2.png\" alt=\"\" width=\"1999\" height=\"1002\" class=\"aligncenter size-full wp-image-11721\" />\n\nAt this point, a careful observer might also notice that we don\u2019t specify information such as the hostname, username, and password to a Databricks shard anywhere in our DAG. To configure this we use the <a href=\"https://airflow.incubator.apache.org/concepts.html?highlight=connections#connections\">connection</a> primitive of Airflow that allows us to reference credentials stored in a database from our DAG. By default, all <em>DatabricksSubmitRunOperator</em> set the <a href=\"http://airflow.readthedocs.io/en/latest/integration.html?highlight=databricks_conn_id#databrickssubmitrunoperator\">databricks_conn_id</a> parameter to \u201cdatabricks_default,\u201d so for our DAG, we\u2019ll have to add a connection with the ID \u201cdatabricks_default.\u201d\n\nThe easiest way to do this is through the web UI. Clicking into the \u201cAdmin\u201d on the top and then \u201cConnections\u201d in the dropdown will show you all your current connections. For our use case, we\u2019ll add a connection for \u201cdatabricks_default.\u201d The final connection should look something like this:\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/07/image3-2.png\" alt=\"\" width=\"1999\" height=\"1045\" class=\"aligncenter size-full wp-image-11722\" />\n\nNow that we have everything set up for our DAG, it\u2019s time to test each task. To do this for the notebook_task we would run, <code>airflow test example_databricks_operator notebook_task 2017-07-01</code> and for the <code>spark_jar_task</code> we would run <code>airflow test example_databricks_operator spark_jar_task 2017-07-01.</code> To run the DAG on a schedule, you would invoke the scheduler daemon process with the command airflow scheduler.\n\nIf everything goes well, after starting the scheduler, you should be able to see backfilled runs of your DAG start to run in the web UI.\n\n<h2>Next Steps</h2>\n\nIn conclusion, this blog post provides an easy example of setting up Airflow integration with Databricks. It demonstrates how Databricks extension to and integration with Airflow allows access via Databricks <a href=\"https://docs.databricks.com/api/latest/jobs.html#runs-submit\">Runs Submit</a> API to invoke computation on the Databricks platform. For more detailed instructions on how to set up a production Airflow deployment, please look at the official <a href=\"https://airflow.incubator.apache.org/configuration.html\">Airflow documentation</a>.\n\nAlso, if you want to try this tutorial on Databricks, sign up for a <a href=\"https://databricks.com/try-databricks\">free trial today</a>."}
{"status": "publish", "description": null, "creator": "jules_damji", "link": "https://databricks.com/blog/2017/07/26/breaking-the-curse-of-dimensionality-in-genomics-using-wide-random-forests.html", "authors": null, "id": 11751, "categories": ["Apache Spark", "Ecosystem", "Engineering Blog", "Machine Learning"], "dates": {"publishedOn": "2017-07-26", "tz": "UTC", "createdOn": "2017-07-26"}, "title": "Breaking the \u201ccurse of dimensionality\u201d in Genomics using \u201cwide\u201d Random Forests", "slug": "breaking-the-curse-of-dimensionality-in-genomics-using-wide-random-forests", "content": "<em>This is a guest blog from members of <a href=\"https://www.csiro.au/\">CSIRO\u2019s</a> transformational bioinformatics team in Sydney, Australia. CSIRO, Australia\u2019s government research agency, is in the top 1% of global research institutions with inventions like fast WiFi, the Hendra virus vaccine, and polymer banknotes. It is their technical account of a scalable <a href=\"https://github.com/aehrc/VariantSpark\">VariantSpark</a> toolkit for genomic analysis at scale using Apache Spark on Databricks.</em>\n\n[dbce_cta href=\"https://aehrc.github.io/VariantSpark/notebook-examples/VariantSpark_HipsterIndex.html\"]Try this notebook in Databricks[/dbce_cta]\n\nWe are in the midst of the digital revolution where consumers and businesses demand decisions be based on evidence collected from data. The resulting datafication of almost everything produces datasets that are not only growing <strong>vertically</strong>, by capturing more events, but also <strong>horizontally</strong> by capturing more information about these events.\n\nThe challenge of big and \u201cwide'' data is especially pronounced in the health and bioinformatics space where, for example, whole genome sequencing (WGS) technology enables researchers to interrogate all 3 billion base pairs of the human genome.\n\nData acquisition in this space is predicted to outpace that of traditional big data disciplines, such as astronomy or youtube <sup id=\"fnref-11751-1\"><a href=\"#fn-11751-1\" class=\"jetpack-footnote\">1</a></sup>, as 50% of the world\u2019s population will have been sequenced to inform a medical decision by 2030. <sup id=\"fnref-11751-2\"><a href=\"#fn-11751-2\" class=\"jetpack-footnote\">2</a></sup>\n\nAs such, the analysis of medical genomics data is at the forefront of this growing need to apply sophisticated machine learning methods to large high-dimensional datasets. A common task in this field is to identify disease genes, that is where small errors in the gene sequence have had a detrimental health impact, such as neurodegenerative diseases or cancer. This has typically been done by looking at one genomic location at a time and assessing whether it is mutated in a large number of affected individuals compared to a healthy test group.\n\nHowever, biology is much more complicated than that. For most common diseases, like Alzheimer's or stroke, small differences may individually have no or only a small effect; joined together they can trigger the \u2018perfect storm.\u2019 It can get even more complicated as variants may interact with each other and between individuals it can be different sets of interacting variants that cause disease.\n\nIn this blog, we explain why we needed to design a novel parallelization algorithm for Random Forests. Although Apache Spark\u2019s MLlib is designed for the common use cases in which there are hundreds of thousands of features, in genomics, we needed to scale to the millions of genomic features. We give details on the new algorithm being based on Spark Core as it provides the parallelization agility needed to orchestrate this massively distributed machine learning task.\n\nFurthermore, we highlight that VariantSpark can be triggered from a Databricks notebook, which enables researchers to just point it to their data in an S3 bucket and start analyzing without worrying about cluster specs or data transfer. This frees up time to do more research and fosters better collaborations as notebooks can be worked on simultaneously and provide a reproducible record conducted experiments.\n\n<h2>Random Forest models disease biology</h2>\n\nFinding such interacting sets of \u2018needles\u2019 in the \u2018haystack\u2019 is impossible for statistical models due to the size of the combinatorial space they need to interrogate. Machine Learning methods, in particular Random Forest (RF), on the other hand are well suited to identify sets of features (e.g., mutations or more generally variants) that are predictive or associated with a label (e.g., disease).\n\nRF has also a reduced risk of overfitting compared to other machine learning methods, which is crucial for situations where the dataset has many more features than samples. These situations suffer from the \u201ccurse of dimensionality,\u201d and RF overcomes this by building independent decision trees each trained on a sub-sampled range of the dataset with the global decision based on all ensemble of trees.\n\n<h2>Making Random Forest scalable to modern data sizes</h2>\n\nHowever, to deal with big datasets, the generation of RF need to be parallelized. Current Big Data algorithms for Decision Trees, such as Google\u2019s PLANET or YGGDRASIL led by MIT and UCLA are designed for lower dimensional data. YGGDRASIL was showcased at last year\u2019s Spark Summit for datasets with 3,500 features, which is orders of magnitude smaller than what is necessary for genomics data ranging in the millions.\n\n<a href=\"https://www.csiro.au/\">CSIRO</a>, Australia\u2019s government research agency, developed a new parallelization strategy to cater for this new discipline of machine learning on high-dimensional data that can be applied to forests of decision trees.\n\n<a href=\"https://github.com/aehrc/VariantSpark\">VariantSpark RF</a> starts by randomly assigning subsets of the data to Spark Executors for decision tree building (Fig 1). It then calculates the best split over all nodes and trees simultaneously. This implementation avoids communication bottlenecks between Spark Driver and Executors as information exchange is minimal, allowing it to build large numbers of trees efficiently. This surveys the solution space appropriately to cater for millions of features and thousands of samples.\n\nFurthermore, VariantSpark RF has memory efficient representation of genomics data, optimized communication patterns and computation batching. It also provides efficient implementation of Out-Of-Bag (OOB) error, which substantially simplifies parameter tuning over the computationally more costly alternative of cross-validation.\n\nWe implemented VariantSpark RF in scala as it is the most performant interface languages to Apache Spark. Also, new updates to Spark and the interacting APIs will be deployed in scala first, which has been important when working on top of a fast evolving framework.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/07/variant_spark.jpg\" alt=\"\" width=\"1231\" height=\"872\" class=\"aligncenter size-full wp-image-11773\" />\n\n<h2>VariantSpark RF scales to Thousands of samples with Millions of features</h2>\n\nIn 2015, CSIRO developed the first version of VariantSpark, which was limited to unsupervised clustering and was built on top of Spark MLlib. In the resulting peer-reviewed publication <sup id=\"fnref-11751-3\"><a href=\"#fn-11751-3\" class=\"jetpack-footnote\">3</a></sup>, we clustered individuals from the 1000 Genomes Project to identify their ethnicity. This dataset contained ~2,500 individuals with ~80 Million genomic variants and we achieved a correct prediction rate of 82% (accuracy).\n\nWe wanted to re-evaluate this dataset using the random forest implementation in Spark MLlib to improve the accuracy using supervised learning. However,  MLlib\u2019s RF was not able to process the entire dataset and ran out of memory on an on-premise Hadoop-cluster with 12 Executors (16 Intel Xeon E5-2660@2.20GHz CPU cores and 128GB of RAM) for even a small subset of the original data (2,504 samples 6,450,364 features).\n\nIn contrast, the new version of VariantSpark, which implements the RF with a novel parallelization algorithm built on Spark Core directly, was able to process the entire dataset using the same cluster setup. It is processing over 15 Million variants per second from the 202 Billion variants in the dataset and was finishing in 3 hours. Being the only method to use the whole dataset, VariantSpark RF achieved a higher accuracy 0.96 (OOB=0.02) (Fig 2).\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/07/1000genomesRuntime.jpg\" alt=\"\" width=\"750\" height=\"600\" class=\"aligncenter size-full wp-image-11775\" />\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/07/1000genomesAccuracy.jpg\" alt=\"\" width=\"750\" height=\"600\" class=\"aligncenter size-full wp-image-11776\" />\n\n<em><strong>Fig. 2</strong> Performance comparison of VariantSpark RF and Spark MLlib on 1000 Genomes Project dataset. <strong>Top</strong> Runtime over different dataset sizes, cross marks the last dataset analyzed successfully. <strong>Bottom</strong> Accuracy achieved at termination point and number of features processed per second.</em>\n\n<h2>VariantSpark RF helps disease gene association discovery</h2>\n\nThe main achievement of VariantSpark RF is to enable -- for the first time -- the identification of disease causing variants by taking the higher-order genome-wide interactions between genomic loci into account.\n\nTo illustrate the benefit of this new powerful analysis, we created a synthetic dataset that simulates the mechanics of a complex disease or phenotype. We call this synthetic affliction \u201cHipsterism.\u201d To create this, we first identified peer-reviewed and published traits, such as propensity for facial hair or higher coffee consumption, that are commonly associated with being a Hipster.\n\nWe then score each individual in the 1000 Genomes Project dataset with the formula below, which joins information from these genome-wide locations in a similarly non-purely-additive way as a real complex phenotype would:\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/07/Screen-Shot-2017-07-25-at-11.22.59-AM.png\" alt=\"\" width=\"748\" height=\"49\" class=\"aligncenter size-full wp-image-11757\" />\n\nWhere GT stands for the genotype at this position with <em>homozygous</em> reference encoded as 0, <em>heterozygote</em> as 1, and <em>homozygote alternative</em> as 2. We then label individuals with a score above 10 as being a Hipster. The genomic information from all individuals with the synthetic Hipster label was then used to train VariantSpark RF to find the features that are most predictive or associated with this synthetic Hipster-phenotype.\n\nVariantSpark RF was able to correctly identify the 4 correct locations purely from the given Hipster label. Not only that, it identified the location in order of their exact weighting in the score\u2019s formula, which similar tools were not able to achieve (Fig 3).\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/07/HistperSignatureGraphic.jpg\" alt=\"\" width=\"945\" height=\"1293\" class=\"aligncenter size-full wp-image-11779\" />\n\n<em><strong>Fig. 3</strong> Synthetic phenotype demonstrating the ability of VariantSpark RF to identify sets of variants contributing to the phenotype even in a non-additive way.</em>\n\n<h2>Running VariantSpark RF</h2>\n\nUnlike other statistical approaches, RF has the advantage of not needing the data to be extensively processed. Furthermore, we deploy the jar file for VariantSpark RF through Maven Central, which enables the users to import the latest version of the software directly into their Databricks notebook. Together with VariantSpark\u2019s API designed to ingest standard genomics variant data formats (VCF), getting to the association testing started is done with three easy commands:\n\nFirstly, import the genomic data. Here, we show genomic data in VCF format, which looks like this:\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/07/VCFblack.jpg\" alt=\"\" width=\"1891\" height=\"225\" class=\"aligncenter size-full wp-image-11782\" />\n\n<pre><code class=\"scala\">import au.csiro.variantspark.api.VSContext\nimplicit val vsContext = VSContext(spark)\nval featureSource = vsContext.featureSource(\"/vs-datasets/hipsterIndex/hipster.vcf.bz2\")\n</code></pre>\n\nSecondly, import the label data. Here, we show the synthetic hipster index which has two categories 1=yes, 0=no hipster\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/07/labelblack.jpg\" alt=\"\" width=\"1891\" height=\"214\" class=\"aligncenter size-full wp-image-11785\" />\n\n<pre><code class=\"scala\">val labelSource  = vsContext.labelSource(\"/vs-datasets/hipsterIndex/hipster_labels.txt\", \"label\")\n</code></pre>\n\nThirdly, run the <code>ImportanceAnalysis</code> of VariantSpark RF to identify the features that are most associated with the label. Here, it will return the genomic locations that are most associated with the binary Hipster label.\n\n<pre><code class=\"scala\">import au.csiro.variantspark.api.ImportanceAnalysis\nval importanceAnalysis = ImportanceAnalysis(featureSource, labelSource, nTrees = 1000)\nval variableImportance = importanceAnalysis.variableImportance\nvariableImportance.cache().registerTempTable(\"importance\")\n</code></pre>\n\nNote, apart from VCF and CSV file format, VariantSpark also works with the ADAM <sup id=\"fnref-11751-4\"><a href=\"#fn-11751-4\" class=\"jetpack-footnote\">4</a></sup> data schema, implemented on top of Avro and Parquet, as well as the HAIL API <sup id=\"fnref-11751-5\"><a href=\"#fn-11751-5\" class=\"jetpack-footnote\">5</a></sup> for variant pre-processing.\n\n<h3>Conclusion</h3>\n\nIn this blog post, we introduced VariantSpark RF, a new library that allows random forest to be applied to high-dimensional datasets. The novel Spark-based parallelization allows a large number of trees to be built simultaneously, hence enabling the solution space to be searched more exhaustively than other methods.\n\nWhile genomics is currently the discipline producing the largest volumes of complex data,  the ongoing datafication will bring similar analysis challenges to other disciplines.\n\nVariantSpark RF may hence be capable of converting these challenges to opportunities on those disciplines as well.\n\nHence running VariantSpark through API calls in a Databricks notebook makes sophisticated machine learning on Big Data very accessible. Databricks has specifically simplified the use of otherwise complex Spark infrastructure and enables teams of researchers to co-develop and share research workflows.\n\nTo learn more about the VariantSpark RF, run the Databricks notebook<sup id=\"fnref-11751-6\"><a href=\"#fn-11751-6\" class=\"jetpack-footnote\">6</a></sup> with the HipsterIndex example yourself. You can also apply VariantSpark RF to your own data on <a href=\"https://databricks.com/try\">Databricks today</a> or by downloading the source code from github repository.\n\n<h2>What\u2019s Next?</h2>\n\nVariantSpark RF <sup id=\"fnref-11751-7\"><a href=\"#fn-11751-7\" class=\"jetpack-footnote\">7</a></sup> will continued to be expanded to cover different applications area. For example, we will extend the current multi-nominal classification to a full regression analysis to cover continuous response variables or scores. Also, we will support a mix of categorical and continuous features as well as allow vastly different feature value ranges, e.g. needed for gene expression analysis.\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n\n<li id=\"fn-11751-1\">\nhttps://www.ncbi.nlm.nih.gov/pubmed/26151137&#160;<a href=\"#fnref-11751-1\">&#8617;</a>\n</li>\n\n<li id=\"fn-11751-2\">\nFrost &amp; Sullivan: Global Precision Market Growth Opportunities, Forecast to 2015 2017&#160;<a href=\"#fnref-11751-2\">&#8617;</a>\n</li>\n\n<li id=\"fn-11751-3\">\nO\u2019Brien et al. BMC Genomics 2015 https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-015-2269-7&#160;<a href=\"#fnref-11751-3\">&#8617;</a>\n</li>\n\n<li id=\"fn-11751-4\">\nhttps://github.com/bigdatagenomics/adam&#160;<a href=\"#fnref-11751-4\">&#8617;</a>\n</li>\n\n<li id=\"fn-11751-5\">\nhttps://hail.is&#160;<a href=\"#fnref-11751-5\">&#8617;</a>\n</li>\n\n<li id=\"fn-11751-6\">\nhttps://aehrc.github.io/VariantSpark/notebook-examples/VariantSpark_HipsterIndex.html&#160;<a href=\"#fnref-11751-6\">&#8617;</a>\n</li>\n\n<li id=\"fn-11751-7\">\nhttps://github.com/aehrc/VariantSpark&#160;<a href=\"#fnref-11751-7\">&#8617;</a>\n</li>\n\n</ol>\n</div>"}
{"status": "publish", "description": null, "creator": "jules_damji", "link": "https://databricks.com/blog/2017/07/31/on-demand-webinar-and-faq-accelerate-data-science-with-better-data-engineering-on-databricks.html", "authors": null, "id": 11796, "categories": ["Company Blog", "Customers", "Product"], "dates": {"publishedOn": "2017-07-31", "tz": "UTC", "createdOn": "2017-07-31"}, "title": "On-Demand Webinar and FAQ: Accelerate Data Science with Better Data Engineering on Databricks", "slug": "on-demand-webinar-and-faq-accelerate-data-science-with-better-data-engineering-on-databricks", "content": "On July 13th, we hosted a live webinar \u2014 <a href=\"http://go.databricks.com/mediamath-webinar\">Accelerate Data Science with Better Data Engineering on Databricks</a>. This webinar focused on the use of PySpark in transforming petabytes of data for ad-hoc analysis and generating downstream queries. In particular, we covered:\n\n<ul>\n<li>Transforming TBs of data with RDDs and PySpark responsibly</li>\n<li>Using the JDBC connector to write results to production databases seamlessly</li>\n<li>Comparisons with a similar approach using Hive</li>\n</ul>\n\nIf you missed the webinar, you can view it <a href=\"http://go.databricks.com/mediamath-webinar\">on-demand here</a> and the <a href=\"https://www.slideshare.net/databricks/accelerating-data-science-with-better-data-engineering-on-databricks\">slides are accessible</a> as attachments to the webinar.\n\nToward the end, we held a Q &amp; A, and below are all the questions with links to forums with their answers. (<em>Follow the link to view the answers.</em>)\n\n<a href=\"https://forums.databricks.com/questions/12099/i-heard-that-rdds-are-going-to-disappear-in-the-ne.html#answer-12100\">I heard that RDD's are going to disappear in the next version of Apache Spark, and DataFrames will replace RDD's. Is that true?</a>\n\n<a href=\"https://forums.databricks.com/questions/12102/is-there-a-whole-pipeline-about-how-you-were-able.html#answer-12103\">Is there a whole pipeline about how you were able to build up the recommendation engine with dealing massive (TBs) of data?</a>\n\n<a href=\"https://forums.databricks.com/questions/12104/whats-the-frequency-at-mediamath-of-how-often-the.html#answer-12105\">What\u2019s the frequency of how often the raw data is deposited onto S3, and the frequency of your ETL?</a>\n\n<a href=\"https://forums.databricks.com/questions/12106/what-version-of-apache-spark-did-you-use-at-mediam.html#answer-12107\">What version of Apache Spark did you use and why?</a>\n\n<a href=\"https://forums.databricks.com/questions/12108/could-you-please-explain-a-bit-more-about-the-4-gr.html#answer-12109\">Could you please explain a bit more about the 4 groups involved in the Index calculation (s, G, s.p &amp; G.p)?</a>\n\n<a href=\"https://forums.databricks.com/questions/12110/what-is-actually-databricks-helping-in-at-mediamat.html#answer-12111\">What is actually Databricks helping in? Only working on Apache Spark?</a>\n\n<a href=\"https://forums.databricks.com/questions/12112/do-you-think-datasetsdataframes-are-faster-than-rd.html#answer-12113\">Do you think Datasets/DataFrames are faster than RDDs, and can improve the performance in this case or your use case?</a>\n\n<a href=\"https://forums.databricks.com/questions/12114/in-pyspark-how-do-we-differentiate-dataset-from-da.html#answer-12115\">In Pyspark how do we differentiate Dataset from Dataframe?</a>\n\nIf you\u2019d like free access to Databricks, you can access the <a href=\"https://databricks.com/try-databricks\">free trial here</a>."}
{"status": "publish", "description": null, "creator": "jakebellacera", "link": "https://databricks.com/blog/2017/08/08/databricks-named-strong-performer-forrester-wave-insight-platforms-service-q3-2017.html", "authors": null, "id": 11807, "categories": ["Announcements", "Company Blog", "Product"], "dates": {"publishedOn": "2017-08-08", "tz": "UTC", "createdOn": "2017-08-08"}, "title": "Databricks Named as a Strong Performer in The Forrester Wave: Insight Platforms-as-a-Service, Q3 2017", "slug": "databricks-named-strong-performer-forrester-wave-insight-platforms-service-q3-2017", "content": "Forrester recently published <a href=\"http://go.databricks.com/forrester-wave-2017\">The Forrester Wave: Insight Platforms-as-a-Service Wave, Q3 2017</a>. In its 36-criteria evaluation of insight platform-as-a-service (PaaS) providers, Forrester identified the eight most significant players and researched, analyzed, and scored them.\n\nDatabricks \u2014 founded by the creators of Apache Spark to help customers accelerate innovation by unifying data science, engineering and the business \u2014 was named a Strong Performer. We were excited to be amongst the highest scores for Product Vision and Performance.\n\n<h2>Forrester defines an insight PaaS as:</h2>\n\nAn integrated set of data management, analytics, and insight application development and management components, offered as a platform the enterprise does not own or control.\n\nIn the current offering category, Forrester evaluated each vendor\u2019s current data management, analytics, and platform management services. Given the need for collaboration across multiple stakeholders, Forrester strongly weighted vendors\u2019 insight application development tooling as well as their platform\u2019s ability to function as an integrated whole.\n\nHere are some of our key takeaways from the report:\n\n<ul>\n<li><strong>\u201cDatabricks offers a next-generation platform for insight applications.\u201d</strong>\nThe <a href=\"https://databricks.com/product/databricks\">Databricks Unified Analytics Platform</a> continues to add proprietary features through its notebooks supporting most data analytics development languages, including SQL, and a comprehensive set of machine learning and visualization libraries.</li>\n<li><strong>\u201cDatabricks\u2019 polished user interface and advanced features make it ideal for teams of citizen data scientists.\u201d</strong>\nGiven our mission to accelerate innovation by reducing the time-to-insight, our powerful user interface allows data scientists of varying skills to easily leverage advanced capabilities and collaborate with each other and the business.</li>\n<li><strong>\u201c[Databricks] provides the benefits of enterprise Spark while reducing the need for complex tuning.\u201d</strong>\nDatabricks, founded by the creators of Apache Spark, continues to remove complexity for enterprises to run and manage Spark. We make it very simple for customers to select different versions of Spark for different workloads providing great migration flexibility to our customers.</li>\n</ul>\n\nWe appreciate the support of all our customers, partners, and global community who have helped Databricks grow. If you\u2019d like an expert at Databricks to walk you through our product, <a href=\"http://go.databricks.com/contact-databricks\">contact us here</a>.\n\n<a href=\"http://go.databricks.com/forrester-wave-2017\">Get your copy of the Forrester Wave report here.</a>"}
{"status": "publish", "description": null, "creator": "jules_damji", "link": "https://databricks.com/blog/2017/08/09/apache-sparks-structured-streaming-with-amazon-kinesis-on-databricks.html", "authors": null, "id": 11820, "categories": ["Company Blog", "Product"], "dates": {"publishedOn": "2017-08-09", "tz": "UTC", "createdOn": "2017-08-09"}, "title": "Apache Spark\u2019s Structured Streaming with Amazon Kinesis on Databricks", "slug": "apache-sparks-structured-streaming-with-amazon-kinesis-on-databricks", "content": "On July 11, 2017, we announced the general availability of <a href=\"https://databricks.com/blog/2017/07/11/introducing-apache-spark-2-2.html\">Apache Spark 2.2.0</a> as part of <a href=\"https://docs.databricks.com/release-notes/runtime/3.0.html\">Databricks Runtime 3.0</a> (DBR) for the <a href=\"https://databricks.com/unified-analytics-platform\">Unified Analytics Platform</a>. To augment the scope of Structured Streaming on DBR, we support <a href=\"https://docs.databricks.com/spark/latest/structured-streaming/kinesis.html\">AWS Kinesis Connector</a> as a source (to read streams from), giving developers the freedom to do three things.\n\nFirst, you can choose either Apache Kafka or Amazon\u2019s Kinesis as a <em>source</em> to read streaming data. Second, you are not shackled to Kinesis Analytics for doing analytics but can use Spark SQL and Structured APIs. And finally, you can use Apache Spark on the <a href=\"https://databricks.com/product/databricks\">Unified Databricks Platform</a>, along with other workloads, to write your end-to-end <a href=\"https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html\">continuous applications</a>.\n\nIn this blog, we\u2019ll discuss four aspects of the Kinesis connector for Structured Streaming so that you can get started quickly on Databricks, and with minimal changes, you can switch to other streaming <em>sources</em> and <em>sinks</em> of your choice.\n\n<ol>\n<li>Kinesis Data Schema</li>\n<li>Configuration Parameters</li>\n<li>Authentication with AWS Kinesis</li>\n<li>Anatomy of a Kinesis Structured Streaming Application</li>\n</ol>\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/08/kinesis_blog_final.jpg\" alt=\"\" width=\"720\" height=\"405\" class=\"aligncenter size-full wp-image-11821\" />\n\n<h2>Kinesis Data Schema</h2>\n\nKnowing what Kinesis records you\u2019ve read from a stream and understanding how those records map to a defined schema make developers\u2019 lives easier. Even better if a <a href=\"http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/kinesis/model/Record.html\">Kinesis record</a> maps to <a href=\"https://spark.apache.org/docs/latest/sql-programming-guide.html\">Apache Spark\u2019s DataFrames</a>, with named columns and their associated types. Then you can select the desired payload from the Kinesis record, by accessing the column from the resulting DataFrame and employ DataFrame APIs operations.\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/08/Screen-Shot-2017-08-08-at-1.20.03-PM.png\" alt=\"\" width=\"696\" height=\"182\" class=\"aligncenter size-full wp-image-11822\" />\n\nLet\u2019s assume you send JSON blobs to Kinesis as your records. To access the binary data payload, which is your JSON encoded data, you can use the DataFrame API method as <code>cast(data as STRING) as JsonData</code> to deserialize your binary payload data into a JSON string. Furthermore, once converted to a JSON string, you can then use <code>from_json()</code> <a href=\"https://databricks.com/blog/2017/06/13/five-spark-sql-utility-functions-extract-explore-complex-data-types.html\">SQL utility functions</a> to explode into respective DataFrame columns.\n\nHence, knowing the Kinesis schema and how it maps to a DataFrame makes things easier to do streaming ETL, whether your data is simple, such as words, or structured and complex, such as nested JSON.\n\n<h2>Kinesis Configuration</h2>\n\nJust as important as understanding Kinesis record and its schema is knowing the right <a href=\"https://docs.databricks.com/spark/latest/structured-streaming/kinesis.html#configuring\">configuration parameters and options</a> to supply in your Kinesis connector code. While options are many, few import ones are worthy of note:\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/08/Screen-Shot-2017-08-08-at-1.29.11-PM.png\" alt=\"\" width=\"693\" height=\"304\" class=\"aligncenter size-full wp-image-11826\" />\n\nFor detailed options read <a href=\"https://docs.databricks.com/spark/latest/structured-streaming/kinesis.html#configuring\">Kinesis configuration documentation</a>.\n\nNow that we know the format of our DataFrame derived from the <a href=\"http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/kinesis/model/Record.html\">Kinesis record</a> and understand what options we can supply to Kinesis connector to read a stream, we can write our code, as shown below in the anatomy of a Kinesis streaming application. But first, we must pass through AWS security gatekeepers for authentication.\n\n<h2>Authentication with AWS Kinesis</h2>\n\nBy default, the Kinesis connector resorts to <a href=\"http://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html#credentials-default\">Amazon\u2019s default credential provider chain</a>, so if you have created an IAM role for your Databricks cluster that includes access to Kinesis then access will be automatically granted. Additionally, depending on your IAM role access, the same default credentials will grant you access to AWS S3 buckets for writing.\n\nAlternatively, you can explicitly supply credentials as part of the \u201coptions\u201d to the Kinesis connector. When supplying explicit secret keys, use two \u201coption\u201d parameters: <code>awsAccessKey</code> and <code>awsSecretKey</code>. However, we recommend using <a href=\"http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\">AWS IAM Roles</a> instead of providing keys in production.\n\n<h2>Anatomy of a Kinesis Structured Streaming Application</h2>\n\nSo far we introduced three concepts that enable us to write our Structured Streaming application using the Kinesis connector. A Structured Streaming application has a distinct anatomy, serial steps, regardless of your streaming <em>sources</em> or <em>sinks</em>. Let\u2019s study each step.\n\n<h3>Step 1: Defining your data\u2019s schema</h3>\n\nAlthough the Kinesis connector can read any encoded data\u2014including JSON, Avro, bytes\u2014as long as you can decode it in your receiving Spark code, for this blog we will assume that our Kinesis stream is fed with device data encoded as a JSON string, with the following schemas.\n\n[code_tabs]\n\n<pre><code class=\"scala\">val jsonSchema = new StructType()\n        .add(\"battery_level\", LongType)\n        .add(\"c02_level\", LongType)\n        .add(\"cca3\",StringType)\n        .add(\"cn\", StringType)\n        .add(\"device_id\", LongType)\n        .add(\"device_type\", StringType)\n        .add(\"signal\", LongType)\n        .add(\"ip\", StringType)\n        .add(\"temp\", LongType)\n        .add(\"timestamp\", TimestampType)\n\n</code></pre>\n\n<pre><code class=\"python\">from pyspark.sql.types import *\n\npythonSchema = StructType() \\\n          .add(\"battery_level\", LongType()) \\   \n          .add(\"c02_level\", LongType()) \\\n          .add(\"cca3\", StringType()) \\\n          .add (\"cn\", StringType()) \\\n          .add (\"device_id\", LongType()) \\\n          .add(\"device_type\", StringType()) \\\n          .add (\"signal\", LongType()) \\\n          .add(\"ip\", StringType()) \\\n          .add(\"temp\", LongType()) \\\n          .add(\"timestamp\", TimestampType())\n</code></pre>\n\n[/code_tabs]\n\n<h3>Step 2: Reading from your source</h3>\n\nOne you have defined your schema, the next step is to read your stream, using Kinesis connector. By only specifying your source format, namely \u201ckinesis,\u201d Databricks will automatically use the Kinesis connector for reading. It will handle all aspects of what shard to read from and keep track of all the metadata. You need not worry about it.\n\nSomething to note here is that if my <em>source</em> were other than \u201ckinesis,\u201d I would simply change this to indicate \u201ckafka\u201d or \u201csocket,\u201d and drop the AWS credentials.\n\n[code_tabs]\n\n<pre><code class=\"scala\">// read the data stream from Kinesis using the connector\nval kinesisDF = spark.readStream\n  .format(\"kinesis\")\n  .option(\"streamName\", \"devices\")\n  .option(\"initialPosition\", \"earliest\")\n  .option(\"region\", \"us-west-2\")\n  .option(\"awsAccessKey\", awsAccessKey)\n  .option(\"awsSecretKey\", awsSecretKey)\n  .load()\n</code></pre>\n\n<pre><code class=\"python\">kinesisDF = spark \\\n  .readStream \\\n  .format(\"kinesis\") \\\n  .option(\"streamName\", \"devices\") \\\n  .option(\"initialPosition\", \"earliest\") \\\n  .option(\"region\", \"us-west-2\") \\\n  .option(\"awsAccessKey\", awsAccessKey) \\\n  .option(\"awsSecretKey\", awsSecretKey) \\\n  .load()\n</code></pre>\n\n[/code_tabs]\n\n<h3>Step 3: Exploring or Transforming streams</h3>\n\nOnce we have our data loaded and the Kinesis records now have been mapped to DataFrames, we can use the SQL and DataFrames/Datasets API to process. And the underlying streaming engines will ensure exactly-once semantics and fault-tolerance. To learn more about how Spark Streaming achieves this vital functionality in Structured Streaming, view our deep dive <a href=\"https://spark-summit.org/2017/events/easy-scalable-fault-tolerant-stream-processing-with-structured-streaming-in-apache-spark/\">Spark Summit session</a>.\n\n[code_tabs]\n\n<pre><code class=\"scala\">// extract data from the payload and use transformation to do\n// your analytics\nval dataDevicesDF = kinesisDF\n  .selectExpr(\"cast (data as STRING) jsonData\"))\n  .select(from_json(\"jsonData\", jsonSchema).as(\"devices\"))\n  // explode into its equivalent DataFrame column names\n  .select(\"devices.*\")\n  // filter out some devices with certain attribute values\n  .filter($\"devices.temp\" &gt; 10 and $\"devices.signal\" &gt; 15)\n</code></pre>\n\n<pre><code class=\"python\">#extract data from the payload and use transformation to do your analytics\ndataDevicesDF = kinesisDF \\\n  .selectExpr(\"cast (data as STRING) jsonData\") \\\n  .select(from_json(\"jsonData\", pythonSchema).alias(\"devices\")) \\\n  .select(\"devices.*\") \\\n  .filter(\"devices.temp &gt; 10 and devices.signal &gt; 15\")\n</code></pre>\n\n[/code_tabs]\n\nThis step is where most of your analytics is done and where your actionable insights are derived from. By using Spark\u2019s Structured APIs in this step, you get all the merits of Spark SQL performance and compact-code generation from Tungsten, without using another SQL engine or programming in a separate SDK to conduct your ETL or streaming analytics.\n\n<h3>Step 4: Saving your transformed stream</h3>\n\nFinally, you can optionally write your transformed stream to a parquet file at the specified location in your S3 bucket, partitioned by \u201cdate\u201d or \u201ctimestamp.\u201d To inform Spark to ensure fault-tolerance, you can specify an option parameter \u201ccheckpointLocation,\u201d and the underlying engine will maintain the state.\n\n[code_tabs]\n\n<pre><code class=\"scala\">val dataDevicesQuery = kinesisDF\n  .selectExpr(\"cast (data as STRING) jsonData\"))\n  .select(from_json(\"jsonData\", jsonSchema).as(\"devices\"))\n // explode into its equivalent DataFrame column names\n .select(\"devices.*\")\n // filter out some devices with certain attribute values\n .filter($\"devices.temp\" &gt; 10 and $\"devices.signal\" &gt; 15)\n .writeStream\n // write to Parquet file\n .partitionBy(\"timestamp\")\n .format(\"parquet\")\n // specify the checkpoint location\n .option(\"checkpointLocation\", \"/parquetCheckpoint\")\n // location where parquet partition files will be written\n .start(\"/parquetDeviceTable\")\n</code></pre>\n\n<pre><code class=\"python\">#extract data from the payload and use transformation to do your analytics\ndataDevicesDF = kinesisDF \\\n  .selectExpr(\"cast (data as STRING) jsonData\") \\\n  .select(from_json(\"jsonData\", pythonSchema).alias(\"devices\")) \\\n  .select(\"devices.*\") \\\n  .filter(\"devices.temp &gt; 10 and devices.signal &gt; 15\") \\\n  # write to Parquet file\n  .writeStream \\\n  .partitionBy(\"timestamp\") \\\n  .format(\"parquet\") \\\n  # specify the checkpoint location\n  .option(\"checkpointLocation\", \"/parquetCheckpoint\") \\\n  # location to store parquet partition files\n  .start(\"/parquetDeviceTable\")\n</code></pre>\n\n[/code_tabs]\n\nThese four basic steps encapsulate an anatomy of a typical Structured Streaming application. Whether your source is Kinesis or Kafka or socket or a local filesystem, you can follow these guidelines and structure your Structured Streaming computation.\n\nWhat if you want to write your transformed stream to, for instance, your own <em>sink</em>, such as Kinesis or NoSQL, presently not supported by Spark\u2019s Structured Streaming. You can write your own <em>sink</em> functionality by implementing the <a href=\"https://docs.databricks.com/spark/latest/structured-streaming/kinesis.html#structured-streaming-kinesis-sink\"><code>ForeachSink</code>interface to write data to Kinesis</a>.\n\n<h2>What\u2019s Next</h2>\n\nInstead of cluttering this blog with a complete code example showing a Kinesis connector streaming application, I\u2019ll refer you to examine and explore the code to do the quintessential \u201cHello World\u201d of distributed computing <a href=\"https://docs.databricks.com/spark/latest/structured-streaming/kinesis.html#quickstart\">WordCount on Kinesis</a>. Even better, you can import this <a href=\"https://docs.databricks.com/_static/notebooks/structured-streaming-kinesis.html\">WordCount notebook</a> and supply your AWS credentials\u2014and get on with it.\n\nThere\u2019s no need for you to install or attach any Kinesis library. No need to access an external Kinesis SDK. You simply write your Structured Streaming on <a href=\"https://docs.databricks.com/release-notes/runtime/3.0.html\">Databricks Runtime 3.0</a>. We will do the rest.\n\nIf you don\u2019t have an account on Databricks, <a href=\"https://databricks.com/try-databricks\">get one today</a>.\n\n<h2>Read More</h2>\n\nWe have a series of <a href=\"https://databricks.com/blog/2017/01/19/real-time-streaming-etl-structured-streaming-apache-spark-2-1.html\">Structured Streaming blogs</a> that expound on many of its features, \nand you can consult our <a href=\"https://docs.databricks.com/spark/latest/structured-streaming/kinesis.html#\">Kinesis Connector</a> documentation along with \n<a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\">Structured Streaming Programming guide</a> for some immersive reading."}
